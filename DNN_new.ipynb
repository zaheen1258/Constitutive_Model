{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90614396-4866-4e01-94fa-fe8e74d2f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLabFEM as FE\n",
    "from pyLabMaterial import Material\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e1c8aa5-990b-4914-a908-6db83874d0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yield loci of anisotropic reference material and isotropic material\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'define two elastic-plastic materials with identical yield strength and elastic properties'\n",
    "E=200.e3\n",
    "nu=0.3\n",
    "sy = 150.\n",
    "'anistropic Hill-material as reference'\n",
    "mat_h = Material(name='anisotropic Hill')\n",
    "mat_h.elasticity(E=E, nu=nu)\n",
    "mat_h.plasticity(sy=sy, hill=[0.7,1.,1.4], drucker=0., khard=0.)\n",
    "'isotropic material for ML flow rule'\n",
    "mat_ml = Material(name='ML flow rule')\n",
    "mat_ml.elasticity(E=E, nu=nu)\n",
    "mat_ml.plasticity(sy=sy, hill=[1.,1.,1.], drucker=0., khard=0.)\n",
    "print('Yield loci of anisotropic reference material and isotropic material')\n",
    "ax = mat_h.plot_yield_locus(xstart=-1.5, xend=1.5, iso=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbfa0726-cde5-4191-9e6f-03e4eb666343",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Create training data in deviatoric stress space for components seq and theta'\n",
    "def create_data(N, mat, extend=False, rand=False):\n",
    "    # create stresses along unit circle normal to hydrostatic axis\n",
    "    if not rand:\n",
    "        theta = np.linspace(-np.pi, np.pi, N)\n",
    "    else:\n",
    "        theta = 2.*(np.random.rand(N)-0.5)*np.pi\n",
    "    sig = FE.sp_cart(np.array([np.ones(N)*np.sqrt(3/2), theta]).T)\n",
    "    offs = 0.01\n",
    "    x = offs*sig\n",
    "    N = 23\n",
    "    for i in range(N):\n",
    "        hh = offs + (1.4-offs)*(i+1)/N\n",
    "        x = np.append(x, hh*sig, axis=0)\n",
    "    if extend:\n",
    "        # add training points in plastic regime to avoid fallback of SVC decision fct. to zero\n",
    "        x = np.append(x, 2.*sig, axis=0)\n",
    "        x = np.append(x, 3.*sig, axis=0)\n",
    "        x = np.append(x, 4.*sig, axis=0) \n",
    "        x = np.append(x, 5.*sig, axis=0)\n",
    "    'result data for ML yield function (only sign is considered)'\n",
    "    y = np.sign(mat.calc_yf(x*mat.sy, ana=True))\n",
    "    return x,y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a4374f8-13db-42b1-ae97-bcde303f7cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Training and testing data for ML yield function, based on reference Material mat_h'\n",
    "ndata = 36\n",
    "ntest = np.maximum(20, int(ndata/10))\n",
    "x_train, y_train = create_data(ndata, mat_h, extend=True) \n",
    "x_test, y_test = create_data(ntest, mat_h, rand=True)\n",
    "x_train *= mat_h.sy\n",
    "x_test *= mat_h.sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "788bc003-32df-42c4-8a06-4a66e0281701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1008, 3) (480, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96036d1a-012b-4b4b-b617-4b11bcedd5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ef71e2b-2b46-402e-9b98-8f42d465b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin = Binarizer()\n",
    "y_train = bin.transform(y_train.reshape(-1, 1)).astype(float)\n",
    "y_test = bin.transform(y_test.reshape(-1, 1)).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d14514dc-2e48-43f5-bc5b-b1fc7ddbdd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b210734-7071-48ec-a520-ea98dfaa8591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e3cfe5c-9f98-44f2-9334-e8dc6b9ee495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = []\n",
    "        in_units = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(in_units, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_units = h_dim\n",
    "        layers.append(nn.Linear(in_units, output_dim))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    input_dim = 3\n",
    "    output_dim = 1\n",
    "    hidden_dims = [trial.suggest_int(f'hidden_dim_{i}', 16, 512) for i in range(trial.suggest_int('n_layers', 1, 7))]\n",
    "    # hidden_layers = trial.suggest_int('hidden_layers', 1, 5)\n",
    "    # hidden_units = trial.suggest_int('hidden_units', 8, 64)\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 4, 100)\n",
    "\n",
    "    # Create model, optimizer and loss function\n",
    "    model = SimpleNN(input_dim, hidden_dims, output_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Convert data to PyTorch tensors and DataLoader\n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # Reshape to [batch_size, 1]\n",
    "    x_val_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)  # Reshape to [batch_size, 1]\n",
    "\n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(100):  # Set a fixed number of epochs or make it tunable\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, labels = batch\n",
    "            outputs = model(inputs).view(-1, 1)  # Ensure output is [batch_size, 1]\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(x_val_tensor).view(-1, 1)  # Ensure output is [batch_size, 1]\n",
    "        val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "\n",
    "    # Convert outputs to binary predictions\n",
    "    binary = Binarizer(threshold=0.5)\n",
    "    val_pred = binary.transform(val_outputs.numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_val_tensor.numpy(), val_pred)\n",
    "    return accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e41a4757-f087-47ad-89dd-55450bbd54fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-25 16:01:57,055] A new study created in memory with name: no-name-5e711a13-7f4f-442e-894e-e8f91d51fc72\n",
      "[I 2024-08-25 16:01:59,616] Trial 0 finished with value: 0.9958333333333333 and parameters: {'n_layers': 7, 'hidden_dim_0': 197, 'hidden_dim_1': 222, 'hidden_dim_2': 131, 'hidden_dim_3': 423, 'hidden_dim_4': 105, 'hidden_dim_5': 18, 'hidden_dim_6': 409, 'lr': 0.0008201791996857502, 'batch_size': 98}. Best is trial 0 with value: 0.9958333333333333.\n",
      "[I 2024-08-25 16:02:00,444] Trial 1 finished with value: 0.8958333333333334 and parameters: {'n_layers': 3, 'hidden_dim_0': 245, 'hidden_dim_1': 60, 'hidden_dim_2': 87, 'lr': 0.01572750915125478, 'batch_size': 64}. Best is trial 0 with value: 0.9958333333333333.\n",
      "[I 2024-08-25 16:02:01,259] Trial 2 finished with value: 0.93125 and parameters: {'n_layers': 2, 'hidden_dim_0': 348, 'hidden_dim_1': 103, 'lr': 0.0021019495659926443, 'batch_size': 79}. Best is trial 0 with value: 0.9958333333333333.\n",
      "[I 2024-08-25 16:02:04,391] Trial 3 finished with value: 0.42291666666666666 and parameters: {'n_layers': 6, 'hidden_dim_0': 115, 'hidden_dim_1': 206, 'hidden_dim_2': 486, 'hidden_dim_3': 405, 'hidden_dim_4': 303, 'hidden_dim_5': 158, 'lr': 0.03702592918202507, 'batch_size': 57}. Best is trial 0 with value: 0.9958333333333333.\n",
      "[W 2024-08-25 16:02:10,715] Trial 4 failed with parameters: {'n_layers': 4, 'hidden_dim_0': 324, 'hidden_dim_1': 98, 'hidden_dim_2': 317, 'hidden_dim_3': 509, 'lr': 2.864696158194166e-05, 'batch_size': 5} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/zaheennasir/mambaforge/envs/Torch/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/_p/dm3zzj416j7_t1st_vhm8mcr0000gn/T/ipykernel_16980/3412776389.py\", line 50, in objective\n",
      "    optimizer.step()\n",
      "  File \"/Users/zaheennasir/mambaforge/envs/Torch/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 385, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/zaheennasir/mambaforge/envs/Torch/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/zaheennasir/mambaforge/envs/Torch/lib/python3.12/site-packages/torch/optim/adam.py\", line 166, in step\n",
      "    adam(\n",
      "  File \"/Users/zaheennasir/mambaforge/envs/Torch/lib/python3.12/site-packages/torch/optim/adam.py\", line 316, in adam\n",
      "    func(params,\n",
      "  File \"/Users/zaheennasir/mambaforge/envs/Torch/lib/python3.12/site-packages/torch/optim/adam.py\", line 439, in _single_tensor_adam\n",
      "    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-08-25 16:02:10,717] Trial 4 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)  \u001b[38;5;66;03m# Increase n_trials for more comprehensive tuning\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Trial: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/Torch/lib/python3.12/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    453\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/Torch/lib/python3.12/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         _optimize_sequential(\n\u001b[1;32m     63\u001b[0m             study,\n\u001b[1;32m     64\u001b[0m             func,\n\u001b[1;32m     65\u001b[0m             n_trials,\n\u001b[1;32m     66\u001b[0m             timeout,\n\u001b[1;32m     67\u001b[0m             catch,\n\u001b[1;32m     68\u001b[0m             callbacks,\n\u001b[1;32m     69\u001b[0m             gc_after_trial,\n\u001b[1;32m     70\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m     73\u001b[0m         )\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/envs/Torch/lib/python3.12/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/mambaforge/envs/Torch/lib/python3.12/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/mambaforge/envs/Torch/lib/python3.12/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[9], line 50\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     48\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     49\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 50\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[1;32m     53\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/mambaforge/envs/Torch/lib/python3.12/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/Torch/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/mambaforge/envs/Torch/lib/python3.12/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     adam(\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[1;32m    168\u001b[0m         grads,\n\u001b[1;32m    169\u001b[0m         exp_avgs,\n\u001b[1;32m    170\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    171\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    172\u001b[0m         state_steps,\n\u001b[1;32m    173\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    174\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    175\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    176\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    177\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    178\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    179\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    180\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    181\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    182\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    183\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    184\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    185\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    186\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/mambaforge/envs/Torch/lib/python3.12/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m func(params,\n\u001b[1;32m    317\u001b[0m      grads,\n\u001b[1;32m    318\u001b[0m      exp_avgs,\n\u001b[1;32m    319\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    320\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    321\u001b[0m      state_steps,\n\u001b[1;32m    322\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    323\u001b[0m      has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    324\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    325\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    326\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    327\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    328\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    329\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    330\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    331\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    332\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    333\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[0;32m~/mambaforge/envs/Torch/lib/python3.12/site-packages/torch/optim/adam.py:439\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    437\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    441\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)  # Increase n_trials for more comprehensive tuning\n",
    "\n",
    "print(f\"Best Trial: {study.best_trial.params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "067e5cab-3504-4643-ae62-e220875a0b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20000], Training Loss: 0.6636134322200503, Validation Loss: 0.5917924003941673\n",
      "Epoch [2/20000], Training Loss: 0.4976429747683661, Validation Loss: 0.46895896962710787\n",
      "Epoch [3/20000], Training Loss: 0.39284846612385343, Validation Loss: 0.41790767120463507\n",
      "Epoch [4/20000], Training Loss: 0.5000927884663854, Validation Loss: 0.43847753533295225\n",
      "Epoch [5/20000], Training Loss: 0.3792297052485602, Validation Loss: 0.29913094746215\n",
      "Epoch [6/20000], Training Loss: 0.273915205683027, Validation Loss: 0.3977535500058106\n",
      "Epoch [7/20000], Training Loss: 0.31440004387072157, Validation Loss: 0.310938259852784\n",
      "Epoch [8/20000], Training Loss: 0.2406832299062184, Validation Loss: 0.2374940389501197\n",
      "Epoch [9/20000], Training Loss: 0.21198655026299612, Validation Loss: 0.2761820130316274\n",
      "Epoch [10/20000], Training Loss: 0.21801408912454331, Validation Loss: 0.1718866109315838\n",
      "Epoch [11/20000], Training Loss: 0.14803324426923478, Validation Loss: 0.1410423144698143\n",
      "Epoch [12/20000], Training Loss: 0.13015020425830567, Validation Loss: 0.13120861378099238\n",
      "Epoch [13/20000], Training Loss: 0.10991599783301353, Validation Loss: 0.10886313122630652\n",
      "Epoch [14/20000], Training Loss: 0.09978552481957845, Validation Loss: 0.10758227551455743\n",
      "Epoch [15/20000], Training Loss: 0.1045553306383746, Validation Loss: 0.1123429417992676\n",
      "Epoch [16/20000], Training Loss: 0.08409344910510949, Validation Loss: 0.09569929612916894\n",
      "Epoch [17/20000], Training Loss: 0.10159652759986264, Validation Loss: 0.09357626430989642\n",
      "Epoch [18/20000], Training Loss: 0.07994117960333824, Validation Loss: 0.1239558503153343\n",
      "Epoch [19/20000], Training Loss: 0.09644571132957935, Validation Loss: 0.21646985340651423\n",
      "Epoch [20/20000], Training Loss: 0.12822034396231174, Validation Loss: 0.08669323531440958\n",
      "Epoch [21/20000], Training Loss: 0.08282622295830931, Validation Loss: 0.07395461548730964\n",
      "Epoch [22/20000], Training Loss: 0.1050766333937645, Validation Loss: 0.11935047531057665\n",
      "Epoch [23/20000], Training Loss: 0.0961848430867706, Validation Loss: 0.07570680398878592\n",
      "Epoch [24/20000], Training Loss: 0.08682526594826154, Validation Loss: 0.12514392162763346\n",
      "Epoch [25/20000], Training Loss: 0.11254943481513432, Validation Loss: 0.1310495433678755\n",
      "Epoch [26/20000], Training Loss: 0.08862910659185477, Validation Loss: 0.0872042855456974\n",
      "Epoch [27/20000], Training Loss: 0.06200708874634334, Validation Loss: 0.07097789422914502\n",
      "Epoch [28/20000], Training Loss: 0.07101989138339247, Validation Loss: 0.06251478810226477\n",
      "Epoch [29/20000], Training Loss: 0.14550141963575566, Validation Loss: 0.10194904810537812\n",
      "Epoch [30/20000], Training Loss: 0.08434661238321237, Validation Loss: 0.06114358271878067\n",
      "Epoch [31/20000], Training Loss: 0.05917997232505253, Validation Loss: 0.05563616640549997\n",
      "Epoch [32/20000], Training Loss: 0.05842329854411738, Validation Loss: 0.05220551282676159\n",
      "Epoch [33/20000], Training Loss: 0.06141413588609014, Validation Loss: 0.05699101258908458\n",
      "Epoch [34/20000], Training Loss: 0.061026793771556447, Validation Loss: 0.052662836231133624\n",
      "Epoch [35/20000], Training Loss: 0.09142338896968535, Validation Loss: 0.07241826004129186\n",
      "Epoch [36/20000], Training Loss: 0.0861412709844964, Validation Loss: 0.10509351786213969\n",
      "Epoch [37/20000], Training Loss: 0.06259379948356322, Validation Loss: 0.05339555751756058\n",
      "Epoch [38/20000], Training Loss: 0.05471467040479183, Validation Loss: 0.09453589158722282\n",
      "Epoch [39/20000], Training Loss: 0.09704058364565883, Validation Loss: 0.11307995958295436\n",
      "Epoch [40/20000], Training Loss: 0.07925166668636459, Validation Loss: 0.10232134296267363\n",
      "Epoch [41/20000], Training Loss: 0.05782109445759228, Validation Loss: 0.0556927850896403\n",
      "Epoch [42/20000], Training Loss: 0.05280988131250654, Validation Loss: 0.0500847787666849\n",
      "Epoch [43/20000], Training Loss: 0.04940081188189132, Validation Loss: 0.05945897074484114\n",
      "Epoch [44/20000], Training Loss: 0.05798598511942795, Validation Loss: 0.04429992631529599\n",
      "Epoch [45/20000], Training Loss: 0.04162115231156349, Validation Loss: 0.048973101716591784\n",
      "Epoch [46/20000], Training Loss: 0.0403167400509119, Validation Loss: 0.04911486089922358\n",
      "Epoch [47/20000], Training Loss: 0.049602677513446124, Validation Loss: 0.05561209973674914\n",
      "Epoch [48/20000], Training Loss: 0.043262536371392865, Validation Loss: 0.04748311735201849\n",
      "Epoch [49/20000], Training Loss: 0.043236241942005496, Validation Loss: 0.03706662088894345\n",
      "Epoch [50/20000], Training Loss: 0.05264487822673151, Validation Loss: 0.037265276557655165\n",
      "Epoch [51/20000], Training Loss: 0.06605166888662747, Validation Loss: 0.04612355023291111\n",
      "Epoch [52/20000], Training Loss: 0.09213488189769643, Validation Loss: 0.06427978508826092\n",
      "Epoch [53/20000], Training Loss: 0.04648820754872369, Validation Loss: 0.04258531457672093\n",
      "Epoch [54/20000], Training Loss: 0.051634786783584526, Validation Loss: 0.035416971865796114\n",
      "Epoch [55/20000], Training Loss: 0.04240942613354751, Validation Loss: 0.06256747744226475\n",
      "Epoch [56/20000], Training Loss: 0.0509852957246559, Validation Loss: 0.061444685661167675\n",
      "Epoch [57/20000], Training Loss: 0.055522293118493896, Validation Loss: 0.07054929278947165\n",
      "Epoch [58/20000], Training Loss: 0.06938107923737594, Validation Loss: 0.10757905708836117\n",
      "Epoch [59/20000], Training Loss: 0.1387438988313079, Validation Loss: 0.0499501653228247\n",
      "Epoch [60/20000], Training Loss: 0.12902314069547824, Validation Loss: 0.17242228809157364\n",
      "Epoch [61/20000], Training Loss: 0.09274775242166859, Validation Loss: 0.10151061943788202\n",
      "Epoch [62/20000], Training Loss: 0.06425088657332319, Validation Loss: 0.04621601260494401\n",
      "Epoch [63/20000], Training Loss: 0.05789285113236734, Validation Loss: 0.04136717174315262\n",
      "Epoch [64/20000], Training Loss: 0.04828457121870348, Validation Loss: 0.04028179385112475\n",
      "Epoch [65/20000], Training Loss: 0.037506379054061005, Validation Loss: 0.054666891564500046\n",
      "Epoch [66/20000], Training Loss: 0.041527266481093, Validation Loss: 0.04110152656764649\n",
      "Epoch [67/20000], Training Loss: 0.03872918789940221, Validation Loss: 0.04153249644317901\n",
      "Epoch [68/20000], Training Loss: 0.0414341746696404, Validation Loss: 0.04636140914364151\n",
      "Epoch [69/20000], Training Loss: 0.044038615588630946, Validation Loss: 0.04870207426677239\n",
      "Epoch [70/20000], Training Loss: 0.037927967961877584, Validation Loss: 0.03396033417394106\n",
      "Epoch [71/20000], Training Loss: 0.03605264804459044, Validation Loss: 0.0426420773945339\n",
      "Epoch [72/20000], Training Loss: 0.038611283392778466, Validation Loss: 0.04402361654866792\n",
      "Epoch [73/20000], Training Loss: 0.037020999672157426, Validation Loss: 0.04471357929421247\n",
      "Epoch [74/20000], Training Loss: 0.07611571970794882, Validation Loss: 0.11278214711303058\n",
      "Epoch [75/20000], Training Loss: 0.08437840261363558, Validation Loss: 0.11164547978152306\n",
      "Epoch [76/20000], Training Loss: 0.061185273341834545, Validation Loss: 0.06435706382544645\n",
      "Epoch [77/20000], Training Loss: 0.060434563151959865, Validation Loss: 0.05062987960399312\n",
      "Epoch [78/20000], Training Loss: 0.049441436604995816, Validation Loss: 0.043656804867911916\n",
      "Epoch [79/20000], Training Loss: 0.03503538202494383, Validation Loss: 0.038306438583186296\n",
      "Epoch [80/20000], Training Loss: 0.03427920663463218, Validation Loss: 0.03216037796368203\n",
      "Epoch [81/20000], Training Loss: 0.03309615608304739, Validation Loss: 0.028355628013411245\n",
      "Epoch [82/20000], Training Loss: 0.035254541252340586, Validation Loss: 0.030287354579564697\n",
      "Epoch [83/20000], Training Loss: 0.04240998725539872, Validation Loss: 0.03588980633522153\n",
      "Epoch [84/20000], Training Loss: 0.03347560091476355, Validation Loss: 0.036428036058286296\n",
      "Epoch [85/20000], Training Loss: 0.05574060883373022, Validation Loss: 0.0873198616622728\n",
      "Epoch [86/20000], Training Loss: 0.0671359361814601, Validation Loss: 0.030414470022029065\n",
      "Epoch [87/20000], Training Loss: 0.06697973455967647, Validation Loss: 0.17326169369245972\n",
      "Epoch [88/20000], Training Loss: 0.08304725041879076, Validation Loss: 0.03008822103124956\n",
      "Epoch [89/20000], Training Loss: 0.04328402876853943, Validation Loss: 0.029078082151702624\n",
      "Epoch [90/20000], Training Loss: 0.029784702429813997, Validation Loss: 0.0331993578640689\n",
      "Epoch [91/20000], Training Loss: 0.03452040950235512, Validation Loss: 0.030939856520293536\n",
      "Epoch [92/20000], Training Loss: 0.030573821293988397, Validation Loss: 0.03417421214490048\n",
      "Epoch [93/20000], Training Loss: 0.0325005986461682, Validation Loss: 0.02916676196979186\n",
      "Epoch [94/20000], Training Loss: 0.028849993400009617, Validation Loss: 0.0424757616284635\n",
      "Epoch [95/20000], Training Loss: 0.034354436809995344, Validation Loss: 0.061502843579433135\n",
      "Epoch [96/20000], Training Loss: 0.04348096624016762, Validation Loss: 0.057967006913059005\n",
      "Epoch [97/20000], Training Loss: 0.05441339447029999, Validation Loss: 0.030041744983990072\n",
      "Epoch [98/20000], Training Loss: 0.044228630007377694, Validation Loss: 0.026627710859662\n",
      "Epoch [99/20000], Training Loss: 0.03429693755294595, Validation Loss: 0.02464433908964559\n",
      "Epoch [100/20000], Training Loss: 0.039817306745265214, Validation Loss: 0.02496449442877851\n",
      "Epoch [101/20000], Training Loss: 0.03373891697265208, Validation Loss: 0.04590559610268574\n",
      "Epoch [102/20000], Training Loss: 0.0301941525789776, Validation Loss: 0.0853962854647037\n",
      "Epoch [103/20000], Training Loss: 0.05240558047911951, Validation Loss: 0.039252211018631085\n",
      "Epoch [104/20000], Training Loss: 0.059049698091777306, Validation Loss: 0.08023983862424576\n",
      "Epoch [105/20000], Training Loss: 0.05642594724694001, Validation Loss: 0.1245881912846852\n",
      "Epoch [106/20000], Training Loss: 0.0706810172040215, Validation Loss: 0.26468479532020883\n",
      "Epoch [107/20000], Training Loss: 0.10170397277189684, Validation Loss: 0.1383234000558444\n",
      "Epoch [108/20000], Training Loss: 0.09193769190460443, Validation Loss: 0.17650318706782123\n",
      "Epoch [109/20000], Training Loss: 0.08238529427243131, Validation Loss: 0.10697713546253844\n",
      "Epoch [110/20000], Training Loss: 0.061461096429931264, Validation Loss: 0.02967645995361211\n",
      "Epoch [111/20000], Training Loss: 0.0364540859923831, Validation Loss: 0.03419191738491015\n",
      "Epoch [112/20000], Training Loss: 0.05692886321672371, Validation Loss: 0.03224575429591144\n",
      "Epoch [113/20000], Training Loss: 0.03940960285919053, Validation Loss: 0.03046578210939204\n",
      "Epoch [114/20000], Training Loss: 0.02953544937606369, Validation Loss: 0.033667823944934616\n",
      "Epoch [115/20000], Training Loss: 0.04056289825322373, Validation Loss: 0.04452021665611521\n",
      "Epoch [116/20000], Training Loss: 0.03540939059374588, Validation Loss: 0.04335671035671424\n",
      "Epoch [117/20000], Training Loss: 0.042513759208044836, Validation Loss: 0.051804874915903784\n",
      "Epoch [118/20000], Training Loss: 0.03677128255367279, Validation Loss: 0.029069276751040953\n",
      "Epoch [119/20000], Training Loss: 0.0287909441561039, Validation Loss: 0.06450034662210927\n",
      "Epoch [120/20000], Training Loss: 0.03565035308046, Validation Loss: 0.04625592903159433\n",
      "Epoch [121/20000], Training Loss: 0.06024815661034414, Validation Loss: 0.04850280007211167\n",
      "Epoch [122/20000], Training Loss: 0.059456924708294015, Validation Loss: 0.03738532448898751\n",
      "Epoch [123/20000], Training Loss: 0.03172318038663694, Validation Loss: 0.04099968612100224\n",
      "Epoch [124/20000], Training Loss: 0.04530504698465977, Validation Loss: 0.04278348978712184\n",
      "Epoch [125/20000], Training Loss: 0.027185746709749634, Validation Loss: 0.0258880574139801\n",
      "Epoch [126/20000], Training Loss: 0.033856576907315424, Validation Loss: 0.031154305585998764\n",
      "Epoch [127/20000], Training Loss: 0.030898430916879858, Validation Loss: 0.05382368715682713\n",
      "Epoch [128/20000], Training Loss: 0.043896525857105316, Validation Loss: 0.02715500482651776\n",
      "Epoch [129/20000], Training Loss: 0.0263499494176358, Validation Loss: 0.031339879061846425\n",
      "Epoch [130/20000], Training Loss: 0.03232323644416673, Validation Loss: 0.027465567072952474\n",
      "Epoch [131/20000], Training Loss: 0.05249246868437955, Validation Loss: 0.07788172372280018\n",
      "Epoch [132/20000], Training Loss: 0.04466797780644681, Validation Loss: 0.02445334715659633\n",
      "Epoch [133/20000], Training Loss: 0.026501719307686602, Validation Loss: 0.023060932996439264\n",
      "Epoch [134/20000], Training Loss: 0.025405286911076734, Validation Loss: 0.022107621538658122\n",
      "Epoch [135/20000], Training Loss: 0.020707047045496956, Validation Loss: 0.023606592798472943\n",
      "Epoch [136/20000], Training Loss: 0.02341350181294339, Validation Loss: 0.02067532129745379\n",
      "Epoch [137/20000], Training Loss: 0.022489175682754388, Validation Loss: 0.031104252169094904\n",
      "Epoch [138/20000], Training Loss: 0.024051521705197438, Validation Loss: 0.04548014291723956\n",
      "Epoch [139/20000], Training Loss: 0.0426673721438939, Validation Loss: 0.039993666352762726\n",
      "Epoch [140/20000], Training Loss: 0.03292592468538454, Validation Loss: 0.035902199333889814\n",
      "Epoch [141/20000], Training Loss: 0.027313942488815104, Validation Loss: 0.024164008681675876\n",
      "Epoch [142/20000], Training Loss: 0.027372085614063377, Validation Loss: 0.03168829760933891\n",
      "Epoch [143/20000], Training Loss: 0.024619100357605412, Validation Loss: 0.02088339505378679\n",
      "Epoch [144/20000], Training Loss: 0.02231152561892356, Validation Loss: 0.01828254373381486\n",
      "Epoch [145/20000], Training Loss: 0.0229877566385507, Validation Loss: 0.02454181654553309\n",
      "Epoch [146/20000], Training Loss: 0.024507065569715842, Validation Loss: 0.08305713491297609\n",
      "Epoch [147/20000], Training Loss: 0.05343977195609893, Validation Loss: 0.03724887104698243\n",
      "Epoch [148/20000], Training Loss: 0.04665738384106329, Validation Loss: 0.10601666029842308\n",
      "Epoch [149/20000], Training Loss: 0.038983477728574405, Validation Loss: 0.023887376416274246\n",
      "Epoch [150/20000], Training Loss: 0.027494446840137243, Validation Loss: 0.037877426333212075\n",
      "Epoch [151/20000], Training Loss: 0.03218454418570867, Validation Loss: 0.0385754683664834\n",
      "Epoch [152/20000], Training Loss: 0.03301806029464517, Validation Loss: 0.0242783339705089\n",
      "Epoch [153/20000], Training Loss: 0.022755587290573333, Validation Loss: 0.06379398935667893\n",
      "Epoch [154/20000], Training Loss: 0.03912367219371455, Validation Loss: 0.044345328286801454\n",
      "Epoch [155/20000], Training Loss: 0.037662108428776264, Validation Loss: 0.04581827454225224\n",
      "Epoch [156/20000], Training Loss: 0.0487734479829669, Validation Loss: 0.08511864757609791\n",
      "Epoch [157/20000], Training Loss: 0.03683874380242612, Validation Loss: 0.05203384688455775\n",
      "Epoch [158/20000], Training Loss: 0.07095996416839105, Validation Loss: 0.05190466300361207\n",
      "Epoch [159/20000], Training Loss: 0.03504294286748128, Validation Loss: 0.024875926310931988\n",
      "Epoch [160/20000], Training Loss: 0.04915839846112898, Validation Loss: 0.058119182648046865\n",
      "Epoch [161/20000], Training Loss: 0.06148414979023593, Validation Loss: 0.053310273548560896\n",
      "Epoch [162/20000], Training Loss: 0.06660965830087662, Validation Loss: 0.041248736124718396\n",
      "Epoch [163/20000], Training Loss: 0.06258504984102078, Validation Loss: 0.04038207393240607\n",
      "Epoch [164/20000], Training Loss: 0.03197292398129191, Validation Loss: 0.08944039702728195\n",
      "Epoch [165/20000], Training Loss: 0.037467560331736295, Validation Loss: 0.03146678874377919\n",
      "Epoch [166/20000], Training Loss: 0.030723398717652475, Validation Loss: 0.021922348794533402\n",
      "Epoch [167/20000], Training Loss: 0.04992314697509365, Validation Loss: 0.023284046330475434\n",
      "Epoch [168/20000], Training Loss: 0.04488951825935926, Validation Loss: 0.0636411596886961\n",
      "Epoch [169/20000], Training Loss: 0.03589955541039152, Validation Loss: 0.03154766488397952\n",
      "Epoch [170/20000], Training Loss: 0.037664596091157625, Validation Loss: 0.023269280118681044\n",
      "Epoch [171/20000], Training Loss: 0.04222185271126883, Validation Loss: 0.02461168354035904\n",
      "Epoch [172/20000], Training Loss: 0.04343997075089386, Validation Loss: 0.022327451649767415\n",
      "Epoch [173/20000], Training Loss: 0.03312521910161844, Validation Loss: 0.033576892303367294\n",
      "Epoch [174/20000], Training Loss: 0.04559052043727466, Validation Loss: 0.026299267382782293\n",
      "Epoch [175/20000], Training Loss: 0.03189268186854731, Validation Loss: 0.02155314547498381\n",
      "Epoch [176/20000], Training Loss: 0.026369956240939376, Validation Loss: 0.021816190003215576\n",
      "Epoch [177/20000], Training Loss: 0.021863623156345317, Validation Loss: 0.025066262931384507\n",
      "Epoch [178/20000], Training Loss: 0.02241632585147662, Validation Loss: 0.024376717623211457\n",
      "Epoch [179/20000], Training Loss: 0.024057459884456227, Validation Loss: 0.040405070537501496\n",
      "Epoch [180/20000], Training Loss: 0.02047399842246835, Validation Loss: 0.07692431217338738\n",
      "Epoch [181/20000], Training Loss: 0.04465041056807552, Validation Loss: 0.020850627492735235\n",
      "Epoch [182/20000], Training Loss: 0.035866776714101434, Validation Loss: 0.05643733285280618\n",
      "Epoch [183/20000], Training Loss: 0.024143358958618983, Validation Loss: 0.020999948084783397\n",
      "Epoch [184/20000], Training Loss: 0.029454734775104692, Validation Loss: 0.04600805799551573\n",
      "Epoch [185/20000], Training Loss: 0.02596713787975854, Validation Loss: 0.04198792730740984\n",
      "Epoch [186/20000], Training Loss: 0.02032164411087121, Validation Loss: 0.020141939898914798\n",
      "Epoch [187/20000], Training Loss: 0.019896446727216244, Validation Loss: 0.029266691180676516\n",
      "Epoch [188/20000], Training Loss: 0.021963146714759723, Validation Loss: 0.022550844240456076\n",
      "Epoch [189/20000], Training Loss: 0.02434364476773356, Validation Loss: 0.01621154502501352\n",
      "Epoch [190/20000], Training Loss: 0.022372903967542306, Validation Loss: 0.04954622904395685\n",
      "Epoch [191/20000], Training Loss: 0.047773521094183834, Validation Loss: 0.029871440758926483\n",
      "Epoch [192/20000], Training Loss: 0.029349552683665285, Validation Loss: 0.11091251805270437\n",
      "Epoch [193/20000], Training Loss: 0.05820508547393339, Validation Loss: 0.0649625147972842\n",
      "Epoch [194/20000], Training Loss: 0.046608721770878346, Validation Loss: 0.032792553464327946\n",
      "Epoch [195/20000], Training Loss: 0.03114720793174846, Validation Loss: 0.023773568674313628\n",
      "Epoch [196/20000], Training Loss: 0.027989864382626756, Validation Loss: 0.016144118168739403\n",
      "Epoch [197/20000], Training Loss: 0.026939938493472124, Validation Loss: 0.015676868729158862\n",
      "Epoch [198/20000], Training Loss: 0.022988310948546444, Validation Loss: 0.016910015908577648\n",
      "Epoch [199/20000], Training Loss: 0.02257269561024649, Validation Loss: 0.017775949934476544\n",
      "Epoch [200/20000], Training Loss: 0.023284318624064326, Validation Loss: 0.018388500894762468\n",
      "Epoch [201/20000], Training Loss: 0.03157947288127616, Validation Loss: 0.043621024637308735\n",
      "Epoch [202/20000], Training Loss: 0.04836485402691843, Validation Loss: 0.02555443608050062\n",
      "Epoch [203/20000], Training Loss: 0.032350895194602866, Validation Loss: 0.01921630353949209\n",
      "Epoch [204/20000], Training Loss: 0.02244054725659745, Validation Loss: 0.01626212412348696\n",
      "Epoch [205/20000], Training Loss: 0.048790044034831226, Validation Loss: 0.11325701655044541\n",
      "Epoch [206/20000], Training Loss: 0.06727566962529506, Validation Loss: 0.023709745222928687\n",
      "Epoch [207/20000], Training Loss: 0.026508868850734352, Validation Loss: 0.025374355132094795\n",
      "Epoch [208/20000], Training Loss: 0.03393311356194317, Validation Loss: 0.0750802054233882\n",
      "Epoch [209/20000], Training Loss: 0.04831635331668492, Validation Loss: 0.051949390402213004\n",
      "Epoch [210/20000], Training Loss: 0.02344790195846664, Validation Loss: 0.02126238336882596\n",
      "Epoch [211/20000], Training Loss: 0.017556757615238894, Validation Loss: 0.015408065096711523\n",
      "Epoch [212/20000], Training Loss: 0.02367691745582436, Validation Loss: 0.044737429689686206\n",
      "Epoch [213/20000], Training Loss: 0.022392199420469945, Validation Loss: 0.03711810812886603\n",
      "Epoch [214/20000], Training Loss: 0.024140978764210428, Validation Loss: 0.019811209093233705\n",
      "Epoch [215/20000], Training Loss: 0.02281858236528933, Validation Loss: 0.05068717211035307\n",
      "Epoch [216/20000], Training Loss: 0.028599335545940057, Validation Loss: 0.018342477910898936\n",
      "Epoch [217/20000], Training Loss: 0.017504593091351644, Validation Loss: 0.016079470488201814\n",
      "Epoch [218/20000], Training Loss: 0.0229869547765702, Validation Loss: 0.012778465544091743\n",
      "Epoch [219/20000], Training Loss: 0.01556351308993596, Validation Loss: 0.047633793645200484\n",
      "Epoch [220/20000], Training Loss: 0.026231393051732863, Validation Loss: 0.09274108503773094\n",
      "Epoch [221/20000], Training Loss: 0.09388895342791719, Validation Loss: 0.2178176258679095\n",
      "Epoch [222/20000], Training Loss: 0.07071150065998413, Validation Loss: 0.04585095684173573\n",
      "Epoch [223/20000], Training Loss: 0.05321509570681623, Validation Loss: 0.04294546064175871\n",
      "Epoch [224/20000], Training Loss: 0.08195133773343903, Validation Loss: 0.05901500912915379\n",
      "Epoch [225/20000], Training Loss: 0.1677854255373989, Validation Loss: 0.03322880580385366\n",
      "Epoch [226/20000], Training Loss: 0.10794455597975425, Validation Loss: 0.07741962033311615\n",
      "Epoch [227/20000], Training Loss: 0.054111136722245386, Validation Loss: 0.028216606753251668\n",
      "Epoch [228/20000], Training Loss: 0.027921526825853756, Validation Loss: 0.027011354217745705\n",
      "Epoch [229/20000], Training Loss: 0.029117660962843468, Validation Loss: 0.024307393758587908\n",
      "Epoch [230/20000], Training Loss: 0.025370383435594186, Validation Loss: 0.03105920042165625\n",
      "Epoch [231/20000], Training Loss: 0.027387425702597414, Validation Loss: 0.02434325713804274\n",
      "Epoch [232/20000], Training Loss: 0.02238293577517782, Validation Loss: 0.050750784484559434\n",
      "Epoch [233/20000], Training Loss: 0.023033736972138286, Validation Loss: 0.022459553764046587\n",
      "Epoch [234/20000], Training Loss: 0.021827336766624024, Validation Loss: 0.02035122208260248\n",
      "Epoch [235/20000], Training Loss: 0.02306063029183341, Validation Loss: 0.021764944277132194\n",
      "Epoch [236/20000], Training Loss: 0.026300248024719104, Validation Loss: 0.021087457957300437\n",
      "Epoch [237/20000], Training Loss: 0.033839452013905556, Validation Loss: 0.03817056095091082\n",
      "Epoch [238/20000], Training Loss: 0.039018108775573115, Validation Loss: 0.02919119949269449\n",
      "Epoch [239/20000], Training Loss: 0.04132078520654302, Validation Loss: 0.02705310557878693\n",
      "Epoch [240/20000], Training Loss: 0.025148159874204015, Validation Loss: 0.02452164356354179\n",
      "Epoch [241/20000], Training Loss: 0.05656033498235047, Validation Loss: 0.02398071447177816\n",
      "Epoch [242/20000], Training Loss: 0.03424637930999909, Validation Loss: 0.03888618492788625\n",
      "Epoch [243/20000], Training Loss: 0.040596947739166875, Validation Loss: 0.0797564846497819\n",
      "Epoch [244/20000], Training Loss: 0.0624226966340627, Validation Loss: 0.04907902483450197\n",
      "Epoch [245/20000], Training Loss: 0.0404110709537885, Validation Loss: 0.04696874631367265\n",
      "Epoch [246/20000], Training Loss: 0.035594629107176194, Validation Loss: 0.022079338655915017\n",
      "Epoch [247/20000], Training Loss: 0.02677927353319579, Validation Loss: 0.023721734909350763\n",
      "Epoch [248/20000], Training Loss: 0.06540977579217204, Validation Loss: 0.021264553560470252\n",
      "Epoch [249/20000], Training Loss: 0.030421469269120798, Validation Loss: 0.023955370114996922\n",
      "Epoch [250/20000], Training Loss: 0.02979998160818858, Validation Loss: 0.020349695544265384\n",
      "Epoch [251/20000], Training Loss: 0.02499517127164706, Validation Loss: 0.08571460437249812\n",
      "Epoch [252/20000], Training Loss: 0.0367135894484818, Validation Loss: 0.022315161627711595\n",
      "Epoch [253/20000], Training Loss: 0.037807252790246694, Validation Loss: 0.017296130733757138\n",
      "Epoch [254/20000], Training Loss: 0.02039896833178188, Validation Loss: 0.016147043523995087\n",
      "Epoch [255/20000], Training Loss: 0.01892207419898893, Validation Loss: 0.01688230851964282\n",
      "Epoch [256/20000], Training Loss: 0.017538854918841804, Validation Loss: 0.014609875094025657\n",
      "Epoch [257/20000], Training Loss: 0.019899668604401604, Validation Loss: 0.01455311188113687\n",
      "Epoch [258/20000], Training Loss: 0.021918960059078278, Validation Loss: 0.026085739817256353\n",
      "Epoch [259/20000], Training Loss: 0.01909596531602022, Validation Loss: 0.04831344547485334\n",
      "Epoch [260/20000], Training Loss: 0.03644203470321372, Validation Loss: 0.019251566124087356\n",
      "Epoch [261/20000], Training Loss: 0.038019215721370916, Validation Loss: 0.03941921082978033\n",
      "Epoch [262/20000], Training Loss: 0.03732430103367993, Validation Loss: 0.015572724887623023\n",
      "Epoch [263/20000], Training Loss: 0.061295658782390613, Validation Loss: 0.021948947804838376\n",
      "Epoch [264/20000], Training Loss: 0.032356943344763876, Validation Loss: 0.024578322222918687\n",
      "Epoch [265/20000], Training Loss: 0.029005490925296078, Validation Loss: 0.03470747763864969\n",
      "Epoch [266/20000], Training Loss: 0.025878240645397455, Validation Loss: 0.022144804697659438\n",
      "Epoch [267/20000], Training Loss: 0.016890484540324126, Validation Loss: 0.017332472432076633\n",
      "Epoch [268/20000], Training Loss: 0.02070237842521497, Validation Loss: 0.02951278019334236\n",
      "Epoch [269/20000], Training Loss: 0.025088983587920666, Validation Loss: 0.02007480082869303\n",
      "Epoch [270/20000], Training Loss: 0.027758423393866645, Validation Loss: 0.18527361968684122\n",
      "Epoch [271/20000], Training Loss: 0.09224849799647927, Validation Loss: 0.06624600659107673\n",
      "Epoch [272/20000], Training Loss: 0.04471493063361517, Validation Loss: 0.025754571784896946\n",
      "Epoch [273/20000], Training Loss: 0.024785299951742803, Validation Loss: 0.02178496648420086\n",
      "Epoch [274/20000], Training Loss: 0.02905620395072869, Validation Loss: 0.019508971191997508\n",
      "Epoch [275/20000], Training Loss: 0.04393297478756202, Validation Loss: 0.030078332485244025\n",
      "Epoch [276/20000], Training Loss: 0.030954900729869093, Validation Loss: 0.031087479207156172\n",
      "Epoch [277/20000], Training Loss: 0.03270632800246988, Validation Loss: 0.029833302894892156\n",
      "Epoch [278/20000], Training Loss: 0.02930239395105413, Validation Loss: 0.03471798778885825\n",
      "Epoch [279/20000], Training Loss: 0.03720029918726401, Validation Loss: 0.10071316562564903\n",
      "Epoch [280/20000], Training Loss: 0.05064927294733934, Validation Loss: 0.1039884655497413\n",
      "Epoch [281/20000], Training Loss: 0.029176334950274656, Validation Loss: 0.0204915841332881\n",
      "Epoch [282/20000], Training Loss: 0.022113718641256646, Validation Loss: 0.016035896817830495\n",
      "Epoch [283/20000], Training Loss: 0.02065140414716942, Validation Loss: 0.016964210505126864\n",
      "Epoch [284/20000], Training Loss: 0.016415690149837507, Validation Loss: 0.01979912626741179\n",
      "Epoch [285/20000], Training Loss: 0.016883787865351354, Validation Loss: 0.014978609732117778\n",
      "Epoch [286/20000], Training Loss: 0.01745926609562178, Validation Loss: 0.01475911318730283\n",
      "Epoch [287/20000], Training Loss: 0.018204359703564217, Validation Loss: 0.019874950685101162\n",
      "Epoch [288/20000], Training Loss: 0.01996223525410252, Validation Loss: 0.023471303623293034\n",
      "Epoch [289/20000], Training Loss: 0.021734745973455056, Validation Loss: 0.0207586866824612\n",
      "Epoch [290/20000], Training Loss: 0.04220662432323609, Validation Loss: 0.039578175852758815\n",
      "Epoch [291/20000], Training Loss: 0.03428641364111432, Validation Loss: 0.034670030344353515\n",
      "Epoch [292/20000], Training Loss: 0.04522364669745522, Validation Loss: 0.028786139304994154\n",
      "Epoch [293/20000], Training Loss: 0.04376990394666791, Validation Loss: 0.03207313682902616\n",
      "Epoch [294/20000], Training Loss: 0.07660231566062846, Validation Loss: 0.04754580055971352\n",
      "Epoch [295/20000], Training Loss: 0.025733279546589723, Validation Loss: 0.024298821003365285\n",
      "Epoch [296/20000], Training Loss: 0.021839188345308815, Validation Loss: 0.014213645078319206\n",
      "Epoch [297/20000], Training Loss: 0.01777964647460197, Validation Loss: 0.06319424485716064\n",
      "Epoch [298/20000], Training Loss: 0.027442802359083935, Validation Loss: 0.020859867670555206\n",
      "Epoch [299/20000], Training Loss: 0.017969277930595644, Validation Loss: 0.04244977673832562\n",
      "Epoch [300/20000], Training Loss: 0.02052671557092773, Validation Loss: 0.028483273283742148\n",
      "Epoch [301/20000], Training Loss: 0.03783941546654595, Validation Loss: 0.037562414810741504\n",
      "Epoch [302/20000], Training Loss: 0.035337021724054854, Validation Loss: 0.01460829096223039\n",
      "Epoch [303/20000], Training Loss: 0.05100174407873835, Validation Loss: 0.014703035436528162\n",
      "Epoch [304/20000], Training Loss: 0.05680506137598838, Validation Loss: 0.020613429120307915\n",
      "Epoch [305/20000], Training Loss: 0.035114956296248626, Validation Loss: 0.024041901070309075\n",
      "Epoch [306/20000], Training Loss: 0.027635847783780525, Validation Loss: 0.01601639789707881\n",
      "Epoch [307/20000], Training Loss: 0.033385485876351595, Validation Loss: 0.014379546318813092\n",
      "Epoch [308/20000], Training Loss: 0.040866804018151015, Validation Loss: 0.033969432090256824\n",
      "Epoch [309/20000], Training Loss: 0.027827955782413483, Validation Loss: 0.01697850856655947\n",
      "Epoch [310/20000], Training Loss: 0.028138165195871676, Validation Loss: 0.08587221787136302\n",
      "Epoch [311/20000], Training Loss: 0.09307375430528607, Validation Loss: 0.07050999178942571\n",
      "Epoch [312/20000], Training Loss: 0.06287692706765872, Validation Loss: 0.059238086737201864\n",
      "Epoch [313/20000], Training Loss: 0.035129505993349346, Validation Loss: 0.026538040044744304\n",
      "Epoch [314/20000], Training Loss: 0.024853617285511324, Validation Loss: 0.019299599277541293\n",
      "Epoch [315/20000], Training Loss: 0.03575402161472344, Validation Loss: 0.01935188934048896\n",
      "Epoch [316/20000], Training Loss: 0.033063732885888646, Validation Loss: 0.018324411845414574\n",
      "Epoch [317/20000], Training Loss: 0.030664732114278843, Validation Loss: 0.023202093252093166\n",
      "Epoch [318/20000], Training Loss: 0.024628562935894088, Validation Loss: 0.024629840336725297\n",
      "Epoch [319/20000], Training Loss: 0.020280059593330537, Validation Loss: 0.016306755079668278\n",
      "Epoch [320/20000], Training Loss: 0.018924617008971318, Validation Loss: 0.01713423235994441\n",
      "Epoch [321/20000], Training Loss: 0.01741081373100834, Validation Loss: 0.025674129473201468\n",
      "Epoch [322/20000], Training Loss: 0.02481678094980972, Validation Loss: 0.02898716207333177\n",
      "Epoch [323/20000], Training Loss: 0.021890391828492284, Validation Loss: 0.017930695965816312\n",
      "Epoch [324/20000], Training Loss: 0.020921844856015274, Validation Loss: 0.020351224190274003\n",
      "Epoch [325/20000], Training Loss: 0.048370898784404356, Validation Loss: 0.01896757942230463\n",
      "Epoch [326/20000], Training Loss: 0.04670576711318323, Validation Loss: 0.09983609209485447\n",
      "Epoch [327/20000], Training Loss: 0.05950095479576183, Validation Loss: 0.037102378337925145\n",
      "Epoch [328/20000], Training Loss: 0.021931406210309694, Validation Loss: 0.015917868160653166\n",
      "Epoch [329/20000], Training Loss: 0.01931862363458744, Validation Loss: 0.014857253035853344\n",
      "Epoch [330/20000], Training Loss: 0.01710779206561191, Validation Loss: 0.01848739471128472\n",
      "Epoch [331/20000], Training Loss: 0.021881022224468843, Validation Loss: 0.024636127200864673\n",
      "Epoch [332/20000], Training Loss: 0.018235381060679044, Validation Loss: 0.015893821367551908\n",
      "Epoch [333/20000], Training Loss: 0.022230206701871275, Validation Loss: 0.02397272842127407\n",
      "Epoch [334/20000], Training Loss: 0.02119869347994349, Validation Loss: 0.023447111548843682\n",
      "Epoch [335/20000], Training Loss: 0.023105290152930787, Validation Loss: 0.02107979038370316\n",
      "Epoch [336/20000], Training Loss: 0.012980689011913325, Validation Loss: 0.014860820351318844\n",
      "Epoch [337/20000], Training Loss: 0.015155627633378441, Validation Loss: 0.011348320403302406\n",
      "Epoch [338/20000], Training Loss: 0.01584009891043284, Validation Loss: 0.011484593461477122\n",
      "Epoch [339/20000], Training Loss: 0.012155176872121436, Validation Loss: 0.010757960506143987\n",
      "Epoch [340/20000], Training Loss: 0.0175571522268001, Validation Loss: 0.013670935282535776\n",
      "Epoch [341/20000], Training Loss: 0.017715537234055643, Validation Loss: 0.014607442290830759\n",
      "Epoch [342/20000], Training Loss: 0.017585563005663322, Validation Loss: 0.05720625634157406\n",
      "Epoch [343/20000], Training Loss: 0.06583285862247326, Validation Loss: 0.16833104753961894\n",
      "Epoch [344/20000], Training Loss: 0.12390255997888744, Validation Loss: 0.07908073150076968\n",
      "Epoch [345/20000], Training Loss: 0.3468307903302567, Validation Loss: 0.12495574515300786\n",
      "Epoch [346/20000], Training Loss: 0.08157865064484733, Validation Loss: 0.07940790936543865\n",
      "Epoch [347/20000], Training Loss: 0.065988226128476, Validation Loss: 0.04998587979923891\n",
      "Epoch [348/20000], Training Loss: 0.04496724012174776, Validation Loss: 0.03478664945654727\n",
      "Epoch [349/20000], Training Loss: 0.04173188137688807, Validation Loss: 0.044817077748888955\n",
      "Epoch [350/20000], Training Loss: 0.041630630930220444, Validation Loss: 0.028138173616820132\n",
      "Epoch [351/20000], Training Loss: 0.04505936971067318, Validation Loss: 0.02729020973947085\n",
      "Epoch [352/20000], Training Loss: 0.027926300585802113, Validation Loss: 0.02526227842350583\n",
      "Epoch [353/20000], Training Loss: 0.029304042791149447, Validation Loss: 0.02467251839067369\n",
      "Epoch [354/20000], Training Loss: 0.024724583141505718, Validation Loss: 0.02182826003018957\n",
      "Epoch [355/20000], Training Loss: 0.02493536179619176, Validation Loss: 0.03369013707378595\n",
      "Epoch [356/20000], Training Loss: 0.0292666237136083, Validation Loss: 0.02791569725013862\n",
      "Epoch [357/20000], Training Loss: 0.031659412247660966, Validation Loss: 0.02969799301147059\n",
      "Epoch [358/20000], Training Loss: 0.025583468843251467, Validation Loss: 0.029842467974870633\n",
      "Epoch [359/20000], Training Loss: 0.021560492858822857, Validation Loss: 0.03799662684731282\n",
      "Epoch [360/20000], Training Loss: 0.03213483535052676, Validation Loss: 0.02896409768718221\n",
      "Epoch [361/20000], Training Loss: 0.029234286697049226, Validation Loss: 0.02090321380230836\n",
      "Epoch [362/20000], Training Loss: 0.02177763923204371, Validation Loss: 0.023408487639935308\n",
      "Epoch [363/20000], Training Loss: 0.04006742034107447, Validation Loss: 0.02865674803055645\n",
      "Epoch [364/20000], Training Loss: 0.05611638062899666, Validation Loss: 0.043248402626846615\n",
      "Epoch [365/20000], Training Loss: 0.041326842087853165, Validation Loss: 0.054874736666395564\n",
      "Epoch [366/20000], Training Loss: 0.03076790988312236, Validation Loss: 0.020547776100611752\n",
      "Epoch [367/20000], Training Loss: 0.023494189532357268, Validation Loss: 0.021589580891275145\n",
      "Epoch [368/20000], Training Loss: 0.02226659833520119, Validation Loss: 0.03581831891568229\n",
      "Epoch [369/20000], Training Loss: 0.046455405153600235, Validation Loss: 0.02239023596669928\n",
      "Epoch [370/20000], Training Loss: 0.045150154270231724, Validation Loss: 0.029179693884617195\n",
      "Epoch [371/20000], Training Loss: 0.032369197878454416, Validation Loss: 0.027574367792457748\n",
      "Epoch [372/20000], Training Loss: 0.06449830575313951, Validation Loss: 0.033180352912779675\n",
      "Epoch [373/20000], Training Loss: 0.02466875866853765, Validation Loss: 0.04026145663031733\n",
      "Epoch [374/20000], Training Loss: 0.020404244041336433, Validation Loss: 0.02111701692970694\n",
      "Epoch [375/20000], Training Loss: 0.021121522644534707, Validation Loss: 0.017986339404013514\n",
      "Epoch [376/20000], Training Loss: 0.019728777298171605, Validation Loss: 0.018372992677023702\n",
      "Epoch [377/20000], Training Loss: 0.02037931872837362, Validation Loss: 0.050903533299216194\n",
      "Epoch [378/20000], Training Loss: 0.033716765764568536, Validation Loss: 0.052364744269022365\n",
      "Epoch [379/20000], Training Loss: 0.03573630534161306, Validation Loss: 0.018607091745337583\n",
      "Epoch [380/20000], Training Loss: 0.020116286485322883, Validation Loss: 0.015241346372790874\n",
      "Epoch [381/20000], Training Loss: 0.021253179407462346, Validation Loss: 0.027909919587600723\n",
      "Epoch [382/20000], Training Loss: 0.021526750276929567, Validation Loss: 0.01524465072380225\n",
      "Epoch [383/20000], Training Loss: 0.017818768142855594, Validation Loss: 0.014238677905606815\n",
      "Epoch [384/20000], Training Loss: 0.01820402203260788, Validation Loss: 0.02066498129277453\n",
      "Epoch [385/20000], Training Loss: 0.016533027062126036, Validation Loss: 0.01432005770379859\n",
      "Epoch [386/20000], Training Loss: 0.023930386507085392, Validation Loss: 0.014471964858652887\n",
      "Epoch [387/20000], Training Loss: 0.027283952504928623, Validation Loss: 0.030383958771333746\n",
      "Epoch [388/20000], Training Loss: 0.026610385593292967, Validation Loss: 0.019775182959633954\n",
      "Epoch [389/20000], Training Loss: 0.02416144876873919, Validation Loss: 0.014559664594482156\n",
      "Epoch [390/20000], Training Loss: 0.0199077059348513, Validation Loss: 0.013412499290513148\n",
      "Epoch [391/20000], Training Loss: 0.06908247077704541, Validation Loss: 0.08253040984799474\n",
      "Epoch [392/20000], Training Loss: 0.037983764372516556, Validation Loss: 0.032753863074096674\n",
      "Epoch [393/20000], Training Loss: 0.025800546198817238, Validation Loss: 0.019629465625317383\n",
      "Epoch [394/20000], Training Loss: 0.019383614284119437, Validation Loss: 0.01547467561092442\n",
      "Epoch [395/20000], Training Loss: 0.02172685679813315, Validation Loss: 0.014689946332581128\n",
      "Epoch [396/20000], Training Loss: 0.01742656627071223, Validation Loss: 0.014898654408077807\n",
      "Epoch [397/20000], Training Loss: 0.01296166014591498, Validation Loss: 0.021193131613992833\n",
      "Epoch [398/20000], Training Loss: 0.016609627653711607, Validation Loss: 0.013834400054703903\n",
      "Epoch [399/20000], Training Loss: 0.01853557029140315, Validation Loss: 0.01764128941316635\n",
      "Epoch [400/20000], Training Loss: 0.02072028215375862, Validation Loss: 0.01770880982641285\n",
      "Epoch [401/20000], Training Loss: 0.03239414157412414, Validation Loss: 0.015580887175132471\n",
      "Epoch [402/20000], Training Loss: 0.023677580690543567, Validation Loss: 0.016712213240216912\n",
      "Epoch [403/20000], Training Loss: 0.02694908086310274, Validation Loss: 0.02812056304252063\n",
      "Epoch [404/20000], Training Loss: 0.033167414899383275, Validation Loss: 0.051399540255633154\n",
      "Epoch [405/20000], Training Loss: 0.028511702981502043, Validation Loss: 0.03460839149148322\n",
      "Epoch [406/20000], Training Loss: 0.03719855286180973, Validation Loss: 0.017621502487035152\n",
      "Epoch [407/20000], Training Loss: 0.017637094133533537, Validation Loss: 0.01816119424476925\n",
      "Epoch [408/20000], Training Loss: 0.023400203773884902, Validation Loss: 0.04096914693972707\n",
      "Epoch [409/20000], Training Loss: 0.036015461947369785, Validation Loss: 0.040907342473006376\n",
      "Epoch [410/20000], Training Loss: 0.055284373029800396, Validation Loss: 0.020031543028846584\n",
      "Epoch [411/20000], Training Loss: 0.10040407082332033, Validation Loss: 0.09889737591156797\n",
      "Epoch [412/20000], Training Loss: 0.06877014852528061, Validation Loss: 0.020583880091524537\n",
      "Epoch [413/20000], Training Loss: 0.027858555616278733, Validation Loss: 0.023751764094610495\n",
      "Epoch [414/20000], Training Loss: 0.019257484935224056, Validation Loss: 0.017768765737926354\n",
      "Epoch [415/20000], Training Loss: 0.023720631450747272, Validation Loss: 0.014915083531286944\n",
      "Epoch [416/20000], Training Loss: 0.019974384090996215, Validation Loss: 0.016404901430662863\n",
      "Epoch [417/20000], Training Loss: 0.02873796354314046, Validation Loss: 0.022046834152438854\n",
      "Epoch [418/20000], Training Loss: 0.023722723945476382, Validation Loss: 0.015118111975589552\n",
      "Epoch [419/20000], Training Loss: 0.017847003905834363, Validation Loss: 0.017605936280686855\n",
      "Epoch [420/20000], Training Loss: 0.03059534471581823, Validation Loss: 0.022602392425937277\n",
      "Epoch [421/20000], Training Loss: 0.06034651987387666, Validation Loss: 0.021059228887986947\n",
      "Epoch [422/20000], Training Loss: 0.07082249618127077, Validation Loss: 0.039711509851326716\n",
      "Epoch [423/20000], Training Loss: 0.07613675016909838, Validation Loss: 0.04792870699225589\n",
      "Epoch [424/20000], Training Loss: 0.03954603947100362, Validation Loss: 0.018620883340567964\n",
      "Epoch [425/20000], Training Loss: 0.029034840974158475, Validation Loss: 0.01962725508031096\n",
      "Epoch [426/20000], Training Loss: 0.0210498786514758, Validation Loss: 0.020304986571074952\n",
      "Epoch [427/20000], Training Loss: 0.020444677519013306, Validation Loss: 0.03172283251233855\n",
      "Epoch [428/20000], Training Loss: 0.022017299447075596, Validation Loss: 0.0292560449287211\n",
      "Epoch [429/20000], Training Loss: 0.02993460849393159, Validation Loss: 0.05450831473606188\n",
      "Epoch [430/20000], Training Loss: 0.03686736605595797, Validation Loss: 0.050582884877972224\n",
      "Epoch [431/20000], Training Loss: 0.029515708331018686, Validation Loss: 0.018225157098816606\n",
      "Epoch [432/20000], Training Loss: 0.021415734896436334, Validation Loss: 0.02200461012326012\n",
      "Epoch [433/20000], Training Loss: 0.01989895537761705, Validation Loss: 0.014598055536201051\n",
      "Epoch [434/20000], Training Loss: 0.016731663823260794, Validation Loss: 0.014627890678462508\n",
      "Epoch [435/20000], Training Loss: 0.02051688855447407, Validation Loss: 0.012744409314277871\n",
      "Epoch [436/20000], Training Loss: 0.019368276498945698, Validation Loss: 0.02153451366714168\n",
      "Epoch [437/20000], Training Loss: 0.033536014812333245, Validation Loss: 0.020528092136278937\n",
      "Epoch [438/20000], Training Loss: 0.025814062782696316, Validation Loss: 0.012972547631681429\n",
      "Epoch [439/20000], Training Loss: 0.018238562757947614, Validation Loss: 0.012390746651936815\n",
      "Epoch [440/20000], Training Loss: 0.012945589692597943, Validation Loss: 0.0117820636362989\n",
      "Epoch [441/20000], Training Loss: 0.019606618848878758, Validation Loss: 0.01276585416926552\n",
      "Epoch [442/20000], Training Loss: 0.016187459937230284, Validation Loss: 0.013830191070161651\n",
      "Epoch [443/20000], Training Loss: 0.027757082217639045, Validation Loss: 0.04895211617350427\n",
      "Epoch [444/20000], Training Loss: 0.025796287759606327, Validation Loss: 0.017641806462094723\n",
      "Epoch [445/20000], Training Loss: 0.029300190741196275, Validation Loss: 0.010257372399959386\n",
      "Epoch [446/20000], Training Loss: 0.03651195113447362, Validation Loss: 0.017279348366565656\n",
      "Epoch [447/20000], Training Loss: 0.024945758449445878, Validation Loss: 0.08783730978755665\n",
      "Epoch [448/20000], Training Loss: 0.04818834677904046, Validation Loss: 0.017764428854600615\n",
      "Epoch [449/20000], Training Loss: 0.019712330158134655, Validation Loss: 0.015823922109236253\n",
      "Epoch [450/20000], Training Loss: 0.026384401634069427, Validation Loss: 0.05427593289430908\n",
      "Epoch [451/20000], Training Loss: 0.02893494929386569, Validation Loss: 0.012882486906056157\n",
      "Epoch [452/20000], Training Loss: 0.015482354160797383, Validation Loss: 0.025189063002387973\n",
      "Epoch [453/20000], Training Loss: 0.028762522685740675, Validation Loss: 0.01806846125808877\n",
      "Epoch [454/20000], Training Loss: 0.04265188332647085, Validation Loss: 0.05262408777997009\n",
      "Epoch [455/20000], Training Loss: 0.043933200483609526, Validation Loss: 0.013515621469972263\n",
      "Epoch [456/20000], Training Loss: 0.018181278087597872, Validation Loss: 0.011096828514774256\n",
      "Epoch [457/20000], Training Loss: 0.016985066812984378, Validation Loss: 0.020067877413430347\n",
      "Epoch [458/20000], Training Loss: 0.016670576662623456, Validation Loss: 0.028441478249958886\n",
      "Epoch [459/20000], Training Loss: 0.022556343232281506, Validation Loss: 0.026949027795671442\n",
      "Epoch [460/20000], Training Loss: 0.021786452720074782, Validation Loss: 0.01249335801735228\n",
      "Epoch [461/20000], Training Loss: 0.03022016920814557, Validation Loss: 0.06939201467033738\n",
      "Epoch [462/20000], Training Loss: 0.03699858305377087, Validation Loss: 0.030859208017530077\n",
      "Epoch [463/20000], Training Loss: 0.017622057481535842, Validation Loss: 0.022626554563623804\n",
      "Epoch [464/20000], Training Loss: 0.019823958265728185, Validation Loss: 0.01636210366944766\n",
      "Epoch [465/20000], Training Loss: 0.016733059376877333, Validation Loss: 0.012288992800112525\n",
      "Epoch [466/20000], Training Loss: 0.01484463004661458, Validation Loss: 0.012967066952773023\n",
      "Epoch [467/20000], Training Loss: 0.011567791907249816, Validation Loss: 0.010682026257170713\n",
      "Epoch [468/20000], Training Loss: 0.01455434292022671, Validation Loss: 0.011017167333807908\n",
      "Epoch [469/20000], Training Loss: 0.01857088597431097, Validation Loss: 0.04363934237690281\n",
      "Epoch [470/20000], Training Loss: 0.04329283693472722, Validation Loss: 0.06538000888376513\n",
      "Epoch [471/20000], Training Loss: 0.07182199650976274, Validation Loss: 0.053705558617458675\n",
      "Epoch [472/20000], Training Loss: 0.046369786407532435, Validation Loss: 0.03745311609422259\n",
      "Epoch [473/20000], Training Loss: 0.04463679815775582, Validation Loss: 0.014191295178345928\n",
      "Epoch [474/20000], Training Loss: 0.022512316770319427, Validation Loss: 0.018584324524498597\n",
      "Epoch [475/20000], Training Loss: 0.052847508856627555, Validation Loss: 0.05830466246284849\n",
      "Epoch [476/20000], Training Loss: 0.044408740874912055, Validation Loss: 0.03935385795368819\n",
      "Epoch [477/20000], Training Loss: 0.041810998427016396, Validation Loss: 0.03737267510448442\n",
      "Epoch [478/20000], Training Loss: 0.030074973871316097, Validation Loss: 0.02853657451669917\n",
      "Epoch [479/20000], Training Loss: 0.044342627466124086, Validation Loss: 0.016190917557162825\n",
      "Epoch [480/20000], Training Loss: 0.055717208505874236, Validation Loss: 0.024911888666918795\n",
      "Epoch [481/20000], Training Loss: 0.024704205919988453, Validation Loss: 0.021082296308165948\n",
      "Epoch [482/20000], Training Loss: 0.0202163316176406, Validation Loss: 0.016495420881929398\n",
      "Epoch [483/20000], Training Loss: 0.016643989465332458, Validation Loss: 0.019511503970663934\n",
      "Epoch [484/20000], Training Loss: 0.018832948225151216, Validation Loss: 0.018140946953797643\n",
      "Epoch [485/20000], Training Loss: 0.024799817374774387, Validation Loss: 0.014734036169667753\n",
      "Epoch [486/20000], Training Loss: 0.021999306777226075, Validation Loss: 0.011354936658891802\n",
      "Epoch [487/20000], Training Loss: 0.01754751246023391, Validation Loss: 0.04570625387452181\n",
      "Epoch [488/20000], Training Loss: 0.021163294786154956, Validation Loss: 0.01969204627010266\n",
      "Epoch [489/20000], Training Loss: 0.023087660582470044, Validation Loss: 0.05153241665372561\n",
      "Epoch [490/20000], Training Loss: 0.029577746388635466, Validation Loss: 0.011060529174738765\n",
      "Epoch [491/20000], Training Loss: 0.018652064925325767, Validation Loss: 0.018643526685160342\n",
      "Epoch [492/20000], Training Loss: 0.017520728595887443, Validation Loss: 0.017975696115332875\n",
      "Epoch [493/20000], Training Loss: 0.015098739490245603, Validation Loss: 0.023186527959662938\n",
      "Epoch [494/20000], Training Loss: 0.021201969107746015, Validation Loss: 0.025458404063841682\n",
      "Epoch [495/20000], Training Loss: 0.027112699309197654, Validation Loss: 0.02651063900554617\n",
      "Epoch [496/20000], Training Loss: 0.026808541908394545, Validation Loss: 0.012091251557102528\n",
      "Epoch [497/20000], Training Loss: 0.019990465470722744, Validation Loss: 0.019989141354337936\n",
      "Epoch [498/20000], Training Loss: 0.03104901995642909, Validation Loss: 0.011063421359097141\n",
      "Epoch [499/20000], Training Loss: 0.020200479459682747, Validation Loss: 0.009487964489239923\n",
      "Epoch [500/20000], Training Loss: 0.012919492416715781, Validation Loss: 0.011472036608417472\n",
      "Epoch [501/20000], Training Loss: 0.024615761757429157, Validation Loss: 0.011749136787004831\n",
      "Epoch [502/20000], Training Loss: 0.01911493294340159, Validation Loss: 0.008349666114033667\n",
      "Epoch [503/20000], Training Loss: 0.017687951447442174, Validation Loss: 0.01786592936905949\n",
      "Epoch [504/20000], Training Loss: 0.02050290599332324, Validation Loss: 0.020112048890260255\n",
      "Epoch [505/20000], Training Loss: 0.024632650304868418, Validation Loss: 0.045518467714189666\n",
      "Epoch [506/20000], Training Loss: 0.03410864939048354, Validation Loss: 0.03462913038398358\n",
      "Epoch [507/20000], Training Loss: 0.027522304207585484, Validation Loss: 0.01426901791091866\n",
      "Epoch [508/20000], Training Loss: 0.01581856353108638, Validation Loss: 0.0130089164247137\n",
      "Epoch [509/20000], Training Loss: 0.026510781024366485, Validation Loss: 0.1027883871719953\n",
      "Epoch [510/20000], Training Loss: 0.059339525864093697, Validation Loss: 0.014571951376989764\n",
      "Epoch [511/20000], Training Loss: 0.05882520235276648, Validation Loss: 0.02265214643589105\n",
      "Epoch [512/20000], Training Loss: 0.07764676605750408, Validation Loss: 0.21000444592290718\n",
      "Epoch [513/20000], Training Loss: 0.06835825062756028, Validation Loss: 0.1199012304508315\n",
      "Epoch [514/20000], Training Loss: 0.06518324770565544, Validation Loss: 0.03946349095483555\n",
      "Epoch [515/20000], Training Loss: 0.03986739766384874, Validation Loss: 0.05411656379819179\n",
      "Epoch [516/20000], Training Loss: 0.033792799072606225, Validation Loss: 0.05568629250128967\n",
      "Epoch [517/20000], Training Loss: 0.03381916567949312, Validation Loss: 0.021020224241294478\n",
      "Epoch [518/20000], Training Loss: 0.02857993361872754, Validation Loss: 0.018561703707199305\n",
      "Epoch [519/20000], Training Loss: 0.02271334471047989, Validation Loss: 0.02347612522574377\n",
      "Epoch [520/20000], Training Loss: 0.021075480251706073, Validation Loss: 0.04393135974837085\n",
      "Epoch [521/20000], Training Loss: 0.022195733385160565, Validation Loss: 0.018721996197484103\n",
      "Epoch [522/20000], Training Loss: 0.01751152687938884, Validation Loss: 0.01101377561028673\n",
      "Epoch [523/20000], Training Loss: 0.013895725467591546, Validation Loss: 0.014038282445834823\n",
      "Epoch [524/20000], Training Loss: 0.025089116816941117, Validation Loss: 0.010984231529405078\n",
      "Epoch [525/20000], Training Loss: 0.013859778168677752, Validation Loss: 0.03022693586756899\n",
      "Epoch [526/20000], Training Loss: 0.023731381000418748, Validation Loss: 0.011891015732118235\n",
      "Epoch [527/20000], Training Loss: 0.014859884362002569, Validation Loss: 0.02667786347584108\n",
      "Epoch [528/20000], Training Loss: 0.03034676579409279, Validation Loss: 0.010894818699645035\n",
      "Epoch [529/20000], Training Loss: 0.03338695904572627, Validation Loss: 0.01852453419529607\n",
      "Epoch [530/20000], Training Loss: 0.021085460661976998, Validation Loss: 0.010612702787537513\n",
      "Epoch [531/20000], Training Loss: 0.012526901006432516, Validation Loss: 0.012535947234589246\n",
      "Epoch [532/20000], Training Loss: 0.013492891764534371, Validation Loss: 0.011591785909800895\n",
      "Epoch [533/20000], Training Loss: 0.01455872436054051, Validation Loss: 0.032817105574383866\n",
      "Epoch [534/20000], Training Loss: 0.04080019046419433, Validation Loss: 0.04901793564465542\n",
      "Epoch [535/20000], Training Loss: 0.13434844962154915, Validation Loss: 0.06674601962379859\n",
      "Epoch [536/20000], Training Loss: 0.11483291403523513, Validation Loss: 0.02966643701650134\n",
      "Epoch [537/20000], Training Loss: 0.06319188752344676, Validation Loss: 0.05961822750163658\n",
      "Epoch [538/20000], Training Loss: 0.037912967814398665, Validation Loss: 0.0339804345282974\n",
      "Epoch [539/20000], Training Loss: 0.03354304391957287, Validation Loss: 0.02644547508186015\n",
      "Epoch [540/20000], Training Loss: 0.022113568149507046, Validation Loss: 0.016234295032415157\n",
      "Epoch [541/20000], Training Loss: 0.016402142588049173, Validation Loss: 0.024405768365597883\n",
      "Epoch [542/20000], Training Loss: 0.03249846857839397, Validation Loss: 0.03318705715868648\n",
      "Epoch [543/20000], Training Loss: 0.025680615001225045, Validation Loss: 0.01635611393991128\n",
      "Epoch [544/20000], Training Loss: 0.018500076875040707, Validation Loss: 0.014508804492600596\n",
      "Epoch [545/20000], Training Loss: 0.02079488694601293, Validation Loss: 0.01714980422851785\n",
      "Epoch [546/20000], Training Loss: 0.014809565851464868, Validation Loss: 0.019223271733021224\n",
      "Epoch [547/20000], Training Loss: 0.019936295159693276, Validation Loss: 0.012271196286152442\n",
      "Epoch [548/20000], Training Loss: 0.01572410968531455, Validation Loss: 0.012028950989671642\n",
      "Epoch [549/20000], Training Loss: 0.012453282589246686, Validation Loss: 0.010529313046426321\n",
      "Epoch [550/20000], Training Loss: 0.013549159397371113, Validation Loss: 0.012252877888219561\n",
      "Epoch [551/20000], Training Loss: 0.01251913229602256, Validation Loss: 0.009135566894212395\n",
      "Epoch [552/20000], Training Loss: 0.013014456294643293, Validation Loss: 0.009355601686291539\n",
      "Epoch [553/20000], Training Loss: 0.013778444645660264, Validation Loss: 0.008703539831564489\n",
      "Epoch [554/20000], Training Loss: 0.012543927466530087, Validation Loss: 0.00984780235876842\n",
      "Epoch [555/20000], Training Loss: 0.02434016132195081, Validation Loss: 0.012772736075916214\n",
      "Epoch [556/20000], Training Loss: 0.01840062055271119, Validation Loss: 0.007817478993131757\n",
      "Epoch [557/20000], Training Loss: 0.015297787858539127, Validation Loss: 0.012738030357149864\n",
      "Epoch [558/20000], Training Loss: 0.010878102404863707, Validation Loss: 0.01938324536605898\n",
      "Epoch [559/20000], Training Loss: 0.01743043535056391, Validation Loss: 0.025929142517890216\n",
      "Epoch [560/20000], Training Loss: 0.015630357932033285, Validation Loss: 0.009038499610139522\n",
      "Epoch [561/20000], Training Loss: 0.03185797866067982, Validation Loss: 0.016323965393048086\n",
      "Epoch [562/20000], Training Loss: 0.12227880467461157, Validation Loss: 0.06170660036141339\n",
      "Epoch [563/20000], Training Loss: 0.07885071256064943, Validation Loss: 0.0360536241568674\n",
      "Epoch [564/20000], Training Loss: 0.07919456410620894, Validation Loss: 0.0350110827322276\n",
      "Epoch [565/20000], Training Loss: 0.03507501498929092, Validation Loss: 0.03853812712809986\n",
      "Epoch [566/20000], Training Loss: 0.02851719908150179, Validation Loss: 0.03427307517607837\n",
      "Epoch [567/20000], Training Loss: 0.02248286728614143, Validation Loss: 0.028324736013976413\n",
      "Epoch [568/20000], Training Loss: 0.027288624684193304, Validation Loss: 0.04919130446351138\n",
      "Epoch [569/20000], Training Loss: 0.02076883357949555, Validation Loss: 0.013164447290579697\n",
      "Epoch [570/20000], Training Loss: 0.015410402789711952, Validation Loss: 0.01474295849514127\n",
      "Epoch [571/20000], Training Loss: 0.014811441414557132, Validation Loss: 0.013022955011140463\n",
      "Epoch [572/20000], Training Loss: 0.014281896283916597, Validation Loss: 0.022488325463608023\n",
      "Epoch [573/20000], Training Loss: 0.01669954709775214, Validation Loss: 0.00943630356308708\n",
      "Epoch [574/20000], Training Loss: 0.014747730680807893, Validation Loss: 0.010128457980215598\n",
      "Epoch [575/20000], Training Loss: 0.026236337784212083, Validation Loss: 0.014130478011430208\n",
      "Epoch [576/20000], Training Loss: 0.019057562558113465, Validation Loss: 0.009529107782196924\n",
      "Epoch [577/20000], Training Loss: 0.015385874671795006, Validation Loss: 0.026485694181094928\n",
      "Epoch [578/20000], Training Loss: 0.029533346241805702, Validation Loss: 0.046197545188034636\n",
      "Epoch [579/20000], Training Loss: 0.04894004956752594, Validation Loss: 0.06587692118518305\n",
      "Epoch [580/20000], Training Loss: 0.050909225695899556, Validation Loss: 0.03205602746026502\n",
      "Epoch [581/20000], Training Loss: 0.03764766107113766, Validation Loss: 0.027857416284949605\n",
      "Epoch [582/20000], Training Loss: 0.055341643830096085, Validation Loss: 0.02798492561569188\n",
      "Epoch [583/20000], Training Loss: 0.03152523231359997, Validation Loss: 0.02191742253426244\n",
      "Epoch [584/20000], Training Loss: 0.03343119252739208, Validation Loss: 0.040274859016828604\n",
      "Epoch [585/20000], Training Loss: 0.017570154641621878, Validation Loss: 0.017497566304846876\n",
      "Epoch [586/20000], Training Loss: 0.016333971738017032, Validation Loss: 0.01953504034916866\n",
      "Epoch [587/20000], Training Loss: 0.015365808487071522, Validation Loss: 0.012922691349608795\n",
      "Epoch [588/20000], Training Loss: 0.012458655866794288, Validation Loss: 0.012481436303977344\n",
      "Epoch [589/20000], Training Loss: 0.016466510981055244, Validation Loss: 0.009352668490677814\n",
      "Epoch [590/20000], Training Loss: 0.014212564646316293, Validation Loss: 0.01342738644314547\n",
      "Epoch [591/20000], Training Loss: 0.07661801744740972, Validation Loss: 0.07747694920565233\n",
      "Epoch [592/20000], Training Loss: 0.08331259416965102, Validation Loss: 0.059940885113614804\n",
      "Epoch [593/20000], Training Loss: 0.050096136484561224, Validation Loss: 0.040098682820006185\n",
      "Epoch [594/20000], Training Loss: 0.042394451930054596, Validation Loss: 0.030430333903939046\n",
      "Epoch [595/20000], Training Loss: 0.02912166528403759, Validation Loss: 0.021551156553226838\n",
      "Epoch [596/20000], Training Loss: 0.02113984658249787, Validation Loss: 0.014628337756895269\n",
      "Epoch [597/20000], Training Loss: 0.017880048835650086, Validation Loss: 0.012857382537396768\n",
      "Epoch [598/20000], Training Loss: 0.017539278271475008, Validation Loss: 0.025418440854207792\n",
      "Epoch [599/20000], Training Loss: 0.025593188691086004, Validation Loss: 0.010470387171914893\n",
      "Epoch [600/20000], Training Loss: 0.02360046999196389, Validation Loss: 0.012562655945446316\n",
      "Epoch [601/20000], Training Loss: 0.05407646245190075, Validation Loss: 0.08269604718140712\n",
      "Epoch [602/20000], Training Loss: 0.054215001865356625, Validation Loss: 0.014153963893103931\n",
      "Epoch [603/20000], Training Loss: 0.02142268171467419, Validation Loss: 0.012543787561232242\n",
      "Epoch [604/20000], Training Loss: 0.023834855204248533, Validation Loss: 0.06181950335581215\n",
      "Epoch [605/20000], Training Loss: 0.0307032943195996, Validation Loss: 0.03017174097522237\n",
      "Epoch [606/20000], Training Loss: 0.025917735561961308, Validation Loss: 0.013515413679546328\n",
      "Epoch [607/20000], Training Loss: 0.01799690001644194, Validation Loss: 0.024487377113856918\n",
      "Epoch [608/20000], Training Loss: 0.021801283558098867, Validation Loss: 0.015590185615644502\n",
      "Epoch [609/20000], Training Loss: 0.025058162606520846, Validation Loss: 0.020502980773987673\n",
      "Epoch [610/20000], Training Loss: 0.022913549760622636, Validation Loss: 0.01947730591459732\n",
      "Epoch [611/20000], Training Loss: 0.019986338306417956, Validation Loss: 0.01620975229883374\n",
      "Epoch [612/20000], Training Loss: 0.017734948125767654, Validation Loss: 0.019189630001408213\n",
      "Epoch [613/20000], Training Loss: 0.016987236956733147, Validation Loss: 0.022120157790277227\n",
      "Epoch [614/20000], Training Loss: 0.01400457784932639, Validation Loss: 0.008695653876705501\n",
      "Epoch [615/20000], Training Loss: 0.02146304320818412, Validation Loss: 0.01629438119724292\n",
      "Epoch [616/20000], Training Loss: 0.01205715591121199, Validation Loss: 0.01666859984264217\n",
      "Epoch [617/20000], Training Loss: 0.02804146526614204, Validation Loss: 0.016950130116438773\n",
      "Epoch [618/20000], Training Loss: 0.024492794747597406, Validation Loss: 0.023051355335474234\n",
      "Epoch [619/20000], Training Loss: 0.02783355991622167, Validation Loss: 0.015170679728579028\n",
      "Epoch [620/20000], Training Loss: 0.03737634829719484, Validation Loss: 0.02405097735521996\n",
      "Epoch [621/20000], Training Loss: 0.05401300273037383, Validation Loss: 0.06636728774032712\n",
      "Epoch [622/20000], Training Loss: 0.0279273344536445, Validation Loss: 0.03925500205069515\n",
      "Epoch [623/20000], Training Loss: 0.024562041158787906, Validation Loss: 0.02758799917462978\n",
      "Epoch [624/20000], Training Loss: 0.01761035987042955, Validation Loss: 0.018144539927875736\n",
      "Epoch [625/20000], Training Loss: 0.014719059042233442, Validation Loss: 0.010010284016255222\n",
      "Epoch [626/20000], Training Loss: 0.010912333548601185, Validation Loss: 0.012905204914595183\n",
      "Epoch [627/20000], Training Loss: 0.011503993625020874, Validation Loss: 0.00978617591218428\n",
      "Epoch [628/20000], Training Loss: 0.010079001899742122, Validation Loss: 0.013032711715106542\n",
      "Epoch [629/20000], Training Loss: 0.01112516964245255, Validation Loss: 0.010190513499549636\n",
      "Epoch [630/20000], Training Loss: 0.013019514153711498, Validation Loss: 0.028795702497192214\n",
      "Epoch [631/20000], Training Loss: 0.03131568625602605, Validation Loss: 0.008982622500184893\n",
      "Epoch [632/20000], Training Loss: 0.0284025643320222, Validation Loss: 0.015246115901021734\n",
      "Epoch [633/20000], Training Loss: 0.011928987705947034, Validation Loss: 0.01162272330787538\n",
      "Epoch [634/20000], Training Loss: 0.031907604791090956, Validation Loss: 0.06670941748341311\n",
      "Epoch [635/20000], Training Loss: 0.03684535206827734, Validation Loss: 0.037390474683708055\n",
      "Epoch [636/20000], Training Loss: 0.029428911495155523, Validation Loss: 0.01735929683329083\n",
      "Epoch [637/20000], Training Loss: 0.016268494389285997, Validation Loss: 0.018469147812702934\n",
      "Epoch [638/20000], Training Loss: 0.017870498954185417, Validation Loss: 0.00997279632871721\n",
      "Epoch [639/20000], Training Loss: 0.012613352371512778, Validation Loss: 0.012622747620139419\n",
      "Epoch [640/20000], Training Loss: 0.017739148145275458, Validation Loss: 0.008033228554412531\n",
      "Epoch [641/20000], Training Loss: 0.015563227103224822, Validation Loss: 0.022418273926111516\n",
      "Epoch [642/20000], Training Loss: 0.019557529162349447, Validation Loss: 0.017378669934769848\n",
      "Epoch [643/20000], Training Loss: 0.018634369131177664, Validation Loss: 0.016236080926344276\n",
      "Epoch [644/20000], Training Loss: 0.032813011252853484, Validation Loss: 0.014354924585723555\n",
      "Epoch [645/20000], Training Loss: 0.0727273316920868, Validation Loss: 0.1801696746760622\n",
      "Epoch [646/20000], Training Loss: 0.11071866650932602, Validation Loss: 0.05176548986138547\n",
      "Epoch [647/20000], Training Loss: 0.07176823820918798, Validation Loss: 0.050069498855698995\n",
      "Epoch [648/20000], Training Loss: 0.031465155812579075, Validation Loss: 0.02936659539723721\n",
      "Epoch [649/20000], Training Loss: 0.027708182876397456, Validation Loss: 0.026045316541156067\n",
      "Epoch [650/20000], Training Loss: 0.03188772260078362, Validation Loss: 0.02506192750543832\n",
      "Epoch [651/20000], Training Loss: 0.0323633737903687, Validation Loss: 0.01882034738738659\n",
      "Epoch [652/20000], Training Loss: 0.017109462101611177, Validation Loss: 0.012439622451309095\n",
      "Epoch [653/20000], Training Loss: 0.013596925684916121, Validation Loss: 0.01111558207655723\n",
      "Epoch [654/20000], Training Loss: 0.012796067699257816, Validation Loss: 0.0096126996169868\n",
      "Epoch [655/20000], Training Loss: 0.010973891438750018, Validation Loss: 0.01742461265492733\n",
      "Epoch [656/20000], Training Loss: 0.01337661366311035, Validation Loss: 0.02554320584182021\n",
      "Epoch [657/20000], Training Loss: 0.028419248248350674, Validation Loss: 0.018974140212511037\n",
      "Epoch [658/20000], Training Loss: 0.01935653278737196, Validation Loss: 0.014649229696439456\n",
      "Epoch [659/20000], Training Loss: 0.020921250938304832, Validation Loss: 0.043561445265822105\n",
      "Epoch [660/20000], Training Loss: 0.02610642943182029, Validation Loss: 0.014342475049668336\n",
      "Epoch [661/20000], Training Loss: 0.015868750600410358, Validation Loss: 0.014193087600445515\n",
      "Epoch [662/20000], Training Loss: 0.011420677800612924, Validation Loss: 0.017641105498594645\n",
      "Epoch [663/20000], Training Loss: 0.01274511156537171, Validation Loss: 0.011703466621722397\n",
      "Epoch [664/20000], Training Loss: 0.017796185680448877, Validation Loss: 0.021765234239865695\n",
      "Epoch [665/20000], Training Loss: 0.012400368648481421, Validation Loss: 0.01495417967733294\n",
      "Epoch [666/20000], Training Loss: 0.018279772010698383, Validation Loss: 0.04287193828220605\n",
      "Epoch [667/20000], Training Loss: 0.12301302854237813, Validation Loss: 0.07538255531596778\n",
      "Epoch [668/20000], Training Loss: 0.0907736536048885, Validation Loss: 0.0249010817438532\n",
      "Epoch [669/20000], Training Loss: 0.037582113374290724, Validation Loss: 0.04108556613504628\n",
      "Epoch [670/20000], Training Loss: 0.029064992609034692, Validation Loss: 0.030882321083344582\n",
      "Epoch [671/20000], Training Loss: 0.026612532957057868, Validation Loss: 0.020607691057410754\n",
      "Epoch [672/20000], Training Loss: 0.01636857502827687, Validation Loss: 0.016361077759618262\n",
      "Epoch [673/20000], Training Loss: 0.014156174413593752, Validation Loss: 0.01881395659245744\n",
      "Epoch [674/20000], Training Loss: 0.020677969408487634, Validation Loss: 0.013622923738575292\n",
      "Epoch [675/20000], Training Loss: 0.04625301167003012, Validation Loss: 0.047479493648126946\n",
      "Epoch [676/20000], Training Loss: 0.03649817017971405, Validation Loss: 0.03117746210563709\n",
      "Epoch [677/20000], Training Loss: 0.017917882218690857, Validation Loss: 0.015131011177700122\n",
      "Epoch [678/20000], Training Loss: 0.01485340912560267, Validation Loss: 0.03564324632641616\n",
      "Epoch [679/20000], Training Loss: 0.017242149682715535, Validation Loss: 0.010590598121340547\n",
      "Epoch [680/20000], Training Loss: 0.012389860739598848, Validation Loss: 0.009582246500548822\n",
      "Epoch [681/20000], Training Loss: 0.017051878850907087, Validation Loss: 0.05960847347521018\n",
      "Epoch [682/20000], Training Loss: 0.0341535847567554, Validation Loss: 0.02003033153598377\n",
      "Epoch [683/20000], Training Loss: 0.026085273736888275, Validation Loss: 0.03339920946384022\n",
      "Epoch [684/20000], Training Loss: 0.029627721607019857, Validation Loss: 0.0150682376138238\n",
      "Epoch [685/20000], Training Loss: 0.017241128969804516, Validation Loss: 0.012101197012974508\n",
      "Epoch [686/20000], Training Loss: 0.014160506493811096, Validation Loss: 0.014875694266568845\n",
      "Epoch [687/20000], Training Loss: 0.014763451451601992, Validation Loss: 0.015621086120114923\n",
      "Epoch [688/20000], Training Loss: 0.011802661116234958, Validation Loss: 0.008261169952551694\n",
      "Epoch [689/20000], Training Loss: 0.010967223572411708, Validation Loss: 0.009198042241105332\n",
      "Epoch [690/20000], Training Loss: 0.014075772737019829, Validation Loss: 0.010735049809965484\n",
      "Epoch [691/20000], Training Loss: 0.01594385152982016, Validation Loss: 0.008802728246886039\n",
      "Epoch [692/20000], Training Loss: 0.027199153123157366, Validation Loss: 0.013000504971353199\n",
      "Epoch [693/20000], Training Loss: 0.011282138206297532, Validation Loss: 0.011158992514792217\n",
      "Epoch [694/20000], Training Loss: 0.023303805185215815, Validation Loss: 0.012311857349273808\n",
      "Epoch [695/20000], Training Loss: 0.023976168571867414, Validation Loss: 0.021859330243185194\n",
      "Epoch [696/20000], Training Loss: 0.02070790544218783, Validation Loss: 0.035615434607935516\n",
      "Epoch [697/20000], Training Loss: 0.023781030388948108, Validation Loss: 0.023934399547700345\n",
      "Epoch [698/20000], Training Loss: 0.020637002295448577, Validation Loss: 0.02249473897136952\n",
      "Epoch [699/20000], Training Loss: 0.037775015009434094, Validation Loss: 0.018087300954660667\n",
      "Epoch [700/20000], Training Loss: 0.014530977365211584, Validation Loss: 0.012981324937720662\n",
      "Epoch [701/20000], Training Loss: 0.016527307191738925, Validation Loss: 0.01812269393732566\n",
      "Epoch [702/20000], Training Loss: 0.014683202391357295, Validation Loss: 0.013869979920204818\n",
      "Epoch [703/20000], Training Loss: 0.009679797413160227, Validation Loss: 0.01103593468810041\n",
      "Epoch [704/20000], Training Loss: 0.011908546705464167, Validation Loss: 0.007910143236808961\n",
      "Epoch [705/20000], Training Loss: 0.029429960580143546, Validation Loss: 0.04002571616490252\n",
      "Epoch [706/20000], Training Loss: 0.031441257713595405, Validation Loss: 0.02554118786705925\n",
      "Epoch [707/20000], Training Loss: 0.024360270837600444, Validation Loss: 0.02116705523979289\n",
      "Epoch [708/20000], Training Loss: 0.020715057350961224, Validation Loss: 0.017682565980507138\n",
      "Epoch [709/20000], Training Loss: 0.020215394873438135, Validation Loss: 0.020891459248896602\n",
      "Epoch [710/20000], Training Loss: 0.026567918680874363, Validation Loss: 0.013839327726553854\n",
      "Epoch [711/20000], Training Loss: 0.012636153056519106, Validation Loss: 0.012009664001194001\n",
      "Epoch [712/20000], Training Loss: 0.010255984895463501, Validation Loss: 0.008467787808320342\n",
      "Epoch [713/20000], Training Loss: 0.010983494003344927, Validation Loss: 0.008151480335963529\n",
      "Epoch [714/20000], Training Loss: 0.010023682993570609, Validation Loss: 0.006864746832477237\n",
      "Epoch [715/20000], Training Loss: 0.01379421588041753, Validation Loss: 0.0370317532968109\n",
      "Epoch [716/20000], Training Loss: 0.022247019685372443, Validation Loss: 0.006885479519354388\n",
      "Epoch [717/20000], Training Loss: 0.014111852514491017, Validation Loss: 0.010584140796261212\n",
      "Epoch [718/20000], Training Loss: 0.01473612975470522, Validation Loss: 0.012725249855132178\n",
      "Epoch [719/20000], Training Loss: 0.009673692577052861, Validation Loss: 0.0071264914225419845\n",
      "Epoch [720/20000], Training Loss: 0.01873919028626655, Validation Loss: 0.03336092836637889\n",
      "Epoch [721/20000], Training Loss: 0.015411066997330636, Validation Loss: 0.006739667202237771\n",
      "Epoch [722/20000], Training Loss: 0.01339572143271133, Validation Loss: 0.022164265126414767\n",
      "Epoch [723/20000], Training Loss: 0.015377063927839376, Validation Loss: 0.020072946587714913\n",
      "Epoch [724/20000], Training Loss: 0.010950522135577298, Validation Loss: 0.02646309998912203\n",
      "Epoch [725/20000], Training Loss: 0.021772888721898198, Validation Loss: 0.011262773093635112\n",
      "Epoch [726/20000], Training Loss: 0.033109312671253326, Validation Loss: 0.03253845818400764\n",
      "Epoch [727/20000], Training Loss: 0.021921178206217258, Validation Loss: 0.04504824879480339\n",
      "Epoch [728/20000], Training Loss: 0.02995036356982642, Validation Loss: 0.03999668938971925\n",
      "Epoch [729/20000], Training Loss: 0.0521470756087053, Validation Loss: 0.0879429904868941\n",
      "Epoch [730/20000], Training Loss: 0.054090598332030435, Validation Loss: 0.08563262219802995\n",
      "Epoch [731/20000], Training Loss: 0.032079856377094984, Validation Loss: 0.01620577210098557\n",
      "Epoch [732/20000], Training Loss: 0.03198101321218668, Validation Loss: 0.0346637893164244\n",
      "Epoch [733/20000], Training Loss: 0.09548930151088696, Validation Loss: 0.042771017949343536\n",
      "Epoch [734/20000], Training Loss: 0.034559841899733455, Validation Loss: 0.018965558549774404\n",
      "Epoch [735/20000], Training Loss: 0.018130951461249163, Validation Loss: 0.01618534277739226\n",
      "Epoch [736/20000], Training Loss: 0.01778291119262576, Validation Loss: 0.016496636900257333\n",
      "Epoch [737/20000], Training Loss: 0.01339646934398583, Validation Loss: 0.021271496401117123\n",
      "Epoch [738/20000], Training Loss: 0.022192113267789994, Validation Loss: 0.0174267398505052\n",
      "Epoch [739/20000], Training Loss: 0.021126102090680172, Validation Loss: 0.036908092964747445\n",
      "Epoch [740/20000], Training Loss: 0.028416794442039515, Validation Loss: 0.07651804755856757\n",
      "Epoch [741/20000], Training Loss: 0.07055775133111249, Validation Loss: 0.06170038316740704\n",
      "Epoch [742/20000], Training Loss: 0.04659601926895058, Validation Loss: 0.037887399455186044\n",
      "Epoch [743/20000], Training Loss: 0.03944269980170897, Validation Loss: 0.048297358710690776\n",
      "Epoch [744/20000], Training Loss: 0.03510370443940961, Validation Loss: 0.016516485402490195\n",
      "Epoch [745/20000], Training Loss: 0.019961330815151865, Validation Loss: 0.02262847951004109\n",
      "Epoch [746/20000], Training Loss: 0.022887676463661983, Validation Loss: 0.02275044083119892\n",
      "Epoch [747/20000], Training Loss: 0.018710243748500943, Validation Loss: 0.011945802972183438\n",
      "Epoch [748/20000], Training Loss: 0.014825395981980754, Validation Loss: 0.010416117828595696\n",
      "Epoch [749/20000], Training Loss: 0.017354117784561498, Validation Loss: 0.06296074135238437\n",
      "Epoch [750/20000], Training Loss: 0.05040344750575189, Validation Loss: 0.021523979546628696\n",
      "Epoch [751/20000], Training Loss: 0.031322369584813714, Validation Loss: 0.01954188677583169\n",
      "Epoch [752/20000], Training Loss: 0.026377221261749843, Validation Loss: 0.05208740614443817\n",
      "Epoch [753/20000], Training Loss: 0.04019712259261204, Validation Loss: 0.025867624536717187\n",
      "Epoch [754/20000], Training Loss: 0.019267592379557236, Validation Loss: 0.015486726216658846\n",
      "Epoch [755/20000], Training Loss: 0.01492790152717914, Validation Loss: 0.01426604448975662\n",
      "Epoch [756/20000], Training Loss: 0.01593818884742047, Validation Loss: 0.02082853075447405\n",
      "Epoch [757/20000], Training Loss: 0.01467668462178803, Validation Loss: 0.02095401908835606\n",
      "Epoch [758/20000], Training Loss: 0.02195211110769638, Validation Loss: 0.014435865062596148\n",
      "Epoch [759/20000], Training Loss: 0.01558827024668322, Validation Loss: 0.02419013765340277\n",
      "Epoch [760/20000], Training Loss: 0.01602123273603086, Validation Loss: 0.012246229555153452\n",
      "Epoch [761/20000], Training Loss: 0.013790594688284077, Validation Loss: 0.011396253167000393\n",
      "Epoch [762/20000], Training Loss: 0.012891728401882574, Validation Loss: 0.009111863793709584\n",
      "Epoch [763/20000], Training Loss: 0.012894360284657782, Validation Loss: 0.014339271748335328\n",
      "Epoch [764/20000], Training Loss: 0.020649487605234235, Validation Loss: 0.02611287435411703\n",
      "Epoch [765/20000], Training Loss: 0.026699244086298028, Validation Loss: 0.01648769389110422\n",
      "Epoch [766/20000], Training Loss: 0.059625962882169654, Validation Loss: 0.014290110828231414\n",
      "Epoch [767/20000], Training Loss: 0.034636100072280636, Validation Loss: 0.01509360139799997\n",
      "Epoch [768/20000], Training Loss: 0.025743704322459444, Validation Loss: 0.022184411647620465\n",
      "Epoch [769/20000], Training Loss: 0.021386540162244012, Validation Loss: 0.00966992957600404\n",
      "Epoch [770/20000], Training Loss: 0.014608133472003309, Validation Loss: 0.021912000664826892\n",
      "Epoch [771/20000], Training Loss: 0.018442772100895257, Validation Loss: 0.02133887487871326\n",
      "Epoch [772/20000], Training Loss: 0.012196303081249684, Validation Loss: 0.025195850728613967\n",
      "Epoch [773/20000], Training Loss: 0.018076512297349318, Validation Loss: 0.027556811035777982\n",
      "Epoch [774/20000], Training Loss: 0.028812988630046936, Validation Loss: 0.023939442092755375\n",
      "Epoch [775/20000], Training Loss: 0.0538148221728209, Validation Loss: 0.03936340220580048\n",
      "Epoch [776/20000], Training Loss: 0.024939362321414853, Validation Loss: 0.05062535873451934\n",
      "Epoch [777/20000], Training Loss: 0.029492453281168958, Validation Loss: 0.025240924464408756\n",
      "Epoch [778/20000], Training Loss: 0.015631522272347605, Validation Loss: 0.016157877339225177\n",
      "Epoch [779/20000], Training Loss: 0.016153262069565244, Validation Loss: 0.02279563727386538\n",
      "Epoch [780/20000], Training Loss: 0.026836549389242594, Validation Loss: 0.05208961376809328\n",
      "Epoch [781/20000], Training Loss: 0.0528196871081101, Validation Loss: 0.01817013296324357\n",
      "Epoch [782/20000], Training Loss: 0.04264091652294155, Validation Loss: 0.07307024313392652\n",
      "Epoch [783/20000], Training Loss: 0.060617222084796855, Validation Loss: 0.05607767510523801\n",
      "Epoch [784/20000], Training Loss: 0.06412129749410919, Validation Loss: 0.09374239985552059\n",
      "Epoch [785/20000], Training Loss: 0.03170104108617774, Validation Loss: 0.018837398846966925\n",
      "Epoch [786/20000], Training Loss: 0.020894572837278247, Validation Loss: 0.015685359372577607\n",
      "Epoch [787/20000], Training Loss: 0.015396681780527745, Validation Loss: 0.01228738388140584\n",
      "Epoch [788/20000], Training Loss: 0.0115121898457541, Validation Loss: 0.007136915415451569\n",
      "Epoch [789/20000], Training Loss: 0.013265304084468101, Validation Loss: 0.023485958883470142\n",
      "Epoch [790/20000], Training Loss: 0.015259580004827253, Validation Loss: 0.017889588663697425\n",
      "Epoch [791/20000], Training Loss: 0.0245332041938257, Validation Loss: 0.014486618549144034\n",
      "Epoch [792/20000], Training Loss: 0.022735934737803682, Validation Loss: 0.017550451841894896\n",
      "Epoch [793/20000], Training Loss: 0.015446612511628441, Validation Loss: 0.009476613586236724\n",
      "Epoch [794/20000], Training Loss: 0.011406004216821333, Validation Loss: 0.009242224549601328\n",
      "Epoch [795/20000], Training Loss: 0.013678555931879341, Validation Loss: 0.009058377506617667\n",
      "Epoch [796/20000], Training Loss: 0.010305000597976946, Validation Loss: 0.010430745695005603\n",
      "Epoch [797/20000], Training Loss: 0.008350081804175196, Validation Loss: 0.0073540673757424724\n",
      "Epoch [798/20000], Training Loss: 0.009981510636862367, Validation Loss: 0.008668372259088324\n",
      "Epoch [799/20000], Training Loss: 0.011234598371499618, Validation Loss: 0.012632033330586415\n",
      "Epoch [800/20000], Training Loss: 0.030679749507856156, Validation Loss: 0.04116879778783591\n",
      "Epoch [801/20000], Training Loss: 0.026049032166026467, Validation Loss: 0.00970892357391812\n",
      "Epoch [802/20000], Training Loss: 0.03716646706119978, Validation Loss: 0.03910329943026715\n",
      "Epoch [803/20000], Training Loss: 0.03391325693311436, Validation Loss: 0.03495435748885779\n",
      "Epoch [804/20000], Training Loss: 0.0603126556213413, Validation Loss: 0.08817882504198292\n",
      "Epoch [805/20000], Training Loss: 0.04422777970986707, Validation Loss: 0.024589706691928594\n",
      "Epoch [806/20000], Training Loss: 0.017859252810792117, Validation Loss: 0.016927380073504094\n",
      "Epoch [807/20000], Training Loss: 0.014388327453551548, Validation Loss: 0.014870599595754633\n",
      "Epoch [808/20000], Training Loss: 0.01299059995549864, Validation Loss: 0.011568979715904303\n",
      "Epoch [809/20000], Training Loss: 0.011950130607666714, Validation Loss: 0.01758840156157217\n",
      "Epoch [810/20000], Training Loss: 0.014753667417348229, Validation Loss: 0.011633459850320261\n",
      "Epoch [811/20000], Training Loss: 0.011872802012866097, Validation Loss: 0.020450612524660128\n",
      "Epoch [812/20000], Training Loss: 0.010527155744577093, Validation Loss: 0.022293312208363787\n",
      "Epoch [813/20000], Training Loss: 0.01351627431410764, Validation Loss: 0.013508430227098318\n",
      "Epoch [814/20000], Training Loss: 0.009750665485626087, Validation Loss: 0.02281272755219649\n",
      "Epoch [815/20000], Training Loss: 0.022421178757213056, Validation Loss: 0.01299780868761363\n",
      "Epoch [816/20000], Training Loss: 0.024707276777397574, Validation Loss: 0.012432376309303977\n",
      "Epoch [817/20000], Training Loss: 0.02904124208725989, Validation Loss: 0.04233032809623728\n",
      "Epoch [818/20000], Training Loss: 0.05194875777150238, Validation Loss: 0.06015659446040274\n",
      "Epoch [819/20000], Training Loss: 0.03245350192966206, Validation Loss: 0.03732884549732569\n",
      "Epoch [820/20000], Training Loss: 0.027355272389416183, Validation Loss: 0.032680440412024135\n",
      "Epoch [821/20000], Training Loss: 0.033008346716607254, Validation Loss: 0.043983766017714664\n",
      "Epoch [822/20000], Training Loss: 0.030998158526407287, Validation Loss: 0.08980331648556751\n",
      "Epoch [823/20000], Training Loss: 0.0674300781850304, Validation Loss: 0.027396783830724195\n",
      "Epoch [824/20000], Training Loss: 0.03706023286628936, Validation Loss: 0.0803527443353836\n",
      "Epoch [825/20000], Training Loss: 0.031170435752885948, Validation Loss: 0.016242374687680732\n",
      "Epoch [826/20000], Training Loss: 0.01573833653570286, Validation Loss: 0.0166637510673107\n",
      "Epoch [827/20000], Training Loss: 0.013302918656596116, Validation Loss: 0.011663399835088585\n",
      "Epoch [828/20000], Training Loss: 0.010758693034793916, Validation Loss: 0.005981346967344419\n",
      "Epoch [829/20000], Training Loss: 0.01040440657275862, Validation Loss: 0.010058524580852128\n",
      "Epoch [830/20000], Training Loss: 0.0248416464751894, Validation Loss: 0.017717007250149703\n",
      "Epoch [831/20000], Training Loss: 0.022704624742670734, Validation Loss: 0.015204364496653014\n",
      "Epoch [832/20000], Training Loss: 0.015614143522855426, Validation Loss: 0.009744234961491512\n",
      "Epoch [833/20000], Training Loss: 0.008676378330294807, Validation Loss: 0.014429437132683005\n",
      "Epoch [834/20000], Training Loss: 0.00895602728789007, Validation Loss: 0.011343380392287753\n",
      "Epoch [835/20000], Training Loss: 0.015777899434656968, Validation Loss: 0.011797020740980319\n",
      "Epoch [836/20000], Training Loss: 0.03681862639450628, Validation Loss: 0.036910660313682155\n",
      "Epoch [837/20000], Training Loss: 0.026315630762837827, Validation Loss: 0.025809395437825632\n",
      "Epoch [838/20000], Training Loss: 0.019024731042528793, Validation Loss: 0.04132694765019963\n",
      "Epoch [839/20000], Training Loss: 0.031456369445160295, Validation Loss: 0.0164878368063732\n",
      "Epoch [840/20000], Training Loss: 0.03253730555713576, Validation Loss: 0.013400571548738274\n",
      "Epoch [841/20000], Training Loss: 0.027165008642311608, Validation Loss: 0.015375452779731054\n",
      "Epoch [842/20000], Training Loss: 0.019718796394077538, Validation Loss: 0.022347693836254342\n",
      "Epoch [843/20000], Training Loss: 0.015518474143131502, Validation Loss: 0.012301997822509845\n",
      "Epoch [844/20000], Training Loss: 0.018690019309620505, Validation Loss: 0.012711131903541279\n",
      "Epoch [845/20000], Training Loss: 0.010261076102324296, Validation Loss: 0.010182548574221736\n",
      "Epoch [846/20000], Training Loss: 0.010477267137113293, Validation Loss: 0.01194229357929351\n",
      "Epoch [847/20000], Training Loss: 0.01215983099661701, Validation Loss: 0.017185081038643017\n",
      "Epoch [848/20000], Training Loss: 0.01292352785822004, Validation Loss: 0.020218701306585478\n",
      "Epoch [849/20000], Training Loss: 0.014680603926535696, Validation Loss: 0.014162822949872186\n",
      "Epoch [850/20000], Training Loss: 0.027064737797315632, Validation Loss: 0.06434134040520054\n",
      "Epoch [851/20000], Training Loss: 0.052348774381763566, Validation Loss: 0.08314639537841528\n",
      "Epoch [852/20000], Training Loss: 0.05796615755285269, Validation Loss: 0.03904133983435191\n",
      "Epoch [853/20000], Training Loss: 0.04204178914161665, Validation Loss: 0.0370003851902536\n",
      "Epoch [854/20000], Training Loss: 0.028040416605238403, Validation Loss: 0.028677927567598772\n",
      "Epoch [855/20000], Training Loss: 0.01714254230526941, Validation Loss: 0.013498655699118881\n",
      "Epoch [856/20000], Training Loss: 0.008205485613351422, Validation Loss: 0.004266973368122771\n",
      "Epoch [857/20000], Training Loss: 0.00927951729889693, Validation Loss: 0.006398826010118841\n",
      "Epoch [858/20000], Training Loss: 0.013551796489212262, Validation Loss: 0.010791585193868022\n",
      "Epoch [859/20000], Training Loss: 0.028382448108719212, Validation Loss: 0.17244400078943217\n",
      "Epoch [860/20000], Training Loss: 0.0613824436815256, Validation Loss: 0.07348840761341655\n",
      "Epoch [861/20000], Training Loss: 0.02771833058172238, Validation Loss: 0.01989518842605123\n",
      "Epoch [862/20000], Training Loss: 0.019438588359792317, Validation Loss: 0.012487121542822154\n",
      "Epoch [863/20000], Training Loss: 0.013375192198769323, Validation Loss: 0.019413689544719132\n",
      "Epoch [864/20000], Training Loss: 0.013272730404943494, Validation Loss: 0.00803524014636358\n",
      "Epoch [865/20000], Training Loss: 0.023808606235044345, Validation Loss: 0.011716765459758928\n",
      "Epoch [866/20000], Training Loss: 0.015759817745285027, Validation Loss: 0.027297518288017412\n",
      "Epoch [867/20000], Training Loss: 0.01501735344728721, Validation Loss: 0.017412618628462054\n",
      "Epoch [868/20000], Training Loss: 0.018555428225746646, Validation Loss: 0.010298198411402842\n",
      "Epoch [869/20000], Training Loss: 0.013123943953749924, Validation Loss: 0.008158202255833214\n",
      "Epoch [870/20000], Training Loss: 0.015611096451591169, Validation Loss: 0.00787675581905018\n",
      "Epoch [871/20000], Training Loss: 0.013563459485469918, Validation Loss: 0.010449678112002167\n",
      "Epoch [872/20000], Training Loss: 0.012530851227763509, Validation Loss: 0.008419983368438437\n",
      "Epoch [873/20000], Training Loss: 0.009468764956441842, Validation Loss: 0.008960277083321751\n",
      "Epoch [874/20000], Training Loss: 0.020059883448993787, Validation Loss: 0.014916715971106302\n",
      "Epoch [875/20000], Training Loss: 0.029673329887113402, Validation Loss: 0.010449196479502472\n",
      "Epoch [876/20000], Training Loss: 0.01836385316814163, Validation Loss: 0.010396179044506825\n",
      "Epoch [877/20000], Training Loss: 0.015772668308012987, Validation Loss: 0.013112997277190442\n",
      "Epoch [878/20000], Training Loss: 0.015346331981293457, Validation Loss: 0.0098121710512221\n",
      "Epoch [879/20000], Training Loss: 0.013636395216703281, Validation Loss: 0.00598944718345971\n",
      "Epoch [880/20000], Training Loss: 0.012300395897390055, Validation Loss: 0.0068276939481015765\n",
      "Epoch [881/20000], Training Loss: 0.008478951542721396, Validation Loss: 0.005775513269872858\n",
      "Epoch [882/20000], Training Loss: 0.021594077352866798, Validation Loss: 0.014017233118257906\n",
      "Epoch [883/20000], Training Loss: 0.02427412340434135, Validation Loss: 0.017288043253968093\n",
      "Epoch [884/20000], Training Loss: 0.030535371623825216, Validation Loss: 0.018702036828180217\n",
      "Epoch [885/20000], Training Loss: 0.022585087923549248, Validation Loss: 0.019288736721799645\n",
      "Epoch [886/20000], Training Loss: 0.014200757035203944, Validation Loss: 0.015756469786538916\n",
      "Epoch [887/20000], Training Loss: 0.019226507134070352, Validation Loss: 0.013534648295129417\n",
      "Epoch [888/20000], Training Loss: 0.034361197746225765, Validation Loss: 0.1530473780862532\n",
      "Epoch [889/20000], Training Loss: 0.09181921290499824, Validation Loss: 0.066073109834906\n",
      "Epoch [890/20000], Training Loss: 0.039645011037854214, Validation Loss: 0.03853307787001042\n",
      "Epoch [891/20000], Training Loss: 0.03668601435076978, Validation Loss: 0.017553102636516993\n",
      "Epoch [892/20000], Training Loss: 0.024058179821752543, Validation Loss: 0.015142834496612507\n",
      "Epoch [893/20000], Training Loss: 0.017454524391463826, Validation Loss: 0.010910024794610455\n",
      "Epoch [894/20000], Training Loss: 0.012478018329212708, Validation Loss: 0.011489605545595657\n",
      "Epoch [895/20000], Training Loss: 0.01083204896921026, Validation Loss: 0.009024206745735827\n",
      "Epoch [896/20000], Training Loss: 0.009729178979926343, Validation Loss: 0.008045796260538642\n",
      "Epoch [897/20000], Training Loss: 0.009523403639572539, Validation Loss: 0.011784786577830355\n",
      "Epoch [898/20000], Training Loss: 0.009309354236133263, Validation Loss: 0.008781182587023066\n",
      "Epoch [899/20000], Training Loss: 0.012619686895050108, Validation Loss: 0.010391029343184326\n",
      "Epoch [900/20000], Training Loss: 0.016247544924096604, Validation Loss: 0.04120976891524429\n",
      "Epoch [901/20000], Training Loss: 0.04388887117758194, Validation Loss: 0.008076291561522548\n",
      "Epoch [902/20000], Training Loss: 0.024769530569236458, Validation Loss: 0.012881063848538066\n",
      "Epoch [903/20000], Training Loss: 0.01602716828762953, Validation Loss: 0.009114404121832185\n",
      "Epoch [904/20000], Training Loss: 0.03129209737692561, Validation Loss: 0.03825492904986412\n",
      "Epoch [905/20000], Training Loss: 0.03468965639227203, Validation Loss: 0.07553068269470305\n",
      "Epoch [906/20000], Training Loss: 0.038678524566681256, Validation Loss: 0.016835579034408472\n",
      "Epoch [907/20000], Training Loss: 0.01728430373727211, Validation Loss: 0.017841134632552925\n",
      "Epoch [908/20000], Training Loss: 0.010471833799134142, Validation Loss: 0.031218135408061207\n",
      "Epoch [909/20000], Training Loss: 0.044908743053643514, Validation Loss: 0.008785276559903025\n",
      "Epoch [910/20000], Training Loss: 0.03587045236158052, Validation Loss: 0.04046835406045844\n",
      "Epoch [911/20000], Training Loss: 0.027774861523149803, Validation Loss: 0.012503594642235619\n",
      "Epoch [912/20000], Training Loss: 0.026019844359585216, Validation Loss: 0.017347602187823153\n",
      "Epoch [913/20000], Training Loss: 0.012984178072656505, Validation Loss: 0.025482695839092885\n",
      "Epoch [914/20000], Training Loss: 0.01698511148736413, Validation Loss: 0.009503916658237849\n",
      "Epoch [915/20000], Training Loss: 0.010350804908999376, Validation Loss: 0.008895056521841738\n",
      "Epoch [916/20000], Training Loss: 0.010401336351476078, Validation Loss: 0.011492588537373172\n",
      "Epoch [917/20000], Training Loss: 0.010149524214544467, Validation Loss: 0.019112725695693222\n",
      "Epoch [918/20000], Training Loss: 0.010594584750443963, Validation Loss: 0.00766838715235693\n",
      "Epoch [919/20000], Training Loss: 0.010878998703057212, Validation Loss: 0.014428920874903372\n",
      "Epoch [920/20000], Training Loss: 0.010472753384549702, Validation Loss: 0.010395632879735292\n",
      "Epoch [921/20000], Training Loss: 0.007765669596795176, Validation Loss: 0.005359604286780659\n",
      "Epoch [922/20000], Training Loss: 0.010074406709319663, Validation Loss: 0.04477586538913046\n",
      "Epoch [923/20000], Training Loss: 0.029474326496970855, Validation Loss: 0.020244550186754192\n",
      "Epoch [924/20000], Training Loss: 0.023311058335821144, Validation Loss: 0.011072710621828618\n",
      "Epoch [925/20000], Training Loss: 0.00889376931757267, Validation Loss: 0.0162569246996069\n",
      "Epoch [926/20000], Training Loss: 0.015142664151166432, Validation Loss: 0.006801794198074962\n",
      "Epoch [927/20000], Training Loss: 0.01319627185668131, Validation Loss: 0.03099778822927585\n",
      "Epoch [928/20000], Training Loss: 0.03567230808714937, Validation Loss: 0.01516262401951562\n",
      "Epoch [929/20000], Training Loss: 0.026678433770679737, Validation Loss: 0.0317956984639771\n",
      "Epoch [930/20000], Training Loss: 0.041991666092404296, Validation Loss: 0.015785554951264883\n",
      "Epoch [931/20000], Training Loss: 0.026171564877066494, Validation Loss: 0.01712354046874347\n",
      "Epoch [932/20000], Training Loss: 0.02114984323270619, Validation Loss: 0.009145763950041358\n",
      "Epoch [933/20000], Training Loss: 0.013173962510856134, Validation Loss: 0.011726663450289651\n",
      "Epoch [934/20000], Training Loss: 0.014306987020453172, Validation Loss: 0.008353487067217556\n",
      "Epoch [935/20000], Training Loss: 0.010707281039295984, Validation Loss: 0.007201588052974048\n",
      "Epoch [936/20000], Training Loss: 0.00909698218155037, Validation Loss: 0.02215949117412546\n",
      "Epoch [937/20000], Training Loss: 0.02332064080733939, Validation Loss: 0.018292529073420903\n",
      "Epoch [938/20000], Training Loss: 0.03276247182345417, Validation Loss: 0.02061645332894187\n",
      "Epoch [939/20000], Training Loss: 0.027176031194228147, Validation Loss: 0.038002582034769715\n",
      "Epoch [940/20000], Training Loss: 0.03413443458599171, Validation Loss: 0.01924204805453566\n",
      "Epoch [941/20000], Training Loss: 0.01555839688184538, Validation Loss: 0.012313895453928997\n",
      "Epoch [942/20000], Training Loss: 0.01427108004281763, Validation Loss: 0.01701623200194574\n",
      "Epoch [943/20000], Training Loss: 0.012178284281357523, Validation Loss: 0.013116364088440071\n",
      "Epoch [944/20000], Training Loss: 0.012728674290493862, Validation Loss: 0.03624422082711408\n",
      "Epoch [945/20000], Training Loss: 0.022553333862950758, Validation Loss: 0.011027078249244706\n",
      "Epoch [946/20000], Training Loss: 0.015713842735359713, Validation Loss: 0.010986087652483573\n",
      "Epoch [947/20000], Training Loss: 0.023215824767248705, Validation Loss: 0.028690335238517887\n",
      "Epoch [948/20000], Training Loss: 0.030234894433566036, Validation Loss: 0.04694174062417268\n",
      "Epoch [949/20000], Training Loss: 0.028825941051763948, Validation Loss: 0.016977409798763658\n",
      "Epoch [950/20000], Training Loss: 0.04376919073651412, Validation Loss: 0.026949792646609048\n",
      "Epoch [951/20000], Training Loss: 0.038412510483924835, Validation Loss: 0.04806222412480548\n",
      "Epoch [952/20000], Training Loss: 0.03416966374165246, Validation Loss: 0.015380881442818142\n",
      "Epoch [953/20000], Training Loss: 0.015317437347091203, Validation Loss: 0.017099519171573308\n",
      "Epoch [954/20000], Training Loss: 0.01226965048616486, Validation Loss: 0.016225834283345574\n",
      "Epoch [955/20000], Training Loss: 0.011220542738945889, Validation Loss: 0.009739055649804606\n",
      "Epoch [956/20000], Training Loss: 0.00963364154033895, Validation Loss: 0.00787635319502736\n",
      "Epoch [957/20000], Training Loss: 0.011252697864879988, Validation Loss: 0.00798150205882434\n",
      "Epoch [958/20000], Training Loss: 0.009745354614486652, Validation Loss: 0.011862589756720705\n",
      "Epoch [959/20000], Training Loss: 0.010389837236808879, Validation Loss: 0.011737247248982757\n",
      "Epoch [960/20000], Training Loss: 0.012189103922407542, Validation Loss: 0.010608989612766517\n",
      "Epoch [961/20000], Training Loss: 0.008903118831637715, Validation Loss: 0.00929925153396574\n",
      "Epoch [962/20000], Training Loss: 0.009770823307917453, Validation Loss: 0.023640826261701896\n",
      "Epoch [963/20000], Training Loss: 0.025622855098585466, Validation Loss: 0.006641769641488818\n",
      "Epoch [964/20000], Training Loss: 0.01834125943631599, Validation Loss: 0.029304371647572665\n",
      "Epoch [965/20000], Training Loss: 0.027933915524045005, Validation Loss: 0.00997172333730074\n",
      "Epoch [966/20000], Training Loss: 0.02643376582169107, Validation Loss: 0.042138655239413014\n",
      "Epoch [967/20000], Training Loss: 0.05399109806291692, Validation Loss: 0.023103678642432228\n",
      "Epoch [968/20000], Training Loss: 0.07382931450514921, Validation Loss: 0.06398382381762242\n",
      "Epoch [969/20000], Training Loss: 0.052170616688921916, Validation Loss: 0.022696845008943003\n",
      "Epoch [970/20000], Training Loss: 0.036839400206060545, Validation Loss: 0.017872411469465405\n",
      "Epoch [971/20000], Training Loss: 0.0234442761367453, Validation Loss: 0.014494002931885705\n",
      "Epoch [972/20000], Training Loss: 0.015051221425112869, Validation Loss: 0.016809621479078145\n",
      "Epoch [973/20000], Training Loss: 0.015102257157975276, Validation Loss: 0.015701219464282053\n",
      "Epoch [974/20000], Training Loss: 0.01203082303982228, Validation Loss: 0.009504698845070988\n",
      "Epoch [975/20000], Training Loss: 0.008777236112759315, Validation Loss: 0.005286570699348306\n",
      "Epoch [976/20000], Training Loss: 0.014914700193912722, Validation Loss: 0.013613609688453985\n",
      "Epoch [977/20000], Training Loss: 0.023556587949964784, Validation Loss: 0.022252213957515667\n",
      "Epoch [978/20000], Training Loss: 0.01970408612396568, Validation Loss: 0.04619564871994334\n",
      "Epoch [979/20000], Training Loss: 0.05295214768765228, Validation Loss: 0.0324512309137563\n",
      "Epoch [980/20000], Training Loss: 0.05190234066269893, Validation Loss: 0.015141063903488896\n",
      "Epoch [981/20000], Training Loss: 0.042794525157660246, Validation Loss: 0.03914782008154715\n",
      "Epoch [982/20000], Training Loss: 0.027084650471806526, Validation Loss: 0.018869762829526165\n",
      "Epoch [983/20000], Training Loss: 0.016691708072487797, Validation Loss: 0.012862519573915229\n",
      "Epoch [984/20000], Training Loss: 0.013665946682782046, Validation Loss: 0.009681523911038381\n",
      "Epoch [985/20000], Training Loss: 0.01594536626778011, Validation Loss: 0.008618800935113552\n",
      "Epoch [986/20000], Training Loss: 0.010234885312489237, Validation Loss: 0.011115480815880025\n",
      "Epoch [987/20000], Training Loss: 0.009807944863236375, Validation Loss: 0.007244457165811519\n",
      "Epoch [988/20000], Training Loss: 0.008237005307039778, Validation Loss: 0.021071939813037427\n",
      "Epoch [989/20000], Training Loss: 0.019852907654629753, Validation Loss: 0.0241545467315816\n",
      "Epoch [990/20000], Training Loss: 0.03164685173084893, Validation Loss: 0.09004202913231636\n",
      "Epoch [991/20000], Training Loss: 0.04719138361646661, Validation Loss: 0.022709513441569764\n",
      "Epoch [992/20000], Training Loss: 0.03583433016735528, Validation Loss: 0.017716447073340538\n",
      "Epoch [993/20000], Training Loss: 0.024156478699296713, Validation Loss: 0.0389757559762625\n",
      "Epoch [994/20000], Training Loss: 0.018824282559632723, Validation Loss: 0.012297958124049444\n",
      "Epoch [995/20000], Training Loss: 0.025181180543898205, Validation Loss: 0.03575421500532545\n",
      "Epoch [996/20000], Training Loss: 0.03366674848283375, Validation Loss: 0.11088966334610376\n",
      "Epoch [997/20000], Training Loss: 0.040400163403579166, Validation Loss: 0.01834514192341132\n",
      "Epoch [998/20000], Training Loss: 0.014726909858706807, Validation Loss: 0.014213164315930064\n",
      "Epoch [999/20000], Training Loss: 0.013251006320518042, Validation Loss: 0.015415463015012907\n",
      "Epoch [1000/20000], Training Loss: 0.00914056400714409, Validation Loss: 0.009072757319527293\n",
      "Epoch [1001/20000], Training Loss: 0.010820363290674453, Validation Loss: 0.014946314327463016\n",
      "Epoch [1002/20000], Training Loss: 0.021686857957060317, Validation Loss: 0.008942148009613463\n",
      "Epoch [1003/20000], Training Loss: 0.01543301042901086, Validation Loss: 0.01107979266022484\n",
      "Epoch [1004/20000], Training Loss: 0.02098867900869144, Validation Loss: 0.03027184684920779\n",
      "Epoch [1005/20000], Training Loss: 0.03175363147498241, Validation Loss: 0.01593872981664668\n",
      "Epoch [1006/20000], Training Loss: 0.022626327026435838, Validation Loss: 0.020793885369884805\n",
      "Epoch [1007/20000], Training Loss: 0.02872418945688488, Validation Loss: 0.03867191120386162\n",
      "Epoch [1008/20000], Training Loss: 0.01951897261980256, Validation Loss: 0.00848317178860362\n",
      "Epoch [1009/20000], Training Loss: 0.015168703976087272, Validation Loss: 0.007364878028893124\n",
      "Epoch [1010/20000], Training Loss: 0.013801501290540077, Validation Loss: 0.02386450058199553\n",
      "Epoch [1011/20000], Training Loss: 0.018691280829703567, Validation Loss: 0.10898423851944591\n",
      "Epoch [1012/20000], Training Loss: 0.038715174521452615, Validation Loss: 0.03935965735193621\n",
      "Epoch [1013/20000], Training Loss: 0.02590059395463738, Validation Loss: 0.016637313963302636\n",
      "Epoch [1014/20000], Training Loss: 0.01951050477301968, Validation Loss: 0.014519143040975546\n",
      "Epoch [1015/20000], Training Loss: 0.01536328622439344, Validation Loss: 0.01181634543865977\n",
      "Epoch [1016/20000], Training Loss: 0.017550690003970106, Validation Loss: 0.010364415345278386\n",
      "Epoch [1017/20000], Training Loss: 0.012776151381591003, Validation Loss: 0.01306501148894224\n",
      "Epoch [1018/20000], Training Loss: 0.011690448774191151, Validation Loss: 0.005854133930797727\n",
      "Epoch [1019/20000], Training Loss: 0.0070177905602447155, Validation Loss: 0.00772487477732339\n",
      "Epoch [1020/20000], Training Loss: 0.008834821945388935, Validation Loss: 0.006781468122930032\n",
      "Epoch [1021/20000], Training Loss: 0.011865837166884117, Validation Loss: 0.0059704059932357435\n",
      "Epoch [1022/20000], Training Loss: 0.008824798733777632, Validation Loss: 0.0056796010632452635\n",
      "Epoch [1023/20000], Training Loss: 0.04930321834815134, Validation Loss: 0.013867219448758586\n",
      "Epoch [1024/20000], Training Loss: 0.030608512548496947, Validation Loss: 0.01416669349763115\n",
      "Epoch [1025/20000], Training Loss: 0.032404574482435625, Validation Loss: 0.013210312648488157\n",
      "Epoch [1026/20000], Training Loss: 0.01220009648906333, Validation Loss: 0.009357355668570004\n",
      "Epoch [1027/20000], Training Loss: 0.01113762816281191, Validation Loss: 0.009096180273539867\n",
      "Epoch [1028/20000], Training Loss: 0.012931883879770924, Validation Loss: 0.015853648925239685\n",
      "Epoch [1029/20000], Training Loss: 0.00973974579080407, Validation Loss: 0.0086443829879771\n",
      "Epoch [1030/20000], Training Loss: 0.017545481692650355, Validation Loss: 0.007995292193090633\n",
      "Epoch [1031/20000], Training Loss: 0.009924216490097544, Validation Loss: 0.013615445168947657\n",
      "Epoch [1032/20000], Training Loss: 0.02681988349234286, Validation Loss: 0.016099266702877854\n",
      "Epoch [1033/20000], Training Loss: 0.03408528342177825, Validation Loss: 0.007006778972394696\n",
      "Epoch [1034/20000], Training Loss: 0.012456312842134918, Validation Loss: 0.01468166901120547\n",
      "Epoch [1035/20000], Training Loss: 0.019060554248946055, Validation Loss: 0.014205385172411643\n",
      "Epoch [1036/20000], Training Loss: 0.054057957887250395, Validation Loss: 0.04789608134396304\n",
      "Epoch [1037/20000], Training Loss: 0.015150383612698144, Validation Loss: 0.013473790393731798\n",
      "Epoch [1038/20000], Training Loss: 0.011694406283952827, Validation Loss: 0.024448401703846896\n",
      "Epoch [1039/20000], Training Loss: 0.02627989137960997, Validation Loss: 0.02043669581986682\n",
      "Epoch [1040/20000], Training Loss: 0.021139665818606903, Validation Loss: 0.012420735908014043\n",
      "Epoch [1041/20000], Training Loss: 0.010536016457032409, Validation Loss: 0.011818083908849118\n",
      "Epoch [1042/20000], Training Loss: 0.011095357576518186, Validation Loss: 0.007110702463994209\n",
      "Epoch [1043/20000], Training Loss: 0.008544897889286014, Validation Loss: 0.007249962094529501\n",
      "Epoch [1044/20000], Training Loss: 0.012969052784943156, Validation Loss: 0.006453033431505912\n",
      "Epoch [1045/20000], Training Loss: 0.007945826125381115, Validation Loss: 0.013329549509101452\n",
      "Epoch [1046/20000], Training Loss: 0.050223330684405355, Validation Loss: 0.07476110898187278\n",
      "Epoch [1047/20000], Training Loss: 0.07266947015263472, Validation Loss: 0.027550707564870637\n",
      "Epoch [1048/20000], Training Loss: 0.032081296401364465, Validation Loss: 0.04612977818055227\n",
      "Epoch [1049/20000], Training Loss: 0.026997255369289114, Validation Loss: 0.02204919162765352\n",
      "Epoch [1050/20000], Training Loss: 0.01844437506848148, Validation Loss: 0.014576519253961953\n",
      "Epoch [1051/20000], Training Loss: 0.015719919565266798, Validation Loss: 0.009546755041329886\n",
      "Epoch [1052/20000], Training Loss: 0.01325416227336973, Validation Loss: 0.012876596627942152\n",
      "Epoch [1053/20000], Training Loss: 0.01286211984032499, Validation Loss: 0.008349615758597334\n",
      "Epoch [1054/20000], Training Loss: 0.02090304554440081, Validation Loss: 0.013784322428291645\n",
      "Epoch [1055/20000], Training Loss: 0.01759158365894109, Validation Loss: 0.01917419423943264\n",
      "Epoch [1056/20000], Training Loss: 0.027699893756237413, Validation Loss: 0.013792554240914832\n",
      "Epoch [1057/20000], Training Loss: 0.009047436236869544, Validation Loss: 0.050005854129522236\n",
      "Epoch [1058/20000], Training Loss: 0.03060160035550195, Validation Loss: 0.008533156961295101\n",
      "Epoch [1059/20000], Training Loss: 0.03853091309844915, Validation Loss: 0.016464103905251583\n",
      "Epoch [1060/20000], Training Loss: 0.04175888661744206, Validation Loss: 0.014703755810334144\n",
      "Epoch [1061/20000], Training Loss: 0.025319998268969357, Validation Loss: 0.020554584132842835\n",
      "Epoch [1062/20000], Training Loss: 0.0215203866828233, Validation Loss: 0.011653532649152582\n",
      "Epoch [1063/20000], Training Loss: 0.014456613413390837, Validation Loss: 0.014964772887997057\n",
      "Epoch [1064/20000], Training Loss: 0.013618743329841112, Validation Loss: 0.008691405740903437\n",
      "Epoch [1065/20000], Training Loss: 0.009820778945660484, Validation Loss: 0.016595186093210524\n",
      "Epoch [1066/20000], Training Loss: 0.009711869121799412, Validation Loss: 0.0068924492807496416\n",
      "Epoch [1067/20000], Training Loss: 0.010655572668058864, Validation Loss: 0.0058834795145640325\n",
      "Epoch [1068/20000], Training Loss: 0.007104188311911587, Validation Loss: 0.005026829555221181\n",
      "Epoch [1069/20000], Training Loss: 0.008585453711150746, Validation Loss: 0.010169534103470379\n",
      "Epoch [1070/20000], Training Loss: 0.018476444657218444, Validation Loss: 0.015785150450173955\n",
      "Epoch [1071/20000], Training Loss: 0.015633299374550397, Validation Loss: 0.01979339460855337\n",
      "Epoch [1072/20000], Training Loss: 0.019204976204491686, Validation Loss: 0.022662284472541244\n",
      "Epoch [1073/20000], Training Loss: 0.023696311638429637, Validation Loss: 0.056930944820099874\n",
      "Epoch [1074/20000], Training Loss: 0.04351205080872335, Validation Loss: 0.02326195507099219\n",
      "Epoch [1075/20000], Training Loss: 0.03230580408126116, Validation Loss: 0.028044815075876226\n",
      "Epoch [1076/20000], Training Loss: 0.02611683267501316, Validation Loss: 0.015239747698488293\n",
      "Epoch [1077/20000], Training Loss: 0.02176993301899139, Validation Loss: 0.015518648802184858\n",
      "Epoch [1078/20000], Training Loss: 0.012346123307777037, Validation Loss: 0.008433782932510975\n",
      "Epoch [1079/20000], Training Loss: 0.011174801167986257, Validation Loss: 0.009249413853841735\n",
      "Epoch [1080/20000], Training Loss: 0.010100547390590821, Validation Loss: 0.019278527508636893\n",
      "Epoch [1081/20000], Training Loss: 0.01614506495648779, Validation Loss: 0.08007855784819869\n",
      "Epoch [1082/20000], Training Loss: 0.02838265964029623, Validation Loss: 0.01749982644136715\n",
      "Epoch [1083/20000], Training Loss: 0.01571357231088249, Validation Loss: 0.010947900957782438\n",
      "Epoch [1084/20000], Training Loss: 0.010716965584184177, Validation Loss: 0.013400156282868555\n",
      "Epoch [1085/20000], Training Loss: 0.015602897698824694, Validation Loss: 0.008455358157870702\n",
      "Epoch [1086/20000], Training Loss: 0.04172086278621074, Validation Loss: 0.031075923085317415\n",
      "Epoch [1087/20000], Training Loss: 0.018895983733403097, Validation Loss: 0.01852666421192627\n",
      "Epoch [1088/20000], Training Loss: 0.00882566490742777, Validation Loss: 0.014928469397047514\n",
      "Epoch [1089/20000], Training Loss: 0.010427557530679874, Validation Loss: 0.007285391652420718\n",
      "Epoch [1090/20000], Training Loss: 0.014652508699717666, Validation Loss: 0.04276819550075288\n",
      "Epoch [1091/20000], Training Loss: 0.07642391910277573, Validation Loss: 0.1194513108755461\n",
      "Epoch [1092/20000], Training Loss: 0.07376810903328337, Validation Loss: 0.017790846948378753\n",
      "Epoch [1093/20000], Training Loss: 0.03800508399893131, Validation Loss: 0.0359019297791614\n",
      "Epoch [1094/20000], Training Loss: 0.025781715588111962, Validation Loss: 0.017526045658207052\n",
      "Epoch [1095/20000], Training Loss: 0.016417974473110268, Validation Loss: 0.013850541919979682\n",
      "Epoch [1096/20000], Training Loss: 0.012774405003126179, Validation Loss: 0.00932474555092389\n",
      "Epoch [1097/20000], Training Loss: 0.013779826828145556, Validation Loss: 0.012672019360781472\n",
      "Epoch [1098/20000], Training Loss: 0.011441080215653139, Validation Loss: 0.015581277710890573\n",
      "Epoch [1099/20000], Training Loss: 0.015487354947254062, Validation Loss: 0.03394535710326622\n",
      "Epoch [1100/20000], Training Loss: 0.017027138137824034, Validation Loss: 0.01641985638515951\n",
      "Epoch [1101/20000], Training Loss: 0.013976631731826014, Validation Loss: 0.015305628642327334\n",
      "Epoch [1102/20000], Training Loss: 0.017484821304346303, Validation Loss: 0.0210967543429234\n",
      "Epoch [1103/20000], Training Loss: 0.03871947825869678, Validation Loss: 0.080735597767405\n",
      "Epoch [1104/20000], Training Loss: 0.12847872290564868, Validation Loss: 0.14413615059535442\n",
      "Epoch [1105/20000], Training Loss: 0.05159634215358112, Validation Loss: 0.026182593128088176\n",
      "Epoch [1106/20000], Training Loss: 0.034532034170946906, Validation Loss: 0.01871368676543033\n",
      "Epoch [1107/20000], Training Loss: 0.019996979997293756, Validation Loss: 0.010897528219804739\n",
      "Epoch [1108/20000], Training Loss: 0.013361774651067597, Validation Loss: 0.011746209743780156\n",
      "Epoch [1109/20000], Training Loss: 0.007932692225689866, Validation Loss: 0.015228792577304062\n",
      "Epoch [1110/20000], Training Loss: 0.021171847380173858, Validation Loss: 0.005940327612176064\n",
      "Epoch [1111/20000], Training Loss: 0.03479954753337162, Validation Loss: 0.018273691065781\n",
      "Epoch [1112/20000], Training Loss: 0.0223499878110098, Validation Loss: 0.013123915706036932\n",
      "Epoch [1113/20000], Training Loss: 0.016699584633378044, Validation Loss: 0.005362016938892981\n",
      "Epoch [1114/20000], Training Loss: 0.011404069888937687, Validation Loss: 0.007777550614696808\n",
      "Epoch [1115/20000], Training Loss: 0.0159223379534004, Validation Loss: 0.022637892628586873\n",
      "Epoch [1116/20000], Training Loss: 0.05625743147850569, Validation Loss: 0.032807867482889934\n",
      "Epoch [1117/20000], Training Loss: 0.042454176083473225, Validation Loss: 0.03579720347487431\n",
      "Epoch [1118/20000], Training Loss: 0.031648618135867376, Validation Loss: 0.04477381827313755\n",
      "Epoch [1119/20000], Training Loss: 0.02895549505150744, Validation Loss: 0.018865838619894947\n",
      "Epoch [1120/20000], Training Loss: 0.019499275941468244, Validation Loss: 0.012801894015755839\n",
      "Epoch [1121/20000], Training Loss: 0.015689752297475934, Validation Loss: 0.012691979395338145\n",
      "Epoch [1122/20000], Training Loss: 0.012881412004519786, Validation Loss: 0.009275733003292588\n",
      "Epoch [1123/20000], Training Loss: 0.009735990398829537, Validation Loss: 0.007462646483335135\n",
      "Epoch [1124/20000], Training Loss: 0.01529349952137896, Validation Loss: 0.02005768684047136\n",
      "Epoch [1125/20000], Training Loss: 0.011508166966710373, Validation Loss: 0.02065098068877358\n",
      "Epoch [1126/20000], Training Loss: 0.008440433265475025, Validation Loss: 0.018414706592726482\n",
      "Epoch [1127/20000], Training Loss: 0.022775112532794344, Validation Loss: 0.011466539812867608\n",
      "Epoch [1128/20000], Training Loss: 0.016005162370025312, Validation Loss: 0.004745304363583843\n",
      "Epoch [1129/20000], Training Loss: 0.01168299098675821, Validation Loss: 0.02570516127541738\n",
      "Epoch [1130/20000], Training Loss: 0.01918897719796015, Validation Loss: 0.012216374977145006\n",
      "Epoch [1131/20000], Training Loss: 0.012280335224724175, Validation Loss: 0.013335498394902532\n",
      "Epoch [1132/20000], Training Loss: 0.017692084503193786, Validation Loss: 0.012968952368403532\n",
      "Epoch [1133/20000], Training Loss: 0.019632804389402736, Validation Loss: 0.020390746743408603\n",
      "Epoch [1134/20000], Training Loss: 0.02798562225819166, Validation Loss: 0.010721803092790794\n",
      "Epoch [1135/20000], Training Loss: 0.0146328314814517, Validation Loss: 0.01154527404275032\n",
      "Epoch [1136/20000], Training Loss: 0.012632760847087152, Validation Loss: 0.007672544120164164\n",
      "Epoch [1137/20000], Training Loss: 0.009483955817164056, Validation Loss: 0.008095338420304776\n",
      "Epoch [1138/20000], Training Loss: 0.020363657756076594, Validation Loss: 0.024221076926599305\n",
      "Epoch [1139/20000], Training Loss: 0.010319956528422023, Validation Loss: 0.011378264936636193\n",
      "Epoch [1140/20000], Training Loss: 0.010829586443443466, Validation Loss: 0.04248333969205958\n",
      "Epoch [1141/20000], Training Loss: 0.02155612436269751, Validation Loss: 0.042358134272049916\n",
      "Epoch [1142/20000], Training Loss: 0.019361373872795542, Validation Loss: 0.010134282922114348\n",
      "Epoch [1143/20000], Training Loss: 0.011548104273970239, Validation Loss: 0.016103913178150124\n",
      "Epoch [1144/20000], Training Loss: 0.013188855155021884, Validation Loss: 0.022273151967082017\n",
      "Epoch [1145/20000], Training Loss: 0.018660401330894923, Validation Loss: 0.012571752753759022\n",
      "Epoch [1146/20000], Training Loss: 0.03167762922385009, Validation Loss: 0.02755345010337462\n",
      "Epoch [1147/20000], Training Loss: 0.024053840931238874, Validation Loss: 0.019875933837395103\n",
      "Epoch [1148/20000], Training Loss: 0.027875593216906833, Validation Loss: 0.06785107426056838\n",
      "Epoch [1149/20000], Training Loss: 0.034276913186269145, Validation Loss: 0.01095687005207351\n",
      "Epoch [1150/20000], Training Loss: 0.013538066123146564, Validation Loss: 0.025756999141363774\n",
      "Epoch [1151/20000], Training Loss: 0.0203580398977335, Validation Loss: 0.02846450508421854\n",
      "Epoch [1152/20000], Training Loss: 0.017382348388699547, Validation Loss: 0.0134174819980316\n",
      "Epoch [1153/20000], Training Loss: 0.01268373359926045, Validation Loss: 0.010488975627103353\n",
      "Epoch [1154/20000], Training Loss: 0.012430740586881126, Validation Loss: 0.009870049365375739\n",
      "Epoch [1155/20000], Training Loss: 0.04222058974846732, Validation Loss: 0.0647440681183478\n",
      "Epoch [1156/20000], Training Loss: 0.06645601848140359, Validation Loss: 0.043308955701200254\n",
      "Epoch [1157/20000], Training Loss: 0.06387381531697299, Validation Loss: 0.05310288633532829\n",
      "Epoch [1158/20000], Training Loss: 0.039474316706348746, Validation Loss: 0.021113544682682468\n",
      "Epoch [1159/20000], Training Loss: 0.021481377883381874, Validation Loss: 0.018387429771546893\n",
      "Epoch [1160/20000], Training Loss: 0.019134711640487825, Validation Loss: 0.015825470269407907\n",
      "Epoch [1161/20000], Training Loss: 0.016073853742065176, Validation Loss: 0.012364466530161973\n",
      "Epoch [1162/20000], Training Loss: 0.018790439846010747, Validation Loss: 0.011134043690359205\n",
      "Epoch [1163/20000], Training Loss: 0.03059367316641978, Validation Loss: 0.03777897643063847\n",
      "Epoch [1164/20000], Training Loss: 0.0222851868269832, Validation Loss: 0.015365641124440527\n",
      "Epoch [1165/20000], Training Loss: 0.01270631973498634, Validation Loss: 0.011426356392361179\n",
      "Epoch [1166/20000], Training Loss: 0.012722331182365971, Validation Loss: 0.011877469694944135\n",
      "Epoch [1167/20000], Training Loss: 0.013833348788986248, Validation Loss: 0.0242214770461091\n",
      "Epoch [1168/20000], Training Loss: 0.010798853323129671, Validation Loss: 0.006936601652534789\n",
      "Epoch [1169/20000], Training Loss: 0.009816069890803192, Validation Loss: 0.015874669841205842\n",
      "Epoch [1170/20000], Training Loss: 0.01468814620082932, Validation Loss: 0.009237480899117112\n",
      "Epoch [1171/20000], Training Loss: 0.008241899371829018, Validation Loss: 0.0056218459458307534\n",
      "Epoch [1172/20000], Training Loss: 0.007807412152552777, Validation Loss: 0.01494573293781352\n",
      "Epoch [1173/20000], Training Loss: 0.007763589788477735, Validation Loss: 0.04045916978480052\n",
      "Epoch [1174/20000], Training Loss: 0.04186211598737698, Validation Loss: 0.009296181758956279\n",
      "Epoch [1175/20000], Training Loss: 0.10867738732278147, Validation Loss: 0.06507507370950957\n",
      "Epoch [1176/20000], Training Loss: 0.05378395086154342, Validation Loss: 0.020014100471953015\n",
      "Epoch [1177/20000], Training Loss: 0.029213391261042228, Validation Loss: 0.017401466110228546\n",
      "Epoch [1178/20000], Training Loss: 0.017408424988389015, Validation Loss: 0.013559773211097281\n",
      "Epoch [1179/20000], Training Loss: 0.015169656403096659, Validation Loss: 0.013097776854374224\n",
      "Epoch [1180/20000], Training Loss: 0.012688056931697897, Validation Loss: 0.021491853035692354\n",
      "Epoch [1181/20000], Training Loss: 0.03267771432209494, Validation Loss: 0.025318775862253267\n",
      "Epoch [1182/20000], Training Loss: 0.03214533971289971, Validation Loss: 0.03574643080234056\n",
      "Epoch [1183/20000], Training Loss: 0.026889586289014136, Validation Loss: 0.013219663490656426\n",
      "Epoch [1184/20000], Training Loss: 0.023725050119017915, Validation Loss: 0.014794338063390587\n",
      "Epoch [1185/20000], Training Loss: 0.012490938143205963, Validation Loss: 0.03050832189027177\n",
      "Epoch [1186/20000], Training Loss: 0.021650932321790606, Validation Loss: 0.019588241858374458\n",
      "Epoch [1187/20000], Training Loss: 0.015063295540000712, Validation Loss: 0.007451823739227942\n",
      "Epoch [1188/20000], Training Loss: 0.009130321418134761, Validation Loss: 0.008469581851758222\n",
      "Epoch [1189/20000], Training Loss: 0.012995078448771633, Validation Loss: 0.010890192930471955\n",
      "Epoch [1190/20000], Training Loss: 0.024080282740344825, Validation Loss: 0.029303503036095236\n",
      "Epoch [1191/20000], Training Loss: 0.02337712882685342, Validation Loss: 0.022118069129511343\n",
      "Epoch [1192/20000], Training Loss: 0.027721678319461977, Validation Loss: 0.020905194532064738\n",
      "Epoch [1193/20000], Training Loss: 0.017382422245905867, Validation Loss: 0.009333671302261022\n",
      "Epoch [1194/20000], Training Loss: 0.015891889806620645, Validation Loss: 0.01936338579509536\n",
      "Epoch [1195/20000], Training Loss: 0.02372599360231626, Validation Loss: 0.007064870633132249\n",
      "Epoch [1196/20000], Training Loss: 0.018670244932374253, Validation Loss: 0.007890089885507703\n",
      "Epoch [1197/20000], Training Loss: 0.015750537392575228, Validation Loss: 0.008553376706167338\n",
      "Epoch [1198/20000], Training Loss: 0.013171658543537237, Validation Loss: 0.010516567402028398\n",
      "Epoch [1199/20000], Training Loss: 0.024718860006292483, Validation Loss: 0.11086113282348933\n",
      "Epoch [1200/20000], Training Loss: 0.03931786394345441, Validation Loss: 0.09149990342269895\n",
      "Epoch [1201/20000], Training Loss: 0.045412496264491765, Validation Loss: 0.027404156005860176\n",
      "Epoch [1202/20000], Training Loss: 0.026507781552416936, Validation Loss: 0.015437655812853553\n",
      "Epoch [1203/20000], Training Loss: 0.02939321454115478, Validation Loss: 0.015628763227343915\n",
      "Epoch [1204/20000], Training Loss: 0.026757510129495392, Validation Loss: 0.010409563966635744\n",
      "Epoch [1205/20000], Training Loss: 0.014642497790711266, Validation Loss: 0.031164037684777955\n",
      "Epoch [1206/20000], Training Loss: 0.01844312737063904, Validation Loss: 0.012886757636361757\n",
      "Epoch [1207/20000], Training Loss: 0.009992457218946324, Validation Loss: 0.01063446317940428\n",
      "Epoch [1208/20000], Training Loss: 0.010647494429057198, Validation Loss: 0.010392366177175266\n",
      "Epoch [1209/20000], Training Loss: 0.012140250789733338, Validation Loss: 0.007404264325719152\n",
      "Epoch [1210/20000], Training Loss: 0.01662496101094543, Validation Loss: 0.03552409527574125\n",
      "Epoch [1211/20000], Training Loss: 0.04246613077495048, Validation Loss: 0.011862903002893959\n",
      "Epoch [1212/20000], Training Loss: 0.05032333185746601, Validation Loss: 0.023879843790299247\n",
      "Epoch [1213/20000], Training Loss: 0.028228284558281302, Validation Loss: 0.021742914063322766\n",
      "Epoch [1214/20000], Training Loss: 0.023232586710946634, Validation Loss: 0.01674871792243391\n",
      "Epoch [1215/20000], Training Loss: 0.0144579951052687, Validation Loss: 0.010950036320604777\n",
      "Epoch [1216/20000], Training Loss: 0.014126247593334742, Validation Loss: 0.015378162172148099\n",
      "Epoch [1217/20000], Training Loss: 0.013755223504371574, Validation Loss: 0.010149431841215108\n",
      "Epoch [1218/20000], Training Loss: 0.010498466689438959, Validation Loss: 0.008472205095543688\n",
      "Epoch [1219/20000], Training Loss: 0.007692852107408855, Validation Loss: 0.008304662525383203\n",
      "Epoch [1220/20000], Training Loss: 0.010364231196165617, Validation Loss: 0.015028194154502137\n",
      "Epoch [1221/20000], Training Loss: 0.008974936923810415, Validation Loss: 0.007305405350588911\n",
      "Epoch [1222/20000], Training Loss: 0.017074920213222607, Validation Loss: 0.030878009685718653\n",
      "Epoch [1223/20000], Training Loss: 0.014857101462049676, Validation Loss: 0.00913726354797639\n",
      "Epoch [1224/20000], Training Loss: 0.0107674722426704, Validation Loss: 0.009088204766175362\n",
      "Epoch [1225/20000], Training Loss: 0.013264894784827317, Validation Loss: 0.007391067381061678\n",
      "Epoch [1226/20000], Training Loss: 0.006097954001818705, Validation Loss: 0.007095224368774673\n",
      "Epoch [1227/20000], Training Loss: 0.008766977488059118, Validation Loss: 0.008072136144232729\n",
      "Epoch [1228/20000], Training Loss: 0.022779976423472754, Validation Loss: 0.027495177332088654\n",
      "Epoch [1229/20000], Training Loss: 0.028522828519011716, Validation Loss: 0.02068850016662452\n",
      "Epoch [1230/20000], Training Loss: 0.02053468729302819, Validation Loss: 0.012162785403893539\n",
      "Epoch [1231/20000], Training Loss: 0.02102750669499593, Validation Loss: 0.03858811574904576\n",
      "Epoch [1232/20000], Training Loss: 0.028502639092039317, Validation Loss: 0.068425238662646\n",
      "Epoch [1233/20000], Training Loss: 0.02748540709061282, Validation Loss: 0.0402168380200252\n",
      "Epoch [1234/20000], Training Loss: 0.017620523381213258, Validation Loss: 0.010026156309874386\n",
      "Epoch [1235/20000], Training Loss: 0.014124880486633629, Validation Loss: 0.01383931335034602\n",
      "Epoch [1236/20000], Training Loss: 0.010966140017054775, Validation Loss: 0.010513971803469132\n",
      "Epoch [1237/20000], Training Loss: 0.014521129483390334, Validation Loss: 0.01703603267276612\n",
      "Epoch [1238/20000], Training Loss: 0.028363337458618583, Validation Loss: 0.018617516778553517\n",
      "Epoch [1239/20000], Training Loss: 0.01404847766389139, Validation Loss: 0.0176596284574013\n",
      "Epoch [1240/20000], Training Loss: 0.01619367915347019, Validation Loss: 0.010009788433703632\n",
      "Epoch [1241/20000], Training Loss: 0.026482706382271966, Validation Loss: 0.0376938726013647\n",
      "Epoch [1242/20000], Training Loss: 0.02097751835494169, Validation Loss: 0.02008296555168266\n",
      "Epoch [1243/20000], Training Loss: 0.015191665380760761, Validation Loss: 0.009293452227971738\n",
      "Epoch [1244/20000], Training Loss: 0.02177992192030485, Validation Loss: 0.011309689926794554\n",
      "Epoch [1245/20000], Training Loss: 0.079280635475048, Validation Loss: 0.030126088130170713\n",
      "Epoch [1246/20000], Training Loss: 0.049044280645570586, Validation Loss: 0.030734238870958813\n",
      "Epoch [1247/20000], Training Loss: 0.028585832299930707, Validation Loss: 0.020869646416066794\n",
      "Epoch [1248/20000], Training Loss: 0.01564980168560786, Validation Loss: 0.016136074449300018\n",
      "Epoch [1249/20000], Training Loss: 0.013298859021493368, Validation Loss: 0.011839988267138803\n",
      "Epoch [1250/20000], Training Loss: 0.01426955931154745, Validation Loss: 0.010500355611606157\n",
      "Epoch [1251/20000], Training Loss: 0.01278195648410474, Validation Loss: 0.011158468855352596\n",
      "Epoch [1252/20000], Training Loss: 0.014411506546561472, Validation Loss: 0.022642467661601932\n",
      "Epoch [1253/20000], Training Loss: 0.02070254412995252, Validation Loss: 0.024884997722319852\n",
      "Epoch [1254/20000], Training Loss: 0.026452541575833623, Validation Loss: 0.018355488552787274\n",
      "Epoch [1255/20000], Training Loss: 0.0346392381636958, Validation Loss: 0.017678046926732312\n",
      "Epoch [1256/20000], Training Loss: 0.024174991828788604, Validation Loss: 0.020762792534248738\n",
      "Epoch [1257/20000], Training Loss: 0.01738051581196487, Validation Loss: 0.010477962749185674\n",
      "Epoch [1258/20000], Training Loss: 0.008788147918364433, Validation Loss: 0.007448144079612454\n",
      "Epoch [1259/20000], Training Loss: 0.008919246309752842, Validation Loss: 0.011630616740610233\n",
      "Epoch [1260/20000], Training Loss: 0.00915907379905028, Validation Loss: 0.007420415637984422\n",
      "Epoch [1261/20000], Training Loss: 0.00977996619518048, Validation Loss: 0.018762541872307612\n",
      "Epoch [1262/20000], Training Loss: 0.01021579814758817, Validation Loss: 0.020710873654805503\n",
      "Epoch [1263/20000], Training Loss: 0.011221063595647658, Validation Loss: 0.007985327104232236\n",
      "Epoch [1264/20000], Training Loss: 0.007285520317964256, Validation Loss: 0.007750625484342772\n",
      "Epoch [1265/20000], Training Loss: 0.013951412628687519, Validation Loss: 0.029969857021310574\n",
      "Epoch [1266/20000], Training Loss: 0.02185219012192517, Validation Loss: 0.018087609436796808\n",
      "Epoch [1267/20000], Training Loss: 0.018168887476869195, Validation Loss: 0.02835858013824342\n",
      "Epoch [1268/20000], Training Loss: 0.030092220021677867, Validation Loss: 0.06939884189965659\n",
      "Epoch [1269/20000], Training Loss: 0.05415309574787638, Validation Loss: 0.044042796319954856\n",
      "Epoch [1270/20000], Training Loss: 0.023854495699717, Validation Loss: 0.01599633539674415\n",
      "Epoch [1271/20000], Training Loss: 0.01692583566182293, Validation Loss: 0.022877180115891856\n",
      "Epoch [1272/20000], Training Loss: 0.02171857440095794, Validation Loss: 0.03949043699829902\n",
      "Epoch [1273/20000], Training Loss: 0.020387538076777543, Validation Loss: 0.05953973486043674\n",
      "Epoch [1274/20000], Training Loss: 0.026700065225928223, Validation Loss: 0.01764074578071513\n",
      "Epoch [1275/20000], Training Loss: 0.01661613816395402, Validation Loss: 0.01585715364730791\n",
      "Epoch [1276/20000], Training Loss: 0.016792614582560157, Validation Loss: 0.010813538088739416\n",
      "Epoch [1277/20000], Training Loss: 0.013996679552032478, Validation Loss: 0.008667741087948723\n",
      "Epoch [1278/20000], Training Loss: 0.012655939333074327, Validation Loss: 0.01027702691862997\n",
      "Epoch [1279/20000], Training Loss: 0.03247760614613071, Validation Loss: 0.01334953354095199\n",
      "Epoch [1280/20000], Training Loss: 0.0300362450153833, Validation Loss: 0.019907051419849026\n",
      "Epoch [1281/20000], Training Loss: 0.022884955288775797, Validation Loss: 0.021316738411983543\n",
      "Epoch [1282/20000], Training Loss: 0.015695320932926343, Validation Loss: 0.02192333957854155\n",
      "Epoch [1283/20000], Training Loss: 0.016554549557830405, Validation Loss: 0.018187262176590842\n",
      "Epoch [1284/20000], Training Loss: 0.016616141921466415, Validation Loss: 0.011773580441205444\n",
      "Epoch [1285/20000], Training Loss: 0.009434527219647342, Validation Loss: 0.006047062725770442\n",
      "Epoch [1286/20000], Training Loss: 0.010930114777043596, Validation Loss: 0.0069113814014539745\n",
      "Epoch [1287/20000], Training Loss: 0.012418497008703915, Validation Loss: 0.010539097052775132\n",
      "Epoch [1288/20000], Training Loss: 0.012976797186898434, Validation Loss: 0.03472096165829331\n",
      "Epoch [1289/20000], Training Loss: 0.041905821267781515, Validation Loss: 0.022213439380207207\n",
      "Epoch [1290/20000], Training Loss: 0.029682653278411766, Validation Loss: 0.009011910237166385\n",
      "Epoch [1291/20000], Training Loss: 0.04189123525949461, Validation Loss: 0.011892101673914806\n",
      "Epoch [1292/20000], Training Loss: 0.01542225677985698, Validation Loss: 0.009488394972366341\n",
      "Epoch [1293/20000], Training Loss: 0.008969399387881691, Validation Loss: 0.009588015704398075\n",
      "Epoch [1294/20000], Training Loss: 0.010393375198223762, Validation Loss: 0.009651241599005645\n",
      "Epoch [1295/20000], Training Loss: 0.010925623621525509, Validation Loss: 0.006386236491871547\n",
      "Epoch [1296/20000], Training Loss: 0.01411686927817105, Validation Loss: 0.008783245835169705\n",
      "Epoch [1297/20000], Training Loss: 0.012630159792024642, Validation Loss: 0.026648832025565377\n",
      "Epoch [1298/20000], Training Loss: 0.010833743785042316, Validation Loss: 0.026943805871697032\n",
      "Epoch [1299/20000], Training Loss: 0.01758437040760847, Validation Loss: 0.031945306113618344\n",
      "Epoch [1300/20000], Training Loss: 0.024667210870542737, Validation Loss: 0.04859132647612874\n",
      "Epoch [1301/20000], Training Loss: 0.04875308246534717, Validation Loss: 0.0642710163989174\n",
      "Epoch [1302/20000], Training Loss: 0.04648807107670499, Validation Loss: 0.06437403747186188\n",
      "Epoch [1303/20000], Training Loss: 0.0378783795583461, Validation Loss: 0.07510101756731828\n",
      "Epoch [1304/20000], Training Loss: 0.0392015215142497, Validation Loss: 0.016192128542165684\n",
      "Epoch [1305/20000], Training Loss: 0.014945345606455314, Validation Loss: 0.012658486206528341\n",
      "Epoch [1306/20000], Training Loss: 0.013357857134126658, Validation Loss: 0.010219192506067235\n",
      "Epoch [1307/20000], Training Loss: 0.015284323141843612, Validation Loss: 0.013153839984323883\n",
      "Epoch [1308/20000], Training Loss: 0.01847573067893141, Validation Loss: 0.015672328814152606\n",
      "Epoch [1309/20000], Training Loss: 0.013319602760020643, Validation Loss: 0.02460107400032955\n",
      "Epoch [1310/20000], Training Loss: 0.03548739034574412, Validation Loss: 0.06467030487492939\n",
      "Epoch [1311/20000], Training Loss: 0.03468257857353559, Validation Loss: 0.047379260325549226\n",
      "Epoch [1312/20000], Training Loss: 0.039944200161179264, Validation Loss: 0.027388540590641216\n",
      "Epoch [1313/20000], Training Loss: 0.020351400374368365, Validation Loss: 0.019027486753654257\n",
      "Epoch [1314/20000], Training Loss: 0.01690582330671272, Validation Loss: 0.018833036358104124\n",
      "Epoch [1315/20000], Training Loss: 0.01753223847065653, Validation Loss: 0.02116955563750797\n",
      "Epoch [1316/20000], Training Loss: 0.015645170883674706, Validation Loss: 0.01717482025321362\n",
      "Epoch [1317/20000], Training Loss: 0.0186997765569166, Validation Loss: 0.04264377045920971\n",
      "Epoch [1318/20000], Training Loss: 0.01675394974050245, Validation Loss: 0.02299709470917799\n",
      "Epoch [1319/20000], Training Loss: 0.024726806304118196, Validation Loss: 0.02727821997868578\n",
      "Epoch [1320/20000], Training Loss: 0.024217069381847978, Validation Loss: 0.02218136415260743\n",
      "Epoch [1321/20000], Training Loss: 0.018260099999939224, Validation Loss: 0.011141921237132193\n",
      "Epoch [1322/20000], Training Loss: 0.019795085133020102, Validation Loss: 0.013781851085973292\n",
      "Epoch [1323/20000], Training Loss: 0.013553697948477097, Validation Loss: 0.007878444241848569\n",
      "Epoch [1324/20000], Training Loss: 0.008218444804827283, Validation Loss: 0.0062750405041602855\n",
      "Epoch [1325/20000], Training Loss: 0.008610303879582457, Validation Loss: 0.007873479128680864\n",
      "Epoch [1326/20000], Training Loss: 0.006824841789369073, Validation Loss: 0.004999904743353508\n",
      "Epoch [1327/20000], Training Loss: 0.00870005095826595, Validation Loss: 0.004453375952217751\n",
      "Epoch [1328/20000], Training Loss: 0.01655803359712341, Validation Loss: 0.007136837546957923\n",
      "Epoch [1329/20000], Training Loss: 0.020371944269364967, Validation Loss: 0.021089339418042415\n",
      "Epoch [1330/20000], Training Loss: 0.022176221958943643, Validation Loss: 0.016334242155218396\n",
      "Epoch [1331/20000], Training Loss: 0.026150655358963246, Validation Loss: 0.007873937931435693\n",
      "Epoch [1332/20000], Training Loss: 0.039994316907333474, Validation Loss: 0.02389627362208582\n",
      "Epoch [1333/20000], Training Loss: 0.0304990681387218, Validation Loss: 0.050333776332472394\n",
      "Epoch [1334/20000], Training Loss: 0.02742271901973124, Validation Loss: 0.01770219374813301\n",
      "Epoch [1335/20000], Training Loss: 0.013275567891209252, Validation Loss: 0.03498189806670994\n",
      "Epoch [1336/20000], Training Loss: 0.020606790374066413, Validation Loss: 0.015247106867848702\n",
      "Epoch [1337/20000], Training Loss: 0.01832445045563093, Validation Loss: 0.014602516156353397\n",
      "Epoch [1338/20000], Training Loss: 0.01561621343716979, Validation Loss: 0.008114441459076593\n",
      "Epoch [1339/20000], Training Loss: 0.012766809198572966, Validation Loss: 0.007061139590226857\n",
      "Epoch [1340/20000], Training Loss: 0.013640879615975012, Validation Loss: 0.00815175363433294\n",
      "Epoch [1341/20000], Training Loss: 0.012410035278922546, Validation Loss: 0.005692767761307304\n",
      "Epoch [1342/20000], Training Loss: 0.014578380784119613, Validation Loss: 0.01270006991936671\n",
      "Epoch [1343/20000], Training Loss: 0.017072719355512942, Validation Loss: 0.012566059218264713\n",
      "Epoch [1344/20000], Training Loss: 0.017950005306837675, Validation Loss: 0.026317162674266533\n",
      "Epoch [1345/20000], Training Loss: 0.03869442638408925, Validation Loss: 0.04340698120982547\n",
      "Epoch [1346/20000], Training Loss: 0.03850489595372762, Validation Loss: 0.07464807318008217\n",
      "Epoch [1347/20000], Training Loss: 0.054552784838181524, Validation Loss: 0.050729665461777665\n",
      "Epoch [1348/20000], Training Loss: 0.025473409398858036, Validation Loss: 0.013719630660565226\n",
      "Epoch [1349/20000], Training Loss: 0.011957713561839358, Validation Loss: 0.01930513279546894\n",
      "Epoch [1350/20000], Training Loss: 0.014837010859212438, Validation Loss: 0.01207365261882565\n",
      "Epoch [1351/20000], Training Loss: 0.025276973220960435, Validation Loss: 0.02613554027853937\n",
      "Epoch [1352/20000], Training Loss: 0.04240326293178701, Validation Loss: 0.09250207966714467\n",
      "Epoch [1353/20000], Training Loss: 0.03542748506047896, Validation Loss: 0.015276867056375596\n",
      "Epoch [1354/20000], Training Loss: 0.017318192777955637, Validation Loss: 0.012080179470583347\n",
      "Epoch [1355/20000], Training Loss: 0.012939416013458478, Validation Loss: 0.010038017219974047\n",
      "Epoch [1356/20000], Training Loss: 0.011181197965925094, Validation Loss: 0.015809403442327848\n",
      "Epoch [1357/20000], Training Loss: 0.0165108400272272, Validation Loss: 0.01712772504569495\n",
      "Epoch [1358/20000], Training Loss: 0.018445225565561225, Validation Loss: 0.008870577936701549\n",
      "Epoch [1359/20000], Training Loss: 0.014001892796451492, Validation Loss: 0.013870624081132793\n",
      "Epoch [1360/20000], Training Loss: 0.012539758429349084, Validation Loss: 0.008643666601822085\n",
      "Epoch [1361/20000], Training Loss: 0.010137555214376854, Validation Loss: 0.016005192640299825\n",
      "Epoch [1362/20000], Training Loss: 0.014648661574548376, Validation Loss: 0.0131142019979843\n",
      "Epoch [1363/20000], Training Loss: 0.010170665253618998, Validation Loss: 0.007692972047770281\n",
      "Epoch [1364/20000], Training Loss: 0.007637144656785365, Validation Loss: 0.008985128789780595\n",
      "Epoch [1365/20000], Training Loss: 0.012723730049661494, Validation Loss: 0.006571685085093961\n",
      "Epoch [1366/20000], Training Loss: 0.006585579436172598, Validation Loss: 0.005741587977939055\n",
      "Epoch [1367/20000], Training Loss: 0.007224803881919277, Validation Loss: 0.004334587960037048\n",
      "Epoch [1368/20000], Training Loss: 0.007472889822175992, Validation Loss: 0.004360880759252604\n",
      "Epoch [1369/20000], Training Loss: 0.007593896884015651, Validation Loss: 0.00858663322133323\n",
      "Epoch [1370/20000], Training Loss: 0.01740440150648023, Validation Loss: 0.016650849418803258\n",
      "Epoch [1371/20000], Training Loss: 0.024942566619886617, Validation Loss: 0.014219339772223208\n",
      "Epoch [1372/20000], Training Loss: 0.02397405592949196, Validation Loss: 0.013936629358113366\n",
      "Epoch [1373/20000], Training Loss: 0.011056621523623886, Validation Loss: 0.023302191234076477\n",
      "Epoch [1374/20000], Training Loss: 0.01568173749546986, Validation Loss: 0.00787806701531835\n",
      "Epoch [1375/20000], Training Loss: 0.014369289902138658, Validation Loss: 0.006165878737062659\n",
      "Epoch [1376/20000], Training Loss: 0.009945046605675347, Validation Loss: 0.0057138364832261535\n",
      "Epoch [1377/20000], Training Loss: 0.030282987755656774, Validation Loss: 0.03246987736268893\n",
      "Epoch [1378/20000], Training Loss: 0.042166818864643574, Validation Loss: 0.021333247716809094\n",
      "Epoch [1379/20000], Training Loss: 0.027969380192059492, Validation Loss: 0.018028991157248344\n",
      "Epoch [1380/20000], Training Loss: 0.028897556443033472, Validation Loss: 0.028200379328547343\n",
      "Epoch [1381/20000], Training Loss: 0.03821944211709446, Validation Loss: 0.06642563209091833\n",
      "Epoch [1382/20000], Training Loss: 0.02947355873350586, Validation Loss: 0.011738895721373014\n",
      "Epoch [1383/20000], Training Loss: 0.009531297829068665, Validation Loss: 0.02239318927105975\n",
      "Epoch [1384/20000], Training Loss: 0.013814614852890372, Validation Loss: 0.014016260848985123\n",
      "Epoch [1385/20000], Training Loss: 0.014141343267900603, Validation Loss: 0.008266452917434424\n",
      "Epoch [1386/20000], Training Loss: 0.007333018318084734, Validation Loss: 0.01372731217520729\n",
      "Epoch [1387/20000], Training Loss: 0.01354464256188034, Validation Loss: 0.01608790869332675\n",
      "Epoch [1388/20000], Training Loss: 0.012510651689288872, Validation Loss: 0.005623123233233598\n",
      "Epoch [1389/20000], Training Loss: 0.010650036755513221, Validation Loss: 0.02352492872421507\n",
      "Epoch [1390/20000], Training Loss: 0.020864607670643767, Validation Loss: 0.023032850012051926\n",
      "Epoch [1391/20000], Training Loss: 0.01551012047171493, Validation Loss: 0.023501903149463987\n",
      "Epoch [1392/20000], Training Loss: 0.023285584292027903, Validation Loss: 0.02809687386651933\n",
      "Epoch [1393/20000], Training Loss: 0.031761392490547484, Validation Loss: 0.019438182737255415\n",
      "Epoch [1394/20000], Training Loss: 0.01837975363014266, Validation Loss: 0.01041093484309452\n",
      "Epoch [1395/20000], Training Loss: 0.015615609782149218, Validation Loss: 0.007726941247321918\n",
      "Epoch [1396/20000], Training Loss: 0.01752428893918737, Validation Loss: 0.007180683643258618\n",
      "Epoch [1397/20000], Training Loss: 0.011045159589100097, Validation Loss: 0.00959266487906747\n",
      "Epoch [1398/20000], Training Loss: 0.016538034983178868, Validation Loss: 0.013459012201760415\n",
      "Epoch [1399/20000], Training Loss: 0.025624825036045098, Validation Loss: 0.03940406488358683\n",
      "Epoch [1400/20000], Training Loss: 0.03172783472525355, Validation Loss: 0.018794768273697787\n",
      "Epoch [1401/20000], Training Loss: 0.010097602026300072, Validation Loss: 0.01686290967362415\n",
      "Epoch [1402/20000], Training Loss: 0.012486079030817823, Validation Loss: 0.009310110300946898\n",
      "Epoch [1403/20000], Training Loss: 0.01066218743134024, Validation Loss: 0.016334004953600222\n",
      "Epoch [1404/20000], Training Loss: 0.009461397560828897, Validation Loss: 0.009688059414665032\n",
      "Epoch [1405/20000], Training Loss: 0.01507112942636013, Validation Loss: 0.037766929449760116\n",
      "Epoch [1406/20000], Training Loss: 0.05449385018021401, Validation Loss: 0.08122241076489575\n",
      "Epoch [1407/20000], Training Loss: 0.07854099419949177, Validation Loss: 0.03082626541534044\n",
      "Epoch [1408/20000], Training Loss: 0.046554959628597965, Validation Loss: 0.06486318856659688\n",
      "Epoch [1409/20000], Training Loss: 0.040335202880669385, Validation Loss: 0.03140237304814279\n",
      "Epoch [1410/20000], Training Loss: 0.03535103505211217, Validation Loss: 0.028060944066960063\n",
      "Epoch [1411/20000], Training Loss: 0.026183783442580273, Validation Loss: 0.021490450939485872\n",
      "Epoch [1412/20000], Training Loss: 0.017927462684123645, Validation Loss: 0.012424983143950614\n",
      "Epoch [1413/20000], Training Loss: 0.01266885143039482, Validation Loss: 0.0149034954088637\n",
      "Epoch [1414/20000], Training Loss: 0.014353243401274085, Validation Loss: 0.009908989687673427\n",
      "Epoch [1415/20000], Training Loss: 0.014193190935267401, Validation Loss: 0.009547064077960576\n",
      "Epoch [1416/20000], Training Loss: 0.01586965687706002, Validation Loss: 0.007365848793483831\n",
      "Epoch [1417/20000], Training Loss: 0.010094447890878655, Validation Loss: 0.007775379850110265\n",
      "Epoch [1418/20000], Training Loss: 0.008871124390030414, Validation Loss: 0.017713560426532896\n",
      "Epoch [1419/20000], Training Loss: 0.01825977702433842, Validation Loss: 0.016570625030941043\n",
      "Epoch [1420/20000], Training Loss: 0.011323981445717177, Validation Loss: 0.09542867511716377\n",
      "Epoch [1421/20000], Training Loss: 0.0325285255426674, Validation Loss: 0.07162318396255987\n",
      "Epoch [1422/20000], Training Loss: 0.039828363964001516, Validation Loss: 0.032327257202872335\n",
      "Epoch [1423/20000], Training Loss: 0.02095055234219347, Validation Loss: 0.011063208760746276\n",
      "Epoch [1424/20000], Training Loss: 0.014440135094836088, Validation Loss: 0.009690260116785202\n",
      "Epoch [1425/20000], Training Loss: 0.013209830237818616, Validation Loss: 0.010394874226358552\n",
      "Epoch [1426/20000], Training Loss: 0.00921349790379671, Validation Loss: 0.014102813498281411\n",
      "Epoch [1427/20000], Training Loss: 0.03217236423266253, Validation Loss: 0.01926175667280697\n",
      "Epoch [1428/20000], Training Loss: 0.05824861339975281, Validation Loss: 0.011601181049624097\n",
      "Epoch [1429/20000], Training Loss: 0.04090674655578498, Validation Loss: 0.0535706500093129\n",
      "Epoch [1430/20000], Training Loss: 0.03950726722541731, Validation Loss: 0.02869694274779854\n",
      "Epoch [1431/20000], Training Loss: 0.022340196450906142, Validation Loss: 0.01803492741618961\n",
      "Epoch [1432/20000], Training Loss: 0.013495684734412603, Validation Loss: 0.018079526850104038\n",
      "Epoch [1433/20000], Training Loss: 0.02161106329211699, Validation Loss: 0.019634333189724792\n",
      "Epoch [1434/20000], Training Loss: 0.01415647444082424, Validation Loss: 0.03730842355331361\n",
      "Epoch [1435/20000], Training Loss: 0.01969848998539549, Validation Loss: 0.009270193736094254\n",
      "Epoch [1436/20000], Training Loss: 0.02244124877012967, Validation Loss: 0.00888160875477532\n",
      "Epoch [1437/20000], Training Loss: 0.014399403972285134, Validation Loss: 0.008136526391338739\n",
      "Epoch [1438/20000], Training Loss: 0.01053433956777943, Validation Loss: 0.008111354489009648\n",
      "Epoch [1439/20000], Training Loss: 0.010498387320175035, Validation Loss: 0.009354702517856034\n",
      "Epoch [1440/20000], Training Loss: 0.009963330330460199, Validation Loss: 0.007028132297883927\n",
      "Epoch [1441/20000], Training Loss: 0.014118911561256806, Validation Loss: 0.008455146676819179\n",
      "Epoch [1442/20000], Training Loss: 0.009293164922772641, Validation Loss: 0.009354306675764491\n",
      "Epoch [1443/20000], Training Loss: 0.010471303182254945, Validation Loss: 0.01492182499387719\n",
      "Epoch [1444/20000], Training Loss: 0.011918518672051246, Validation Loss: 0.005326406771132498\n",
      "Epoch [1445/20000], Training Loss: 0.030037691858784195, Validation Loss: 0.019937473132521127\n",
      "Epoch [1446/20000], Training Loss: 0.01872265511026074, Validation Loss: 0.011985580938553139\n",
      "Epoch [1447/20000], Training Loss: 0.01410805217476861, Validation Loss: 0.013603604763764985\n",
      "Epoch [1448/20000], Training Loss: 0.01603902279540697, Validation Loss: 0.012781783226598375\n",
      "Epoch [1449/20000], Training Loss: 0.012437489852475534, Validation Loss: 0.007398058512291888\n",
      "Epoch [1450/20000], Training Loss: 0.020901048838693117, Validation Loss: 0.035564373872711386\n",
      "Epoch [1451/20000], Training Loss: 0.031349911799355014, Validation Loss: 0.019725690043872347\n",
      "Epoch [1452/20000], Training Loss: 0.03043233495970656, Validation Loss: 0.03564942631471388\n",
      "Epoch [1453/20000], Training Loss: 0.031223233240390464, Validation Loss: 0.015666895247136946\n",
      "Epoch [1454/20000], Training Loss: 0.05133953426099781, Validation Loss: 0.041857203215069534\n",
      "Epoch [1455/20000], Training Loss: 0.023916894985762025, Validation Loss: 0.025267786973427266\n",
      "Epoch [1456/20000], Training Loss: 0.01827704710220652, Validation Loss: 0.011081661146845372\n",
      "Epoch [1457/20000], Training Loss: 0.017068152648529837, Validation Loss: 0.041710888713585845\n",
      "Epoch [1458/20000], Training Loss: 0.021892079439047456, Validation Loss: 0.02124581081004969\n",
      "Epoch [1459/20000], Training Loss: 0.013451363044233793, Validation Loss: 0.014748986502258097\n",
      "Epoch [1460/20000], Training Loss: 0.017620291045334722, Validation Loss: 0.008312472378926818\n",
      "Epoch [1461/20000], Training Loss: 0.014621721019336422, Validation Loss: 0.007744133264850536\n",
      "Epoch [1462/20000], Training Loss: 0.013050094438118063, Validation Loss: 0.007985538324387014\n",
      "Epoch [1463/20000], Training Loss: 0.01164633415672662, Validation Loss: 0.006310267557085252\n",
      "Epoch [1464/20000], Training Loss: 0.017349649883856597, Validation Loss: 0.011426314718976658\n",
      "Epoch [1465/20000], Training Loss: 0.016741579843385677, Validation Loss: 0.01094707293280628\n",
      "Epoch [1466/20000], Training Loss: 0.009674101664651451, Validation Loss: 0.00907337281692631\n",
      "Epoch [1467/20000], Training Loss: 0.008612193840755416, Validation Loss: 0.0068908221421679705\n",
      "Epoch [1468/20000], Training Loss: 0.008135865617077798, Validation Loss: 0.008424814024611548\n",
      "Epoch [1469/20000], Training Loss: 0.007186808630162237, Validation Loss: 0.05092481646315783\n",
      "Epoch [1470/20000], Training Loss: 0.035373130921340944, Validation Loss: 0.0378273907290192\n",
      "Epoch [1471/20000], Training Loss: 0.01364667592445455, Validation Loss: 0.01005171215963078\n",
      "Epoch [1472/20000], Training Loss: 0.015141633746679872, Validation Loss: 0.005117950719977685\n",
      "Epoch [1473/20000], Training Loss: 0.01704980014515708, Validation Loss: 0.04228374250795092\n",
      "Epoch [1474/20000], Training Loss: 0.02156268762441219, Validation Loss: 0.009115480114425237\n",
      "Epoch [1475/20000], Training Loss: 0.014631934987846762, Validation Loss: 0.005595833419647567\n",
      "Epoch [1476/20000], Training Loss: 0.03734412094386893, Validation Loss: 0.09684254485390402\n",
      "Epoch [1477/20000], Training Loss: 0.03806047873305423, Validation Loss: 0.013503367742358345\n",
      "Epoch [1478/20000], Training Loss: 0.014066427235125698, Validation Loss: 0.011570188098014352\n",
      "Epoch [1479/20000], Training Loss: 0.01938438049650618, Validation Loss: 0.015907025068439096\n",
      "Epoch [1480/20000], Training Loss: 0.011949186412883657, Validation Loss: 0.02507793500809044\n",
      "Epoch [1481/20000], Training Loss: 0.016587745922151953, Validation Loss: 0.008991365514768047\n",
      "Epoch [1482/20000], Training Loss: 0.01348689039670197, Validation Loss: 0.011434925948314129\n",
      "Epoch [1483/20000], Training Loss: 0.012628477133278335, Validation Loss: 0.010358698375333817\n",
      "Epoch [1484/20000], Training Loss: 0.03195118042328561, Validation Loss: 0.03078927476839258\n",
      "Epoch [1485/20000], Training Loss: 0.022069008139494275, Validation Loss: 0.01883483544923351\n",
      "Epoch [1486/20000], Training Loss: 0.019314653834044293, Validation Loss: 0.011112781861032253\n",
      "Epoch [1487/20000], Training Loss: 0.008609714198558192, Validation Loss: 0.007564797099235387\n",
      "Epoch [1488/20000], Training Loss: 0.013424074497639335, Validation Loss: 0.014088758030620124\n",
      "Epoch [1489/20000], Training Loss: 0.013122084047479023, Validation Loss: 0.01293636538853868\n",
      "Epoch [1490/20000], Training Loss: 0.010286433231418155, Validation Loss: 0.012976100387472659\n",
      "Epoch [1491/20000], Training Loss: 0.011951035365200369, Validation Loss: 0.01517124754028439\n",
      "Epoch [1492/20000], Training Loss: 0.03918706753756851, Validation Loss: 0.03565935738821452\n",
      "Epoch [1493/20000], Training Loss: 0.08713638241481801, Validation Loss: 0.04625963954296947\n",
      "Epoch [1494/20000], Training Loss: 0.04632672855430948, Validation Loss: 0.030456429406169053\n",
      "Epoch [1495/20000], Training Loss: 0.02150614356755146, Validation Loss: 0.016037437131578496\n",
      "Epoch [1496/20000], Training Loss: 0.018101570067561364, Validation Loss: 0.02336913983318037\n",
      "Epoch [1497/20000], Training Loss: 0.014643834188713558, Validation Loss: 0.011938016621482236\n",
      "Epoch [1498/20000], Training Loss: 0.011368847684934735, Validation Loss: 0.009138108588918907\n",
      "Epoch [1499/20000], Training Loss: 0.00916054449044168, Validation Loss: 0.007629200258456073\n",
      "Epoch [1500/20000], Training Loss: 0.011430012543444588, Validation Loss: 0.008388436114858403\n",
      "Epoch [1501/20000], Training Loss: 0.016314078305315758, Validation Loss: 0.030262495550102526\n",
      "Epoch [1502/20000], Training Loss: 0.019399036075420945, Validation Loss: 0.008597599958323252\n",
      "Epoch [1503/20000], Training Loss: 0.010383343831303396, Validation Loss: 0.006559746143206478\n",
      "Epoch [1504/20000], Training Loss: 0.007805744406401313, Validation Loss: 0.00860529923318178\n",
      "Epoch [1505/20000], Training Loss: 0.010371453083020501, Validation Loss: 0.0070188042622508625\n",
      "Epoch [1506/20000], Training Loss: 0.01159935600922576, Validation Loss: 0.007391625137090939\n",
      "Epoch [1507/20000], Training Loss: 0.011899172299308702, Validation Loss: 0.0047159173663463404\n",
      "Epoch [1508/20000], Training Loss: 0.006870300708604711, Validation Loss: 0.00444993862064231\n",
      "Epoch [1509/20000], Training Loss: 0.006742821985556345, Validation Loss: 0.010984724942119328\n",
      "Epoch [1510/20000], Training Loss: 0.007853874757143688, Validation Loss: 0.021584090436590486\n",
      "Epoch [1511/20000], Training Loss: 0.009994541018386371, Validation Loss: 0.011663191707027312\n",
      "Epoch [1512/20000], Training Loss: 0.023067475521072214, Validation Loss: 0.007047492482797679\n",
      "Epoch [1513/20000], Training Loss: 0.06404495368146204, Validation Loss: 0.03163184743581431\n",
      "Epoch [1514/20000], Training Loss: 0.04619684019209152, Validation Loss: 0.07187696042956102\n",
      "Epoch [1515/20000], Training Loss: 0.045618913834914565, Validation Loss: 0.02078375400106684\n",
      "Epoch [1516/20000], Training Loss: 0.014172647979908757, Validation Loss: 0.012038465952758025\n",
      "Epoch [1517/20000], Training Loss: 0.010669075050308103, Validation Loss: 0.018323400763523224\n",
      "Epoch [1518/20000], Training Loss: 0.023405866098723242, Validation Loss: 0.013814677353740602\n",
      "Epoch [1519/20000], Training Loss: 0.013295317668832598, Validation Loss: 0.00824803874307539\n",
      "Epoch [1520/20000], Training Loss: 0.008878025226294994, Validation Loss: 0.007752881454459105\n",
      "Epoch [1521/20000], Training Loss: 0.009880141654450978, Validation Loss: 0.00737910878111944\n",
      "Epoch [1522/20000], Training Loss: 0.015470853620042493, Validation Loss: 0.006552932214877469\n",
      "Epoch [1523/20000], Training Loss: 0.0210340654344431, Validation Loss: 0.029196218050076696\n",
      "Epoch [1524/20000], Training Loss: 0.02192626306454518, Validation Loss: 0.009092323318847259\n",
      "Epoch [1525/20000], Training Loss: 0.026772374245670756, Validation Loss: 0.008176924456295509\n",
      "Epoch [1526/20000], Training Loss: 0.020411909295944497, Validation Loss: 0.007766029082631578\n",
      "Epoch [1527/20000], Training Loss: 0.024250132447507764, Validation Loss: 0.008126059774310438\n",
      "Epoch [1528/20000], Training Loss: 0.010275834921880491, Validation Loss: 0.012171774415151812\n",
      "Epoch [1529/20000], Training Loss: 0.011675433866912499, Validation Loss: 0.0061573548431884525\n",
      "Epoch [1530/20000], Training Loss: 0.013179830749452646, Validation Loss: 0.005834760198013101\n",
      "Epoch [1531/20000], Training Loss: 0.010682611900847405, Validation Loss: 0.012893043827992945\n",
      "Epoch [1532/20000], Training Loss: 0.007423231244112165, Validation Loss: 0.010538845593990638\n",
      "Epoch [1533/20000], Training Loss: 0.018305984275814677, Validation Loss: 0.014085306985496768\n",
      "Epoch [1534/20000], Training Loss: 0.025445564126130193, Validation Loss: 0.007224190907956134\n",
      "Epoch [1535/20000], Training Loss: 0.008402412277064286, Validation Loss: 0.007276852357530775\n",
      "Epoch [1536/20000], Training Loss: 0.009717365647832463, Validation Loss: 0.008500202110293944\n",
      "Epoch [1537/20000], Training Loss: 0.017334806039538568, Validation Loss: 0.00986904023495429\n",
      "Epoch [1538/20000], Training Loss: 0.010909780227978314, Validation Loss: 0.01379714516569508\n",
      "Epoch [1539/20000], Training Loss: 0.007813203271195692, Validation Loss: 0.0071437070921547274\n",
      "Epoch [1540/20000], Training Loss: 0.011707627613778828, Validation Loss: 0.030416780536037486\n",
      "Epoch [1541/20000], Training Loss: 0.019770343799401808, Validation Loss: 0.017855479653852462\n",
      "Epoch [1542/20000], Training Loss: 0.021715779938468977, Validation Loss: 0.08353579016966352\n",
      "Epoch [1543/20000], Training Loss: 0.07381568392988161, Validation Loss: 0.04810620079208811\n",
      "Epoch [1544/20000], Training Loss: 0.043602815396817665, Validation Loss: 0.07476689076523257\n",
      "Epoch [1545/20000], Training Loss: 0.027275394315698316, Validation Loss: 0.012159402605882599\n",
      "Epoch [1546/20000], Training Loss: 0.01945573793325041, Validation Loss: 0.02052768830323681\n",
      "Epoch [1547/20000], Training Loss: 0.03618499992548355, Validation Loss: 0.03901827323971145\n",
      "Epoch [1548/20000], Training Loss: 0.026443420734722167, Validation Loss: 0.029455695382653503\n",
      "Epoch [1549/20000], Training Loss: 0.020735934476501176, Validation Loss: 0.015263909645275324\n",
      "Epoch [1550/20000], Training Loss: 0.023236594216931344, Validation Loss: 0.010571222064869143\n",
      "Epoch [1551/20000], Training Loss: 0.010824784935851182, Validation Loss: 0.008816212397003222\n",
      "Epoch [1552/20000], Training Loss: 0.008835866531756307, Validation Loss: 0.006797025177109498\n",
      "Epoch [1553/20000], Training Loss: 0.006331749084243451, Validation Loss: 0.007896676069841695\n",
      "Epoch [1554/20000], Training Loss: 0.01483630905152365, Validation Loss: 0.01365988760289838\n",
      "Epoch [1555/20000], Training Loss: 0.010600071479400088, Validation Loss: 0.006406287977759868\n",
      "Epoch [1556/20000], Training Loss: 0.008941767373601448, Validation Loss: 0.01837801499784421\n",
      "Epoch [1557/20000], Training Loss: 0.029133446977149497, Validation Loss: 0.0064185874642654584\n",
      "Epoch [1558/20000], Training Loss: 0.02414808839343355, Validation Loss: 0.06961294085462935\n",
      "Epoch [1559/20000], Training Loss: 0.052891805834536045, Validation Loss: 0.0588048419964358\n",
      "Epoch [1560/20000], Training Loss: 0.024114315572660416, Validation Loss: 0.03468553731716639\n",
      "Epoch [1561/20000], Training Loss: 0.021472463750147393, Validation Loss: 0.010078938010342446\n",
      "Epoch [1562/20000], Training Loss: 0.021380216341640335, Validation Loss: 0.009713498599499612\n",
      "Epoch [1563/20000], Training Loss: 0.014688776756104614, Validation Loss: 0.009947793259308517\n",
      "Epoch [1564/20000], Training Loss: 0.011495553073473275, Validation Loss: 0.007501268565079282\n",
      "Epoch [1565/20000], Training Loss: 0.011407213601549821, Validation Loss: 0.010325222431415395\n",
      "Epoch [1566/20000], Training Loss: 0.016470280130826204, Validation Loss: 0.007085011881356097\n",
      "Epoch [1567/20000], Training Loss: 0.02019280073831656, Validation Loss: 0.01531309185286643\n",
      "Epoch [1568/20000], Training Loss: 0.02319549907198442, Validation Loss: 0.006648738304664329\n",
      "Epoch [1569/20000], Training Loss: 0.02183894766312733, Validation Loss: 0.017686489598385875\n",
      "Epoch [1570/20000], Training Loss: 0.021584474858108087, Validation Loss: 0.013515782793093909\n",
      "Epoch [1571/20000], Training Loss: 0.014523914915376477, Validation Loss: 0.02623279619532846\n",
      "Epoch [1572/20000], Training Loss: 0.014177243544898894, Validation Loss: 0.01839049422548173\n",
      "Epoch [1573/20000], Training Loss: 0.012958800711203367, Validation Loss: 0.015649251064135\n",
      "Epoch [1574/20000], Training Loss: 0.010709636832221545, Validation Loss: 0.015440723504622968\n",
      "Epoch [1575/20000], Training Loss: 0.010201074178829523, Validation Loss: 0.006288029023924732\n",
      "Epoch [1576/20000], Training Loss: 0.007649439480571475, Validation Loss: 0.0052325225196260105\n",
      "Epoch [1577/20000], Training Loss: 0.008649464353116596, Validation Loss: 0.017837775672627947\n",
      "Epoch [1578/20000], Training Loss: 0.011072632196633745, Validation Loss: 0.005609589854803484\n",
      "Epoch [1579/20000], Training Loss: 0.01250340671270221, Validation Loss: 0.008391179923704735\n",
      "Epoch [1580/20000], Training Loss: 0.05616588609908441, Validation Loss: 0.05110378682825251\n",
      "Epoch [1581/20000], Training Loss: 0.0782953369198367, Validation Loss: 0.08701070800558293\n",
      "Epoch [1582/20000], Training Loss: 0.038270239917827506, Validation Loss: 0.02590241861739437\n",
      "Epoch [1583/20000], Training Loss: 0.03276285110041499, Validation Loss: 0.013763865284634334\n",
      "Epoch [1584/20000], Training Loss: 0.01686106454248407, Validation Loss: 0.014616672095604435\n",
      "Epoch [1585/20000], Training Loss: 0.014592400826846383, Validation Loss: 0.009824758838516892\n",
      "Epoch [1586/20000], Training Loss: 0.012136694771470502, Validation Loss: 0.010296251468081297\n",
      "Epoch [1587/20000], Training Loss: 0.01144642846442626, Validation Loss: 0.008470604874414093\n",
      "Epoch [1588/20000], Training Loss: 0.012573677514280592, Validation Loss: 0.008495937931913822\n",
      "Epoch [1589/20000], Training Loss: 0.010301360770037198, Validation Loss: 0.018834747397315443\n",
      "Epoch [1590/20000], Training Loss: 0.025464714426301986, Validation Loss: 0.013052893296833195\n",
      "Epoch [1591/20000], Training Loss: 0.07005071181837204, Validation Loss: 0.04288399992435601\n",
      "Epoch [1592/20000], Training Loss: 0.052811620490891595, Validation Loss: 0.062686866141136\n",
      "Epoch [1593/20000], Training Loss: 0.03605739097110927, Validation Loss: 0.02176220708799811\n",
      "Epoch [1594/20000], Training Loss: 0.01882448945460575, Validation Loss: 0.02390486242415849\n",
      "Epoch [1595/20000], Training Loss: 0.017068801181656972, Validation Loss: 0.017489868598288914\n",
      "Epoch [1596/20000], Training Loss: 0.019534802363653268, Validation Loss: 0.012281484807922492\n",
      "Epoch [1597/20000], Training Loss: 0.016153643589599857, Validation Loss: 0.009910273668420384\n",
      "Epoch [1598/20000], Training Loss: 0.012061305023962632, Validation Loss: 0.01879161924661089\n",
      "Epoch [1599/20000], Training Loss: 0.02039820679679646, Validation Loss: 0.008681676368785864\n",
      "Epoch [1600/20000], Training Loss: 0.007632591520502631, Validation Loss: 0.007880189151174687\n",
      "Epoch [1601/20000], Training Loss: 0.010499013698011237, Validation Loss: 0.004818377467973736\n",
      "Epoch [1602/20000], Training Loss: 0.008664851534246867, Validation Loss: 0.02927323232581474\n",
      "Epoch [1603/20000], Training Loss: 0.030456511401748036, Validation Loss: 0.012700123692220584\n",
      "Epoch [1604/20000], Training Loss: 0.061791097707880126, Validation Loss: 0.05540758503885258\n",
      "Epoch [1605/20000], Training Loss: 0.06279160970422838, Validation Loss: 0.01907342811401466\n",
      "Epoch [1606/20000], Training Loss: 0.031095521790640696, Validation Loss: 0.015381619648035803\n",
      "Epoch [1607/20000], Training Loss: 0.017504772054962814, Validation Loss: 0.016554713986292508\n",
      "Epoch [1608/20000], Training Loss: 0.014101244442697083, Validation Loss: 0.011649584103080358\n",
      "Epoch [1609/20000], Training Loss: 0.011566248223451632, Validation Loss: 0.006745344276496493\n",
      "Epoch [1610/20000], Training Loss: 0.00920016742124322, Validation Loss: 0.007570723038117718\n",
      "Epoch [1611/20000], Training Loss: 0.0254624965178404, Validation Loss: 0.011770045271289912\n",
      "Epoch [1612/20000], Training Loss: 0.020462992492736833, Validation Loss: 0.021493177266508835\n",
      "Epoch [1613/20000], Training Loss: 0.010769713819042539, Validation Loss: 0.006573357787820985\n",
      "Epoch [1614/20000], Training Loss: 0.01423710914761094, Validation Loss: 0.005314641408949673\n",
      "Epoch [1615/20000], Training Loss: 0.01694892245411341, Validation Loss: 0.011648151326957897\n",
      "Epoch [1616/20000], Training Loss: 0.019508514946989765, Validation Loss: 0.006874861572355907\n",
      "Epoch [1617/20000], Training Loss: 0.012653026442941544, Validation Loss: 0.00991720348658007\n",
      "Epoch [1618/20000], Training Loss: 0.009851675308189962, Validation Loss: 0.008316157067542507\n",
      "Epoch [1619/20000], Training Loss: 0.01907679206176129, Validation Loss: 0.00755881092048042\n",
      "Epoch [1620/20000], Training Loss: 0.043553431878016066, Validation Loss: 0.11508147491828558\n",
      "Epoch [1621/20000], Training Loss: 0.039776887046173215, Validation Loss: 0.022163826530423307\n",
      "Epoch [1622/20000], Training Loss: 0.026121613949986307, Validation Loss: 0.02007763163571581\n",
      "Epoch [1623/20000], Training Loss: 0.025118596086810743, Validation Loss: 0.02157685890768246\n",
      "Epoch [1624/20000], Training Loss: 0.01602569533445473, Validation Loss: 0.03147542655971707\n",
      "Epoch [1625/20000], Training Loss: 0.016499774297699332, Validation Loss: 0.013544913728503036\n",
      "Epoch [1626/20000], Training Loss: 0.01572453889197537, Validation Loss: 0.012068197528406288\n",
      "Epoch [1627/20000], Training Loss: 0.021976540571423748, Validation Loss: 0.024646892611991884\n",
      "Epoch [1628/20000], Training Loss: 0.014340054185595363, Validation Loss: 0.011252393109051973\n",
      "Epoch [1629/20000], Training Loss: 0.0112585923426585, Validation Loss: 0.007326593120043253\n",
      "Epoch [1630/20000], Training Loss: 0.02146999237343802, Validation Loss: 0.007502277426040273\n",
      "Epoch [1631/20000], Training Loss: 0.019641426075915142, Validation Loss: 0.008122306484471552\n",
      "Epoch [1632/20000], Training Loss: 0.01169977746238666, Validation Loss: 0.005394134694154851\n",
      "Epoch [1633/20000], Training Loss: 0.02378294024882572, Validation Loss: 0.020699182958135488\n",
      "Epoch [1634/20000], Training Loss: 0.013193460206301617, Validation Loss: 0.008774772793658287\n",
      "Epoch [1635/20000], Training Loss: 0.0158258009302829, Validation Loss: 0.006660138263800855\n",
      "Epoch [1636/20000], Training Loss: 0.02869326856827164, Validation Loss: 0.013019492765523114\n",
      "Epoch [1637/20000], Training Loss: 0.019772585814020464, Validation Loss: 0.007455717441654098\n",
      "Epoch [1638/20000], Training Loss: 0.012546521784575557, Validation Loss: 0.006481663942148226\n",
      "Epoch [1639/20000], Training Loss: 0.010744223445986531, Validation Loss: 0.012700664692093062\n",
      "Epoch [1640/20000], Training Loss: 0.009649968577182986, Validation Loss: 0.0075727112288702545\n",
      "Epoch [1641/20000], Training Loss: 0.008444137966892282, Validation Loss: 0.0065260303511325505\n",
      "Epoch [1642/20000], Training Loss: 0.006510229664854705, Validation Loss: 0.004595135294657543\n",
      "Epoch [1643/20000], Training Loss: 0.007301234836112209, Validation Loss: 0.004493291160608389\n",
      "Epoch [1644/20000], Training Loss: 0.00999816529779959, Validation Loss: 0.010202789445145414\n",
      "Epoch [1645/20000], Training Loss: 0.011626458996033762, Validation Loss: 0.021228981073672305\n",
      "Epoch [1646/20000], Training Loss: 0.010197165492822282, Validation Loss: 0.009435715454913073\n",
      "Epoch [1647/20000], Training Loss: 0.008424720078307604, Validation Loss: 0.006619725925020502\n",
      "Epoch [1648/20000], Training Loss: 0.007521214502633354, Validation Loss: 0.003394612141879271\n",
      "Epoch [1649/20000], Training Loss: 0.010646143133531691, Validation Loss: 0.009745357727427226\n",
      "Epoch [1650/20000], Training Loss: 0.009756499421720426, Validation Loss: 0.009892379041746507\n",
      "Epoch [1651/20000], Training Loss: 0.013562481858701046, Validation Loss: 0.03021800909066668\n",
      "Epoch [1652/20000], Training Loss: 0.026264350699680757, Validation Loss: 0.02278002579883574\n",
      "Epoch [1653/20000], Training Loss: 0.017938535593982254, Validation Loss: 0.05276990176750484\n",
      "Epoch [1654/20000], Training Loss: 0.0753650177794043, Validation Loss: 0.09188253152869881\n",
      "Epoch [1655/20000], Training Loss: 0.053561262640869245, Validation Loss: 0.1233653503994793\n",
      "Epoch [1656/20000], Training Loss: 0.04099799778279183, Validation Loss: 0.020107118194915594\n",
      "Epoch [1657/20000], Training Loss: 0.023771833179385533, Validation Loss: 0.011828070439762736\n",
      "Epoch [1658/20000], Training Loss: 0.018762595851772597, Validation Loss: 0.03921015292193716\n",
      "Epoch [1659/20000], Training Loss: 0.025620723109958426, Validation Loss: 0.013749593652776632\n",
      "Epoch [1660/20000], Training Loss: 0.014194135903380811, Validation Loss: 0.01380275023577591\n",
      "Epoch [1661/20000], Training Loss: 0.011819159711844154, Validation Loss: 0.007985874646176368\n",
      "Epoch [1662/20000], Training Loss: 0.019145888426075026, Validation Loss: 0.011539474241936737\n",
      "Epoch [1663/20000], Training Loss: 0.011766167330538988, Validation Loss: 0.011718409406187727\n",
      "Epoch [1664/20000], Training Loss: 0.009029249976655203, Validation Loss: 0.010914627387364944\n",
      "Epoch [1665/20000], Training Loss: 0.010849119164049625, Validation Loss: 0.008470238630999579\n",
      "Epoch [1666/20000], Training Loss: 0.018999370416395323, Validation Loss: 0.026571240967245207\n",
      "Epoch [1667/20000], Training Loss: 0.014164248483471706, Validation Loss: 0.00708464841039004\n",
      "Epoch [1668/20000], Training Loss: 0.009394619636753174, Validation Loss: 0.00634846481719772\n",
      "Epoch [1669/20000], Training Loss: 0.016347222994746908, Validation Loss: 0.022089130728458275\n",
      "Epoch [1670/20000], Training Loss: 0.015071395098597609, Validation Loss: 0.007489976757564626\n",
      "Epoch [1671/20000], Training Loss: 0.029791067543555982, Validation Loss: 0.02960532903879152\n",
      "Epoch [1672/20000], Training Loss: 0.045107620994843146, Validation Loss: 0.04394754453512312\n",
      "Epoch [1673/20000], Training Loss: 0.02828287315449174, Validation Loss: 0.015013520912784609\n",
      "Epoch [1674/20000], Training Loss: 0.01389625298439309, Validation Loss: 0.014416367931151596\n",
      "Epoch [1675/20000], Training Loss: 0.011488511640761447, Validation Loss: 0.009083637397858573\n",
      "Epoch [1676/20000], Training Loss: 0.0106796574213409, Validation Loss: 0.005787035192724944\n",
      "Epoch [1677/20000], Training Loss: 0.010738971389530758, Validation Loss: 0.008047060539078237\n",
      "Epoch [1678/20000], Training Loss: 0.016248734593578514, Validation Loss: 0.028918954554813223\n",
      "Epoch [1679/20000], Training Loss: 0.03188561953720637, Validation Loss: 0.017778740337624707\n",
      "Epoch [1680/20000], Training Loss: 0.024994299580742205, Validation Loss: 0.013323397487900889\n",
      "Epoch [1681/20000], Training Loss: 0.01066762679589114, Validation Loss: 0.009366487075430514\n",
      "Epoch [1682/20000], Training Loss: 0.006237409419294896, Validation Loss: 0.011090683651200847\n",
      "Epoch [1683/20000], Training Loss: 0.023782927131313563, Validation Loss: 0.011511481000562287\n",
      "Epoch [1684/20000], Training Loss: 0.020456356852914075, Validation Loss: 0.016780512963135433\n",
      "Epoch [1685/20000], Training Loss: 0.014999199500640057, Validation Loss: 0.018041360016429177\n",
      "Epoch [1686/20000], Training Loss: 0.01939910114534931, Validation Loss: 0.012129288913440011\n",
      "Epoch [1687/20000], Training Loss: 0.020371284094705646, Validation Loss: 0.01139416813692618\n",
      "Epoch [1688/20000], Training Loss: 0.009033414385547596, Validation Loss: 0.007433052601216871\n",
      "Epoch [1689/20000], Training Loss: 0.008806255018238776, Validation Loss: 0.006401136057862501\n",
      "Epoch [1690/20000], Training Loss: 0.006457792644921158, Validation Loss: 0.006313160153648994\n",
      "Epoch [1691/20000], Training Loss: 0.006903732789006816, Validation Loss: 0.005470488542682602\n",
      "Epoch [1692/20000], Training Loss: 0.013978805234988354, Validation Loss: 0.006416770023363633\n",
      "Epoch [1693/20000], Training Loss: 0.029633944095361846, Validation Loss: 0.031345226582491434\n",
      "Epoch [1694/20000], Training Loss: 0.02078765710965464, Validation Loss: 0.0091966408946637\n",
      "Epoch [1695/20000], Training Loss: 0.018068417103287566, Validation Loss: 0.009359060756731501\n",
      "Epoch [1696/20000], Training Loss: 0.010188869345061644, Validation Loss: 0.005530132707736877\n",
      "Epoch [1697/20000], Training Loss: 0.0095512246313904, Validation Loss: 0.014928492184329121\n",
      "Epoch [1698/20000], Training Loss: 0.007829864584242128, Validation Loss: 0.00443845694476076\n",
      "Epoch [1699/20000], Training Loss: 0.010301530729131108, Validation Loss: 0.004412828462252221\n",
      "Epoch [1700/20000], Training Loss: 0.0075301640567236715, Validation Loss: 0.0054451893797063345\n",
      "Epoch [1701/20000], Training Loss: 0.010011666774841745, Validation Loss: 0.0435554582948536\n",
      "Epoch [1702/20000], Training Loss: 0.015377393493378935, Validation Loss: 0.00730769719361433\n",
      "Epoch [1703/20000], Training Loss: 0.017293539623746516, Validation Loss: 0.037283517279783344\n",
      "Epoch [1704/20000], Training Loss: 0.019506889998508217, Validation Loss: 0.03502000316165608\n",
      "Epoch [1705/20000], Training Loss: 0.03916108136763796, Validation Loss: 0.010871954421237949\n",
      "Epoch [1706/20000], Training Loss: 0.016706475289538503, Validation Loss: 0.011160410565777332\n",
      "Epoch [1707/20000], Training Loss: 0.014037819720605122, Validation Loss: 0.008189982274823909\n",
      "Epoch [1708/20000], Training Loss: 0.010099091201222368, Validation Loss: 0.01857300679732306\n",
      "Epoch [1709/20000], Training Loss: 0.01995691813018701, Validation Loss: 0.03367255436409757\n",
      "Epoch [1710/20000], Training Loss: 0.061168474391836626, Validation Loss: 0.06516242748876065\n",
      "Epoch [1711/20000], Training Loss: 0.0407765032723546, Validation Loss: 0.01766645057019121\n",
      "Epoch [1712/20000], Training Loss: 0.02642965506363128, Validation Loss: 0.031990013636150824\n",
      "Epoch [1713/20000], Training Loss: 0.018997195758856833, Validation Loss: 0.010169725840551909\n",
      "Epoch [1714/20000], Training Loss: 0.010861747945974847, Validation Loss: 0.009365611031398796\n",
      "Epoch [1715/20000], Training Loss: 0.010622303468153405, Validation Loss: 0.020551025886519584\n",
      "Epoch [1716/20000], Training Loss: 0.011837398591783963, Validation Loss: 0.009539508098390045\n",
      "Epoch [1717/20000], Training Loss: 0.011048031388781965, Validation Loss: 0.008112745021585462\n",
      "Epoch [1718/20000], Training Loss: 0.011947608769073017, Validation Loss: 0.0120512737968332\n",
      "Epoch [1719/20000], Training Loss: 0.0135821192192712, Validation Loss: 0.015474136032496753\n",
      "Epoch [1720/20000], Training Loss: 0.016150729537652557, Validation Loss: 0.006023078551950993\n",
      "Epoch [1721/20000], Training Loss: 0.02096821595789931, Validation Loss: 0.08806574256147216\n",
      "Epoch [1722/20000], Training Loss: 0.03001922502880916, Validation Loss: 0.00953100127810534\n",
      "Epoch [1723/20000], Training Loss: 0.01157313137913921, Validation Loss: 0.0064066420045051425\n",
      "Epoch [1724/20000], Training Loss: 0.013020700854082991, Validation Loss: 0.010331837256267249\n",
      "Epoch [1725/20000], Training Loss: 0.01957526868266411, Validation Loss: 0.006361605099881861\n",
      "Epoch [1726/20000], Training Loss: 0.014333805115061946, Validation Loss: 0.017585187769517473\n",
      "Epoch [1727/20000], Training Loss: 0.014698792394483462, Validation Loss: 0.007906762053130038\n",
      "Epoch [1728/20000], Training Loss: 0.011482893333387827, Validation Loss: 0.01692721709073473\n",
      "Epoch [1729/20000], Training Loss: 0.01243745235634768, Validation Loss: 0.02921856996573818\n",
      "Epoch [1730/20000], Training Loss: 0.027337034305674024, Validation Loss: 0.037123379738093036\n",
      "Epoch [1731/20000], Training Loss: 0.021951415762941906, Validation Loss: 0.018900408839112575\n",
      "Epoch [1732/20000], Training Loss: 0.020133776216035976, Validation Loss: 0.04070672652994044\n",
      "Epoch [1733/20000], Training Loss: 0.016749837768397162, Validation Loss: 0.021866219595092964\n",
      "Epoch [1734/20000], Training Loss: 0.01584531947238637, Validation Loss: 0.021135205462744944\n",
      "Epoch [1735/20000], Training Loss: 0.022818084157604192, Validation Loss: 0.004798295037554003\n",
      "Epoch [1736/20000], Training Loss: 0.031117720911944553, Validation Loss: 0.03956589947723647\n",
      "Epoch [1737/20000], Training Loss: 0.04433341267784791, Validation Loss: 0.029478922477294504\n",
      "Epoch [1738/20000], Training Loss: 0.0197888300296784, Validation Loss: 0.021110902031950513\n",
      "Epoch [1739/20000], Training Loss: 0.01426107286741691, Validation Loss: 0.013528111775964205\n",
      "Epoch [1740/20000], Training Loss: 0.015745984012028202, Validation Loss: 0.009131226773599479\n",
      "Epoch [1741/20000], Training Loss: 0.011260468493544198, Validation Loss: 0.028397313431071818\n",
      "Epoch [1742/20000], Training Loss: 0.011853295503117676, Validation Loss: 0.011693138732105762\n",
      "Epoch [1743/20000], Training Loss: 0.01455025027618311, Validation Loss: 0.02568822445963692\n",
      "Epoch [1744/20000], Training Loss: 0.019886851501983722, Validation Loss: 0.010412316137685496\n",
      "Epoch [1745/20000], Training Loss: 0.00889271571889237, Validation Loss: 0.005666268716337236\n",
      "Epoch [1746/20000], Training Loss: 0.008179140697133594, Validation Loss: 0.005178686036224034\n",
      "Epoch [1747/20000], Training Loss: 0.013873514003145309, Validation Loss: 0.027025883194906685\n",
      "Epoch [1748/20000], Training Loss: 0.03250740248976009, Validation Loss: 0.0072643092470189435\n",
      "Epoch [1749/20000], Training Loss: 0.02197438853285608, Validation Loss: 0.008326278484999151\n",
      "Epoch [1750/20000], Training Loss: 0.018427508463251537, Validation Loss: 0.021058597822957482\n",
      "Epoch [1751/20000], Training Loss: 0.013802549410944007, Validation Loss: 0.017689636871053245\n",
      "Epoch [1752/20000], Training Loss: 0.01449790397808621, Validation Loss: 0.007706322174986646\n",
      "Epoch [1753/20000], Training Loss: 0.021188739321327636, Validation Loss: 0.009624408381065706\n",
      "Epoch [1754/20000], Training Loss: 0.014938083926348813, Validation Loss: 0.006877440653300236\n",
      "Epoch [1755/20000], Training Loss: 0.011230195165678327, Validation Loss: 0.005336517317654726\n",
      "Epoch [1756/20000], Training Loss: 0.008725990445652445, Validation Loss: 0.004303725587306037\n",
      "Epoch [1757/20000], Training Loss: 0.009570176668830754, Validation Loss: 0.007590221446077678\n",
      "Epoch [1758/20000], Training Loss: 0.011020227488448395, Validation Loss: 0.00736071942174034\n",
      "Epoch [1759/20000], Training Loss: 0.01400798830270235, Validation Loss: 0.030824560387951505\n",
      "Epoch [1760/20000], Training Loss: 0.02621818871973249, Validation Loss: 0.007928466614105623\n",
      "Epoch [1761/20000], Training Loss: 0.026022701958156955, Validation Loss: 0.012525084361284062\n",
      "Epoch [1762/20000], Training Loss: 0.015203487917980445, Validation Loss: 0.02314950373435312\n",
      "Epoch [1763/20000], Training Loss: 0.016257330779418617, Validation Loss: 0.0381580369160164\n",
      "Epoch [1764/20000], Training Loss: 0.0467373541341658, Validation Loss: 0.013110519956309696\n",
      "Epoch [1765/20000], Training Loss: 0.016097426323020563, Validation Loss: 0.016160842318413676\n",
      "Epoch [1766/20000], Training Loss: 0.015681376240016625, Validation Loss: 0.017462379920646653\n",
      "Epoch [1767/20000], Training Loss: 0.02366818973262395, Validation Loss: 0.008390835816921825\n",
      "Epoch [1768/20000], Training Loss: 0.008320415076533598, Validation Loss: 0.006771149644870739\n",
      "Epoch [1769/20000], Training Loss: 0.007425194913854024, Validation Loss: 0.005989505705023095\n",
      "Epoch [1770/20000], Training Loss: 0.007768667760371629, Validation Loss: 0.006923222896270028\n",
      "Epoch [1771/20000], Training Loss: 0.009913300746120512, Validation Loss: 0.007889079811385667\n",
      "Epoch [1772/20000], Training Loss: 0.024609252960155054, Validation Loss: 0.04333506262628363\n",
      "Epoch [1773/20000], Training Loss: 0.04358388150909117, Validation Loss: 0.008163408685114913\n",
      "Epoch [1774/20000], Training Loss: 0.014186204269727958, Validation Loss: 0.009215096984186584\n",
      "Epoch [1775/20000], Training Loss: 0.008026346636272916, Validation Loss: 0.00548999459241326\n",
      "Epoch [1776/20000], Training Loss: 0.012089276470110885, Validation Loss: 0.004992497623009397\n",
      "Epoch [1777/20000], Training Loss: 0.014274481261996439, Validation Loss: 0.005857589548116144\n",
      "Epoch [1778/20000], Training Loss: 0.009119017207662441, Validation Loss: 0.011005010724951891\n",
      "Epoch [1779/20000], Training Loss: 0.00873318825116647, Validation Loss: 0.004737672952355112\n",
      "Epoch [1780/20000], Training Loss: 0.01699269561712364, Validation Loss: 0.021488659935909853\n",
      "Epoch [1781/20000], Training Loss: 0.026265528161145215, Validation Loss: 0.020068253272516573\n",
      "Epoch [1782/20000], Training Loss: 0.024358169379411265, Validation Loss: 0.013148995629308064\n",
      "Epoch [1783/20000], Training Loss: 0.024403248025789383, Validation Loss: 0.04030605603890906\n",
      "Epoch [1784/20000], Training Loss: 0.04967267710370444, Validation Loss: 0.03787290151319704\n",
      "Epoch [1785/20000], Training Loss: 0.0988292317405077, Validation Loss: 0.019634324619854007\n",
      "Epoch [1786/20000], Training Loss: 0.056070946622639894, Validation Loss: 0.029755274818598326\n",
      "Epoch [1787/20000], Training Loss: 0.02000801365024277, Validation Loss: 0.016677091271002694\n",
      "Epoch [1788/20000], Training Loss: 0.01692653578772609, Validation Loss: 0.011862693841224903\n",
      "Epoch [1789/20000], Training Loss: 0.01427431610812034, Validation Loss: 0.010964787136861135\n",
      "Epoch [1790/20000], Training Loss: 0.010718176656934832, Validation Loss: 0.010785726479852696\n",
      "Epoch [1791/20000], Training Loss: 0.009618923053104871, Validation Loss: 0.008452361661512092\n",
      "Epoch [1792/20000], Training Loss: 0.009178324336452144, Validation Loss: 0.00617094438744713\n",
      "Epoch [1793/20000], Training Loss: 0.008994867217921865, Validation Loss: 0.007195102818248113\n",
      "Epoch [1794/20000], Training Loss: 0.005674441594335024, Validation Loss: 0.01880732019056525\n",
      "Epoch [1795/20000], Training Loss: 0.035498876222845865, Validation Loss: 0.016357531820469996\n",
      "Epoch [1796/20000], Training Loss: 0.014214328411201547, Validation Loss: 0.010336924437202617\n",
      "Epoch [1797/20000], Training Loss: 0.013672482481134856, Validation Loss: 0.005676961347340687\n",
      "Epoch [1798/20000], Training Loss: 0.012486367619463376, Validation Loss: 0.00545593363807465\n",
      "Epoch [1799/20000], Training Loss: 0.010791551695937025, Validation Loss: 0.010084419495718939\n",
      "Epoch [1800/20000], Training Loss: 0.013277930107765965, Validation Loss: 0.005869973069858759\n",
      "Epoch [1801/20000], Training Loss: 0.016649680679880215, Validation Loss: 0.0066933987476007345\n",
      "Epoch [1802/20000], Training Loss: 0.027092181096252586, Validation Loss: 0.011926920480984704\n",
      "Epoch [1803/20000], Training Loss: 0.02100793052315047, Validation Loss: 0.016651464010622816\n",
      "Epoch [1804/20000], Training Loss: 0.012272937996645592, Validation Loss: 0.008616166029627217\n",
      "Epoch [1805/20000], Training Loss: 0.013425115292193368, Validation Loss: 0.02711245094451508\n",
      "Epoch [1806/20000], Training Loss: 0.014964766646569063, Validation Loss: 0.006253518338737664\n",
      "Epoch [1807/20000], Training Loss: 0.011473403085403788, Validation Loss: 0.01477325628031157\n",
      "Epoch [1808/20000], Training Loss: 0.025032447967013077, Validation Loss: 0.01588332414944131\n",
      "Epoch [1809/20000], Training Loss: 0.01681217120234838, Validation Loss: 0.017191786602062328\n",
      "Epoch [1810/20000], Training Loss: 0.01941792307687657, Validation Loss: 0.02042127849038718\n",
      "Epoch [1811/20000], Training Loss: 0.012286429002415389, Validation Loss: 0.017201256718420228\n",
      "Epoch [1812/20000], Training Loss: 0.011729884701448359, Validation Loss: 0.011681611637259998\n",
      "Epoch [1813/20000], Training Loss: 0.008365104902103277, Validation Loss: 0.012088934940898707\n",
      "Epoch [1814/20000], Training Loss: 0.01309148840976247, Validation Loss: 0.005554674386925188\n",
      "Epoch [1815/20000], Training Loss: 0.011984706333480841, Validation Loss: 0.009198935376381718\n",
      "Epoch [1816/20000], Training Loss: 0.010754217917565256, Validation Loss: 0.008676139553958863\n",
      "Epoch [1817/20000], Training Loss: 0.010069008853731378, Validation Loss: 0.02984102585046014\n",
      "Epoch [1818/20000], Training Loss: 0.012841419138047578, Validation Loss: 0.008992140297139073\n",
      "Epoch [1819/20000], Training Loss: 0.009449164843329885, Validation Loss: 0.006468638047891864\n",
      "Epoch [1820/20000], Training Loss: 0.006328072224278003, Validation Loss: 0.008820069794019806\n",
      "Epoch [1821/20000], Training Loss: 0.022411442498975833, Validation Loss: 0.04753305936964779\n",
      "Epoch [1822/20000], Training Loss: 0.034582292615336234, Validation Loss: 0.013528313455555411\n",
      "Epoch [1823/20000], Training Loss: 0.011429177347703703, Validation Loss: 0.007536224605721703\n",
      "Epoch [1824/20000], Training Loss: 0.010416954606106239, Validation Loss: 0.006186356698487515\n",
      "Epoch [1825/20000], Training Loss: 0.012094661553939139, Validation Loss: 0.0064430793694685625\n",
      "Epoch [1826/20000], Training Loss: 0.009015500291882615, Validation Loss: 0.0046464804693586706\n",
      "Epoch [1827/20000], Training Loss: 0.008024132590175473, Validation Loss: 0.0045063974712495425\n",
      "Epoch [1828/20000], Training Loss: 0.008414814392381231, Validation Loss: 0.005358729036866669\n",
      "Epoch [1829/20000], Training Loss: 0.013497036931637143, Validation Loss: 0.00446277241657402\n",
      "Epoch [1830/20000], Training Loss: 0.013769361464645564, Validation Loss: 0.02485633214415345\n",
      "Epoch [1831/20000], Training Loss: 0.013540327565611474, Validation Loss: 0.01053098588963461\n",
      "Epoch [1832/20000], Training Loss: 0.007497468493446442, Validation Loss: 0.010070121572917829\n",
      "Epoch [1833/20000], Training Loss: 0.02457590314900569, Validation Loss: 0.004608115298911614\n",
      "Epoch [1834/20000], Training Loss: 0.01277293528024919, Validation Loss: 0.008140137369896251\n",
      "Epoch [1835/20000], Training Loss: 0.01048525087701689, Validation Loss: 0.008644110141167066\n",
      "Epoch [1836/20000], Training Loss: 0.019014873830851035, Validation Loss: 0.02457661167159676\n",
      "Epoch [1837/20000], Training Loss: 0.02119851012581161, Validation Loss: 0.0076316985835465975\n",
      "Epoch [1838/20000], Training Loss: 0.013474668367832367, Validation Loss: 0.017601158630209186\n",
      "Epoch [1839/20000], Training Loss: 0.023695175014186783, Validation Loss: 0.025878767199442165\n",
      "Epoch [1840/20000], Training Loss: 0.04103761964610645, Validation Loss: 0.1100539892465046\n",
      "Epoch [1841/20000], Training Loss: 0.0933762383044398, Validation Loss: 0.060191022125506526\n",
      "Epoch [1842/20000], Training Loss: 0.062849582974325, Validation Loss: 0.04589044652012001\n",
      "Epoch [1843/20000], Training Loss: 0.034643319369934034, Validation Loss: 0.01723438829328095\n",
      "Epoch [1844/20000], Training Loss: 0.018211943296981708, Validation Loss: 0.028676679344933086\n",
      "Epoch [1845/20000], Training Loss: 0.023211197112686932, Validation Loss: 0.012110877758151018\n",
      "Epoch [1846/20000], Training Loss: 0.01878830830433539, Validation Loss: 0.011683879537295744\n",
      "Epoch [1847/20000], Training Loss: 0.011608968621918134, Validation Loss: 0.01195986141398667\n",
      "Epoch [1848/20000], Training Loss: 0.010760530638175883, Validation Loss: 0.015108553015249779\n",
      "Epoch [1849/20000], Training Loss: 0.016274114690272005, Validation Loss: 0.03113817523182862\n",
      "Epoch [1850/20000], Training Loss: 0.03374890450920377, Validation Loss: 0.025960873044049038\n",
      "Epoch [1851/20000], Training Loss: 0.026723762483535602, Validation Loss: 0.03587562632355506\n",
      "Epoch [1852/20000], Training Loss: 0.025661914933672442, Validation Loss: 0.05713185968612119\n",
      "Epoch [1853/20000], Training Loss: 0.037035699118860066, Validation Loss: 0.02392182138618994\n",
      "Epoch [1854/20000], Training Loss: 0.02700411367030548, Validation Loss: 0.01726495820269485\n",
      "Epoch [1855/20000], Training Loss: 0.013424067259100931, Validation Loss: 0.01030172175645053\n",
      "Epoch [1856/20000], Training Loss: 0.02959586475377104, Validation Loss: 0.013327053303669583\n",
      "Epoch [1857/20000], Training Loss: 0.03583069923999054, Validation Loss: 0.018204153320458296\n",
      "Epoch [1858/20000], Training Loss: 0.021617836568371525, Validation Loss: 0.018884823373820074\n",
      "Epoch [1859/20000], Training Loss: 0.014636201131257362, Validation Loss: 0.023745098822769047\n",
      "Epoch [1860/20000], Training Loss: 0.01431316977167236, Validation Loss: 0.008510790203689516\n",
      "Epoch [1861/20000], Training Loss: 0.00835828308481723, Validation Loss: 0.006996588533167964\n",
      "Epoch [1862/20000], Training Loss: 0.009528147464152426, Validation Loss: 0.00812016434580162\n",
      "Epoch [1863/20000], Training Loss: 0.015153581060336105, Validation Loss: 0.008353349814456124\n",
      "Epoch [1864/20000], Training Loss: 0.010718462938841964, Validation Loss: 0.017312983071763353\n",
      "Epoch [1865/20000], Training Loss: 0.018024421005975455, Validation Loss: 0.007957950603380734\n",
      "Epoch [1866/20000], Training Loss: 0.01938885620412683, Validation Loss: 0.006415293154304891\n",
      "Epoch [1867/20000], Training Loss: 0.007685517370191519, Validation Loss: 0.0068007632868944\n",
      "Epoch [1868/20000], Training Loss: 0.01153108021078099, Validation Loss: 0.004718739581151797\n",
      "Epoch [1869/20000], Training Loss: 0.0219485548981798, Validation Loss: 0.10052376815219193\n",
      "Epoch [1870/20000], Training Loss: 0.05455911099228875, Validation Loss: 0.022257416990996655\n",
      "Epoch [1871/20000], Training Loss: 0.030831153176092942, Validation Loss: 0.020151587620684537\n",
      "Epoch [1872/20000], Training Loss: 0.024562382742130597, Validation Loss: 0.009301717650002827\n",
      "Epoch [1873/20000], Training Loss: 0.020316540612839162, Validation Loss: 0.011110877481225348\n",
      "Epoch [1874/20000], Training Loss: 0.018573072830414667, Validation Loss: 0.00824790947586226\n",
      "Epoch [1875/20000], Training Loss: 0.013312014801028584, Validation Loss: 0.011803329405735141\n",
      "Epoch [1876/20000], Training Loss: 0.008738834383880853, Validation Loss: 0.008800934836375182\n",
      "Epoch [1877/20000], Training Loss: 0.009522575668857567, Validation Loss: 0.011040022325037821\n",
      "Epoch [1878/20000], Training Loss: 0.010611446886157085, Validation Loss: 0.007635601446054352\n",
      "Epoch [1879/20000], Training Loss: 0.008997079379956372, Validation Loss: 0.008591945540079646\n",
      "Epoch [1880/20000], Training Loss: 0.009408658863061905, Validation Loss: 0.0054712871085728565\n",
      "Epoch [1881/20000], Training Loss: 0.009431852476804383, Validation Loss: 0.007628404732567603\n",
      "Epoch [1882/20000], Training Loss: 0.00810515016213945, Validation Loss: 0.014307899518696234\n",
      "Epoch [1883/20000], Training Loss: 0.010675476394161316, Validation Loss: 0.014037245267380578\n",
      "Epoch [1884/20000], Training Loss: 0.012698296977240326, Validation Loss: 0.06126414146638126\n",
      "Epoch [1885/20000], Training Loss: 0.06642012330017419, Validation Loss: 0.08540733673726209\n",
      "Epoch [1886/20000], Training Loss: 0.04423918403751616, Validation Loss: 0.015183952776366891\n",
      "Epoch [1887/20000], Training Loss: 0.019130677111888872, Validation Loss: 0.009551834325766149\n",
      "Epoch [1888/20000], Training Loss: 0.01677069254219532, Validation Loss: 0.010556206002991278\n",
      "Epoch [1889/20000], Training Loss: 0.011770783590951137, Validation Loss: 0.010257507250214681\n",
      "Epoch [1890/20000], Training Loss: 0.009644986304920167, Validation Loss: 0.007223944099062939\n",
      "Epoch [1891/20000], Training Loss: 0.01057078326786203, Validation Loss: 0.00811280066110547\n",
      "Epoch [1892/20000], Training Loss: 0.010845350550620683, Validation Loss: 0.008858368150739196\n",
      "Epoch [1893/20000], Training Loss: 0.009894335641677441, Validation Loss: 0.01657747728977426\n",
      "Epoch [1894/20000], Training Loss: 0.011980813661856311, Validation Loss: 0.010498867574751936\n",
      "Epoch [1895/20000], Training Loss: 0.010468398578398461, Validation Loss: 0.014671854388099006\n",
      "Epoch [1896/20000], Training Loss: 0.01004284861431058, Validation Loss: 0.00877273351760063\n",
      "Epoch [1897/20000], Training Loss: 0.007052859948349318, Validation Loss: 0.005456712384102502\n",
      "Epoch [1898/20000], Training Loss: 0.009608290565665811, Validation Loss: 0.018523710901775087\n",
      "Epoch [1899/20000], Training Loss: 0.0315844151726716, Validation Loss: 0.07530672256412581\n",
      "Epoch [1900/20000], Training Loss: 0.08142329272232018, Validation Loss: 0.05391543065204855\n",
      "Epoch [1901/20000], Training Loss: 0.052868275537288616, Validation Loss: 0.06623432220229526\n",
      "Epoch [1902/20000], Training Loss: 0.048603281703045856, Validation Loss: 0.05586446693229975\n",
      "Epoch [1903/20000], Training Loss: 0.04325098105307136, Validation Loss: 0.017273282699769816\n",
      "Epoch [1904/20000], Training Loss: 0.01985872207608606, Validation Loss: 0.01566623153235826\n",
      "Epoch [1905/20000], Training Loss: 0.01448696999744113, Validation Loss: 0.013628374306047622\n",
      "Epoch [1906/20000], Training Loss: 0.010649722668209247, Validation Loss: 0.007159974956956321\n",
      "Epoch [1907/20000], Training Loss: 0.010371286038213563, Validation Loss: 0.005461358568307365\n",
      "Epoch [1908/20000], Training Loss: 0.010492032526339503, Validation Loss: 0.008265065925867463\n",
      "Epoch [1909/20000], Training Loss: 0.008952800796708158, Validation Loss: 0.008214611117029591\n",
      "Epoch [1910/20000], Training Loss: 0.007512471350699863, Validation Loss: 0.006965880726666437\n",
      "Epoch [1911/20000], Training Loss: 0.013648187873643889, Validation Loss: 0.004991474574364726\n",
      "Epoch [1912/20000], Training Loss: 0.09965489188159284, Validation Loss: 0.0848180217030274\n",
      "Epoch [1913/20000], Training Loss: 0.030195914025950645, Validation Loss: 0.022209521248327462\n",
      "Epoch [1914/20000], Training Loss: 0.028050313287946795, Validation Loss: 0.035833266963575174\n",
      "Epoch [1915/20000], Training Loss: 0.020572005970669643, Validation Loss: 0.01440477949654853\n",
      "Epoch [1916/20000], Training Loss: 0.012939524470961519, Validation Loss: 0.010387612211655515\n",
      "Epoch [1917/20000], Training Loss: 0.009735491520197164, Validation Loss: 0.011821570727687051\n",
      "Epoch [1918/20000], Training Loss: 0.011570196814968117, Validation Loss: 0.008539067759817885\n",
      "Epoch [1919/20000], Training Loss: 0.007930793225698705, Validation Loss: 0.006944020270712813\n",
      "Epoch [1920/20000], Training Loss: 0.007628356522348311, Validation Loss: 0.01361017136170112\n",
      "Epoch [1921/20000], Training Loss: 0.01774448702105604, Validation Loss: 0.02906013202233661\n",
      "Epoch [1922/20000], Training Loss: 0.018303711615901972, Validation Loss: 0.011597351721159261\n",
      "Epoch [1923/20000], Training Loss: 0.028101387182167464, Validation Loss: 0.01107492879529153\n",
      "Epoch [1924/20000], Training Loss: 0.01914131758426915, Validation Loss: 0.019356896481440348\n",
      "Epoch [1925/20000], Training Loss: 0.013617596152471378, Validation Loss: 0.0069599103605118015\n",
      "Epoch [1926/20000], Training Loss: 0.010764355288951524, Validation Loss: 0.007123660744274376\n",
      "Epoch [1927/20000], Training Loss: 0.009662791454632367, Validation Loss: 0.009593750444083135\n",
      "Epoch [1928/20000], Training Loss: 0.011530525599872428, Validation Loss: 0.005474776116118789\n",
      "Epoch [1929/20000], Training Loss: 0.007420584576071373, Validation Loss: 0.0065590630712385415\n",
      "Epoch [1930/20000], Training Loss: 0.010996839201782547, Validation Loss: 0.005000856089476949\n",
      "Epoch [1931/20000], Training Loss: 0.006320011785386929, Validation Loss: 0.00728372727728909\n",
      "Epoch [1932/20000], Training Loss: 0.011486068247385057, Validation Loss: 0.005854748345784959\n",
      "Epoch [1933/20000], Training Loss: 0.012218755088854647, Validation Loss: 0.005027078190486081\n",
      "Epoch [1934/20000], Training Loss: 0.019362953221259107, Validation Loss: 0.009780748049057117\n",
      "Epoch [1935/20000], Training Loss: 0.02053979884034821, Validation Loss: 0.024680280212643485\n",
      "Epoch [1936/20000], Training Loss: 0.07887934510758246, Validation Loss: 0.014897751310075298\n",
      "Epoch [1937/20000], Training Loss: 0.06188156039986227, Validation Loss: 0.06511439533690554\n",
      "Epoch [1938/20000], Training Loss: 0.053821407757433395, Validation Loss: 0.02132363123696662\n",
      "Epoch [1939/20000], Training Loss: 0.029145467502530664, Validation Loss: 0.01785162212750839\n",
      "Epoch [1940/20000], Training Loss: 0.017002996423148682, Validation Loss: 0.013477089459813218\n",
      "Epoch [1941/20000], Training Loss: 0.010419088399170764, Validation Loss: 0.007110096627859186\n",
      "Epoch [1942/20000], Training Loss: 0.0075772673922723955, Validation Loss: 0.004247837455462737\n",
      "Epoch [1943/20000], Training Loss: 0.007401936448721764, Validation Loss: 0.00497324773922843\n",
      "Epoch [1944/20000], Training Loss: 0.012807641976646014, Validation Loss: 0.0036372387908816983\n",
      "Epoch [1945/20000], Training Loss: 0.007838490478856588, Validation Loss: 0.006084781598160642\n",
      "Epoch [1946/20000], Training Loss: 0.008002480671685979, Validation Loss: 0.016808048710761248\n",
      "Epoch [1947/20000], Training Loss: 0.011207850681525347, Validation Loss: 0.007141549744305318\n",
      "Epoch [1948/20000], Training Loss: 0.023650489737130038, Validation Loss: 0.026915474354944506\n",
      "Epoch [1949/20000], Training Loss: 0.02906381353802447, Validation Loss: 0.026025351111473412\n",
      "Epoch [1950/20000], Training Loss: 0.02635223848795119, Validation Loss: 0.016595756849625644\n",
      "Epoch [1951/20000], Training Loss: 0.01584530571043225, Validation Loss: 0.05178323365349714\n",
      "Epoch [1952/20000], Training Loss: 0.031785506821636646, Validation Loss: 0.03522391928803453\n",
      "Epoch [1953/20000], Training Loss: 0.028788661810852188, Validation Loss: 0.013927899273856554\n",
      "Epoch [1954/20000], Training Loss: 0.018659878888034394, Validation Loss: 0.010824278503621386\n",
      "Epoch [1955/20000], Training Loss: 0.011134554981254041, Validation Loss: 0.008229908248166967\n",
      "Epoch [1956/20000], Training Loss: 0.010205812906081389, Validation Loss: 0.005641125669862356\n",
      "Epoch [1957/20000], Training Loss: 0.01834291512126843, Validation Loss: 0.033425208384784535\n",
      "Epoch [1958/20000], Training Loss: 0.026938008799334057, Validation Loss: 0.07767491915009307\n",
      "Epoch [1959/20000], Training Loss: 0.03753679624060169, Validation Loss: 0.01793920533142233\n",
      "Epoch [1960/20000], Training Loss: 0.015153914889586824, Validation Loss: 0.01246631079359478\n",
      "Epoch [1961/20000], Training Loss: 0.012842069101420097, Validation Loss: 0.013960618758630803\n",
      "Epoch [1962/20000], Training Loss: 0.013170960076552416, Validation Loss: 0.008052189985612777\n",
      "Epoch [1963/20000], Training Loss: 0.011659285797837324, Validation Loss: 0.014812066970213755\n",
      "Epoch [1964/20000], Training Loss: 0.012526761258154042, Validation Loss: 0.017029807931460705\n",
      "Epoch [1965/20000], Training Loss: 0.015928336610418876, Validation Loss: 0.008898631963999712\n",
      "Epoch [1966/20000], Training Loss: 0.028179786635778976, Validation Loss: 0.023309066389370282\n",
      "Epoch [1967/20000], Training Loss: 0.042823971266833336, Validation Loss: 0.016127797025709916\n",
      "Epoch [1968/20000], Training Loss: 0.017740648763719946, Validation Loss: 0.010360143779280998\n",
      "Epoch [1969/20000], Training Loss: 0.014102295373699494, Validation Loss: 0.010624746185466908\n",
      "Epoch [1970/20000], Training Loss: 0.01421608867323292, Validation Loss: 0.007926324974278543\n",
      "Epoch [1971/20000], Training Loss: 0.013169621200566846, Validation Loss: 0.009233113105697823\n",
      "Epoch [1972/20000], Training Loss: 0.010959474336622017, Validation Loss: 0.007169056103781026\n",
      "Epoch [1973/20000], Training Loss: 0.010247633064864203, Validation Loss: 0.00890808300339684\n",
      "Epoch [1974/20000], Training Loss: 0.010038527024724837, Validation Loss: 0.0072494243197850005\n",
      "Epoch [1975/20000], Training Loss: 0.007265871112135106, Validation Loss: 0.0061884112665211485\n",
      "Epoch [1976/20000], Training Loss: 0.008795705274678767, Validation Loss: 0.009242504334273302\n",
      "Epoch [1977/20000], Training Loss: 0.008969491442258004, Validation Loss: 0.004682402263499958\n",
      "Epoch [1978/20000], Training Loss: 0.005983161041805423, Validation Loss: 0.005984626873393091\n",
      "Epoch [1979/20000], Training Loss: 0.013831584796987175, Validation Loss: 0.018452247571709844\n",
      "Epoch [1980/20000], Training Loss: 0.018779515006047274, Validation Loss: 0.0060960808857219466\n",
      "Epoch [1981/20000], Training Loss: 0.012089205153253195, Validation Loss: 0.004080500777880459\n",
      "Epoch [1982/20000], Training Loss: 0.01028590312385599, Validation Loss: 0.014494212492097957\n",
      "Epoch [1983/20000], Training Loss: 0.00802707710265947, Validation Loss: 0.01948040465154983\n",
      "Epoch [1984/20000], Training Loss: 0.011863918864946754, Validation Loss: 0.08533736045204444\n",
      "Epoch [1985/20000], Training Loss: 0.02135380663613822, Validation Loss: 0.07140746696226431\n",
      "Epoch [1986/20000], Training Loss: 0.05630674941598305, Validation Loss: 0.043096410276744886\n",
      "Epoch [1987/20000], Training Loss: 0.05092138569645093, Validation Loss: 0.045157835708098205\n",
      "Epoch [1988/20000], Training Loss: 0.039000781230112934, Validation Loss: 0.013488548837326392\n",
      "Epoch [1989/20000], Training Loss: 0.014425443718209863, Validation Loss: 0.011065789780543497\n",
      "Epoch [1990/20000], Training Loss: 0.010864120737616239, Validation Loss: 0.009584190089510601\n",
      "Epoch [1991/20000], Training Loss: 0.010192699880073113, Validation Loss: 0.007625337227831812\n",
      "Epoch [1992/20000], Training Loss: 0.008824549848213792, Validation Loss: 0.006679581164592363\n",
      "Epoch [1993/20000], Training Loss: 0.007839520054403692, Validation Loss: 0.00904921408852845\n",
      "Epoch [1994/20000], Training Loss: 0.013106950433991318, Validation Loss: 0.0086042320965914\n",
      "Epoch [1995/20000], Training Loss: 0.017782818122733652, Validation Loss: 0.012204031482334747\n",
      "Epoch [1996/20000], Training Loss: 0.010784668003907427, Validation Loss: 0.01031588023105152\n",
      "Epoch [1997/20000], Training Loss: 0.011619358456560544, Validation Loss: 0.020029349570612403\n",
      "Epoch [1998/20000], Training Loss: 0.011490226313721255, Validation Loss: 0.027803618002248447\n",
      "Epoch [1999/20000], Training Loss: 0.02077805863310849, Validation Loss: 0.01437410808070386\n",
      "Epoch [2000/20000], Training Loss: 0.02258041721610685, Validation Loss: 0.015436413826656852\n",
      "Epoch [2001/20000], Training Loss: 0.0266241179228278, Validation Loss: 0.015506899671371295\n",
      "Epoch [2002/20000], Training Loss: 0.012912879799841903, Validation Loss: 0.009310346991047222\n",
      "Epoch [2003/20000], Training Loss: 0.013137733829872949, Validation Loss: 0.007601561240660105\n",
      "Epoch [2004/20000], Training Loss: 0.011663019465881266, Validation Loss: 0.010323456644917834\n",
      "Epoch [2005/20000], Training Loss: 0.00694384751227517, Validation Loss: 0.010472277992188797\n",
      "Epoch [2006/20000], Training Loss: 0.018777738950510475, Validation Loss: 0.022450582291639876\n",
      "Epoch [2007/20000], Training Loss: 0.017949809131096117, Validation Loss: 0.031537288034087396\n",
      "Epoch [2008/20000], Training Loss: 0.021954119263682514, Validation Loss: 0.016259820880674982\n",
      "Epoch [2009/20000], Training Loss: 0.020592576637032574, Validation Loss: 0.03031684294455772\n",
      "Epoch [2010/20000], Training Loss: 0.031241065905695514, Validation Loss: 0.01714668401496103\n",
      "Epoch [2011/20000], Training Loss: 0.03426278643122974, Validation Loss: 0.010972638688794436\n",
      "Epoch [2012/20000], Training Loss: 0.013492748906303729, Validation Loss: 0.012092522648426482\n",
      "Epoch [2013/20000], Training Loss: 0.011578653307099427, Validation Loss: 0.008451418126371988\n",
      "Epoch [2014/20000], Training Loss: 0.013998025874440958, Validation Loss: 0.00734885985782187\n",
      "Epoch [2015/20000], Training Loss: 0.009433333603997849, Validation Loss: 0.006687447926586694\n",
      "Epoch [2016/20000], Training Loss: 0.009866103671291577, Validation Loss: 0.008517752023180947\n",
      "Epoch [2017/20000], Training Loss: 0.01078677746665692, Validation Loss: 0.010604276095234257\n",
      "Epoch [2018/20000], Training Loss: 0.008958068144108568, Validation Loss: 0.00643162125249676\n",
      "Epoch [2019/20000], Training Loss: 0.013837130068558119, Validation Loss: 0.018244212839428137\n",
      "Epoch [2020/20000], Training Loss: 0.00948650626482309, Validation Loss: 0.004870255164860282\n",
      "Epoch [2021/20000], Training Loss: 0.006600934800059933, Validation Loss: 0.007733252632988297\n",
      "Epoch [2022/20000], Training Loss: 0.007061240219627507, Validation Loss: 0.02101082776919481\n",
      "Epoch [2023/20000], Training Loss: 0.026708311758868928, Validation Loss: 0.04722577411231604\n",
      "Epoch [2024/20000], Training Loss: 0.04356880945852026, Validation Loss: 0.014064258666004597\n",
      "Epoch [2025/20000], Training Loss: 0.014140347850375943, Validation Loss: 0.006123157606842662\n",
      "Epoch [2026/20000], Training Loss: 0.019652079489398084, Validation Loss: 0.028534119465413305\n",
      "Epoch [2027/20000], Training Loss: 0.027948965418285558, Validation Loss: 0.0057387967431818\n",
      "Epoch [2028/20000], Training Loss: 0.03327095928917905, Validation Loss: 0.01071100434013975\n",
      "Epoch [2029/20000], Training Loss: 0.018465806264430285, Validation Loss: 0.017283809125033706\n",
      "Epoch [2030/20000], Training Loss: 0.023102685077381984, Validation Loss: 0.020526429853297095\n",
      "Epoch [2031/20000], Training Loss: 0.01645438837087048, Validation Loss: 0.016986763037624808\n",
      "Epoch [2032/20000], Training Loss: 0.03674976542242803, Validation Loss: 0.02806661814788772\n",
      "Epoch [2033/20000], Training Loss: 0.045979942028809874, Validation Loss: 0.014349036920667437\n",
      "Epoch [2034/20000], Training Loss: 0.029790099876533662, Validation Loss: 0.010223935192909634\n",
      "Epoch [2035/20000], Training Loss: 0.016694721749185452, Validation Loss: 0.009951299788943808\n",
      "Epoch [2036/20000], Training Loss: 0.011055527745546507, Validation Loss: 0.012789586870764782\n",
      "Epoch [2037/20000], Training Loss: 0.013508328502731664, Validation Loss: 0.018493033398824212\n",
      "Epoch [2038/20000], Training Loss: 0.018033361906418577, Validation Loss: 0.010649699407492465\n",
      "Epoch [2039/20000], Training Loss: 0.009056001111665475, Validation Loss: 0.008034700347187547\n",
      "Epoch [2040/20000], Training Loss: 0.01391764616346336, Validation Loss: 0.008150327027189113\n",
      "Epoch [2041/20000], Training Loss: 0.007920047484471329, Validation Loss: 0.008461903833222095\n",
      "Epoch [2042/20000], Training Loss: 0.009544040409049817, Validation Loss: 0.01587146556343149\n",
      "Epoch [2043/20000], Training Loss: 0.013654058873986028, Validation Loss: 0.027171452297385237\n",
      "Epoch [2044/20000], Training Loss: 0.012569501014825488, Validation Loss: 0.008414823666157645\n",
      "Epoch [2045/20000], Training Loss: 0.009061639654516642, Validation Loss: 0.005563592022992582\n",
      "Epoch [2046/20000], Training Loss: 0.0090477584162727, Validation Loss: 0.004352358787351867\n",
      "Epoch [2047/20000], Training Loss: 0.008115859484990193, Validation Loss: 0.010168541990651647\n",
      "Epoch [2048/20000], Training Loss: 0.01123483272177899, Validation Loss: 0.00477415800730796\n",
      "Epoch [2049/20000], Training Loss: 0.012701771613235126, Validation Loss: 0.01213886063660701\n",
      "Epoch [2050/20000], Training Loss: 0.022480817179062536, Validation Loss: 0.0060280822230213615\n",
      "Epoch [2051/20000], Training Loss: 0.015777777319142063, Validation Loss: 0.014630015068864364\n",
      "Epoch [2052/20000], Training Loss: 0.02113943269068841, Validation Loss: 0.07022636250867212\n",
      "Epoch [2053/20000], Training Loss: 0.03030282729222173, Validation Loss: 0.044031509635276835\n",
      "Epoch [2054/20000], Training Loss: 0.02160336354115446, Validation Loss: 0.050310763264459775\n",
      "Epoch [2055/20000], Training Loss: 0.0617088892862999, Validation Loss: 0.10538955126563719\n",
      "Epoch [2056/20000], Training Loss: 0.12370831768826715, Validation Loss: 0.10103282541834904\n",
      "Epoch [2057/20000], Training Loss: 0.059954882000706027, Validation Loss: 0.032985035294976636\n",
      "Epoch [2058/20000], Training Loss: 0.03332545591651329, Validation Loss: 0.019524735808543272\n",
      "Epoch [2059/20000], Training Loss: 0.03793188895880511, Validation Loss: 0.1067593575657949\n",
      "Epoch [2060/20000], Training Loss: 0.11349515364106212, Validation Loss: 0.07601839688459165\n",
      "Epoch [2061/20000], Training Loss: 0.05005574119942529, Validation Loss: 0.038944226358062904\n",
      "Epoch [2062/20000], Training Loss: 0.026441593604561473, Validation Loss: 0.02669404638829443\n",
      "Epoch [2063/20000], Training Loss: 0.01716278592773181, Validation Loss: 0.006848715029520251\n",
      "Epoch [2064/20000], Training Loss: 0.010490597523000491, Validation Loss: 0.05400723140385748\n",
      "Epoch [2065/20000], Training Loss: 0.01757374586824361, Validation Loss: 0.016381932091476058\n",
      "Epoch [2066/20000], Training Loss: 0.01643138179653241, Validation Loss: 0.009751445977245612\n",
      "Epoch [2067/20000], Training Loss: 0.009842511973277266, Validation Loss: 0.006862087055245902\n",
      "Epoch [2068/20000], Training Loss: 0.005530089882086031, Validation Loss: 0.009798919372236278\n",
      "Epoch [2069/20000], Training Loss: 0.00980522234338618, Validation Loss: 0.027842961439935938\n",
      "Epoch [2070/20000], Training Loss: 0.010761567349878274, Validation Loss: 0.0177782848588188\n",
      "Epoch [2071/20000], Training Loss: 0.009685067587724916, Validation Loss: 0.026214561934969714\n",
      "Epoch [2072/20000], Training Loss: 0.03733987067101095, Validation Loss: 0.01135553166241765\n",
      "Epoch [2073/20000], Training Loss: 0.02368921760769029, Validation Loss: 0.009242967547733507\n",
      "Epoch [2074/20000], Training Loss: 0.01151353179011494, Validation Loss: 0.00995520621870245\n",
      "Epoch [2075/20000], Training Loss: 0.009021926696212696, Validation Loss: 0.007506034322928727\n",
      "Epoch [2076/20000], Training Loss: 0.009626153294188302, Validation Loss: 0.010437539410479051\n",
      "Epoch [2077/20000], Training Loss: 0.014140720166843883, Validation Loss: 0.007891605722334976\n",
      "Epoch [2078/20000], Training Loss: 0.0081225485422952, Validation Loss: 0.013230606124262833\n",
      "Epoch [2079/20000], Training Loss: 0.019657604716485366, Validation Loss: 0.057052658516572465\n",
      "Epoch [2080/20000], Training Loss: 0.03858087972808529, Validation Loss: 0.06790260827548136\n",
      "Epoch [2081/20000], Training Loss: 0.04756921779231301, Validation Loss: 0.02307541455556392\n",
      "Epoch [2082/20000], Training Loss: 0.03601632744539529, Validation Loss: 0.0665816193216758\n",
      "Epoch [2083/20000], Training Loss: 0.036857735798028965, Validation Loss: 0.026493824305739135\n",
      "Epoch [2084/20000], Training Loss: 0.027538182014333352, Validation Loss: 0.028078773264146182\n",
      "Epoch [2085/20000], Training Loss: 0.01632712144471173, Validation Loss: 0.011839467543778546\n",
      "Epoch [2086/20000], Training Loss: 0.012649727486340063, Validation Loss: 0.0105679748397237\n",
      "Epoch [2087/20000], Training Loss: 0.019562922393171384, Validation Loss: 0.007792693650482077\n",
      "Epoch [2088/20000], Training Loss: 0.026214445874627148, Validation Loss: 0.0106574485522192\n",
      "Epoch [2089/20000], Training Loss: 0.016623600785221373, Validation Loss: 0.01446561844580629\n",
      "Epoch [2090/20000], Training Loss: 0.01179460680162135, Validation Loss: 0.024357261742813746\n",
      "Epoch [2091/20000], Training Loss: 0.013458874403296153, Validation Loss: 0.014266074400188357\n",
      "Epoch [2092/20000], Training Loss: 0.0106791525956526, Validation Loss: 0.03559761563705522\n",
      "Epoch [2093/20000], Training Loss: 0.009061914619191416, Validation Loss: 0.00759190304667821\n",
      "Epoch [2094/20000], Training Loss: 0.007229487118560688, Validation Loss: 0.008614202019762734\n",
      "Epoch [2095/20000], Training Loss: 0.011993227419485006, Validation Loss: 0.0076377178057591855\n",
      "Epoch [2096/20000], Training Loss: 0.021248823044256175, Validation Loss: 0.03302472308915574\n",
      "Epoch [2097/20000], Training Loss: 0.027963179529511502, Validation Loss: 0.013864467607415233\n",
      "Epoch [2098/20000], Training Loss: 0.012228417867195926, Validation Loss: 0.0085049191118397\n",
      "Epoch [2099/20000], Training Loss: 0.011120484052558563, Validation Loss: 0.010530402320428284\n",
      "Epoch [2100/20000], Training Loss: 0.014502509702713542, Validation Loss: 0.008535006387258296\n",
      "Epoch [2101/20000], Training Loss: 0.009534349735726469, Validation Loss: 0.005872594640825005\n",
      "Epoch [2102/20000], Training Loss: 0.00922562222876877, Validation Loss: 0.016818873929434753\n",
      "Epoch [2103/20000], Training Loss: 0.014826391940005124, Validation Loss: 0.012421804994026538\n",
      "Epoch [2104/20000], Training Loss: 0.01960258966677689, Validation Loss: 0.014758490247415356\n",
      "Epoch [2105/20000], Training Loss: 0.019019210415925563, Validation Loss: 0.011792533993470909\n",
      "Epoch [2106/20000], Training Loss: 0.015581818499153346, Validation Loss: 0.012817396603506193\n",
      "Epoch [2107/20000], Training Loss: 0.018619597190991044, Validation Loss: 0.03939964234292286\n",
      "Epoch [2108/20000], Training Loss: 0.023170706132077612, Validation Loss: 0.008481465111945332\n",
      "Epoch [2109/20000], Training Loss: 0.013134936897716085, Validation Loss: 0.009379674965079517\n",
      "Epoch [2110/20000], Training Loss: 0.00955197012184986, Validation Loss: 0.006142990295568268\n",
      "Epoch [2111/20000], Training Loss: 0.015292280320344227, Validation Loss: 0.014627705600501003\n",
      "Epoch [2112/20000], Training Loss: 0.017524036128764107, Validation Loss: 0.02484477521758782\n",
      "Epoch [2113/20000], Training Loss: 0.012921514902180726, Validation Loss: 0.025380217711899803\n",
      "Epoch [2114/20000], Training Loss: 0.013303574020808031, Validation Loss: 0.0046727306319451056\n",
      "Epoch [2115/20000], Training Loss: 0.009145990273931861, Validation Loss: 0.005780369385906712\n",
      "Epoch [2116/20000], Training Loss: 0.016603889492606477, Validation Loss: 0.04397452740705658\n",
      "Epoch [2117/20000], Training Loss: 0.03852240283075454, Validation Loss: 0.05434932596053834\n",
      "Epoch [2118/20000], Training Loss: 0.06719454434434217, Validation Loss: 0.17908435904213155\n",
      "Epoch [2119/20000], Training Loss: 0.0863437445701233, Validation Loss: 0.08308031853208021\n",
      "Epoch [2120/20000], Training Loss: 0.0657694365363568, Validation Loss: 0.020997721068697368\n",
      "Epoch [2121/20000], Training Loss: 0.02390123002364167, Validation Loss: 0.018620003341112225\n",
      "Epoch [2122/20000], Training Loss: 0.016713133813547238, Validation Loss: 0.014379417291099315\n",
      "Epoch [2123/20000], Training Loss: 0.015326312576819743, Validation Loss: 0.014642874045179683\n",
      "Epoch [2124/20000], Training Loss: 0.022600621982876743, Validation Loss: 0.006793393327635714\n",
      "Epoch [2125/20000], Training Loss: 0.01810743165801146, Validation Loss: 0.010892619398982993\n",
      "Epoch [2126/20000], Training Loss: 0.02099461863898406, Validation Loss: 0.014223801733883986\n",
      "Epoch [2127/20000], Training Loss: 0.017673916150150553, Validation Loss: 0.022535419346778763\n",
      "Epoch [2128/20000], Training Loss: 0.01567613088159955, Validation Loss: 0.009663700010215224\n",
      "Epoch [2129/20000], Training Loss: 0.014091151600171412, Validation Loss: 0.009678377018612882\n",
      "Epoch [2130/20000], Training Loss: 0.009585512739639463, Validation Loss: 0.006344072352122068\n",
      "Epoch [2131/20000], Training Loss: 0.0265295004948192, Validation Loss: 0.014227227573338086\n",
      "Epoch [2132/20000], Training Loss: 0.021075395713393976, Validation Loss: 0.010843558640122222\n",
      "Epoch [2133/20000], Training Loss: 0.011895606402374272, Validation Loss: 0.01121342083167548\n",
      "Epoch [2134/20000], Training Loss: 0.011561557176589434, Validation Loss: 0.009557357012297402\n",
      "Epoch [2135/20000], Training Loss: 0.012450962327420712, Validation Loss: 0.00755992238372642\n",
      "Epoch [2136/20000], Training Loss: 0.011748911929316819, Validation Loss: 0.009913359861241755\n",
      "Epoch [2137/20000], Training Loss: 0.015478840155992657, Validation Loss: 0.007063427620923944\n",
      "Epoch [2138/20000], Training Loss: 0.012126169672098643, Validation Loss: 0.014189521629569182\n",
      "Epoch [2139/20000], Training Loss: 0.018771509868591756, Validation Loss: 0.02099073685219432\n",
      "Epoch [2140/20000], Training Loss: 0.009951596164942853, Validation Loss: 0.01388017788278051\n",
      "Epoch [2141/20000], Training Loss: 0.012249696409396296, Validation Loss: 0.009602420651001116\n",
      "Epoch [2142/20000], Training Loss: 0.010849724964438272, Validation Loss: 0.007014840239727966\n",
      "Epoch [2143/20000], Training Loss: 0.01162322482144061, Validation Loss: 0.010976518246804913\n",
      "Epoch [2144/20000], Training Loss: 0.012084394181978755, Validation Loss: 0.010627593201206997\n",
      "Epoch [2145/20000], Training Loss: 0.009807712415879775, Validation Loss: 0.005567080672075847\n",
      "Epoch [2146/20000], Training Loss: 0.008605630570465499, Validation Loss: 0.004854449358910205\n",
      "Epoch [2147/20000], Training Loss: 0.00972073819970579, Validation Loss: 0.01104899134121047\n",
      "Epoch [2148/20000], Training Loss: 0.011098609901507319, Validation Loss: 0.07315110802197275\n",
      "Epoch [2149/20000], Training Loss: 0.053739742642002444, Validation Loss: 0.009564728643324303\n",
      "Epoch [2150/20000], Training Loss: 0.01116129589926069, Validation Loss: 0.008564625030495907\n",
      "Epoch [2151/20000], Training Loss: 0.013479399644503636, Validation Loss: 0.007270509598519037\n",
      "Epoch [2152/20000], Training Loss: 0.009336288324058322, Validation Loss: 0.010584962881694157\n",
      "Epoch [2153/20000], Training Loss: 0.007957166216718698, Validation Loss: 0.008696344188138066\n",
      "Epoch [2154/20000], Training Loss: 0.012172020510271457, Validation Loss: 0.007920890187701231\n",
      "Epoch [2155/20000], Training Loss: 0.008386302785116382, Validation Loss: 0.007379448391020575\n",
      "Epoch [2156/20000], Training Loss: 0.02323359451838769, Validation Loss: 0.01662000491346915\n",
      "Epoch [2157/20000], Training Loss: 0.036851388560275415, Validation Loss: 0.025756795994945094\n",
      "Epoch [2158/20000], Training Loss: 0.018407753749800446, Validation Loss: 0.007377584026211782\n",
      "Epoch [2159/20000], Training Loss: 0.012038519173594458, Validation Loss: 0.012661833478497518\n",
      "Epoch [2160/20000], Training Loss: 0.013574087778709196, Validation Loss: 0.0228718033862866\n",
      "Epoch [2161/20000], Training Loss: 0.03612788336717391, Validation Loss: 0.012058847619690448\n",
      "Epoch [2162/20000], Training Loss: 0.035210256030300764, Validation Loss: 0.028399733238205018\n",
      "Epoch [2163/20000], Training Loss: 0.018090785135947435, Validation Loss: 0.014137173478437655\n",
      "Epoch [2164/20000], Training Loss: 0.01816068631264248, Validation Loss: 0.03619056128081877\n",
      "Epoch [2165/20000], Training Loss: 0.0412412736747813, Validation Loss: 0.03975315036524022\n",
      "Epoch [2166/20000], Training Loss: 0.04002074795010101, Validation Loss: 0.055307899511687526\n",
      "Epoch [2167/20000], Training Loss: 0.03731734236602539, Validation Loss: 0.03347655699767941\n",
      "Epoch [2168/20000], Training Loss: 0.018567307311708907, Validation Loss: 0.017322915741608533\n",
      "Epoch [2169/20000], Training Loss: 0.01499740085897169, Validation Loss: 0.009963250051980488\n",
      "Epoch [2170/20000], Training Loss: 0.011890901851334743, Validation Loss: 0.00818451273715203\n",
      "Epoch [2171/20000], Training Loss: 0.01227944775018841, Validation Loss: 0.008460799257071489\n",
      "Epoch [2172/20000], Training Loss: 0.010385216076558988, Validation Loss: 0.00988119102995763\n",
      "Epoch [2173/20000], Training Loss: 0.014733985349136804, Validation Loss: 0.01753058213840375\n",
      "Epoch [2174/20000], Training Loss: 0.01591096542376493, Validation Loss: 0.006305163822357183\n",
      "Epoch [2175/20000], Training Loss: 0.014303296144394803, Validation Loss: 0.006336162431060598\n",
      "Epoch [2176/20000], Training Loss: 0.01187681411725602, Validation Loss: 0.008914342257704839\n",
      "Epoch [2177/20000], Training Loss: 0.00582847319310531, Validation Loss: 0.004904897373697688\n",
      "Epoch [2178/20000], Training Loss: 0.006749789995540466, Validation Loss: 0.007128088820568987\n",
      "Epoch [2179/20000], Training Loss: 0.012144428580151205, Validation Loss: 0.004763893948035859\n",
      "Epoch [2180/20000], Training Loss: 0.008022039488423616, Validation Loss: 0.0056069867368121\n",
      "Epoch [2181/20000], Training Loss: 0.009962164699183111, Validation Loss: 0.009000636277862992\n",
      "Epoch [2182/20000], Training Loss: 0.013358263747379948, Validation Loss: 0.024446518188142363\n",
      "Epoch [2183/20000], Training Loss: 0.016349265429848207, Validation Loss: 0.012589456045589031\n",
      "Epoch [2184/20000], Training Loss: 0.019779688441693515, Validation Loss: 0.02220087610365746\n",
      "Epoch [2185/20000], Training Loss: 0.036867154795410376, Validation Loss: 0.06929456096726685\n",
      "Epoch [2186/20000], Training Loss: 0.026987971143431162, Validation Loss: 0.008654670605275852\n",
      "Epoch [2187/20000], Training Loss: 0.009742314461618662, Validation Loss: 0.006346638591805588\n",
      "Epoch [2188/20000], Training Loss: 0.009718077631467687, Validation Loss: 0.005863923837750917\n",
      "Epoch [2189/20000], Training Loss: 0.008133505720512144, Validation Loss: 0.006572426635882168\n",
      "Epoch [2190/20000], Training Loss: 0.010728332621511072, Validation Loss: 0.00857408876457352\n",
      "Epoch [2191/20000], Training Loss: 0.011675176584893572, Validation Loss: 0.005206204434557549\n",
      "Epoch [2192/20000], Training Loss: 0.011568746864213608, Validation Loss: 0.004368533683337513\n",
      "Epoch [2193/20000], Training Loss: 0.007925578393042088, Validation Loss: 0.004962622229592333\n",
      "Epoch [2194/20000], Training Loss: 0.007531336472823114, Validation Loss: 0.008324545236713934\n",
      "Epoch [2195/20000], Training Loss: 0.009379776431679991, Validation Loss: 0.006618816682467761\n",
      "Epoch [2196/20000], Training Loss: 0.010130528637481413, Validation Loss: 0.005087344411159782\n",
      "Epoch [2197/20000], Training Loss: 0.009954451541505347, Validation Loss: 0.01331724863033352\n",
      "Epoch [2198/20000], Training Loss: 0.02102236153015318, Validation Loss: 0.019275914573102324\n",
      "Epoch [2199/20000], Training Loss: 0.013671904590696369, Validation Loss: 0.008058657223419427\n",
      "Epoch [2200/20000], Training Loss: 0.012497728998174093, Validation Loss: 0.01134152898536708\n",
      "Epoch [2201/20000], Training Loss: 0.010762045568429, Validation Loss: 0.006030878318865364\n",
      "Epoch [2202/20000], Training Loss: 0.00682647925818206, Validation Loss: 0.005421039173963241\n",
      "Epoch [2203/20000], Training Loss: 0.005251277769275475, Validation Loss: 0.01542583250764957\n",
      "Epoch [2204/20000], Training Loss: 0.01053585677124959, Validation Loss: 0.0071270689335766035\n",
      "Epoch [2205/20000], Training Loss: 0.02550140014097581, Validation Loss: 0.04082365659996867\n",
      "Epoch [2206/20000], Training Loss: 0.0318061010506686, Validation Loss: 0.014476970203044661\n",
      "Epoch [2207/20000], Training Loss: 0.01868732552507676, Validation Loss: 0.009679534741475695\n",
      "Epoch [2208/20000], Training Loss: 0.01206447766162455, Validation Loss: 0.01942248257204377\n",
      "Epoch [2209/20000], Training Loss: 0.02076761015424771, Validation Loss: 0.025428165082250383\n",
      "Epoch [2210/20000], Training Loss: 0.0386078206273461, Validation Loss: 0.0134087477468451\n",
      "Epoch [2211/20000], Training Loss: 0.08536172988949277, Validation Loss: 0.03964183450267887\n",
      "Epoch [2212/20000], Training Loss: 0.039416570515771, Validation Loss: 0.023532350649698434\n",
      "Epoch [2213/20000], Training Loss: 0.016736515215598047, Validation Loss: 0.01799696567115916\n",
      "Epoch [2214/20000], Training Loss: 0.01335363189822861, Validation Loss: 0.009653245722412456\n",
      "Epoch [2215/20000], Training Loss: 0.00928365467448852, Validation Loss: 0.009071855253098806\n",
      "Epoch [2216/20000], Training Loss: 0.00960029018045004, Validation Loss: 0.007478958701104789\n",
      "Epoch [2217/20000], Training Loss: 0.014593957183283887, Validation Loss: 0.007727492841583202\n",
      "Epoch [2218/20000], Training Loss: 0.02103792180839394, Validation Loss: 0.030511800403597737\n",
      "Epoch [2219/20000], Training Loss: 0.023361315510036156, Validation Loss: 0.009081419695609145\n",
      "Epoch [2220/20000], Training Loss: 0.0166256314592569, Validation Loss: 0.007041494348090094\n",
      "Epoch [2221/20000], Training Loss: 0.008660488779111932, Validation Loss: 0.009279907126857303\n",
      "Epoch [2222/20000], Training Loss: 0.007136726198950782, Validation Loss: 0.00625468767014224\n",
      "Epoch [2223/20000], Training Loss: 0.007722827661252397, Validation Loss: 0.006805033327899375\n",
      "Epoch [2224/20000], Training Loss: 0.006980844636148374, Validation Loss: 0.004459874942767783\n",
      "Epoch [2225/20000], Training Loss: 0.008264464544481598, Validation Loss: 0.01570936626639455\n",
      "Epoch [2226/20000], Training Loss: 0.00850517354306898, Validation Loss: 0.005123480160396606\n",
      "Epoch [2227/20000], Training Loss: 0.010164934342485919, Validation Loss: 0.005465072593028911\n",
      "Epoch [2228/20000], Training Loss: 0.020761747792026393, Validation Loss: 0.014521467498966989\n",
      "Epoch [2229/20000], Training Loss: 0.009712586510332244, Validation Loss: 0.013127267422286235\n",
      "Epoch [2230/20000], Training Loss: 0.015896560715711012, Validation Loss: 0.00766936453334921\n",
      "Epoch [2231/20000], Training Loss: 0.024760431252486472, Validation Loss: 0.02116182937399052\n",
      "Epoch [2232/20000], Training Loss: 0.041249907592178454, Validation Loss: 0.03766221605474129\n",
      "Epoch [2233/20000], Training Loss: 0.03895637123163657, Validation Loss: 0.06414012951738966\n",
      "Epoch [2234/20000], Training Loss: 0.02681554406548717, Validation Loss: 0.015789741944407205\n",
      "Epoch [2235/20000], Training Loss: 0.03296212079502376, Validation Loss: 0.024026310501282438\n",
      "Epoch [2236/20000], Training Loss: 0.06053508209463741, Validation Loss: 0.029987099414857506\n",
      "Epoch [2237/20000], Training Loss: 0.03005994971525589, Validation Loss: 0.020579613089288223\n",
      "Epoch [2238/20000], Training Loss: 0.015924560870709165, Validation Loss: 0.025998959653699267\n",
      "Epoch [2239/20000], Training Loss: 0.015078921669295855, Validation Loss: 0.015252313877998651\n",
      "Epoch [2240/20000], Training Loss: 0.012128247580091869, Validation Loss: 0.00843656340505307\n",
      "Epoch [2241/20000], Training Loss: 0.011519558104087732, Validation Loss: 0.0074202311333405125\n",
      "Epoch [2242/20000], Training Loss: 0.01235922279634646, Validation Loss: 0.007037520280587588\n",
      "Epoch [2243/20000], Training Loss: 0.010370295528056366, Validation Loss: 0.01565959175897496\n",
      "Epoch [2244/20000], Training Loss: 0.013399613069071035, Validation Loss: 0.023113510323616446\n",
      "Epoch [2245/20000], Training Loss: 0.009440904379256867, Validation Loss: 0.01666740168613095\n",
      "Epoch [2246/20000], Training Loss: 0.01625598468984078, Validation Loss: 0.006768903336576061\n",
      "Epoch [2247/20000], Training Loss: 0.01713145201626633, Validation Loss: 0.009659683365108711\n",
      "Epoch [2248/20000], Training Loss: 0.015485200621437148, Validation Loss: 0.0067005001967412724\n",
      "Epoch [2249/20000], Training Loss: 0.013580347457069106, Validation Loss: 0.01618493187717312\n",
      "Epoch [2250/20000], Training Loss: 0.011895345981299345, Validation Loss: 0.007542787390645701\n",
      "Epoch [2251/20000], Training Loss: 0.007934277535241563, Validation Loss: 0.004842674814296645\n",
      "Epoch [2252/20000], Training Loss: 0.0068780522352816275, Validation Loss: 0.013552323842613576\n",
      "Epoch [2253/20000], Training Loss: 0.008833477111433499, Validation Loss: 0.02673393228664264\n",
      "Epoch [2254/20000], Training Loss: 0.014940746976727886, Validation Loss: 0.02460344165160701\n",
      "Epoch [2255/20000], Training Loss: 0.013229191284543569, Validation Loss: 0.06358558477217306\n",
      "Epoch [2256/20000], Training Loss: 0.0257629952570174, Validation Loss: 0.01034582206849726\n",
      "Epoch [2257/20000], Training Loss: 0.017876037375702123, Validation Loss: 0.005047528538723002\n",
      "Epoch [2258/20000], Training Loss: 0.009883899896522053, Validation Loss: 0.009457828613449237\n",
      "Epoch [2259/20000], Training Loss: 0.010911955375507074, Validation Loss: 0.015371638361102669\n",
      "Epoch [2260/20000], Training Loss: 0.01701763683481009, Validation Loss: 0.016540019802050665\n",
      "Epoch [2261/20000], Training Loss: 0.015270531809489642, Validation Loss: 0.00724080338172176\n",
      "Epoch [2262/20000], Training Loss: 0.01139262022375728, Validation Loss: 0.01151603872702773\n",
      "Epoch [2263/20000], Training Loss: 0.0114278453069606, Validation Loss: 0.0065873799066399185\n",
      "Epoch [2264/20000], Training Loss: 0.007389199701719917, Validation Loss: 0.004579413455303438\n",
      "Epoch [2265/20000], Training Loss: 0.006617498199505333, Validation Loss: 0.004035271084392532\n",
      "Epoch [2266/20000], Training Loss: 0.00675514990250952, Validation Loss: 0.006240762060282837\n",
      "Epoch [2267/20000], Training Loss: 0.006625474016930509, Validation Loss: 0.0031779003722687904\n",
      "Epoch [2268/20000], Training Loss: 0.009678296308265999, Validation Loss: 0.010410666407554661\n",
      "Epoch [2269/20000], Training Loss: 0.025206408684425696, Validation Loss: 0.03611806986165383\n",
      "Epoch [2270/20000], Training Loss: 0.02019580519741534, Validation Loss: 0.01616800361892144\n",
      "Epoch [2271/20000], Training Loss: 0.018003050468645858, Validation Loss: 0.004902676534227014\n",
      "Epoch [2272/20000], Training Loss: 0.014730541558362478, Validation Loss: 0.031592633867278765\n",
      "Epoch [2273/20000], Training Loss: 0.048094313512722565, Validation Loss: 0.2114298807251284\n",
      "Epoch [2274/20000], Training Loss: 0.11326746265509817, Validation Loss: 0.027323503206162534\n",
      "Epoch [2275/20000], Training Loss: 0.058571546737636836, Validation Loss: 0.03410193903031151\n",
      "Epoch [2276/20000], Training Loss: 0.022341146582870612, Validation Loss: 0.015307510364384111\n",
      "Epoch [2277/20000], Training Loss: 0.01295290539772915, Validation Loss: 0.012390921869252582\n",
      "Epoch [2278/20000], Training Loss: 0.012519109486934863, Validation Loss: 0.00921143498008765\n",
      "Epoch [2279/20000], Training Loss: 0.00908241983103965, Validation Loss: 0.013852526919967032\n",
      "Epoch [2280/20000], Training Loss: 0.014950432382517777, Validation Loss: 0.084725820143441\n",
      "Epoch [2281/20000], Training Loss: 0.028800260441909944, Validation Loss: 0.019605709036114517\n",
      "Epoch [2282/20000], Training Loss: 0.020323456061305478, Validation Loss: 0.021484694262787547\n",
      "Epoch [2283/20000], Training Loss: 0.015901062186458148, Validation Loss: 0.007781596176627143\n",
      "Epoch [2284/20000], Training Loss: 0.010223732205174332, Validation Loss: 0.010846630441928908\n",
      "Epoch [2285/20000], Training Loss: 0.0075285839723489645, Validation Loss: 0.008153886532194937\n",
      "Epoch [2286/20000], Training Loss: 0.010488981971450682, Validation Loss: 0.006810007751482815\n",
      "Epoch [2287/20000], Training Loss: 0.00798231435640316, Validation Loss: 0.008361812220495547\n",
      "Epoch [2288/20000], Training Loss: 0.007603801784820722, Validation Loss: 0.004684383536057021\n",
      "Epoch [2289/20000], Training Loss: 0.00910543484081115, Validation Loss: 0.004585066995787206\n",
      "Epoch [2290/20000], Training Loss: 0.009260038198720264, Validation Loss: 0.006975616134565959\n",
      "Epoch [2291/20000], Training Loss: 0.013382087091615955, Validation Loss: 0.0040336861711693945\n",
      "Epoch [2292/20000], Training Loss: 0.009449470734190462, Validation Loss: 0.006250302155681961\n",
      "Epoch [2293/20000], Training Loss: 0.012488558273097234, Validation Loss: 0.01790362819746605\n",
      "Epoch [2294/20000], Training Loss: 0.01170847682598313, Validation Loss: 0.004584061412580403\n",
      "Epoch [2295/20000], Training Loss: 0.009818851689422965, Validation Loss: 0.012019574853703878\n",
      "Epoch [2296/20000], Training Loss: 0.015802210845452334, Validation Loss: 0.03697259267885654\n",
      "Epoch [2297/20000], Training Loss: 0.01981727403887947, Validation Loss: 0.013104725162390553\n",
      "Epoch [2298/20000], Training Loss: 0.032651315212465955, Validation Loss: 0.019322616107404298\n",
      "Epoch [2299/20000], Training Loss: 0.019142703768531128, Validation Loss: 0.013444556510218522\n",
      "Epoch [2300/20000], Training Loss: 0.019582539339483316, Validation Loss: 0.012205529154502617\n",
      "Epoch [2301/20000], Training Loss: 0.015561523136316932, Validation Loss: 0.008760374704471425\n",
      "Epoch [2302/20000], Training Loss: 0.010300400288542733, Validation Loss: 0.005404341510589867\n",
      "Epoch [2303/20000], Training Loss: 0.012285141132971538, Validation Loss: 0.01038664297831409\n",
      "Epoch [2304/20000], Training Loss: 0.012906271070408235, Validation Loss: 0.01064436131657333\n",
      "Epoch [2305/20000], Training Loss: 0.013802137916042869, Validation Loss: 0.007332583876502342\n",
      "Epoch [2306/20000], Training Loss: 0.011472501530079171, Validation Loss: 0.018502951556521503\n",
      "Epoch [2307/20000], Training Loss: 0.01663193082926487, Validation Loss: 0.006314665359501045\n",
      "Epoch [2308/20000], Training Loss: 0.03340529083340828, Validation Loss: 0.008693396822504187\n",
      "Epoch [2309/20000], Training Loss: 0.03513059056630092, Validation Loss: 0.02534384185254677\n",
      "Epoch [2310/20000], Training Loss: 0.01815567580550643, Validation Loss: 0.006657127342444851\n",
      "Epoch [2311/20000], Training Loss: 0.011818425796393837, Validation Loss: 0.005859240248888652\n",
      "Epoch [2312/20000], Training Loss: 0.013495731157101025, Validation Loss: 0.014704096539528934\n",
      "Epoch [2313/20000], Training Loss: 0.03194688248705851, Validation Loss: 0.006472875844468741\n",
      "Epoch [2314/20000], Training Loss: 0.009377066328722452, Validation Loss: 0.009177056823338173\n",
      "Epoch [2315/20000], Training Loss: 0.011012872152995052, Validation Loss: 0.008629393836460567\n",
      "Epoch [2316/20000], Training Loss: 0.009281545914356164, Validation Loss: 0.009195159155858168\n",
      "Epoch [2317/20000], Training Loss: 0.011190778542576092, Validation Loss: 0.006372875174455671\n",
      "Epoch [2318/20000], Training Loss: 0.005065396816852237, Validation Loss: 0.006352385133455797\n",
      "Epoch [2319/20000], Training Loss: 0.007345116407252915, Validation Loss: 0.007038705022515435\n",
      "Epoch [2320/20000], Training Loss: 0.010580252357093351, Validation Loss: 0.005276027115429351\n",
      "Epoch [2321/20000], Training Loss: 0.02090598361350463, Validation Loss: 0.05959427982506632\n",
      "Epoch [2322/20000], Training Loss: 0.047631834321821644, Validation Loss: 0.03648541764207904\n",
      "Epoch [2323/20000], Training Loss: 0.05588467354287526, Validation Loss: 0.025366642716536363\n",
      "Epoch [2324/20000], Training Loss: 0.04523151621937619, Validation Loss: 0.031147582847027407\n",
      "Epoch [2325/20000], Training Loss: 0.027340069962000207, Validation Loss: 0.03529941821765969\n",
      "Epoch [2326/20000], Training Loss: 0.023308674796550934, Validation Loss: 0.00935877031484833\n",
      "Epoch [2327/20000], Training Loss: 0.01039107917209289, Validation Loss: 0.00877555257758577\n",
      "Epoch [2328/20000], Training Loss: 0.012511960205821586, Validation Loss: 0.011885234441566805\n",
      "Epoch [2329/20000], Training Loss: 0.008305502032661545, Validation Loss: 0.0074256301704773795\n",
      "Epoch [2330/20000], Training Loss: 0.007467283360061369, Validation Loss: 0.005925760694352279\n",
      "Epoch [2331/20000], Training Loss: 0.010163495629967656, Validation Loss: 0.009631175964127206\n",
      "Epoch [2332/20000], Training Loss: 0.01877196555142291, Validation Loss: 0.007929195915072228\n",
      "Epoch [2333/20000], Training Loss: 0.013026055838314019, Validation Loss: 0.00820589448890791\n",
      "Epoch [2334/20000], Training Loss: 0.01686888254646744, Validation Loss: 0.01106208145434505\n",
      "Epoch [2335/20000], Training Loss: 0.017183821806351522, Validation Loss: 0.019759564993624502\n",
      "Epoch [2336/20000], Training Loss: 0.01334038718986059, Validation Loss: 0.01295632291424099\n",
      "Epoch [2337/20000], Training Loss: 0.017033425513156026, Validation Loss: 0.015328532822812657\n",
      "Epoch [2338/20000], Training Loss: 0.012688330965050097, Validation Loss: 0.006745257888854092\n",
      "Epoch [2339/20000], Training Loss: 0.014024660822802357, Validation Loss: 0.025677003659828716\n",
      "Epoch [2340/20000], Training Loss: 0.016356088231467374, Validation Loss: 0.00632423585610858\n",
      "Epoch [2341/20000], Training Loss: 0.009957426046769666, Validation Loss: 0.007823136547683266\n",
      "Epoch [2342/20000], Training Loss: 0.018368892909285655, Validation Loss: 0.017355127659729623\n",
      "Epoch [2343/20000], Training Loss: 0.02132580107510356, Validation Loss: 0.008649316295994986\n",
      "Epoch [2344/20000], Training Loss: 0.013185447489377111, Validation Loss: 0.014064211134788302\n",
      "Epoch [2345/20000], Training Loss: 0.015379712072899565, Validation Loss: 0.010184103177879576\n",
      "Epoch [2346/20000], Training Loss: 0.012938533132845416, Validation Loss: 0.02294147610805451\n",
      "Epoch [2347/20000], Training Loss: 0.026671908467635928, Validation Loss: 0.0315219764595156\n",
      "Epoch [2348/20000], Training Loss: 0.04076880056943212, Validation Loss: 0.036286830702530484\n",
      "Epoch [2349/20000], Training Loss: 0.021472907974384725, Validation Loss: 0.01822006014711316\n",
      "Epoch [2350/20000], Training Loss: 0.027836795463891967, Validation Loss: 0.04002015304358305\n",
      "Epoch [2351/20000], Training Loss: 0.04196973643930895, Validation Loss: 0.015781431166552864\n",
      "Epoch [2352/20000], Training Loss: 0.02167545717176316, Validation Loss: 0.009381138197048684\n",
      "Epoch [2353/20000], Training Loss: 0.02346667622415615, Validation Loss: 0.012090734630744176\n",
      "Epoch [2354/20000], Training Loss: 0.02396498467507107, Validation Loss: 0.009778349456610302\n",
      "Epoch [2355/20000], Training Loss: 0.017843805860528455, Validation Loss: 0.020319711162415734\n",
      "Epoch [2356/20000], Training Loss: 0.009496212504538042, Validation Loss: 0.00797415584124145\n",
      "Epoch [2357/20000], Training Loss: 0.009771660469206316, Validation Loss: 0.006968620079922936\n",
      "Epoch [2358/20000], Training Loss: 0.008060228258338091, Validation Loss: 0.011681881274333667\n",
      "Epoch [2359/20000], Training Loss: 0.01174673999984017, Validation Loss: 0.009686328523295226\n",
      "Epoch [2360/20000], Training Loss: 0.009121006920135446, Validation Loss: 0.012812806740904698\n",
      "Epoch [2361/20000], Training Loss: 0.006576031689162066, Validation Loss: 0.009536787752170183\n",
      "Epoch [2362/20000], Training Loss: 0.013101421205127346, Validation Loss: 0.0114648801045405\n",
      "Epoch [2363/20000], Training Loss: 0.01647887155611118, Validation Loss: 0.012023270040011929\n",
      "Epoch [2364/20000], Training Loss: 0.012450560827606491, Validation Loss: 0.020214662723836745\n",
      "Epoch [2365/20000], Training Loss: 0.017526420779176988, Validation Loss: 0.021993571042799993\n",
      "Epoch [2366/20000], Training Loss: 0.012708317374355309, Validation Loss: 0.028974950221902013\n",
      "Epoch [2367/20000], Training Loss: 0.028465769528078715, Validation Loss: 0.06424086001378207\n",
      "Epoch [2368/20000], Training Loss: 0.041141176192987974, Validation Loss: 0.041254019566015675\n",
      "Epoch [2369/20000], Training Loss: 0.020426932826272344, Validation Loss: 0.012299611434822768\n",
      "Epoch [2370/20000], Training Loss: 0.018573214299976826, Validation Loss: 0.015375863423226324\n",
      "Epoch [2371/20000], Training Loss: 0.017786842238690172, Validation Loss: 0.02034673497525676\n",
      "Epoch [2372/20000], Training Loss: 0.012294530444445886, Validation Loss: 0.006247434670577994\n",
      "Epoch [2373/20000], Training Loss: 0.011681139843338835, Validation Loss: 0.008857583993135774\n",
      "Epoch [2374/20000], Training Loss: 0.013103993844457104, Validation Loss: 0.00782442045795051\n",
      "Epoch [2375/20000], Training Loss: 0.018054573498277802, Validation Loss: 0.00923251916528248\n",
      "Epoch [2376/20000], Training Loss: 0.017473176886726702, Validation Loss: 0.01717572051574377\n",
      "Epoch [2377/20000], Training Loss: 0.01586747906512187, Validation Loss: 0.00726758467122376\n",
      "Epoch [2378/20000], Training Loss: 0.008200686133932322, Validation Loss: 0.008128914655458175\n",
      "Epoch [2379/20000], Training Loss: 0.008774404024214684, Validation Loss: 0.005617093778766957\n",
      "Epoch [2380/20000], Training Loss: 0.011907202804845187, Validation Loss: 0.007227941615482791\n",
      "Epoch [2381/20000], Training Loss: 0.010129652283337367, Validation Loss: 0.010973732904693025\n",
      "Epoch [2382/20000], Training Loss: 0.014129577074917117, Validation Loss: 0.01747370171894553\n",
      "Epoch [2383/20000], Training Loss: 0.029576592524660685, Validation Loss: 0.04262367887241477\n",
      "Epoch [2384/20000], Training Loss: 0.04797601921018213, Validation Loss: 0.06192676906551262\n",
      "Epoch [2385/20000], Training Loss: 0.03601193707436323, Validation Loss: 0.01393796702087238\n",
      "Epoch [2386/20000], Training Loss: 0.0311706957041419, Validation Loss: 0.01781281524294642\n",
      "Epoch [2387/20000], Training Loss: 0.029017613876411424, Validation Loss: 0.08692046625776098\n",
      "Epoch [2388/20000], Training Loss: 0.03739995815392051, Validation Loss: 0.011700963742847828\n",
      "Epoch [2389/20000], Training Loss: 0.02383984313512753, Validation Loss: 0.012950250635471436\n",
      "Epoch [2390/20000], Training Loss: 0.012208505891196961, Validation Loss: 0.009277231452879122\n",
      "Epoch [2391/20000], Training Loss: 0.009993222708414708, Validation Loss: 0.007516402905513792\n",
      "Epoch [2392/20000], Training Loss: 0.008683942636123123, Validation Loss: 0.007290760251115602\n",
      "Epoch [2393/20000], Training Loss: 0.010852345666665184, Validation Loss: 0.03100102805673938\n",
      "Epoch [2394/20000], Training Loss: 0.020235380394816666, Validation Loss: 0.019930708860281147\n",
      "Epoch [2395/20000], Training Loss: 0.028537684308373303, Validation Loss: 0.017136483572228058\n",
      "Epoch [2396/20000], Training Loss: 0.04684406867766354, Validation Loss: 0.04966402025560717\n",
      "Epoch [2397/20000], Training Loss: 0.02827078166904227, Validation Loss: 0.02851180348391479\n",
      "Epoch [2398/20000], Training Loss: 0.022298317136509076, Validation Loss: 0.010726447990448291\n",
      "Epoch [2399/20000], Training Loss: 0.01252282822471378, Validation Loss: 0.009401098861888493\n",
      "Epoch [2400/20000], Training Loss: 0.015435954705545945, Validation Loss: 0.008099431277206328\n",
      "Epoch [2401/20000], Training Loss: 0.010549977193087605, Validation Loss: 0.00780009840335489\n",
      "Epoch [2402/20000], Training Loss: 0.008656948663493884, Validation Loss: 0.00929457404137273\n",
      "Epoch [2403/20000], Training Loss: 0.008532046444348193, Validation Loss: 0.009214660288599979\n",
      "Epoch [2404/20000], Training Loss: 0.00715226774004155, Validation Loss: 0.008167371719504704\n",
      "Epoch [2405/20000], Training Loss: 0.011732738664639848, Validation Loss: 0.0244229289350939\n",
      "Epoch [2406/20000], Training Loss: 0.02365891564321438, Validation Loss: 0.05386292338869679\n",
      "Epoch [2407/20000], Training Loss: 0.01679856146178541, Validation Loss: 0.005799597737921301\n",
      "Epoch [2408/20000], Training Loss: 0.0092625804016799, Validation Loss: 0.006961328272057342\n",
      "Epoch [2409/20000], Training Loss: 0.0116967614173648, Validation Loss: 0.008610285762041972\n",
      "Epoch [2410/20000], Training Loss: 0.0124221294660986, Validation Loss: 0.012892641450824678\n",
      "Epoch [2411/20000], Training Loss: 0.01882568290290822, Validation Loss: 0.010415804473138581\n",
      "Epoch [2412/20000], Training Loss: 0.011849896350343312, Validation Loss: 0.005948554475480705\n",
      "Epoch [2413/20000], Training Loss: 0.007746131697786041, Validation Loss: 0.007845002689675442\n",
      "Epoch [2414/20000], Training Loss: 0.007066130600703348, Validation Loss: 0.007253595857296854\n",
      "Epoch [2415/20000], Training Loss: 0.005904974825430794, Validation Loss: 0.005359331526928405\n",
      "Epoch [2416/20000], Training Loss: 0.007951986316194442, Validation Loss: 0.00958652378158474\n",
      "Epoch [2417/20000], Training Loss: 0.01706988949029307, Validation Loss: 0.03173475454056147\n",
      "Epoch [2418/20000], Training Loss: 0.015739789844831518, Validation Loss: 0.012120181014016193\n",
      "Epoch [2419/20000], Training Loss: 0.0162338228884177, Validation Loss: 0.008450479572340035\n",
      "Epoch [2420/20000], Training Loss: 0.013595746210609962, Validation Loss: 0.009799265167690205\n",
      "Epoch [2421/20000], Training Loss: 0.025746600110973565, Validation Loss: 0.011757322215869084\n",
      "Epoch [2422/20000], Training Loss: 0.03465800776445706, Validation Loss: 0.026594974808728824\n",
      "Epoch [2423/20000], Training Loss: 0.0281178364311927, Validation Loss: 0.05772335889738752\n",
      "Epoch [2424/20000], Training Loss: 0.024992239888368722, Validation Loss: 0.007044435103125579\n",
      "Epoch [2425/20000], Training Loss: 0.017620391554893495, Validation Loss: 0.010741966061622017\n",
      "Epoch [2426/20000], Training Loss: 0.016346578028917844, Validation Loss: 0.007915821396311782\n",
      "Epoch [2427/20000], Training Loss: 0.007721511040082467, Validation Loss: 0.006392625367815137\n",
      "Epoch [2428/20000], Training Loss: 0.0117240733151058, Validation Loss: 0.013950423902088005\n",
      "Epoch [2429/20000], Training Loss: 0.009408783799569522, Validation Loss: 0.012697370719333126\n",
      "Epoch [2430/20000], Training Loss: 0.008639283193458271, Validation Loss: 0.00541818210956332\n",
      "Epoch [2431/20000], Training Loss: 0.005546644093036386, Validation Loss: 0.006162606410815182\n",
      "Epoch [2432/20000], Training Loss: 0.013512790953557539, Validation Loss: 0.0075819235911041005\n",
      "Epoch [2433/20000], Training Loss: 0.021216158999387908, Validation Loss: 0.006209528862655134\n",
      "Epoch [2434/20000], Training Loss: 0.01693395421794516, Validation Loss: 0.030212218582619244\n",
      "Epoch [2435/20000], Training Loss: 0.02459182234347931, Validation Loss: 0.02748082732369206\n",
      "Epoch [2436/20000], Training Loss: 0.026737126662088224, Validation Loss: 0.010939930467088743\n",
      "Epoch [2437/20000], Training Loss: 0.011397134346355284, Validation Loss: 0.0149548026017458\n",
      "Epoch [2438/20000], Training Loss: 0.020070950575505515, Validation Loss: 0.010696948089259552\n",
      "Epoch [2439/20000], Training Loss: 0.02368502409496744, Validation Loss: 0.025515099987322692\n",
      "Epoch [2440/20000], Training Loss: 0.020111028373711242, Validation Loss: 0.03262520232488545\n",
      "Epoch [2441/20000], Training Loss: 0.022767941931046413, Validation Loss: 0.01598358932163332\n",
      "Epoch [2442/20000], Training Loss: 0.014663450286856719, Validation Loss: 0.007689422499554764\n",
      "Epoch [2443/20000], Training Loss: 0.01867278219547838, Validation Loss: 0.011798567737466046\n",
      "Epoch [2444/20000], Training Loss: 0.018147413635493388, Validation Loss: 0.008319498598731505\n",
      "Epoch [2445/20000], Training Loss: 0.011555195998101096, Validation Loss: 0.0164582647228469\n",
      "Epoch [2446/20000], Training Loss: 0.009988374989396627, Validation Loss: 0.009619778378042352\n",
      "Epoch [2447/20000], Training Loss: 0.006625591558466632, Validation Loss: 0.010598156128188358\n",
      "Epoch [2448/20000], Training Loss: 0.01203696234525913, Validation Loss: 0.006566790921397114\n",
      "Epoch [2449/20000], Training Loss: 0.009185957909461908, Validation Loss: 0.0053579183316482416\n",
      "Epoch [2450/20000], Training Loss: 0.009127700493991142, Validation Loss: 0.004598935022223876\n",
      "Epoch [2451/20000], Training Loss: 0.02751114791109493, Validation Loss: 0.012602888686712634\n",
      "Epoch [2452/20000], Training Loss: 0.04663584863322155, Validation Loss: 0.04235576325481816\n",
      "Epoch [2453/20000], Training Loss: 0.020065190125023946, Validation Loss: 0.020841120100937718\n",
      "Epoch [2454/20000], Training Loss: 0.016839041324731494, Validation Loss: 0.00857031700330221\n",
      "Epoch [2455/20000], Training Loss: 0.012401537379316454, Validation Loss: 0.008009114122874703\n",
      "Epoch [2456/20000], Training Loss: 0.012309498042408709, Validation Loss: 0.008545885480218982\n",
      "Epoch [2457/20000], Training Loss: 0.010086035701013836, Validation Loss: 0.012511728739558034\n",
      "Epoch [2458/20000], Training Loss: 0.020605239543199007, Validation Loss: 0.005728194646053453\n",
      "Epoch [2459/20000], Training Loss: 0.011315944429952651, Validation Loss: 0.005524639131148693\n",
      "Epoch [2460/20000], Training Loss: 0.011877451249996998, Validation Loss: 0.006834935823722584\n",
      "Epoch [2461/20000], Training Loss: 0.010024794752618098, Validation Loss: 0.013758248299380074\n",
      "Epoch [2462/20000], Training Loss: 0.015100343547861226, Validation Loss: 0.008029218088611043\n",
      "Epoch [2463/20000], Training Loss: 0.011915401565992008, Validation Loss: 0.007041797071130824\n",
      "Epoch [2464/20000], Training Loss: 0.01198957271325136, Validation Loss: 0.020722140335107855\n",
      "Epoch [2465/20000], Training Loss: 0.043499707959459295, Validation Loss: 0.01764554727535374\n",
      "Epoch [2466/20000], Training Loss: 0.01534104917352254, Validation Loss: 0.006462770896717512\n",
      "Epoch [2467/20000], Training Loss: 0.009004022144446415, Validation Loss: 0.011512640651134994\n",
      "Epoch [2468/20000], Training Loss: 0.007810265483320629, Validation Loss: 0.011830900913946748\n",
      "Epoch [2469/20000], Training Loss: 0.008091206121857144, Validation Loss: 0.0159067834770782\n",
      "Epoch [2470/20000], Training Loss: 0.007743685704813937, Validation Loss: 0.010381454274894817\n",
      "Epoch [2471/20000], Training Loss: 0.010496783789546629, Validation Loss: 0.005932840201452627\n",
      "Epoch [2472/20000], Training Loss: 0.011884589057964539, Validation Loss: 0.00698749952514785\n",
      "Epoch [2473/20000], Training Loss: 0.011635005468893464, Validation Loss: 0.005310906999778417\n",
      "Epoch [2474/20000], Training Loss: 0.009913952320597932, Validation Loss: 0.013116665905597213\n",
      "Epoch [2475/20000], Training Loss: 0.020273354112370207, Validation Loss: 0.006884196056440217\n",
      "Epoch [2476/20000], Training Loss: 0.024363056815413335, Validation Loss: 0.012963943978463848\n",
      "Epoch [2477/20000], Training Loss: 0.018789589878273154, Validation Loss: 0.021121188474635334\n",
      "Epoch [2478/20000], Training Loss: 0.0277763838987864, Validation Loss: 0.03903312942192078\n",
      "Epoch [2479/20000], Training Loss: 0.03279641317203641, Validation Loss: 0.045421928018163575\n",
      "Epoch [2480/20000], Training Loss: 0.022216447454411536, Validation Loss: 0.01788568029741037\n",
      "Epoch [2481/20000], Training Loss: 0.015336029352251248, Validation Loss: 0.02558577416717354\n",
      "Epoch [2482/20000], Training Loss: 0.021992295814145888, Validation Loss: 0.028124797960197796\n",
      "Epoch [2483/20000], Training Loss: 0.03965782723389566, Validation Loss: 0.01200403886601097\n",
      "Epoch [2484/20000], Training Loss: 0.01430087951302994, Validation Loss: 0.009207801271824842\n",
      "Epoch [2485/20000], Training Loss: 0.010448779644710677, Validation Loss: 0.007476751011336216\n",
      "Epoch [2486/20000], Training Loss: 0.013209122056390956, Validation Loss: 0.014790425183534484\n",
      "Epoch [2487/20000], Training Loss: 0.009860669242048712, Validation Loss: 0.005416103841541643\n",
      "Epoch [2488/20000], Training Loss: 0.01069303767456274, Validation Loss: 0.005614626224052418\n",
      "Epoch [2489/20000], Training Loss: 0.008875078687976514, Validation Loss: 0.012141087988351342\n",
      "Epoch [2490/20000], Training Loss: 0.009263391234396425, Validation Loss: 0.006371694977331245\n",
      "Epoch [2491/20000], Training Loss: 0.008056755884483988, Validation Loss: 0.020083073355361334\n",
      "Epoch [2492/20000], Training Loss: 0.01983643812855007, Validation Loss: 0.010178965006571996\n",
      "Epoch [2493/20000], Training Loss: 0.010770251384071474, Validation Loss: 0.007506274298763652\n",
      "Epoch [2494/20000], Training Loss: 0.00978923702911873, Validation Loss: 0.005759318002597011\n",
      "Epoch [2495/20000], Training Loss: 0.012109847127508797, Validation Loss: 0.019330562188706244\n",
      "Epoch [2496/20000], Training Loss: 0.01977966326999844, Validation Loss: 0.07087835433604796\n",
      "Epoch [2497/20000], Training Loss: 0.05780820339818352, Validation Loss: 0.08214916005836966\n",
      "Epoch [2498/20000], Training Loss: 0.06487485857880008, Validation Loss: 0.127565975849772\n",
      "Epoch [2499/20000], Training Loss: 0.053695739014074206, Validation Loss: 0.0164356377718804\n",
      "Epoch [2500/20000], Training Loss: 0.021162818784692457, Validation Loss: 0.015292828794103557\n",
      "Epoch [2501/20000], Training Loss: 0.013764958895210708, Validation Loss: 0.010526167211495381\n",
      "Epoch [2502/20000], Training Loss: 0.010524519358503832, Validation Loss: 0.009177019238288333\n",
      "Epoch [2503/20000], Training Loss: 0.006811477690851981, Validation Loss: 0.003522777405875566\n",
      "Epoch [2504/20000], Training Loss: 0.005197595750879762, Validation Loss: 0.006458398519392336\n",
      "Epoch [2505/20000], Training Loss: 0.014800185917920317, Validation Loss: 0.16255384442345\n",
      "Epoch [2506/20000], Training Loss: 0.061656903380285906, Validation Loss: 0.05908596047523883\n",
      "Epoch [2507/20000], Training Loss: 0.030706149950024804, Validation Loss: 0.012864044421166001\n",
      "Epoch [2508/20000], Training Loss: 0.01241071377548256, Validation Loss: 0.009887027004208817\n",
      "Epoch [2509/20000], Training Loss: 0.009430210571736097, Validation Loss: 0.008818838933667621\n",
      "Epoch [2510/20000], Training Loss: 0.00724326096211111, Validation Loss: 0.0055940765097772654\n",
      "Epoch [2511/20000], Training Loss: 0.008074913011764042, Validation Loss: 0.013229003109116943\n",
      "Epoch [2512/20000], Training Loss: 0.016623998654332745, Validation Loss: 0.016145666372053117\n",
      "Epoch [2513/20000], Training Loss: 0.010753558309391207, Validation Loss: 0.013306787067771228\n",
      "Epoch [2514/20000], Training Loss: 0.02292398869758472, Validation Loss: 0.007736568022317752\n",
      "Epoch [2515/20000], Training Loss: 0.011546602439401405, Validation Loss: 0.008438351274115697\n",
      "Epoch [2516/20000], Training Loss: 0.011229310359340161, Validation Loss: 0.0070363516409598075\n",
      "Epoch [2517/20000], Training Loss: 0.011503994899352879, Validation Loss: 0.013577315388342661\n",
      "Epoch [2518/20000], Training Loss: 0.013767646475051671, Validation Loss: 0.017926264060594298\n",
      "Epoch [2519/20000], Training Loss: 0.013769480877921783, Validation Loss: 0.029961297502793166\n",
      "Epoch [2520/20000], Training Loss: 0.018390186404108784, Validation Loss: 0.004996937268995643\n",
      "Epoch [2521/20000], Training Loss: 0.010719393779124533, Validation Loss: 0.011236553830531413\n",
      "Epoch [2522/20000], Training Loss: 0.008040998257846306, Validation Loss: 0.005997747104590815\n",
      "Epoch [2523/20000], Training Loss: 0.009828574222150823, Validation Loss: 0.0033333722662798925\n",
      "Epoch [2524/20000], Training Loss: 0.0049824398058068, Validation Loss: 0.006105032902363246\n",
      "Epoch [2525/20000], Training Loss: 0.011602007925310838, Validation Loss: 0.009793792327640927\n",
      "Epoch [2526/20000], Training Loss: 0.008172942792563325, Validation Loss: 0.007702979190302123\n",
      "Epoch [2527/20000], Training Loss: 0.007289228566930562, Validation Loss: 0.005649946171466074\n",
      "Epoch [2528/20000], Training Loss: 0.011989389591430413, Validation Loss: 0.00923252402004479\n",
      "Epoch [2529/20000], Training Loss: 0.037733840675563055, Validation Loss: 0.0371439790234437\n",
      "Epoch [2530/20000], Training Loss: 0.05841815289048619, Validation Loss: 0.009573458152698595\n",
      "Epoch [2531/20000], Training Loss: 0.02387365570757538, Validation Loss: 0.010329001311439736\n",
      "Epoch [2532/20000], Training Loss: 0.009545760071237705, Validation Loss: 0.00832086643480519\n",
      "Epoch [2533/20000], Training Loss: 0.010501695684589711, Validation Loss: 0.005196161004344969\n",
      "Epoch [2534/20000], Training Loss: 0.016531125509313176, Validation Loss: 0.00585313265491097\n",
      "Epoch [2535/20000], Training Loss: 0.021010645238545424, Validation Loss: 0.012628246069799422\n",
      "Epoch [2536/20000], Training Loss: 0.012517627478311104, Validation Loss: 0.007739531260142264\n",
      "Epoch [2537/20000], Training Loss: 0.008255478252457189, Validation Loss: 0.008494746674594555\n",
      "Epoch [2538/20000], Training Loss: 0.02191682885157726, Validation Loss: 0.010486687507833098\n",
      "Epoch [2539/20000], Training Loss: 0.024974016072129807, Validation Loss: 0.008389662552434562\n",
      "Epoch [2540/20000], Training Loss: 0.023866757946962025, Validation Loss: 0.01109088740414371\n",
      "Epoch [2541/20000], Training Loss: 0.013734302209091507, Validation Loss: 0.009063891560850369\n",
      "Epoch [2542/20000], Training Loss: 0.011398137888006334, Validation Loss: 0.01352879652299325\n",
      "Epoch [2543/20000], Training Loss: 0.01129970489585373, Validation Loss: 0.005261055709634997\n",
      "Epoch [2544/20000], Training Loss: 0.008964120676474912, Validation Loss: 0.011403179971739754\n",
      "Epoch [2545/20000], Training Loss: 0.011686460393580742, Validation Loss: 0.006133559796219841\n",
      "Epoch [2546/20000], Training Loss: 0.014503035316010937, Validation Loss: 0.00582087550363795\n",
      "Epoch [2547/20000], Training Loss: 0.031662897030920636, Validation Loss: 0.06931096822977349\n",
      "Epoch [2548/20000], Training Loss: 0.022850126756696097, Validation Loss: 0.05385798919897238\n",
      "Epoch [2549/20000], Training Loss: 0.04691407314177403, Validation Loss: 0.025659814307107863\n",
      "Epoch [2550/20000], Training Loss: 0.01733728315282081, Validation Loss: 0.017844138708421015\n",
      "Epoch [2551/20000], Training Loss: 0.009873118101885276, Validation Loss: 0.007337425530992225\n",
      "Epoch [2552/20000], Training Loss: 0.008120955886884726, Validation Loss: 0.009061405496418389\n",
      "Epoch [2553/20000], Training Loss: 0.007548022479438389, Validation Loss: 0.007806678019424985\n",
      "Epoch [2554/20000], Training Loss: 0.013441634773747475, Validation Loss: 0.01591855950271703\n",
      "Epoch [2555/20000], Training Loss: 0.02498339451683153, Validation Loss: 0.005878780235759577\n",
      "Epoch [2556/20000], Training Loss: 0.009419341829405832, Validation Loss: 0.005196689050956508\n",
      "Epoch [2557/20000], Training Loss: 0.009563564827528483, Validation Loss: 0.010241871427782089\n",
      "Epoch [2558/20000], Training Loss: 0.007432258055944528, Validation Loss: 0.008336581164257098\n",
      "Epoch [2559/20000], Training Loss: 0.009782606421400228, Validation Loss: 0.017335080617457234\n",
      "Epoch [2560/20000], Training Loss: 0.012251988288231328, Validation Loss: 0.018948980570681697\n",
      "Epoch [2561/20000], Training Loss: 0.010901076756584058, Validation Loss: 0.003890260327313842\n",
      "Epoch [2562/20000], Training Loss: 0.009524963270881212, Validation Loss: 0.008312321496964266\n",
      "Epoch [2563/20000], Training Loss: 0.01161494864832743, Validation Loss: 0.01111275124226833\n",
      "Epoch [2564/20000], Training Loss: 0.016138275599847214, Validation Loss: 0.010186361073615785\n",
      "Epoch [2565/20000], Training Loss: 0.009043854727808918, Validation Loss: 0.0039358125834902695\n",
      "Epoch [2566/20000], Training Loss: 0.00886810189985811, Validation Loss: 0.006828393445021316\n",
      "Epoch [2567/20000], Training Loss: 0.02487613510571204, Validation Loss: 0.017758792299451615\n",
      "Epoch [2568/20000], Training Loss: 0.008348891927328492, Validation Loss: 0.00931814830533532\n",
      "Epoch [2569/20000], Training Loss: 0.012231121047599507, Validation Loss: 0.011656240992241645\n",
      "Epoch [2570/20000], Training Loss: 0.02018449429046346, Validation Loss: 0.019141915619142223\n",
      "Epoch [2571/20000], Training Loss: 0.03381292341925603, Validation Loss: 0.008235952918440415\n",
      "Epoch [2572/20000], Training Loss: 0.036507131998275454, Validation Loss: 0.10571556056089633\n",
      "Epoch [2573/20000], Training Loss: 0.08577156641487298, Validation Loss: 0.11544677382578786\n",
      "Epoch [2574/20000], Training Loss: 0.054289705984826596, Validation Loss: 0.019722731098248753\n",
      "Epoch [2575/20000], Training Loss: 0.02493221274510558, Validation Loss: 0.014788486687070798\n",
      "Epoch [2576/20000], Training Loss: 0.011665268169183816, Validation Loss: 0.01138248063418348\n",
      "Epoch [2577/20000], Training Loss: 0.011971220365791981, Validation Loss: 0.026997988159092625\n",
      "Epoch [2578/20000], Training Loss: 0.013639620808784716, Validation Loss: 0.00914002852232715\n",
      "Epoch [2579/20000], Training Loss: 0.011187439734515334, Validation Loss: 0.006535308968464616\n",
      "Epoch [2580/20000], Training Loss: 0.008900342085066118, Validation Loss: 0.008471993153062674\n",
      "Epoch [2581/20000], Training Loss: 0.008168775690137409, Validation Loss: 0.004049667231797752\n",
      "Epoch [2582/20000], Training Loss: 0.005280063776548819, Validation Loss: 0.014497547009092985\n",
      "Epoch [2583/20000], Training Loss: 0.009588407422727738, Validation Loss: 0.005513072601906819\n",
      "Epoch [2584/20000], Training Loss: 0.005271517377488115, Validation Loss: 0.00647268974364787\n",
      "Epoch [2585/20000], Training Loss: 0.006674626831260146, Validation Loss: 0.004662772225328502\n",
      "Epoch [2586/20000], Training Loss: 0.00898674683412537, Validation Loss: 0.005542122206731758\n",
      "Epoch [2587/20000], Training Loss: 0.027925518109701182, Validation Loss: 0.010420857158672832\n",
      "Epoch [2588/20000], Training Loss: 0.006149130725784094, Validation Loss: 0.0111571705362106\n",
      "Epoch [2589/20000], Training Loss: 0.015320199301640969, Validation Loss: 0.004335115108187176\n",
      "Epoch [2590/20000], Training Loss: 0.013381293230590277, Validation Loss: 0.006818918769586087\n",
      "Epoch [2591/20000], Training Loss: 0.008416129617087011, Validation Loss: 0.0064089948804835605\n",
      "Epoch [2592/20000], Training Loss: 0.005395213268846939, Validation Loss: 0.009315385112088943\n",
      "Epoch [2593/20000], Training Loss: 0.009535178309306502, Validation Loss: 0.004724252563290793\n",
      "Epoch [2594/20000], Training Loss: 0.021345388336158067, Validation Loss: 0.006877335282524232\n",
      "Epoch [2595/20000], Training Loss: 0.016995690043716292, Validation Loss: 0.01805927160496402\n",
      "Epoch [2596/20000], Training Loss: 0.026708545119618066, Validation Loss: 0.011618621227837496\n",
      "Epoch [2597/20000], Training Loss: 0.016714401094109883, Validation Loss: 0.010501838672137177\n",
      "Epoch [2598/20000], Training Loss: 0.01979314782407268, Validation Loss: 0.008576079521086117\n",
      "Epoch [2599/20000], Training Loss: 0.022114479207500284, Validation Loss: 0.016309528677768005\n",
      "Epoch [2600/20000], Training Loss: 0.0170238375721965, Validation Loss: 0.014524918348266299\n",
      "Epoch [2601/20000], Training Loss: 0.012455188139158833, Validation Loss: 0.006612801454709828\n",
      "Epoch [2602/20000], Training Loss: 0.010851314781965422, Validation Loss: 0.005643531050396585\n",
      "Epoch [2603/20000], Training Loss: 0.01971997697754497, Validation Loss: 0.017600435634874272\n",
      "Epoch [2604/20000], Training Loss: 0.012344070760134076, Validation Loss: 0.0125237329738014\n",
      "Epoch [2605/20000], Training Loss: 0.010053863501395557, Validation Loss: 0.008118601106126876\n",
      "Epoch [2606/20000], Training Loss: 0.021608706559553475, Validation Loss: 0.008798641473823574\n",
      "Epoch [2607/20000], Training Loss: 0.00999683264360231, Validation Loss: 0.004578127167962318\n",
      "Epoch [2608/20000], Training Loss: 0.010095585088882737, Validation Loss: 0.005481756964131624\n",
      "Epoch [2609/20000], Training Loss: 0.009094732938527679, Validation Loss: 0.004614366505618946\n",
      "Epoch [2610/20000], Training Loss: 0.007337197157279921, Validation Loss: 0.007977945152091189\n",
      "Epoch [2611/20000], Training Loss: 0.0065309525962220505, Validation Loss: 0.003640099494797409\n",
      "Epoch [2612/20000], Training Loss: 0.007417286247281092, Validation Loss: 0.014834756234275262\n",
      "Epoch [2613/20000], Training Loss: 0.01925135013828237, Validation Loss: 0.017557981182744136\n",
      "Epoch [2614/20000], Training Loss: 0.019935277602469017, Validation Loss: 0.005594042234401525\n",
      "Epoch [2615/20000], Training Loss: 0.010222162423555605, Validation Loss: 0.004795067066686111\n",
      "Epoch [2616/20000], Training Loss: 0.03221325676817547, Validation Loss: 0.004938315238090928\n",
      "Epoch [2617/20000], Training Loss: 0.006659463318198634, Validation Loss: 0.00727801666232683\n",
      "Epoch [2618/20000], Training Loss: 0.007679236855307993, Validation Loss: 0.005709061475932074\n",
      "Epoch [2619/20000], Training Loss: 0.005721380800553432, Validation Loss: 0.00383346205924465\n",
      "Epoch [2620/20000], Training Loss: 0.005808112858046245, Validation Loss: 0.0105358776409509\n",
      "Epoch [2621/20000], Training Loss: 0.008272570215402604, Validation Loss: 0.00394592035859894\n",
      "Epoch [2622/20000], Training Loss: 0.005351417026499153, Validation Loss: 0.006554929384408222\n",
      "Epoch [2623/20000], Training Loss: 0.012323157427861784, Validation Loss: 0.008543083993331813\n",
      "Epoch [2624/20000], Training Loss: 0.013671303791592695, Validation Loss: 0.016900504279427856\n",
      "Epoch [2625/20000], Training Loss: 0.008829823379398607, Validation Loss: 0.0102686620605596\n",
      "Epoch [2626/20000], Training Loss: 0.021774817823565433, Validation Loss: 0.009453379029554803\n",
      "Epoch [2627/20000], Training Loss: 0.015248255665678048, Validation Loss: 0.006497664103397788\n",
      "Epoch [2628/20000], Training Loss: 0.03608143273283661, Validation Loss: 0.012909846075898993\n",
      "Epoch [2629/20000], Training Loss: 0.035210014383275326, Validation Loss: 0.040607819264407465\n",
      "Epoch [2630/20000], Training Loss: 0.035277260517302365, Validation Loss: 0.02389480770410269\n",
      "Epoch [2631/20000], Training Loss: 0.02736635466239282, Validation Loss: 0.02257720557607107\n",
      "Epoch [2632/20000], Training Loss: 0.013330577273986169, Validation Loss: 0.011072300809704092\n",
      "Epoch [2633/20000], Training Loss: 0.010267288507228451, Validation Loss: 0.008991630095450173\n",
      "Epoch [2634/20000], Training Loss: 0.01016585444865216, Validation Loss: 0.006482455025907275\n",
      "Epoch [2635/20000], Training Loss: 0.03401449798444186, Validation Loss: 0.08551630630464656\n",
      "Epoch [2636/20000], Training Loss: 0.04921332841019778, Validation Loss: 0.06142744749116602\n",
      "Epoch [2637/20000], Training Loss: 0.058550562763619904, Validation Loss: 0.061935575918011763\n",
      "Epoch [2638/20000], Training Loss: 0.053721612718488486, Validation Loss: 0.04148398490009605\n",
      "Epoch [2639/20000], Training Loss: 0.02106773214680808, Validation Loss: 0.036245110836598315\n",
      "Epoch [2640/20000], Training Loss: 0.045648385753988156, Validation Loss: 0.019486496811357057\n",
      "Epoch [2641/20000], Training Loss: 0.04020587347414611, Validation Loss: 0.02101430309398974\n",
      "Epoch [2642/20000], Training Loss: 0.026584673722806786, Validation Loss: 0.01660640340077205\n",
      "Epoch [2643/20000], Training Loss: 0.011927348960723196, Validation Loss: 0.008393064315821819\n",
      "Epoch [2644/20000], Training Loss: 0.01081085654108652, Validation Loss: 0.004543103035444779\n",
      "Epoch [2645/20000], Training Loss: 0.01766354900402283, Validation Loss: 0.022000881226970907\n",
      "Epoch [2646/20000], Training Loss: 0.02591070336659738, Validation Loss: 0.011366434419186102\n",
      "Epoch [2647/20000], Training Loss: 0.019535035190139233, Validation Loss: 0.02696517993472109\n",
      "Epoch [2648/20000], Training Loss: 0.017809423534864827, Validation Loss: 0.015273004503930654\n",
      "Epoch [2649/20000], Training Loss: 0.011780429448533272, Validation Loss: 0.007047441326748457\n",
      "Epoch [2650/20000], Training Loss: 0.007415796197684748, Validation Loss: 0.006595824187571806\n",
      "Epoch [2651/20000], Training Loss: 0.008654557035437651, Validation Loss: 0.009916431124825533\n",
      "Epoch [2652/20000], Training Loss: 0.008424046878644731, Validation Loss: 0.004291702215499723\n",
      "Epoch [2653/20000], Training Loss: 0.012459916918097795, Validation Loss: 0.02433372699477209\n",
      "Epoch [2654/20000], Training Loss: 0.026231923299617068, Validation Loss: 0.02492323234991656\n",
      "Epoch [2655/20000], Training Loss: 0.019477000707411207, Validation Loss: 0.011239952629560918\n",
      "Epoch [2656/20000], Training Loss: 0.014436631598593002, Validation Loss: 0.006974053233264598\n",
      "Epoch [2657/20000], Training Loss: 0.010829121404510391, Validation Loss: 0.008205429151045416\n",
      "Epoch [2658/20000], Training Loss: 0.007280136974649005, Validation Loss: 0.007018818222504595\n",
      "Epoch [2659/20000], Training Loss: 0.00787034597098162, Validation Loss: 0.004146914294860278\n",
      "Epoch [2660/20000], Training Loss: 0.008089590239770976, Validation Loss: 0.007541259474901279\n",
      "Epoch [2661/20000], Training Loss: 0.018773967004692946, Validation Loss: 0.02346314060793863\n",
      "Epoch [2662/20000], Training Loss: 0.017475891480755377, Validation Loss: 0.007501194786586264\n",
      "Epoch [2663/20000], Training Loss: 0.011822134511021432, Validation Loss: 0.022124209002126694\n",
      "Epoch [2664/20000], Training Loss: 0.02860391546918046, Validation Loss: 0.06370536530902979\n",
      "Epoch [2665/20000], Training Loss: 0.04583060703173812, Validation Loss: 0.010927943573635333\n",
      "Epoch [2666/20000], Training Loss: 0.042884892872280034, Validation Loss: 0.015425331360378743\n",
      "Epoch [2667/20000], Training Loss: 0.018623857625893185, Validation Loss: 0.030633728289796065\n",
      "Epoch [2668/20000], Training Loss: 0.018989023489601613, Validation Loss: 0.008809518425006473\n",
      "Epoch [2669/20000], Training Loss: 0.010068405619157212, Validation Loss: 0.008122224318734036\n",
      "Epoch [2670/20000], Training Loss: 0.00825724038440967, Validation Loss: 0.0059891976630941255\n",
      "Epoch [2671/20000], Training Loss: 0.008585198006975199, Validation Loss: 0.00486708178509202\n",
      "Epoch [2672/20000], Training Loss: 0.007713923524923822, Validation Loss: 0.007768280350717353\n",
      "Epoch [2673/20000], Training Loss: 0.009799185861733608, Validation Loss: 0.006135711210458526\n",
      "Epoch [2674/20000], Training Loss: 0.007479707470764778, Validation Loss: 0.004347744154303506\n",
      "Epoch [2675/20000], Training Loss: 0.007080787065206096, Validation Loss: 0.005053084039048575\n",
      "Epoch [2676/20000], Training Loss: 0.009171775226215166, Validation Loss: 0.009205823986918075\n",
      "Epoch [2677/20000], Training Loss: 0.01345480275423532, Validation Loss: 0.003170923728135994\n",
      "Epoch [2678/20000], Training Loss: 0.006199128000194573, Validation Loss: 0.00351230855605173\n",
      "Epoch [2679/20000], Training Loss: 0.009050036826270766, Validation Loss: 0.003975700434857251\n",
      "Epoch [2680/20000], Training Loss: 0.02096587554167075, Validation Loss: 0.005566577910324458\n",
      "Epoch [2681/20000], Training Loss: 0.028019011390694817, Validation Loss: 0.005709589777095897\n",
      "Epoch [2682/20000], Training Loss: 0.016555746884218285, Validation Loss: 0.01018538360587744\n",
      "Epoch [2683/20000], Training Loss: 0.025260061863394054, Validation Loss: 0.011059757709698137\n",
      "Epoch [2684/20000], Training Loss: 0.033317777685754536, Validation Loss: 0.00862806720773373\n",
      "Epoch [2685/20000], Training Loss: 0.012819228506746836, Validation Loss: 0.013201514767806769\n",
      "Epoch [2686/20000], Training Loss: 0.013535422956920229, Validation Loss: 0.005826225322822085\n",
      "Epoch [2687/20000], Training Loss: 0.014378113139952933, Validation Loss: 0.005967913889077215\n",
      "Epoch [2688/20000], Training Loss: 0.019516587132654552, Validation Loss: 0.01138294337716253\n",
      "Epoch [2689/20000], Training Loss: 0.0141399622204647, Validation Loss: 0.012691070848214884\n",
      "Epoch [2690/20000], Training Loss: 0.0101556028655198, Validation Loss: 0.016128292963214795\n",
      "Epoch [2691/20000], Training Loss: 0.019499011200553338, Validation Loss: 0.006732969744472991\n",
      "Epoch [2692/20000], Training Loss: 0.009674174572865013, Validation Loss: 0.005104941952664861\n",
      "Epoch [2693/20000], Training Loss: 0.010797956858628563, Validation Loss: 0.00679049435584823\n",
      "Epoch [2694/20000], Training Loss: 0.01244472321039731, Validation Loss: 0.013750738999110643\n",
      "Epoch [2695/20000], Training Loss: 0.03444377391964996, Validation Loss: 0.01005867506583109\n",
      "Epoch [2696/20000], Training Loss: 0.02478696166404656, Validation Loss: 0.0104404028760908\n",
      "Epoch [2697/20000], Training Loss: 0.011693647630246622, Validation Loss: 0.005820623746658684\n",
      "Epoch [2698/20000], Training Loss: 0.007906378807092551, Validation Loss: 0.0046652161062487995\n",
      "Epoch [2699/20000], Training Loss: 0.007825609288244908, Validation Loss: 0.004339583131327352\n",
      "Epoch [2700/20000], Training Loss: 0.008551855689022756, Validation Loss: 0.00871821023098744\n",
      "Epoch [2701/20000], Training Loss: 0.007581081301655753, Validation Loss: 0.003887170148255012\n",
      "Epoch [2702/20000], Training Loss: 0.00954718072067148, Validation Loss: 0.01398001558976014\n",
      "Epoch [2703/20000], Training Loss: 0.010062225483125076, Validation Loss: 0.030532335681238837\n",
      "Epoch [2704/20000], Training Loss: 0.018719013655624752, Validation Loss: 0.005080198474780494\n",
      "Epoch [2705/20000], Training Loss: 0.016115082017807123, Validation Loss: 0.013485425586430613\n",
      "Epoch [2706/20000], Training Loss: 0.02455812309718957, Validation Loss: 0.01762848154639152\n",
      "Epoch [2707/20000], Training Loss: 0.03031295111161723, Validation Loss: 0.03504167628334396\n",
      "Epoch [2708/20000], Training Loss: 0.04180290329635942, Validation Loss: 0.09630542117841596\n",
      "Epoch [2709/20000], Training Loss: 0.048135316431788464, Validation Loss: 0.05121207980268005\n",
      "Epoch [2710/20000], Training Loss: 0.032065864386303086, Validation Loss: 0.017089642679555466\n",
      "Epoch [2711/20000], Training Loss: 0.01677038159687072, Validation Loss: 0.013235302581568256\n",
      "Epoch [2712/20000], Training Loss: 0.011447727782329562, Validation Loss: 0.00973841884935288\n",
      "Epoch [2713/20000], Training Loss: 0.008439888854419613, Validation Loss: 0.009924709147763788\n",
      "Epoch [2714/20000], Training Loss: 0.0075599483763133845, Validation Loss: 0.010722167167868002\n",
      "Epoch [2715/20000], Training Loss: 0.012798839481547475, Validation Loss: 0.007625211345523024\n",
      "Epoch [2716/20000], Training Loss: 0.0070582849628116106, Validation Loss: 0.012680097842627924\n",
      "Epoch [2717/20000], Training Loss: 0.008592851888222088, Validation Loss: 0.007059975885504931\n",
      "Epoch [2718/20000], Training Loss: 0.009333847012645233, Validation Loss: 0.011826479034817178\n",
      "Epoch [2719/20000], Training Loss: 0.011347462094688256, Validation Loss: 0.006456498435066395\n",
      "Epoch [2720/20000], Training Loss: 0.008223159132025362, Validation Loss: 0.006366122956414957\n",
      "Epoch [2721/20000], Training Loss: 0.006120160179437205, Validation Loss: 0.007264537633822837\n",
      "Epoch [2722/20000], Training Loss: 0.0062261193254796255, Validation Loss: 0.009474372610725859\n",
      "Epoch [2723/20000], Training Loss: 0.008461918143439107, Validation Loss: 0.00463280522928646\n",
      "Epoch [2724/20000], Training Loss: 0.009022816195543524, Validation Loss: 0.06810040288536773\n",
      "Epoch [2725/20000], Training Loss: 0.05071699269839363, Validation Loss: 0.01635678870068921\n",
      "Epoch [2726/20000], Training Loss: 0.014755247038140493, Validation Loss: 0.03124787109718722\n",
      "Epoch [2727/20000], Training Loss: 0.028217919046125774, Validation Loss: 0.08311310671492253\n",
      "Epoch [2728/20000], Training Loss: 0.040440894912795296, Validation Loss: 0.012688474372174033\n",
      "Epoch [2729/20000], Training Loss: 0.018679259045581733, Validation Loss: 0.01452274852976269\n",
      "Epoch [2730/20000], Training Loss: 0.01104642650378602, Validation Loss: 0.01485080107913045\n",
      "Epoch [2731/20000], Training Loss: 0.01082353405919483, Validation Loss: 0.008662569773416829\n",
      "Epoch [2732/20000], Training Loss: 0.009599909181166108, Validation Loss: 0.0052741781273034525\n",
      "Epoch [2733/20000], Training Loss: 0.01313852667590254, Validation Loss: 0.011577821754657835\n",
      "Epoch [2734/20000], Training Loss: 0.012386767732095905, Validation Loss: 0.007659531351514399\n",
      "Epoch [2735/20000], Training Loss: 0.02001459787633004, Validation Loss: 0.020638071232563317\n",
      "Epoch [2736/20000], Training Loss: 0.033355861594567875, Validation Loss: 0.014430352094226188\n",
      "Epoch [2737/20000], Training Loss: 0.015177182520606689, Validation Loss: 0.00841650398135399\n",
      "Epoch [2738/20000], Training Loss: 0.018440371113164083, Validation Loss: 0.013258689881443408\n",
      "Epoch [2739/20000], Training Loss: 0.01775648336998919, Validation Loss: 0.014104849510477366\n",
      "Epoch [2740/20000], Training Loss: 0.01028843188708249, Validation Loss: 0.010933227508898292\n",
      "Epoch [2741/20000], Training Loss: 0.008536224831394585, Validation Loss: 0.008518451120106163\n",
      "Epoch [2742/20000], Training Loss: 0.007910220808948256, Validation Loss: 0.005593066976741987\n",
      "Epoch [2743/20000], Training Loss: 0.0054522599530173466, Validation Loss: 0.0032499433272492793\n",
      "Epoch [2744/20000], Training Loss: 0.006271750477026217, Validation Loss: 0.008750955580230684\n",
      "Epoch [2745/20000], Training Loss: 0.008289832913630692, Validation Loss: 0.01687921813235359\n",
      "Epoch [2746/20000], Training Loss: 0.026143839578643174, Validation Loss: 0.017140908158439876\n",
      "Epoch [2747/20000], Training Loss: 0.03459813507340316, Validation Loss: 0.010362850044443825\n",
      "Epoch [2748/20000], Training Loss: 0.035058818660867734, Validation Loss: 0.03426436030845706\n",
      "Epoch [2749/20000], Training Loss: 0.017267023063530878, Validation Loss: 0.047656730223696084\n",
      "Epoch [2750/20000], Training Loss: 0.019151421062166003, Validation Loss: 0.014283941853958232\n",
      "Epoch [2751/20000], Training Loss: 0.014772439508565835, Validation Loss: 0.011872862157919729\n",
      "Epoch [2752/20000], Training Loss: 0.020981530010301088, Validation Loss: 0.01553887396786032\n",
      "Epoch [2753/20000], Training Loss: 0.02530070788426591, Validation Loss: 0.0754133070143509\n",
      "Epoch [2754/20000], Training Loss: 0.04675685564455177, Validation Loss: 0.009491610870849047\n",
      "Epoch [2755/20000], Training Loss: 0.024048292890906202, Validation Loss: 0.011099857422075953\n",
      "Epoch [2756/20000], Training Loss: 0.009906412355901142, Validation Loss: 0.008552738190454743\n",
      "Epoch [2757/20000], Training Loss: 0.008598200045526028, Validation Loss: 0.009145696300220219\n",
      "Epoch [2758/20000], Training Loss: 0.010815895006609415, Validation Loss: 0.006634946055543553\n",
      "Epoch [2759/20000], Training Loss: 0.009199863942090556, Validation Loss: 0.005699083882696428\n",
      "Epoch [2760/20000], Training Loss: 0.01591181602998404, Validation Loss: 0.02206224227236548\n",
      "Epoch [2761/20000], Training Loss: 0.028972171230374703, Validation Loss: 0.030206117362248857\n",
      "Epoch [2762/20000], Training Loss: 0.024830158314476387, Validation Loss: 0.042632244129803216\n",
      "Epoch [2763/20000], Training Loss: 0.02849758048874459, Validation Loss: 0.04252393340718741\n",
      "Epoch [2764/20000], Training Loss: 0.013274176886105644, Validation Loss: 0.008696230825957713\n",
      "Epoch [2765/20000], Training Loss: 0.010295274135257517, Validation Loss: 0.007936980497330371\n",
      "Epoch [2766/20000], Training Loss: 0.007724880714834269, Validation Loss: 0.007903267579696201\n",
      "Epoch [2767/20000], Training Loss: 0.0065435305127071585, Validation Loss: 0.007171970984515967\n",
      "Epoch [2768/20000], Training Loss: 0.008495814866494453, Validation Loss: 0.005458462669183955\n",
      "Epoch [2769/20000], Training Loss: 0.0061240218845861295, Validation Loss: 0.005439022479472118\n",
      "Epoch [2770/20000], Training Loss: 0.007014879575373405, Validation Loss: 0.01198747957754053\n",
      "Epoch [2771/20000], Training Loss: 0.007994640809816442, Validation Loss: 0.005683571648377121\n",
      "Epoch [2772/20000], Training Loss: 0.013600339339713432, Validation Loss: 0.011534438103692464\n",
      "Epoch [2773/20000], Training Loss: 0.018117006250707033, Validation Loss: 0.0071679560378236275\n",
      "Epoch [2774/20000], Training Loss: 0.01949601300709349, Validation Loss: 0.01670807546421055\n",
      "Epoch [2775/20000], Training Loss: 0.03708698004733638, Validation Loss: 0.010320180127609362\n",
      "Epoch [2776/20000], Training Loss: 0.03435245412401855, Validation Loss: 0.012834092913792288\n",
      "Epoch [2777/20000], Training Loss: 0.01262159984824913, Validation Loss: 0.024918664209131247\n",
      "Epoch [2778/20000], Training Loss: 0.012008486190877323, Validation Loss: 0.007286832833021495\n",
      "Epoch [2779/20000], Training Loss: 0.00956094210518391, Validation Loss: 0.00747257015812477\n",
      "Epoch [2780/20000], Training Loss: 0.017774171455779912, Validation Loss: 0.007797269898685913\n",
      "Epoch [2781/20000], Training Loss: 0.01740064807901425, Validation Loss: 0.013439221083903902\n",
      "Epoch [2782/20000], Training Loss: 0.011989081733710398, Validation Loss: 0.0038029389305362592\n",
      "Epoch [2783/20000], Training Loss: 0.007052167384764678, Validation Loss: 0.01606748387111891\n",
      "Epoch [2784/20000], Training Loss: 0.01198106849915348, Validation Loss: 0.0164856865754455\n",
      "Epoch [2785/20000], Training Loss: 0.015809697204531403, Validation Loss: 0.01607368986626402\n",
      "Epoch [2786/20000], Training Loss: 0.011963693099720072, Validation Loss: 0.003535013994357261\n",
      "Epoch [2787/20000], Training Loss: 0.006238964486068913, Validation Loss: 0.004458858891796677\n",
      "Epoch [2788/20000], Training Loss: 0.006365025618184258, Validation Loss: 0.009764940510350808\n",
      "Epoch [2789/20000], Training Loss: 0.013709158154337533, Validation Loss: 0.006271419752125668\n",
      "Epoch [2790/20000], Training Loss: 0.012202786801676016, Validation Loss: 0.006315086832985761\n",
      "Epoch [2791/20000], Training Loss: 0.010821149661621478, Validation Loss: 0.0026602972967979506\n",
      "Epoch [2792/20000], Training Loss: 0.006850935275321847, Validation Loss: 0.004654606962739906\n",
      "Epoch [2793/20000], Training Loss: 0.006059584325287558, Validation Loss: 0.007033527176674935\n",
      "Epoch [2794/20000], Training Loss: 0.009658040315636234, Validation Loss: 0.006378292942043195\n",
      "Epoch [2795/20000], Training Loss: 0.01705626148707649, Validation Loss: 0.008358936975125112\n",
      "Epoch [2796/20000], Training Loss: 0.06300963504976867, Validation Loss: 0.010104450605913631\n",
      "Epoch [2797/20000], Training Loss: 0.012619395000261388, Validation Loss: 0.009884095069643078\n",
      "Epoch [2798/20000], Training Loss: 0.010578904367451156, Validation Loss: 0.006755571463098345\n",
      "Epoch [2799/20000], Training Loss: 0.01617257118258359, Validation Loss: 0.007527309339859566\n",
      "Epoch [2800/20000], Training Loss: 0.016092133068013936, Validation Loss: 0.007367355763897824\n",
      "Epoch [2801/20000], Training Loss: 0.007498315371256987, Validation Loss: 0.005204249054933575\n",
      "Epoch [2802/20000], Training Loss: 0.004832131804368177, Validation Loss: 0.004444859652883724\n",
      "Epoch [2803/20000], Training Loss: 0.004429889142556931, Validation Loss: 0.005275407476503417\n",
      "Epoch [2804/20000], Training Loss: 0.008236664266730389, Validation Loss: 0.007983906896643708\n",
      "Epoch [2805/20000], Training Loss: 0.01686300140266082, Validation Loss: 0.004013284091774853\n",
      "Epoch [2806/20000], Training Loss: 0.019750348824475492, Validation Loss: 0.010599325897902802\n",
      "Epoch [2807/20000], Training Loss: 0.014396699586151434, Validation Loss: 0.014881127395669247\n",
      "Epoch [2808/20000], Training Loss: 0.028017831391480286, Validation Loss: 0.01689453873212247\n",
      "Epoch [2809/20000], Training Loss: 0.0374625263211783, Validation Loss: 0.020231635845695446\n",
      "Epoch [2810/20000], Training Loss: 0.042720140722979395, Validation Loss: 0.025479799046432106\n",
      "Epoch [2811/20000], Training Loss: 0.026077622996776233, Validation Loss: 0.017862851476028094\n",
      "Epoch [2812/20000], Training Loss: 0.014225621740998966, Validation Loss: 0.02269186811376953\n",
      "Epoch [2813/20000], Training Loss: 0.010337759430902744, Validation Loss: 0.01052243994106176\n",
      "Epoch [2814/20000], Training Loss: 0.009828743782626199, Validation Loss: 0.006558980371839919\n",
      "Epoch [2815/20000], Training Loss: 0.01046556863418248, Validation Loss: 0.007007980320693475\n",
      "Epoch [2816/20000], Training Loss: 0.009943469107383862, Validation Loss: 0.009133733356510548\n",
      "Epoch [2817/20000], Training Loss: 0.008272463373162151, Validation Loss: 0.008682454218617516\n",
      "Epoch [2818/20000], Training Loss: 0.008609508343007681, Validation Loss: 0.006208022350028074\n",
      "Epoch [2819/20000], Training Loss: 0.03059526555339939, Validation Loss: 0.017296681753871414\n",
      "Epoch [2820/20000], Training Loss: 0.05399960733939224, Validation Loss: 0.021657047968874403\n",
      "Epoch [2821/20000], Training Loss: 0.02053972227232797, Validation Loss: 0.008303730640882542\n",
      "Epoch [2822/20000], Training Loss: 0.012997775255436344, Validation Loss: 0.015034453190149039\n",
      "Epoch [2823/20000], Training Loss: 0.008091227001776653, Validation Loss: 0.006750191542290972\n",
      "Epoch [2824/20000], Training Loss: 0.008664279968278217, Validation Loss: 0.006213050379886877\n",
      "Epoch [2825/20000], Training Loss: 0.009995035850027176, Validation Loss: 0.0121778111705483\n",
      "Epoch [2826/20000], Training Loss: 0.013787672775963853, Validation Loss: 0.018532827110367085\n",
      "Epoch [2827/20000], Training Loss: 0.006938756168113157, Validation Loss: 0.009649663264819678\n",
      "Epoch [2828/20000], Training Loss: 0.01328326238061501, Validation Loss: 0.005068594424175546\n",
      "Epoch [2829/20000], Training Loss: 0.01337817485078371, Validation Loss: 0.010119895115365094\n",
      "Epoch [2830/20000], Training Loss: 0.010226465196215681, Validation Loss: 0.008737986057767254\n",
      "Epoch [2831/20000], Training Loss: 0.009598587421351112, Validation Loss: 0.0195386335641564\n",
      "Epoch [2832/20000], Training Loss: 0.013596536512652944, Validation Loss: 0.008204727712773367\n",
      "Epoch [2833/20000], Training Loss: 0.009826121948468167, Validation Loss: 0.0051439433321847795\n",
      "Epoch [2834/20000], Training Loss: 0.00626782799164565, Validation Loss: 0.004477576695925812\n",
      "Epoch [2835/20000], Training Loss: 0.006614065082560826, Validation Loss: 0.010191781128005095\n",
      "Epoch [2836/20000], Training Loss: 0.008436306138589447, Validation Loss: 0.0032492554411681007\n",
      "Epoch [2837/20000], Training Loss: 0.00771768521371996, Validation Loss: 0.004925415340803208\n",
      "Epoch [2838/20000], Training Loss: 0.005460312524519395, Validation Loss: 0.008004922512100273\n",
      "Epoch [2839/20000], Training Loss: 0.009150679582879613, Validation Loss: 0.0038416900849480053\n",
      "Epoch [2840/20000], Training Loss: 0.0059248940233374014, Validation Loss: 0.0035397656020807757\n",
      "Epoch [2841/20000], Training Loss: 0.01523182199785619, Validation Loss: 0.01787994884255395\n",
      "Epoch [2842/20000], Training Loss: 0.04110666310095569, Validation Loss: 0.03228001089880379\n",
      "Epoch [2843/20000], Training Loss: 0.021585087454046255, Validation Loss: 0.042498939402194555\n",
      "Epoch [2844/20000], Training Loss: 0.022912603422550352, Validation Loss: 0.03250516417107615\n",
      "Epoch [2845/20000], Training Loss: 0.020243702364885912, Validation Loss: 0.01128349812371002\n",
      "Epoch [2846/20000], Training Loss: 0.009030371421496446, Validation Loss: 0.005860604898127399\n",
      "Epoch [2847/20000], Training Loss: 0.010680825362214819, Validation Loss: 0.014450399895259225\n",
      "Epoch [2848/20000], Training Loss: 0.01830811400317803, Validation Loss: 0.014501702137565287\n",
      "Epoch [2849/20000], Training Loss: 0.007112033586184095, Validation Loss: 0.004449087197894302\n",
      "Epoch [2850/20000], Training Loss: 0.005592332962968223, Validation Loss: 0.0037111374689257457\n",
      "Epoch [2851/20000], Training Loss: 0.005570169862559331, Validation Loss: 0.003686220891398754\n",
      "Epoch [2852/20000], Training Loss: 0.007706522313778156, Validation Loss: 0.005457711185778131\n",
      "Epoch [2853/20000], Training Loss: 0.011849868381562243, Validation Loss: 0.010061570197578541\n",
      "Epoch [2854/20000], Training Loss: 0.008626200773245987, Validation Loss: 0.0038366287387413068\n",
      "Epoch [2855/20000], Training Loss: 0.004766321330472627, Validation Loss: 0.004913289087840424\n",
      "Epoch [2856/20000], Training Loss: 0.006217936371935399, Validation Loss: 0.004308311172549866\n",
      "Epoch [2857/20000], Training Loss: 0.00483050454494917, Validation Loss: 0.010493932558753404\n",
      "Epoch [2858/20000], Training Loss: 0.01253118654962496, Validation Loss: 0.0044912471413718486\n",
      "Epoch [2859/20000], Training Loss: 0.0071461590489759275, Validation Loss: 0.0032713996053749077\n",
      "Epoch [2860/20000], Training Loss: 0.006880061079885179, Validation Loss: 0.008818127304412065\n",
      "Epoch [2861/20000], Training Loss: 0.026891151153774575, Validation Loss: 0.028927344117091934\n",
      "Epoch [2862/20000], Training Loss: 0.052730808154754935, Validation Loss: 0.06166379723417857\n",
      "Epoch [2863/20000], Training Loss: 0.04716127772865418, Validation Loss: 0.029880704403105165\n",
      "Epoch [2864/20000], Training Loss: 0.0177829200900825, Validation Loss: 0.020602483205063558\n",
      "Epoch [2865/20000], Training Loss: 0.01798990834504366, Validation Loss: 0.018163071766639567\n",
      "Epoch [2866/20000], Training Loss: 0.015742613252119293, Validation Loss: 0.021677155921967034\n",
      "Epoch [2867/20000], Training Loss: 0.016129387837801396, Validation Loss: 0.00936902891941109\n",
      "Epoch [2868/20000], Training Loss: 0.009233152202796191, Validation Loss: 0.006322310467244386\n",
      "Epoch [2869/20000], Training Loss: 0.008512133287110504, Validation Loss: 0.00892422946228955\n",
      "Epoch [2870/20000], Training Loss: 0.006091402544240866, Validation Loss: 0.005684496120109202\n",
      "Epoch [2871/20000], Training Loss: 0.010422645842741727, Validation Loss: 0.034650641336844444\n",
      "Epoch [2872/20000], Training Loss: 0.024142856017403704, Validation Loss: 0.006000734187004709\n",
      "Epoch [2873/20000], Training Loss: 0.013333009399015541, Validation Loss: 0.006353112178456371\n",
      "Epoch [2874/20000], Training Loss: 0.00903312312064892, Validation Loss: 0.005053951593319936\n",
      "Epoch [2875/20000], Training Loss: 0.007535075677359211, Validation Loss: 0.004600958869336628\n",
      "Epoch [2876/20000], Training Loss: 0.007483498684970462, Validation Loss: 0.005189285893461138\n",
      "Epoch [2877/20000], Training Loss: 0.009585084807310653, Validation Loss: 0.014577676775008517\n",
      "Epoch [2878/20000], Training Loss: 0.015821125651460273, Validation Loss: 0.007906355189362651\n",
      "Epoch [2879/20000], Training Loss: 0.011225543953644643, Validation Loss: 0.010001461815607368\n",
      "Epoch [2880/20000], Training Loss: 0.010131223541975487, Validation Loss: 0.00736746138081966\n",
      "Epoch [2881/20000], Training Loss: 0.011901013595742629, Validation Loss: 0.006746646126869329\n",
      "Epoch [2882/20000], Training Loss: 0.014724643576690661, Validation Loss: 0.004255119247294039\n",
      "Epoch [2883/20000], Training Loss: 0.009457772132009268, Validation Loss: 0.005289020443472407\n",
      "Epoch [2884/20000], Training Loss: 0.006376732632426345, Validation Loss: 0.007266641595449028\n",
      "Epoch [2885/20000], Training Loss: 0.0068627651198767126, Validation Loss: 0.0053263861327325695\n",
      "Epoch [2886/20000], Training Loss: 0.008892097493766673, Validation Loss: 0.007860729697444172\n",
      "Epoch [2887/20000], Training Loss: 0.009444573629610074, Validation Loss: 0.008103647592642064\n",
      "Epoch [2888/20000], Training Loss: 0.012835564927497347, Validation Loss: 0.047569096734386175\n",
      "Epoch [2889/20000], Training Loss: 0.037682512677357796, Validation Loss: 0.013262791580830475\n",
      "Epoch [2890/20000], Training Loss: 0.055030780879211046, Validation Loss: 0.029406253643494466\n",
      "Epoch [2891/20000], Training Loss: 0.016539756508011903, Validation Loss: 0.008742678831377948\n",
      "Epoch [2892/20000], Training Loss: 0.009759888726486159, Validation Loss: 0.007152398704482528\n",
      "Epoch [2893/20000], Training Loss: 0.009968192931929869, Validation Loss: 0.006966725850178872\n",
      "Epoch [2894/20000], Training Loss: 0.007483986555598676, Validation Loss: 0.004707104283630516\n",
      "Epoch [2895/20000], Training Loss: 0.0066946944778984675, Validation Loss: 0.00761967903372525\n",
      "Epoch [2896/20000], Training Loss: 0.007264294859071795, Validation Loss: 0.013911473859481924\n",
      "Epoch [2897/20000], Training Loss: 0.011896391985958741, Validation Loss: 0.014716416688836682\n",
      "Epoch [2898/20000], Training Loss: 0.017895317099048822, Validation Loss: 0.0162274216722195\n",
      "Epoch [2899/20000], Training Loss: 0.019837601943956024, Validation Loss: 0.045226870220827366\n",
      "Epoch [2900/20000], Training Loss: 0.023041220092896504, Validation Loss: 0.006134874951451242\n",
      "Epoch [2901/20000], Training Loss: 0.013571462544080402, Validation Loss: 0.01097533142092391\n",
      "Epoch [2902/20000], Training Loss: 0.007252697469084524, Validation Loss: 0.006346355455662661\n",
      "Epoch [2903/20000], Training Loss: 0.010472498726033206, Validation Loss: 0.004791386491109268\n",
      "Epoch [2904/20000], Training Loss: 0.01409502778879609, Validation Loss: 0.0060586374177420506\n",
      "Epoch [2905/20000], Training Loss: 0.009346940845716745, Validation Loss: 0.00836105599514064\n",
      "Epoch [2906/20000], Training Loss: 0.008236581001710874, Validation Loss: 0.004293714327104989\n",
      "Epoch [2907/20000], Training Loss: 0.007666117800129412, Validation Loss: 0.011275245835610001\n",
      "Epoch [2908/20000], Training Loss: 0.009962687273010877, Validation Loss: 0.007353179875346264\n",
      "Epoch [2909/20000], Training Loss: 0.022898728253754337, Validation Loss: 0.011557283886629844\n",
      "Epoch [2910/20000], Training Loss: 0.007573428513881352, Validation Loss: 0.00719939864297578\n",
      "Epoch [2911/20000], Training Loss: 0.007920577671029605, Validation Loss: 0.008376508055724964\n",
      "Epoch [2912/20000], Training Loss: 0.011055025793861464, Validation Loss: 0.012117430793296188\n",
      "Epoch [2913/20000], Training Loss: 0.015309176860942639, Validation Loss: 0.010576441239574614\n",
      "Epoch [2914/20000], Training Loss: 0.01432569931992995, Validation Loss: 0.01083147851155844\n",
      "Epoch [2915/20000], Training Loss: 0.02040069007697249, Validation Loss: 0.014723696660342251\n",
      "Epoch [2916/20000], Training Loss: 0.021361758088460192, Validation Loss: 0.03495653844687187\n",
      "Epoch [2917/20000], Training Loss: 0.020333346086512653, Validation Loss: 0.012870698128223059\n",
      "Epoch [2918/20000], Training Loss: 0.008631143078673631, Validation Loss: 0.01040906915394314\n",
      "Epoch [2919/20000], Training Loss: 0.009340588707605743, Validation Loss: 0.017843143272822434\n",
      "Epoch [2920/20000], Training Loss: 0.016148950315255206, Validation Loss: 0.011095174567710168\n",
      "Epoch [2921/20000], Training Loss: 0.010789195068095328, Validation Loss: 0.008901308351754959\n",
      "Epoch [2922/20000], Training Loss: 0.007893570603170832, Validation Loss: 0.008555446918119271\n",
      "Epoch [2923/20000], Training Loss: 0.009062662007701354, Validation Loss: 0.004078320353918288\n",
      "Epoch [2924/20000], Training Loss: 0.007280576808950302, Validation Loss: 0.009967109747698587\n",
      "Epoch [2925/20000], Training Loss: 0.008427290815494157, Validation Loss: 0.004391733499460368\n",
      "Epoch [2926/20000], Training Loss: 0.008738566962295278, Validation Loss: 0.0056869074653442975\n",
      "Epoch [2927/20000], Training Loss: 0.010666232610154631, Validation Loss: 0.006578035934420993\n",
      "Epoch [2928/20000], Training Loss: 0.009812571297004524, Validation Loss: 0.01460607032437829\n",
      "Epoch [2929/20000], Training Loss: 0.01604443804535549, Validation Loss: 0.014898012731082128\n",
      "Epoch [2930/20000], Training Loss: 0.008673103292884272, Validation Loss: 0.01078806875823704\n",
      "Epoch [2931/20000], Training Loss: 0.02423026053916796, Validation Loss: 0.06980847162568755\n",
      "Epoch [2932/20000], Training Loss: 0.04022796765535271, Validation Loss: 0.012463122715010872\n",
      "Epoch [2933/20000], Training Loss: 0.04544817985359779, Validation Loss: 0.023643453998959677\n",
      "Epoch [2934/20000], Training Loss: 0.01657582407850506, Validation Loss: 0.021469607311052345\n",
      "Epoch [2935/20000], Training Loss: 0.01614844172062086, Validation Loss: 0.00934132508622811\n",
      "Epoch [2936/20000], Training Loss: 0.011864498600646454, Validation Loss: 0.0058224627389016565\n",
      "Epoch [2937/20000], Training Loss: 0.008133860901580192, Validation Loss: 0.013853587004541422\n",
      "Epoch [2938/20000], Training Loss: 0.012541086849523708, Validation Loss: 0.012438054053926537\n",
      "Epoch [2939/20000], Training Loss: 0.02008829786726502, Validation Loss: 0.0276348691360734\n",
      "Epoch [2940/20000], Training Loss: 0.020409532347652463, Validation Loss: 0.027828848981008986\n",
      "Epoch [2941/20000], Training Loss: 0.02447445648379341, Validation Loss: 0.029410042784806963\n",
      "Epoch [2942/20000], Training Loss: 0.012865954269987665, Validation Loss: 0.006347077761022857\n",
      "Epoch [2943/20000], Training Loss: 0.008315262235035854, Validation Loss: 0.004393372700462992\n",
      "Epoch [2944/20000], Training Loss: 0.006533495983603643, Validation Loss: 0.004090694265138667\n",
      "Epoch [2945/20000], Training Loss: 0.007913843668315426, Validation Loss: 0.008786735600397572\n",
      "Epoch [2946/20000], Training Loss: 0.015229406209462988, Validation Loss: 0.007390445040316963\n",
      "Epoch [2947/20000], Training Loss: 0.016518486281711375, Validation Loss: 0.016317047945530627\n",
      "Epoch [2948/20000], Training Loss: 0.01065435443050384, Validation Loss: 0.0035790040491252527\n",
      "Epoch [2949/20000], Training Loss: 0.004280994394710953, Validation Loss: 0.0036367589168421048\n",
      "Epoch [2950/20000], Training Loss: 0.007518094918227953, Validation Loss: 0.009660017024064872\n",
      "Epoch [2951/20000], Training Loss: 0.021873153255520656, Validation Loss: 0.004788519325183129\n",
      "Epoch [2952/20000], Training Loss: 0.027744501845778098, Validation Loss: 0.03572147068384172\n",
      "Epoch [2953/20000], Training Loss: 0.023253616901846335, Validation Loss: 0.022292333891789157\n",
      "Epoch [2954/20000], Training Loss: 0.0251322465150484, Validation Loss: 0.012823361033109135\n",
      "Epoch [2955/20000], Training Loss: 0.024466461549829028, Validation Loss: 0.012011361292140662\n",
      "Epoch [2956/20000], Training Loss: 0.015008268728187042, Validation Loss: 0.009494362521521476\n",
      "Epoch [2957/20000], Training Loss: 0.008612779214412771, Validation Loss: 0.006180326449624125\n",
      "Epoch [2958/20000], Training Loss: 0.009711013519579734, Validation Loss: 0.015495338094343957\n",
      "Epoch [2959/20000], Training Loss: 0.011530792582951628, Validation Loss: 0.032807864620541284\n",
      "Epoch [2960/20000], Training Loss: 0.015391966465228637, Validation Loss: 0.02645780468104985\n",
      "Epoch [2961/20000], Training Loss: 0.01048506341164414, Validation Loss: 0.008060560642939765\n",
      "Epoch [2962/20000], Training Loss: 0.00554820204095969, Validation Loss: 0.005214976212430877\n",
      "Epoch [2963/20000], Training Loss: 0.006778515139428366, Validation Loss: 0.005762535731533561\n",
      "Epoch [2964/20000], Training Loss: 0.0075737583922870855, Validation Loss: 0.004850039991922054\n",
      "Epoch [2965/20000], Training Loss: 0.012396987276068623, Validation Loss: 0.010149315586911989\n",
      "Epoch [2966/20000], Training Loss: 0.012609307019114826, Validation Loss: 0.002550181492161411\n",
      "Epoch [2967/20000], Training Loss: 0.007036517056186702, Validation Loss: 0.003994208697137927\n",
      "Epoch [2968/20000], Training Loss: 0.009230965820149453, Validation Loss: 0.0020571347322496355\n",
      "Epoch [2969/20000], Training Loss: 0.009288499791434137, Validation Loss: 0.001987812114748339\n",
      "Epoch [2970/20000], Training Loss: 0.022176385307895025, Validation Loss: 0.08918277069065873\n",
      "Epoch [2971/20000], Training Loss: 0.05082588957572755, Validation Loss: 0.03494739400248868\n",
      "Epoch [2972/20000], Training Loss: 0.03319583593500803, Validation Loss: 0.012393817721199093\n",
      "Epoch [2973/20000], Training Loss: 0.032210844367260245, Validation Loss: 0.008679080549098913\n",
      "Epoch [2974/20000], Training Loss: 0.012603039504028857, Validation Loss: 0.0084051385748707\n",
      "Epoch [2975/20000], Training Loss: 0.012935866111157728, Validation Loss: 0.008624909913739102\n",
      "Epoch [2976/20000], Training Loss: 0.008716168956457946, Validation Loss: 0.01317806883659695\n",
      "Epoch [2977/20000], Training Loss: 0.013218422220753772, Validation Loss: 0.015146803001341731\n",
      "Epoch [2978/20000], Training Loss: 0.011951100779697299, Validation Loss: 0.012149776755965519\n",
      "Epoch [2979/20000], Training Loss: 0.013059501841066126, Validation Loss: 0.011202798865310422\n",
      "Epoch [2980/20000], Training Loss: 0.010907894274818577, Validation Loss: 0.006676145715261321\n",
      "Epoch [2981/20000], Training Loss: 0.011867229045102639, Validation Loss: 0.00626341987763381\n",
      "Epoch [2982/20000], Training Loss: 0.011621191933435122, Validation Loss: 0.007514266604353768\n",
      "Epoch [2983/20000], Training Loss: 0.012837484853142607, Validation Loss: 0.006162735346641901\n",
      "Epoch [2984/20000], Training Loss: 0.011421347315522976, Validation Loss: 0.01815482129150247\n",
      "Epoch [2985/20000], Training Loss: 0.0068984927513936, Validation Loss: 0.00506521247732755\n",
      "Epoch [2986/20000], Training Loss: 0.005676675560867547, Validation Loss: 0.006595793310439961\n",
      "Epoch [2987/20000], Training Loss: 0.009025320797068812, Validation Loss: 0.010502097044100897\n",
      "Epoch [2988/20000], Training Loss: 0.012008263198627642, Validation Loss: 0.018058977535298202\n",
      "Epoch [2989/20000], Training Loss: 0.016280556259776598, Validation Loss: 0.01323302626695567\n",
      "Epoch [2990/20000], Training Loss: 0.008002456038963959, Validation Loss: 0.009428905466979085\n",
      "Epoch [2991/20000], Training Loss: 0.014882546687398903, Validation Loss: 0.007489878930614942\n",
      "Epoch [2992/20000], Training Loss: 0.019166756583477502, Validation Loss: 0.008075378831467172\n",
      "Epoch [2993/20000], Training Loss: 0.011912885465790168, Validation Loss: 0.005984662463997774\n",
      "Epoch [2994/20000], Training Loss: 0.005871035175914585, Validation Loss: 0.02454199779750374\n",
      "Epoch [2995/20000], Training Loss: 0.01585173710181412, Validation Loss: 0.005707982479776732\n",
      "Epoch [2996/20000], Training Loss: 0.013119244316060628, Validation Loss: 0.03440596649220614\n",
      "Epoch [2997/20000], Training Loss: 0.01837846779796694, Validation Loss: 0.021397348450931282\n",
      "Epoch [2998/20000], Training Loss: 0.020646825499300445, Validation Loss: 0.009284547003948904\n",
      "Epoch [2999/20000], Training Loss: 0.009385914259058024, Validation Loss: 0.008456580615497837\n",
      "Epoch [3000/20000], Training Loss: 0.021729585400732634, Validation Loss: 0.005549846802048458\n",
      "Epoch [3001/20000], Training Loss: 0.02125683842626001, Validation Loss: 0.009814756535054054\n",
      "Epoch [3002/20000], Training Loss: 0.016707680512419238, Validation Loss: 0.01762946037209203\n",
      "Epoch [3003/20000], Training Loss: 0.011460342009740998, Validation Loss: 0.006101218251762973\n",
      "Epoch [3004/20000], Training Loss: 0.006772853024553375, Validation Loss: 0.00523250404108281\n",
      "Epoch [3005/20000], Training Loss: 0.006431445030362478, Validation Loss: 0.0052421347370839545\n",
      "Epoch [3006/20000], Training Loss: 0.005172699457164397, Validation Loss: 0.01170927142032068\n",
      "Epoch [3007/20000], Training Loss: 0.02423702716727608, Validation Loss: 0.04075395386046247\n",
      "Epoch [3008/20000], Training Loss: 0.027911380096026863, Validation Loss: 0.005560662507317877\n",
      "Epoch [3009/20000], Training Loss: 0.018058856767311227, Validation Loss: 0.009634783497321763\n",
      "Epoch [3010/20000], Training Loss: 0.009781927848442657, Validation Loss: 0.021508023421392658\n",
      "Epoch [3011/20000], Training Loss: 0.025100555470479385, Validation Loss: 0.023539795854988832\n",
      "Epoch [3012/20000], Training Loss: 0.008212353876193188, Validation Loss: 0.005696097605265642\n",
      "Epoch [3013/20000], Training Loss: 0.006449995495911155, Validation Loss: 0.007385365491282166\n",
      "Epoch [3014/20000], Training Loss: 0.007600179865912533, Validation Loss: 0.004037940204617173\n",
      "Epoch [3015/20000], Training Loss: 0.004779222492028826, Validation Loss: 0.003926928247852968\n",
      "Epoch [3016/20000], Training Loss: 0.0064748004193201526, Validation Loss: 0.005669485914374225\n",
      "Epoch [3017/20000], Training Loss: 0.008537741673145709, Validation Loss: 0.005666485105195271\n",
      "Epoch [3018/20000], Training Loss: 0.006415803523071061, Validation Loss: 0.003133131808846112\n",
      "Epoch [3019/20000], Training Loss: 0.006773735899644505, Validation Loss: 0.023083618541283664\n",
      "Epoch [3020/20000], Training Loss: 0.01910206268699507, Validation Loss: 0.021411640986100215\n",
      "Epoch [3021/20000], Training Loss: 0.02248754145486081, Validation Loss: 0.011479596486171926\n",
      "Epoch [3022/20000], Training Loss: 0.019207227444504888, Validation Loss: 0.042093730062130255\n",
      "Epoch [3023/20000], Training Loss: 0.01915860644242327, Validation Loss: 0.006633037609851298\n",
      "Epoch [3024/20000], Training Loss: 0.013498217815400235, Validation Loss: 0.009043222277573584\n",
      "Epoch [3025/20000], Training Loss: 0.009269014258669423, Validation Loss: 0.004234986509836434\n",
      "Epoch [3026/20000], Training Loss: 0.01118445530716729, Validation Loss: 0.008540775012122843\n",
      "Epoch [3027/20000], Training Loss: 0.014810664333968557, Validation Loss: 0.0085206231368357\n",
      "Epoch [3028/20000], Training Loss: 0.04066831219824962, Validation Loss: 0.03443478383739323\n",
      "Epoch [3029/20000], Training Loss: 0.037097823864314705, Validation Loss: 0.017766685110343072\n",
      "Epoch [3030/20000], Training Loss: 0.018168123667627305, Validation Loss: 0.009125156109210164\n",
      "Epoch [3031/20000], Training Loss: 0.016142366121390035, Validation Loss: 0.0147912368264749\n",
      "Epoch [3032/20000], Training Loss: 0.0126031977207666, Validation Loss: 0.007936676904463563\n",
      "Epoch [3033/20000], Training Loss: 0.011098680277687631, Validation Loss: 0.015599321372063407\n",
      "Epoch [3034/20000], Training Loss: 0.008704943644780931, Validation Loss: 0.006477264350984106\n",
      "Epoch [3035/20000], Training Loss: 0.00655676233925208, Validation Loss: 0.006418786556023248\n",
      "Epoch [3036/20000], Training Loss: 0.007385858966569815, Validation Loss: 0.004430329083289831\n",
      "Epoch [3037/20000], Training Loss: 0.011328657039224968, Validation Loss: 0.00786029869737678\n",
      "Epoch [3038/20000], Training Loss: 0.020264910276377383, Validation Loss: 0.03646859137244218\n",
      "Epoch [3039/20000], Training Loss: 0.029089623540065595, Validation Loss: 0.014550314822568456\n",
      "Epoch [3040/20000], Training Loss: 0.011645161440745662, Validation Loss: 0.04402955726925612\n",
      "Epoch [3041/20000], Training Loss: 0.014252405308070593, Validation Loss: 0.009043949095888192\n",
      "Epoch [3042/20000], Training Loss: 0.011611415627287767, Validation Loss: 0.00451385912372796\n",
      "Epoch [3043/20000], Training Loss: 0.008466904974609082, Validation Loss: 0.004143336283673005\n",
      "Epoch [3044/20000], Training Loss: 0.00934558622611803, Validation Loss: 0.01567446350483379\n",
      "Epoch [3045/20000], Training Loss: 0.025894006091610727, Validation Loss: 0.0186285105293369\n",
      "Epoch [3046/20000], Training Loss: 0.028755434772132764, Validation Loss: 0.008451609228407849\n",
      "Epoch [3047/20000], Training Loss: 0.016747345666122522, Validation Loss: 0.007387257180940593\n",
      "Epoch [3048/20000], Training Loss: 0.010845460729407412, Validation Loss: 0.00842794135071344\n",
      "Epoch [3049/20000], Training Loss: 0.009885511637548916, Validation Loss: 0.006647967841084584\n",
      "Epoch [3050/20000], Training Loss: 0.015136821324371599, Validation Loss: 0.006134116830764118\n",
      "Epoch [3051/20000], Training Loss: 0.013936740209049146, Validation Loss: 0.006091146281236989\n",
      "Epoch [3052/20000], Training Loss: 0.007189976151234337, Validation Loss: 0.005615647362049119\n",
      "Epoch [3053/20000], Training Loss: 0.007349671667075849, Validation Loss: 0.014103456445211745\n",
      "Epoch [3054/20000], Training Loss: 0.010425409401900001, Validation Loss: 0.006025559776771282\n",
      "Epoch [3055/20000], Training Loss: 0.008258620090990527, Validation Loss: 0.008941481159904882\n",
      "Epoch [3056/20000], Training Loss: 0.007210886999798406, Validation Loss: 0.004174745925415865\n",
      "Epoch [3057/20000], Training Loss: 0.007118385809008032, Validation Loss: 0.004660801549362865\n",
      "Epoch [3058/20000], Training Loss: 0.0070909916458674616, Validation Loss: 0.006057127680097035\n",
      "Epoch [3059/20000], Training Loss: 0.007595066301054284, Validation Loss: 0.01090804317105286\n",
      "Epoch [3060/20000], Training Loss: 0.029736308132076568, Validation Loss: 0.06726843183235696\n",
      "Epoch [3061/20000], Training Loss: 0.02425088853903747, Validation Loss: 0.033897092922704763\n",
      "Epoch [3062/20000], Training Loss: 0.01844551968887182, Validation Loss: 0.004014841484798615\n",
      "Epoch [3063/20000], Training Loss: 0.00810414127356905, Validation Loss: 0.004527070725649896\n",
      "Epoch [3064/20000], Training Loss: 0.009180721115886368, Validation Loss: 0.015609346042299271\n",
      "Epoch [3065/20000], Training Loss: 0.009293688133506553, Validation Loss: 0.004692076968192844\n",
      "Epoch [3066/20000], Training Loss: 0.011494479086000606, Validation Loss: 0.033636750019131585\n",
      "Epoch [3067/20000], Training Loss: 0.01001373201214457, Validation Loss: 0.021367418601210025\n",
      "Epoch [3068/20000], Training Loss: 0.012692134530100572, Validation Loss: 0.017603801976252862\n",
      "Epoch [3069/20000], Training Loss: 0.005786494756258824, Validation Loss: 0.030598785377563824\n",
      "Epoch [3070/20000], Training Loss: 0.012935328796239836, Validation Loss: 0.00774977932796301\n",
      "Epoch [3071/20000], Training Loss: 0.011587780757898665, Validation Loss: 0.0032010154683002998\n",
      "Epoch [3072/20000], Training Loss: 0.011403131094376866, Validation Loss: 0.02639980417749215\n",
      "Epoch [3073/20000], Training Loss: 0.01840625278990566, Validation Loss: 0.0548222527908519\n",
      "Epoch [3074/20000], Training Loss: 0.02271587109134998, Validation Loss: 0.060913422411165395\n",
      "Epoch [3075/20000], Training Loss: 0.023063426147148545, Validation Loss: 0.019829099320378962\n",
      "Epoch [3076/20000], Training Loss: 0.020043426626711738, Validation Loss: 0.014884824125572942\n",
      "Epoch [3077/20000], Training Loss: 0.024137311245015423, Validation Loss: 0.0068384552716039765\n",
      "Epoch [3078/20000], Training Loss: 0.010727203753958747, Validation Loss: 0.006388437067159345\n",
      "Epoch [3079/20000], Training Loss: 0.008083163249206595, Validation Loss: 0.006757685858922548\n",
      "Epoch [3080/20000], Training Loss: 0.01504697806389491, Validation Loss: 0.010296006251718845\n",
      "Epoch [3081/20000], Training Loss: 0.012349390146222763, Validation Loss: 0.004519299050712892\n",
      "Epoch [3082/20000], Training Loss: 0.006360108377910885, Validation Loss: 0.01030623543432886\n",
      "Epoch [3083/20000], Training Loss: 0.005238045119248065, Validation Loss: 0.008365662631803094\n",
      "Epoch [3084/20000], Training Loss: 0.0076343730055928715, Validation Loss: 0.004442511172903651\n",
      "Epoch [3085/20000], Training Loss: 0.007082625200772392, Validation Loss: 0.00647673636604381\n",
      "Epoch [3086/20000], Training Loss: 0.006440887897562269, Validation Loss: 0.004051308604966101\n",
      "Epoch [3087/20000], Training Loss: 0.007714373342293713, Validation Loss: 0.008953455786994306\n",
      "Epoch [3088/20000], Training Loss: 0.01580834355195293, Validation Loss: 0.01855024127072753\n",
      "Epoch [3089/20000], Training Loss: 0.01811214681155044, Validation Loss: 0.005202405574682747\n",
      "Epoch [3090/20000], Training Loss: 0.021106510667136393, Validation Loss: 0.006890628861213677\n",
      "Epoch [3091/20000], Training Loss: 0.03323484711538808, Validation Loss: 0.04022131345277992\n",
      "Epoch [3092/20000], Training Loss: 0.025301413373589248, Validation Loss: 0.015174867019921299\n",
      "Epoch [3093/20000], Training Loss: 0.012489742448711436, Validation Loss: 0.00644509553404817\n",
      "Epoch [3094/20000], Training Loss: 0.008438439110510185, Validation Loss: 0.0054489645917372\n",
      "Epoch [3095/20000], Training Loss: 0.012092400670683543, Validation Loss: 0.005327968004522572\n",
      "Epoch [3096/20000], Training Loss: 0.013579143647543137, Validation Loss: 0.01032260201780523\n",
      "Epoch [3097/20000], Training Loss: 0.023366978098887818, Validation Loss: 0.0254238030051045\n",
      "Epoch [3098/20000], Training Loss: 0.02609903655580378, Validation Loss: 0.01236930262997533\n",
      "Epoch [3099/20000], Training Loss: 0.03317903033790311, Validation Loss: 0.05677125466631695\n",
      "Epoch [3100/20000], Training Loss: 0.020996663968877068, Validation Loss: 0.00885784082799682\n",
      "Epoch [3101/20000], Training Loss: 0.017924289002881517, Validation Loss: 0.029983625964060505\n",
      "Epoch [3102/20000], Training Loss: 0.01405087417723345, Validation Loss: 0.017000974137259928\n",
      "Epoch [3103/20000], Training Loss: 0.012397957451737187, Validation Loss: 0.008202235937728107\n",
      "Epoch [3104/20000], Training Loss: 0.024675830622852248, Validation Loss: 0.007197384236212376\n",
      "Epoch [3105/20000], Training Loss: 0.030720769727070416, Validation Loss: 0.015206307436105015\n",
      "Epoch [3106/20000], Training Loss: 0.027023592198799764, Validation Loss: 0.0143595285688497\n",
      "Epoch [3107/20000], Training Loss: 0.024339916262501253, Validation Loss: 0.02525616526504837\n",
      "Epoch [3108/20000], Training Loss: 0.030079389980528504, Validation Loss: 0.03936965701156365\n",
      "Epoch [3109/20000], Training Loss: 0.02085938197394301, Validation Loss: 0.015284560585449317\n",
      "Epoch [3110/20000], Training Loss: 0.015178940608166158, Validation Loss: 0.007561564408659753\n",
      "Epoch [3111/20000], Training Loss: 0.007971330446058087, Validation Loss: 0.00453711992662096\n",
      "Epoch [3112/20000], Training Loss: 0.006893509672831247, Validation Loss: 0.0041302362654077485\n",
      "Epoch [3113/20000], Training Loss: 0.0078931101202865, Validation Loss: 0.011301228112887711\n",
      "Epoch [3114/20000], Training Loss: 0.025704045980792296, Validation Loss: 0.0372478967065218\n",
      "Epoch [3115/20000], Training Loss: 0.028054077297513556, Validation Loss: 0.02062811352326279\n",
      "Epoch [3116/20000], Training Loss: 0.01381957421212324, Validation Loss: 0.0052181065428689105\n",
      "Epoch [3117/20000], Training Loss: 0.011754338779220623, Validation Loss: 0.004471385333585992\n",
      "Epoch [3118/20000], Training Loss: 0.01092655655728387, Validation Loss: 0.00897233475103555\n",
      "Epoch [3119/20000], Training Loss: 0.010791056165804289, Validation Loss: 0.009009626377400461\n",
      "Epoch [3120/20000], Training Loss: 0.008983965365457282, Validation Loss: 0.006376963683065615\n",
      "Epoch [3121/20000], Training Loss: 0.007862068108546996, Validation Loss: 0.004100693258849643\n",
      "Epoch [3122/20000], Training Loss: 0.010896209249040112, Validation Loss: 0.008256926485976401\n",
      "Epoch [3123/20000], Training Loss: 0.007443938321167869, Validation Loss: 0.004961932978023908\n",
      "Epoch [3124/20000], Training Loss: 0.009137774749043663, Validation Loss: 0.009682868108075478\n",
      "Epoch [3125/20000], Training Loss: 0.020185731806642643, Validation Loss: 0.061451445849992334\n",
      "Epoch [3126/20000], Training Loss: 0.05066143539235262, Validation Loss: 0.01991585727356255\n",
      "Epoch [3127/20000], Training Loss: 0.01931426998427404, Validation Loss: 0.014995252808378672\n",
      "Epoch [3128/20000], Training Loss: 0.015193566530277687, Validation Loss: 0.01673329675134846\n",
      "Epoch [3129/20000], Training Loss: 0.01204961832679276, Validation Loss: 0.01233963917063948\n",
      "Epoch [3130/20000], Training Loss: 0.00772043972600451, Validation Loss: 0.00950052773287711\n",
      "Epoch [3131/20000], Training Loss: 0.009528468674813797, Validation Loss: 0.005650120365335806\n",
      "Epoch [3132/20000], Training Loss: 0.007511250952250391, Validation Loss: 0.004328759749691469\n",
      "Epoch [3133/20000], Training Loss: 0.007215419719744075, Validation Loss: 0.0176152031866406\n",
      "Epoch [3134/20000], Training Loss: 0.006235052164161711, Validation Loss: 0.004470331878110635\n",
      "Epoch [3135/20000], Training Loss: 0.013062028707898727, Validation Loss: 0.03899029391862224\n",
      "Epoch [3136/20000], Training Loss: 0.019498758990656433, Validation Loss: 0.0336132661424428\n",
      "Epoch [3137/20000], Training Loss: 0.054162428636378275, Validation Loss: 0.18263206607540663\n",
      "Epoch [3138/20000], Training Loss: 0.09375420455554766, Validation Loss: 0.047425339735427584\n",
      "Epoch [3139/20000], Training Loss: 0.03285819002693253, Validation Loss: 0.02143600075189389\n",
      "Epoch [3140/20000], Training Loss: 0.01812667737249285, Validation Loss: 0.012211228635458718\n",
      "Epoch [3141/20000], Training Loss: 0.011363464989699423, Validation Loss: 0.008756299441050963\n",
      "Epoch [3142/20000], Training Loss: 0.01219710470260387, Validation Loss: 0.007320362006811349\n",
      "Epoch [3143/20000], Training Loss: 0.01686607193135257, Validation Loss: 0.007725354343547106\n",
      "Epoch [3144/20000], Training Loss: 0.01663236426455634, Validation Loss: 0.00662699600455\n",
      "Epoch [3145/20000], Training Loss: 0.011896646307182632, Validation Loss: 0.006709118981401227\n",
      "Epoch [3146/20000], Training Loss: 0.0072797543273606736, Validation Loss: 0.006269339016534709\n",
      "Epoch [3147/20000], Training Loss: 0.006301139780719366, Validation Loss: 0.007085425761105125\n",
      "Epoch [3148/20000], Training Loss: 0.00960833640835647, Validation Loss: 0.012014142147653022\n",
      "Epoch [3149/20000], Training Loss: 0.015114171706954949, Validation Loss: 0.036121882791810604\n",
      "Epoch [3150/20000], Training Loss: 0.014223777568466696, Validation Loss: 0.006432170566942637\n",
      "Epoch [3151/20000], Training Loss: 0.009178186663457868, Validation Loss: 0.008666504221723529\n",
      "Epoch [3152/20000], Training Loss: 0.006804330002134, Validation Loss: 0.004577394357844313\n",
      "Epoch [3153/20000], Training Loss: 0.007699001810984945, Validation Loss: 0.021505776364806217\n",
      "Epoch [3154/20000], Training Loss: 0.00951898427953779, Validation Loss: 0.009091548439021617\n",
      "Epoch [3155/20000], Training Loss: 0.010327876674377226, Validation Loss: 0.007110382207194606\n",
      "Epoch [3156/20000], Training Loss: 0.009722185491617503, Validation Loss: 0.007308097257597845\n",
      "Epoch [3157/20000], Training Loss: 0.010685521928930289, Validation Loss: 0.00785102815029656\n",
      "Epoch [3158/20000], Training Loss: 0.014789974283693092, Validation Loss: 0.010236191980011104\n",
      "Epoch [3159/20000], Training Loss: 0.009934662997709762, Validation Loss: 0.0074051069354814985\n",
      "Epoch [3160/20000], Training Loss: 0.006658722556624396, Validation Loss: 0.003921267308298989\n",
      "Epoch [3161/20000], Training Loss: 0.005197091444902721, Validation Loss: 0.0039940893289208855\n",
      "Epoch [3162/20000], Training Loss: 0.007977183273136948, Validation Loss: 0.00518894138646215\n",
      "Epoch [3163/20000], Training Loss: 0.005839208229320191, Validation Loss: 0.004633048309114659\n",
      "Epoch [3164/20000], Training Loss: 0.032003627981924056, Validation Loss: 0.045610280614904956\n",
      "Epoch [3165/20000], Training Loss: 0.049695214634994045, Validation Loss: 0.014460376649637827\n",
      "Epoch [3166/20000], Training Loss: 0.0316788442183419, Validation Loss: 0.011591193962226952\n",
      "Epoch [3167/20000], Training Loss: 0.016710324720146934, Validation Loss: 0.02012457664348641\n",
      "Epoch [3168/20000], Training Loss: 0.0317010295943224, Validation Loss: 0.011514723978068115\n",
      "Epoch [3169/20000], Training Loss: 0.011851157311217062, Validation Loss: 0.010096524390445191\n",
      "Epoch [3170/20000], Training Loss: 0.009994081944958972, Validation Loss: 0.008823513872281334\n",
      "Epoch [3171/20000], Training Loss: 0.008170593779401056, Validation Loss: 0.00894450865341033\n",
      "Epoch [3172/20000], Training Loss: 0.010633144303158457, Validation Loss: 0.01062429824550216\n",
      "Epoch [3173/20000], Training Loss: 0.009976487324040915, Validation Loss: 0.005937808157730333\n",
      "Epoch [3174/20000], Training Loss: 0.0075675918674928, Validation Loss: 0.008069198350009677\n",
      "Epoch [3175/20000], Training Loss: 0.006821725945753444, Validation Loss: 0.004529572360705836\n",
      "Epoch [3176/20000], Training Loss: 0.007475017004513315, Validation Loss: 0.00394948915840476\n",
      "Epoch [3177/20000], Training Loss: 0.013098759882268496, Validation Loss: 0.009067720357188109\n",
      "Epoch [3178/20000], Training Loss: 0.01663615409916584, Validation Loss: 0.015500129703887812\n",
      "Epoch [3179/20000], Training Loss: 0.015862948510662785, Validation Loss: 0.05102681350475372\n",
      "Epoch [3180/20000], Training Loss: 0.02064047396249537, Validation Loss: 0.020920331870366744\n",
      "Epoch [3181/20000], Training Loss: 0.007528829238643604, Validation Loss: 0.004479019986072375\n",
      "Epoch [3182/20000], Training Loss: 0.007794716190451453, Validation Loss: 0.003774671454664043\n",
      "Epoch [3183/20000], Training Loss: 0.009536540788498573, Validation Loss: 0.0076312091350089675\n",
      "Epoch [3184/20000], Training Loss: 0.010116095113127293, Validation Loss: 0.009762078537215607\n",
      "Epoch [3185/20000], Training Loss: 0.010433794348500669, Validation Loss: 0.01875235058197288\n",
      "Epoch [3186/20000], Training Loss: 0.012702740678962852, Validation Loss: 0.007612999454786463\n",
      "Epoch [3187/20000], Training Loss: 0.010673588170902804, Validation Loss: 0.003475120151249137\n",
      "Epoch [3188/20000], Training Loss: 0.016008330317423054, Validation Loss: 0.00782198435126912\n",
      "Epoch [3189/20000], Training Loss: 0.0156721336451093, Validation Loss: 0.011710518709738242\n",
      "Epoch [3190/20000], Training Loss: 0.01299529878555664, Validation Loss: 0.004729353238063246\n",
      "Epoch [3191/20000], Training Loss: 0.01892363380677255, Validation Loss: 0.008841218032170537\n",
      "Epoch [3192/20000], Training Loss: 0.041565553597008274, Validation Loss: 0.07936749044477268\n",
      "Epoch [3193/20000], Training Loss: 0.05526047203290675, Validation Loss: 0.050018661674240926\n",
      "Epoch [3194/20000], Training Loss: 0.03536002680838075, Validation Loss: 0.02002845402882324\n",
      "Epoch [3195/20000], Training Loss: 0.04632502822538039, Validation Loss: 0.01265941126798391\n",
      "Epoch [3196/20000], Training Loss: 0.01670992958991389, Validation Loss: 0.011049613818355928\n",
      "Epoch [3197/20000], Training Loss: 0.0118240640482067, Validation Loss: 0.014444796543313376\n",
      "Epoch [3198/20000], Training Loss: 0.01087167879865904, Validation Loss: 0.009410459508830219\n",
      "Epoch [3199/20000], Training Loss: 0.008838460930357022, Validation Loss: 0.007751650499955638\n",
      "Epoch [3200/20000], Training Loss: 0.007833363261462572, Validation Loss: 0.006687731550596749\n",
      "Epoch [3201/20000], Training Loss: 0.0072806126013996875, Validation Loss: 0.006361993776865182\n",
      "Epoch [3202/20000], Training Loss: 0.006022338445680881, Validation Loss: 0.009130022523424482\n",
      "Epoch [3203/20000], Training Loss: 0.0064468301092607105, Validation Loss: 0.012581268007220458\n",
      "Epoch [3204/20000], Training Loss: 0.008385137663156326, Validation Loss: 0.004380288233424633\n",
      "Epoch [3205/20000], Training Loss: 0.011065393564682122, Validation Loss: 0.007160483097623344\n",
      "Epoch [3206/20000], Training Loss: 0.009633167484415546, Validation Loss: 0.005342094962137338\n",
      "Epoch [3207/20000], Training Loss: 0.00816070894195166, Validation Loss: 0.006718600714096856\n",
      "Epoch [3208/20000], Training Loss: 0.01369146755730201, Validation Loss: 0.011119467284343636\n",
      "Epoch [3209/20000], Training Loss: 0.01311991925051968, Validation Loss: 0.008615382707886731\n",
      "Epoch [3210/20000], Training Loss: 0.01506525881788028, Validation Loss: 0.005999221296422477\n",
      "Epoch [3211/20000], Training Loss: 0.012931064774290593, Validation Loss: 0.011837721691544302\n",
      "Epoch [3212/20000], Training Loss: 0.005371140298458548, Validation Loss: 0.0034356571350185887\n",
      "Epoch [3213/20000], Training Loss: 0.00684679994563859, Validation Loss: 0.004965261037568813\n",
      "Epoch [3214/20000], Training Loss: 0.05313515071091907, Validation Loss: 0.04535900758479096\n",
      "Epoch [3215/20000], Training Loss: 0.024504289356887705, Validation Loss: 0.011622078338241354\n",
      "Epoch [3216/20000], Training Loss: 0.013284045072006328, Validation Loss: 0.014879047750597027\n",
      "Epoch [3217/20000], Training Loss: 0.009723271534312516, Validation Loss: 0.006816982028152258\n",
      "Epoch [3218/20000], Training Loss: 0.00902725597760374, Validation Loss: 0.01028181218432203\n",
      "Epoch [3219/20000], Training Loss: 0.015519333155160504, Validation Loss: 0.007093868233435405\n",
      "Epoch [3220/20000], Training Loss: 0.01259110560515962, Validation Loss: 0.007769597658864044\n",
      "Epoch [3221/20000], Training Loss: 0.007422962417162385, Validation Loss: 0.008756905739414029\n",
      "Epoch [3222/20000], Training Loss: 0.007389265487810397, Validation Loss: 0.006736401993249459\n",
      "Epoch [3223/20000], Training Loss: 0.00752845400503637, Validation Loss: 0.004379826848816986\n",
      "Epoch [3224/20000], Training Loss: 0.0067013964232631095, Validation Loss: 0.02215365224111335\n",
      "Epoch [3225/20000], Training Loss: 0.02113571944313922, Validation Loss: 0.019999766575954387\n",
      "Epoch [3226/20000], Training Loss: 0.005524689552008307, Validation Loss: 0.0031410160257183675\n",
      "Epoch [3227/20000], Training Loss: 0.010995568508491513, Validation Loss: 0.004713665766673704\n",
      "Epoch [3228/20000], Training Loss: 0.009612516059340643, Validation Loss: 0.00960105851149482\n",
      "Epoch [3229/20000], Training Loss: 0.030387832167824463, Validation Loss: 0.03782873059454799\n",
      "Epoch [3230/20000], Training Loss: 0.026028030325375897, Validation Loss: 0.007681057396857235\n",
      "Epoch [3231/20000], Training Loss: 0.012997397978324443, Validation Loss: 0.008347863277111197\n",
      "Epoch [3232/20000], Training Loss: 0.00970685293889671, Validation Loss: 0.006179011965534177\n",
      "Epoch [3233/20000], Training Loss: 0.013529298502752292, Validation Loss: 0.02148580113900391\n",
      "Epoch [3234/20000], Training Loss: 0.01799592965523646, Validation Loss: 0.010632581999211877\n",
      "Epoch [3235/20000], Training Loss: 0.008741123920109268, Validation Loss: 0.008831096659408703\n",
      "Epoch [3236/20000], Training Loss: 0.01295609964394576, Validation Loss: 0.02427280954960528\n",
      "Epoch [3237/20000], Training Loss: 0.012222463365885363, Validation Loss: 0.016043471257210774\n",
      "Epoch [3238/20000], Training Loss: 0.011643475865379773, Validation Loss: 0.005638640689143023\n",
      "Epoch [3239/20000], Training Loss: 0.01182708837521724, Validation Loss: 0.004634689599242875\n",
      "Epoch [3240/20000], Training Loss: 0.01070121926646347, Validation Loss: 0.005721202277093063\n",
      "Epoch [3241/20000], Training Loss: 0.013055356053103293, Validation Loss: 0.00517340651297159\n",
      "Epoch [3242/20000], Training Loss: 0.007936202328502466, Validation Loss: 0.005469769603780948\n",
      "Epoch [3243/20000], Training Loss: 0.011031637215377746, Validation Loss: 0.017385402921230992\n",
      "Epoch [3244/20000], Training Loss: 0.010542519001839017, Validation Loss: 0.004866726383133785\n",
      "Epoch [3245/20000], Training Loss: 0.01299415408357163, Validation Loss: 0.008764587928803011\n",
      "Epoch [3246/20000], Training Loss: 0.014154470899874079, Validation Loss: 0.03575118525077316\n",
      "Epoch [3247/20000], Training Loss: 0.02629852400735087, Validation Loss: 0.024858912454119586\n",
      "Epoch [3248/20000], Training Loss: 0.016713760127978667, Validation Loss: 0.050222670925462745\n",
      "Epoch [3249/20000], Training Loss: 0.0515536175485717, Validation Loss: 0.0766516865010967\n",
      "Epoch [3250/20000], Training Loss: 0.023007953235459615, Validation Loss: 0.01191515812960468\n",
      "Epoch [3251/20000], Training Loss: 0.012636162590102426, Validation Loss: 0.007990890100440054\n",
      "Epoch [3252/20000], Training Loss: 0.012699929872594242, Validation Loss: 0.007480550509734224\n",
      "Epoch [3253/20000], Training Loss: 0.0087807047696385, Validation Loss: 0.007070039262714803\n",
      "Epoch [3254/20000], Training Loss: 0.006633235948226813, Validation Loss: 0.005545950766130063\n",
      "Epoch [3255/20000], Training Loss: 0.007111746077758393, Validation Loss: 0.005894652259193976\n",
      "Epoch [3256/20000], Training Loss: 0.006791605930110174, Validation Loss: 0.005263787002678298\n",
      "Epoch [3257/20000], Training Loss: 0.007577281023259275, Validation Loss: 0.004058103231630643\n",
      "Epoch [3258/20000], Training Loss: 0.010767587562440895, Validation Loss: 0.015024342446546556\n",
      "Epoch [3259/20000], Training Loss: 0.009111713992232191, Validation Loss: 0.011470721990871234\n",
      "Epoch [3260/20000], Training Loss: 0.02065525642787439, Validation Loss: 0.028810354288832447\n",
      "Epoch [3261/20000], Training Loss: 0.04176252518404259, Validation Loss: 0.014552143114152639\n",
      "Epoch [3262/20000], Training Loss: 0.03505856759979257, Validation Loss: 0.04129333786841135\n",
      "Epoch [3263/20000], Training Loss: 0.035351038519625684, Validation Loss: 0.02868218432247374\n",
      "Epoch [3264/20000], Training Loss: 0.01672387685227607, Validation Loss: 0.018012108853120323\n",
      "Epoch [3265/20000], Training Loss: 0.012373643731864701, Validation Loss: 0.008519902508364865\n",
      "Epoch [3266/20000], Training Loss: 0.009853400090443236, Validation Loss: 0.008926404956583093\n",
      "Epoch [3267/20000], Training Loss: 0.011124999582534656, Validation Loss: 0.0057370001408280575\n",
      "Epoch [3268/20000], Training Loss: 0.009624725473778588, Validation Loss: 0.022232039168557184\n",
      "Epoch [3269/20000], Training Loss: 0.023340890896130468, Validation Loss: 0.02520231535163484\n",
      "Epoch [3270/20000], Training Loss: 0.022740191696876928, Validation Loss: 0.020909911561050636\n",
      "Epoch [3271/20000], Training Loss: 0.037356867596307505, Validation Loss: 0.03166593529749685\n",
      "Epoch [3272/20000], Training Loss: 0.015466523326592454, Validation Loss: 0.013021463958665533\n",
      "Epoch [3273/20000], Training Loss: 0.012151015955688698, Validation Loss: 0.011290090027294789\n",
      "Epoch [3274/20000], Training Loss: 0.010818581221558685, Validation Loss: 0.005100415243593478\n",
      "Epoch [3275/20000], Training Loss: 0.023153122197982157, Validation Loss: 0.011930619090492228\n",
      "Epoch [3276/20000], Training Loss: 0.04351472905338077, Validation Loss: 0.026663639910682175\n",
      "Epoch [3277/20000], Training Loss: 0.012907618739908295, Validation Loss: 0.010942836783899393\n",
      "Epoch [3278/20000], Training Loss: 0.009042238791672779, Validation Loss: 0.0075510731668557655\n",
      "Epoch [3279/20000], Training Loss: 0.008411802532853991, Validation Loss: 0.009250266894274972\n",
      "Epoch [3280/20000], Training Loss: 0.00763472359228347, Validation Loss: 0.010095399212819647\n",
      "Epoch [3281/20000], Training Loss: 0.006927836164582654, Validation Loss: 0.00844217628582568\n",
      "Epoch [3282/20000], Training Loss: 0.008326392587540405, Validation Loss: 0.004778030186701381\n",
      "Epoch [3283/20000], Training Loss: 0.010651238709605033, Validation Loss: 0.005280178976956269\n",
      "Epoch [3284/20000], Training Loss: 0.01052485324908048, Validation Loss: 0.008009800213506888\n",
      "Epoch [3285/20000], Training Loss: 0.012965702551550098, Validation Loss: 0.007570683239683676\n",
      "Epoch [3286/20000], Training Loss: 0.02883491205284372, Validation Loss: 0.0093019934902259\n",
      "Epoch [3287/20000], Training Loss: 0.014799100005932684, Validation Loss: 0.006042598204430807\n",
      "Epoch [3288/20000], Training Loss: 0.01236806676856109, Validation Loss: 0.0193786462119274\n",
      "Epoch [3289/20000], Training Loss: 0.018519917804431834, Validation Loss: 0.008975978184886536\n",
      "Epoch [3290/20000], Training Loss: 0.014953117842326589, Validation Loss: 0.01040437404488365\n",
      "Epoch [3291/20000], Training Loss: 0.010553355486730911, Validation Loss: 0.008133911196409853\n",
      "Epoch [3292/20000], Training Loss: 0.008537751919123236, Validation Loss: 0.00802591505418451\n",
      "Epoch [3293/20000], Training Loss: 0.008287411632149346, Validation Loss: 0.008418056926328233\n",
      "Epoch [3294/20000], Training Loss: 0.010805582629213209, Validation Loss: 0.006798451789444069\n",
      "Epoch [3295/20000], Training Loss: 0.008357745724164747, Validation Loss: 0.007760282010216156\n",
      "Epoch [3296/20000], Training Loss: 0.01073555343568192, Validation Loss: 0.018676725493638257\n",
      "Epoch [3297/20000], Training Loss: 0.01131102666217235, Validation Loss: 0.009526946103960656\n",
      "Epoch [3298/20000], Training Loss: 0.009620040296795196, Validation Loss: 0.02957765787531181\n",
      "Epoch [3299/20000], Training Loss: 0.02607626690431581, Validation Loss: 0.007557577482959743\n",
      "Epoch [3300/20000], Training Loss: 0.046842330833896995, Validation Loss: 0.03791767870676007\n",
      "Epoch [3301/20000], Training Loss: 0.027623938011986735, Validation Loss: 0.008299019059450087\n",
      "Epoch [3302/20000], Training Loss: 0.014693391400669498, Validation Loss: 0.013918130056838786\n",
      "Epoch [3303/20000], Training Loss: 0.015353292297472112, Validation Loss: 0.015226094986214148\n",
      "Epoch [3304/20000], Training Loss: 0.013468470207174375, Validation Loss: 0.018217935942370152\n",
      "Epoch [3305/20000], Training Loss: 0.018404108806862496, Validation Loss: 0.0064912061564277685\n",
      "Epoch [3306/20000], Training Loss: 0.01169258915719443, Validation Loss: 0.005824748858273671\n",
      "Epoch [3307/20000], Training Loss: 0.006448473793820345, Validation Loss: 0.005388165860819888\n",
      "Epoch [3308/20000], Training Loss: 0.009238401386288128, Validation Loss: 0.004750869573078784\n",
      "Epoch [3309/20000], Training Loss: 0.005489353227728445, Validation Loss: 0.005957887716478539\n",
      "Epoch [3310/20000], Training Loss: 0.00873065579019437, Validation Loss: 0.005383021456338051\n",
      "Epoch [3311/20000], Training Loss: 0.00905778027663473, Validation Loss: 0.004848676021298713\n",
      "Epoch [3312/20000], Training Loss: 0.007126512188863542, Validation Loss: 0.005551129387827457\n",
      "Epoch [3313/20000], Training Loss: 0.007853130281936111, Validation Loss: 0.005043528793039138\n",
      "Epoch [3314/20000], Training Loss: 0.011270605945883614, Validation Loss: 0.003340093576140037\n",
      "Epoch [3315/20000], Training Loss: 0.006933847799786642, Validation Loss: 0.0045980017145892714\n",
      "Epoch [3316/20000], Training Loss: 0.007143823320705321, Validation Loss: 0.005597391445470239\n",
      "Epoch [3317/20000], Training Loss: 0.0059552537999089895, Validation Loss: 0.003604000084388405\n",
      "Epoch [3318/20000], Training Loss: 0.00984011505975754, Validation Loss: 0.004130176350568242\n",
      "Epoch [3319/20000], Training Loss: 0.027247846298905642, Validation Loss: 0.005955418489014014\n",
      "Epoch [3320/20000], Training Loss: 0.012260129400861583, Validation Loss: 0.00477763793370357\n",
      "Epoch [3321/20000], Training Loss: 0.006062712970430896, Validation Loss: 0.006298215508572575\n",
      "Epoch [3322/20000], Training Loss: 0.005628029817931487, Validation Loss: 0.011575052679644844\n",
      "Epoch [3323/20000], Training Loss: 0.0061128090372741485, Validation Loss: 0.01268644027987067\n",
      "Epoch [3324/20000], Training Loss: 0.013217438023795174, Validation Loss: 0.004720197121783511\n",
      "Epoch [3325/20000], Training Loss: 0.02459134918379797, Validation Loss: 0.01756642655085942\n",
      "Epoch [3326/20000], Training Loss: 0.031796117960181745, Validation Loss: 0.018570080195787354\n",
      "Epoch [3327/20000], Training Loss: 0.02224757629197224, Validation Loss: 0.006398112384192999\n",
      "Epoch [3328/20000], Training Loss: 0.00943384027673996, Validation Loss: 0.007971434010354913\n",
      "Epoch [3329/20000], Training Loss: 0.00975402652797389, Validation Loss: 0.006474318156410133\n",
      "Epoch [3330/20000], Training Loss: 0.007284890664907705, Validation Loss: 0.003919667263274109\n",
      "Epoch [3331/20000], Training Loss: 0.007472358870602745, Validation Loss: 0.00523534497681695\n",
      "Epoch [3332/20000], Training Loss: 0.012471261619274239, Validation Loss: 0.008849314282641437\n",
      "Epoch [3333/20000], Training Loss: 0.009771713441620835, Validation Loss: 0.008686697942771675\n",
      "Epoch [3334/20000], Training Loss: 0.007797983890798475, Validation Loss: 0.006460660507707675\n",
      "Epoch [3335/20000], Training Loss: 0.00494756630359916, Validation Loss: 0.005978826340893577\n",
      "Epoch [3336/20000], Training Loss: 0.0097779631843358, Validation Loss: 0.007550251498404975\n",
      "Epoch [3337/20000], Training Loss: 0.01644191081244831, Validation Loss: 0.0048823596844173285\n",
      "Epoch [3338/20000], Training Loss: 0.052836346273709624, Validation Loss: 0.0347984960955391\n",
      "Epoch [3339/20000], Training Loss: 0.03523422630587447, Validation Loss: 0.023697297473318106\n",
      "Epoch [3340/20000], Training Loss: 0.021521279787910835, Validation Loss: 0.009555976018031835\n",
      "Epoch [3341/20000], Training Loss: 0.011302613588798392, Validation Loss: 0.010251331079469014\n",
      "Epoch [3342/20000], Training Loss: 0.007989406515012629, Validation Loss: 0.00557105734360961\n",
      "Epoch [3343/20000], Training Loss: 0.009470913268160075, Validation Loss: 0.008136184632358155\n",
      "Epoch [3344/20000], Training Loss: 0.008181373123079538, Validation Loss: 0.005482460457724704\n",
      "Epoch [3345/20000], Training Loss: 0.007426506638564335, Validation Loss: 0.015063805493161013\n",
      "Epoch [3346/20000], Training Loss: 0.013070379936834797, Validation Loss: 0.0059073155824054206\n",
      "Epoch [3347/20000], Training Loss: 0.010285718258403773, Validation Loss: 0.005078424650782222\n",
      "Epoch [3348/20000], Training Loss: 0.006177709462852883, Validation Loss: 0.0076729123646100275\n",
      "Epoch [3349/20000], Training Loss: 0.00840674756701836, Validation Loss: 0.0038441699765274467\n",
      "Epoch [3350/20000], Training Loss: 0.012587475900155758, Validation Loss: 0.009457948717458291\n",
      "Epoch [3351/20000], Training Loss: 0.013353488981790309, Validation Loss: 0.0051898416087173835\n",
      "Epoch [3352/20000], Training Loss: 0.012066394096889001, Validation Loss: 0.012973384133905988\n",
      "Epoch [3353/20000], Training Loss: 0.012208393833134323, Validation Loss: 0.00481988009354241\n",
      "Epoch [3354/20000], Training Loss: 0.005010820722548358, Validation Loss: 0.009938096249965984\n",
      "Epoch [3355/20000], Training Loss: 0.008301592561500521, Validation Loss: 0.006542679056013065\n",
      "Epoch [3356/20000], Training Loss: 0.009881484746334277, Validation Loss: 0.014433304091362426\n",
      "Epoch [3357/20000], Training Loss: 0.05344204502632367, Validation Loss: 0.021020965526467175\n",
      "Epoch [3358/20000], Training Loss: 0.03977724786886938, Validation Loss: 0.02388842656415553\n",
      "Epoch [3359/20000], Training Loss: 0.011679682069890467, Validation Loss: 0.01435088747815748\n",
      "Epoch [3360/20000], Training Loss: 0.008372758914317404, Validation Loss: 0.005954985420041539\n",
      "Epoch [3361/20000], Training Loss: 0.006334822658183319, Validation Loss: 0.008423012017875353\n",
      "Epoch [3362/20000], Training Loss: 0.006985262961409587, Validation Loss: 0.008000719569292453\n",
      "Epoch [3363/20000], Training Loss: 0.007379760851368441, Validation Loss: 0.004729981025859384\n",
      "Epoch [3364/20000], Training Loss: 0.01061583646307034, Validation Loss: 0.004418357058346619\n",
      "Epoch [3365/20000], Training Loss: 0.012084900667624814, Validation Loss: 0.0077996484948326455\n",
      "Epoch [3366/20000], Training Loss: 0.010712801844679884, Validation Loss: 0.004884207250685501\n",
      "Epoch [3367/20000], Training Loss: 0.005619563044287393, Validation Loss: 0.005842773554590757\n",
      "Epoch [3368/20000], Training Loss: 0.005063067123826061, Validation Loss: 0.004190093790139719\n",
      "Epoch [3369/20000], Training Loss: 0.00424620992842912, Validation Loss: 0.005294092952596049\n",
      "Epoch [3370/20000], Training Loss: 0.00629715680510604, Validation Loss: 0.00925896629141321\n",
      "Epoch [3371/20000], Training Loss: 0.013977474784171915, Validation Loss: 0.014996803089114802\n",
      "Epoch [3372/20000], Training Loss: 0.023907852353269327, Validation Loss: 0.01700094749061071\n",
      "Epoch [3373/20000], Training Loss: 0.023195378238597186, Validation Loss: 0.00987878414143804\n",
      "Epoch [3374/20000], Training Loss: 0.0071241345450029515, Validation Loss: 0.01138958679625815\n",
      "Epoch [3375/20000], Training Loss: 0.00981986517061679, Validation Loss: 0.006719934776589549\n",
      "Epoch [3376/20000], Training Loss: 0.015875765695195047, Validation Loss: 0.02046067509116534\n",
      "Epoch [3377/20000], Training Loss: 0.027802532101954318, Validation Loss: 0.05068404151052208\n",
      "Epoch [3378/20000], Training Loss: 0.03139053756603971, Validation Loss: 0.009741909667781355\n",
      "Epoch [3379/20000], Training Loss: 0.016881489222423544, Validation Loss: 0.005969600611324865\n",
      "Epoch [3380/20000], Training Loss: 0.010881603354521627, Validation Loss: 0.005280904138892877\n",
      "Epoch [3381/20000], Training Loss: 0.04087927281307202, Validation Loss: 0.023360598327876198\n",
      "Epoch [3382/20000], Training Loss: 0.03050286286244435, Validation Loss: 0.014015430278198415\n",
      "Epoch [3383/20000], Training Loss: 0.014989534779618095, Validation Loss: 0.013116150804809134\n",
      "Epoch [3384/20000], Training Loss: 0.022796290826850703, Validation Loss: 0.007484242651802223\n",
      "Epoch [3385/20000], Training Loss: 0.023818131369937743, Validation Loss: 0.009627540495323219\n",
      "Epoch [3386/20000], Training Loss: 0.01829562016064301, Validation Loss: 0.015468743831559526\n",
      "Epoch [3387/20000], Training Loss: 0.015507679307899837, Validation Loss: 0.030146014981123875\n",
      "Epoch [3388/20000], Training Loss: 0.023746366340284503, Validation Loss: 0.015468886010338754\n",
      "Epoch [3389/20000], Training Loss: 0.012518630690465216, Validation Loss: 0.012084312778457621\n",
      "Epoch [3390/20000], Training Loss: 0.01609338140302238, Validation Loss: 0.03695452492547514\n",
      "Epoch [3391/20000], Training Loss: 0.012009689876452154, Validation Loss: 0.005132055406439343\n",
      "Epoch [3392/20000], Training Loss: 0.009516485093627125, Validation Loss: 0.011179503205840987\n",
      "Epoch [3393/20000], Training Loss: 0.0077954606489843824, Validation Loss: 0.006054135339315118\n",
      "Epoch [3394/20000], Training Loss: 0.011267821529015367, Validation Loss: 0.004552320967214626\n",
      "Epoch [3395/20000], Training Loss: 0.010239627213325418, Validation Loss: 0.014316402797866947\n",
      "Epoch [3396/20000], Training Loss: 0.00847559636063774, Validation Loss: 0.009117481570402887\n",
      "Epoch [3397/20000], Training Loss: 0.01663543815642145, Validation Loss: 0.008199706787702241\n",
      "Epoch [3398/20000], Training Loss: 0.012268768928022058, Validation Loss: 0.003977636511272515\n",
      "Epoch [3399/20000], Training Loss: 0.00970520348554211, Validation Loss: 0.003820785827325462\n",
      "Epoch [3400/20000], Training Loss: 0.006693341943901032, Validation Loss: 0.005885084231675071\n",
      "Epoch [3401/20000], Training Loss: 0.0053112245529648916, Validation Loss: 0.004023277889432501\n",
      "Epoch [3402/20000], Training Loss: 0.007155199428650251, Validation Loss: 0.009808403919137426\n",
      "Epoch [3403/20000], Training Loss: 0.02097912714920572, Validation Loss: 0.013774675354129772\n",
      "Epoch [3404/20000], Training Loss: 0.010534217146674305, Validation Loss: 0.003628731186965379\n",
      "Epoch [3405/20000], Training Loss: 0.014562044727686694, Validation Loss: 0.01953549153993167\n",
      "Epoch [3406/20000], Training Loss: 0.010670314950402826, Validation Loss: 0.004445013599472454\n",
      "Epoch [3407/20000], Training Loss: 0.008606004720994471, Validation Loss: 0.003716358387958358\n",
      "Epoch [3408/20000], Training Loss: 0.011271330116349938, Validation Loss: 0.01469948420681905\n",
      "Epoch [3409/20000], Training Loss: 0.05257695995637083, Validation Loss: 0.007556619941130549\n",
      "Epoch [3410/20000], Training Loss: 0.0555624877680592, Validation Loss: 0.06537119083490354\n",
      "Epoch [3411/20000], Training Loss: 0.022255805753437535, Validation Loss: 0.011608469674907529\n",
      "Epoch [3412/20000], Training Loss: 0.009653195353166666, Validation Loss: 0.0082079607981664\n",
      "Epoch [3413/20000], Training Loss: 0.011929412189991646, Validation Loss: 0.0075330576185885295\n",
      "Epoch [3414/20000], Training Loss: 0.013984953521035745, Validation Loss: 0.008474214341838787\n",
      "Epoch [3415/20000], Training Loss: 0.010014880509905717, Validation Loss: 0.004858452087433845\n",
      "Epoch [3416/20000], Training Loss: 0.005917107694715794, Validation Loss: 0.003198428034992984\n",
      "Epoch [3417/20000], Training Loss: 0.009992957790797976, Validation Loss: 0.02754776056250419\n",
      "Epoch [3418/20000], Training Loss: 0.05989128945345458, Validation Loss: 0.08178787069628546\n",
      "Epoch [3419/20000], Training Loss: 0.04841834165355457, Validation Loss: 0.04293041467407394\n",
      "Epoch [3420/20000], Training Loss: 0.019134340674749444, Validation Loss: 0.012385436485852762\n",
      "Epoch [3421/20000], Training Loss: 0.010167557832314092, Validation Loss: 0.004741842797333616\n",
      "Epoch [3422/20000], Training Loss: 0.007770640888338676, Validation Loss: 0.007304683634743776\n",
      "Epoch [3423/20000], Training Loss: 0.006712946288534535, Validation Loss: 0.024012661938163007\n",
      "Epoch [3424/20000], Training Loss: 0.02937672641460917, Validation Loss: 0.030593213036720326\n",
      "Epoch [3425/20000], Training Loss: 0.016973115736618638, Validation Loss: 0.014452863480463396\n",
      "Epoch [3426/20000], Training Loss: 0.01313410805804389, Validation Loss: 0.009518973253372162\n",
      "Epoch [3427/20000], Training Loss: 0.01004763609047846, Validation Loss: 0.015132612291478966\n",
      "Epoch [3428/20000], Training Loss: 0.01756473008975133, Validation Loss: 0.016446948854284722\n",
      "Epoch [3429/20000], Training Loss: 0.02138201054335046, Validation Loss: 0.008537138199812944\n",
      "Epoch [3430/20000], Training Loss: 0.013388820044513392, Validation Loss: 0.007708025561795141\n",
      "Epoch [3431/20000], Training Loss: 0.009239799164268854, Validation Loss: 0.0088384165527324\n",
      "Epoch [3432/20000], Training Loss: 0.006985886344409664, Validation Loss: 0.006508715589656536\n",
      "Epoch [3433/20000], Training Loss: 0.006167646173188197, Validation Loss: 0.005250113692804957\n",
      "Epoch [3434/20000], Training Loss: 0.007141996493114026, Validation Loss: 0.004558589836719307\n",
      "Epoch [3435/20000], Training Loss: 0.009270030831279641, Validation Loss: 0.005659314864463535\n",
      "Epoch [3436/20000], Training Loss: 0.008111685115311826, Validation Loss: 0.014509998437569033\n",
      "Epoch [3437/20000], Training Loss: 0.015383286935827658, Validation Loss: 0.012623915628230746\n",
      "Epoch [3438/20000], Training Loss: 0.018420170560213073, Validation Loss: 0.004827495089500644\n",
      "Epoch [3439/20000], Training Loss: 0.011313444285146293, Validation Loss: 0.006488366613921873\n",
      "Epoch [3440/20000], Training Loss: 0.014705008442563536, Validation Loss: 0.010997396670025037\n",
      "Epoch [3441/20000], Training Loss: 0.014202405016736261, Validation Loss: 0.007008454058327221\n",
      "Epoch [3442/20000], Training Loss: 0.008136478275576207, Validation Loss: 0.004414257262928751\n",
      "Epoch [3443/20000], Training Loss: 0.0069645363068307885, Validation Loss: 0.0036875361009671387\n",
      "Epoch [3444/20000], Training Loss: 0.0060621002209830166, Validation Loss: 0.0033161047339019334\n",
      "Epoch [3445/20000], Training Loss: 0.005929344639818217, Validation Loss: 0.009874211139161204\n",
      "Epoch [3446/20000], Training Loss: 0.005703658406024, Validation Loss: 0.004117530940008456\n",
      "Epoch [3447/20000], Training Loss: 0.008932414088381588, Validation Loss: 0.004457836336656433\n",
      "Epoch [3448/20000], Training Loss: 0.011676843370816568, Validation Loss: 0.029129987050894337\n",
      "Epoch [3449/20000], Training Loss: 0.01398481660687269, Validation Loss: 0.029762918397461653\n",
      "Epoch [3450/20000], Training Loss: 0.013082869966215835, Validation Loss: 0.005309288200750257\n",
      "Epoch [3451/20000], Training Loss: 0.006884921123855747, Validation Loss: 0.0054374299798645165\n",
      "Epoch [3452/20000], Training Loss: 0.006703707931072651, Validation Loss: 0.006545247187311769\n",
      "Epoch [3453/20000], Training Loss: 0.008938705658822852, Validation Loss: 0.009477554965973729\n",
      "Epoch [3454/20000], Training Loss: 0.010794252483589974, Validation Loss: 0.01356354731806115\n",
      "Epoch [3455/20000], Training Loss: 0.023940855653823485, Validation Loss: 0.010505471034103866\n",
      "Epoch [3456/20000], Training Loss: 0.028724085165387287, Validation Loss: 0.012325298065824037\n",
      "Epoch [3457/20000], Training Loss: 0.015300801815231222, Validation Loss: 0.011354301979579873\n",
      "Epoch [3458/20000], Training Loss: 0.00909983237839437, Validation Loss: 0.011203167457939351\n",
      "Epoch [3459/20000], Training Loss: 0.008374876901923147, Validation Loss: 0.005397572884155417\n",
      "Epoch [3460/20000], Training Loss: 0.00625159519508348, Validation Loss: 0.0043886065197059464\n",
      "Epoch [3461/20000], Training Loss: 0.009013860751500553, Validation Loss: 0.012658750782683066\n",
      "Epoch [3462/20000], Training Loss: 0.011739664019842166, Validation Loss: 0.005049353545851358\n",
      "Epoch [3463/20000], Training Loss: 0.013930456388542163, Validation Loss: 0.010942682111200546\n",
      "Epoch [3464/20000], Training Loss: 0.013623851282967994, Validation Loss: 0.03161564889962522\n",
      "Epoch [3465/20000], Training Loss: 0.03554538493153814, Validation Loss: 0.016646830803438206\n",
      "Epoch [3466/20000], Training Loss: 0.09712834513058104, Validation Loss: 0.01948400983487653\n",
      "Epoch [3467/20000], Training Loss: 0.024309068685397506, Validation Loss: 0.03469186805481462\n",
      "Epoch [3468/20000], Training Loss: 0.020704162273822085, Validation Loss: 0.01361812979208105\n",
      "Epoch [3469/20000], Training Loss: 0.015993668026307466, Validation Loss: 0.01333127288256532\n",
      "Epoch [3470/20000], Training Loss: 0.007842382021148555, Validation Loss: 0.0057585731727689046\n",
      "Epoch [3471/20000], Training Loss: 0.013226727028398142, Validation Loss: 0.006914812205780681\n",
      "Epoch [3472/20000], Training Loss: 0.008232253306362378, Validation Loss: 0.020461459621012125\n",
      "Epoch [3473/20000], Training Loss: 0.016861664285118292, Validation Loss: 0.0067288386851165685\n",
      "Epoch [3474/20000], Training Loss: 0.0203536889971474, Validation Loss: 0.012495030303398765\n",
      "Epoch [3475/20000], Training Loss: 0.01032114819931199, Validation Loss: 0.004766715015518842\n",
      "Epoch [3476/20000], Training Loss: 0.00668909984441208, Validation Loss: 0.004109808102774575\n",
      "Epoch [3477/20000], Training Loss: 0.006774176482784762, Validation Loss: 0.018332430122005226\n",
      "Epoch [3478/20000], Training Loss: 0.00540783308679238, Validation Loss: 0.012957394188171262\n",
      "Epoch [3479/20000], Training Loss: 0.008564139553495156, Validation Loss: 0.0033606295929920413\n",
      "Epoch [3480/20000], Training Loss: 0.005287691133811937, Validation Loss: 0.004637562688068504\n",
      "Epoch [3481/20000], Training Loss: 0.02034262987919127, Validation Loss: 0.01325338202582317\n",
      "Epoch [3482/20000], Training Loss: 0.042793808577828586, Validation Loss: 0.02806237887778352\n",
      "Epoch [3483/20000], Training Loss: 0.038907275044558834, Validation Loss: 0.01058508200783014\n",
      "Epoch [3484/20000], Training Loss: 0.04183287765564663, Validation Loss: 0.027222317945937984\n",
      "Epoch [3485/20000], Training Loss: 0.05677537573501468, Validation Loss: 0.02392766765247614\n",
      "Epoch [3486/20000], Training Loss: 0.02558659848624042, Validation Loss: 0.020517318933662616\n",
      "Epoch [3487/20000], Training Loss: 0.016833193209354898, Validation Loss: 0.013348563743121777\n",
      "Epoch [3488/20000], Training Loss: 0.011516307796617704, Validation Loss: 0.011765514033738734\n",
      "Epoch [3489/20000], Training Loss: 0.010225250186132533, Validation Loss: 0.011073507370643493\n",
      "Epoch [3490/20000], Training Loss: 0.010530837316764519, Validation Loss: 0.007964482661918742\n",
      "Epoch [3491/20000], Training Loss: 0.009223559761202134, Validation Loss: 0.010487012517062664\n",
      "Epoch [3492/20000], Training Loss: 0.006738283596600273, Validation Loss: 0.005736038764293231\n",
      "Epoch [3493/20000], Training Loss: 0.007340291764454118, Validation Loss: 0.008072393780626694\n",
      "Epoch [3494/20000], Training Loss: 0.007868239972107663, Validation Loss: 0.011550015880808522\n",
      "Epoch [3495/20000], Training Loss: 0.006905953512354505, Validation Loss: 0.007623705860039211\n",
      "Epoch [3496/20000], Training Loss: 0.0056953477717992685, Validation Loss: 0.004477664129808545\n",
      "Epoch [3497/20000], Training Loss: 0.010544137650867924, Validation Loss: 0.00893127141820774\n",
      "Epoch [3498/20000], Training Loss: 0.006942834119919488, Validation Loss: 0.0058566017841518\n",
      "Epoch [3499/20000], Training Loss: 0.006937302931744073, Validation Loss: 0.003134155862312582\n",
      "Epoch [3500/20000], Training Loss: 0.013060082118094365, Validation Loss: 0.007494146299019836\n",
      "Epoch [3501/20000], Training Loss: 0.017055960232807723, Validation Loss: 0.008757040112673817\n",
      "Epoch [3502/20000], Training Loss: 0.03208455940330168, Validation Loss: 0.02183966682961369\n",
      "Epoch [3503/20000], Training Loss: 0.04159667350385072, Validation Loss: 0.02170760061731666\n",
      "Epoch [3504/20000], Training Loss: 0.05017438050292965, Validation Loss: 0.028642985774370442\n",
      "Epoch [3505/20000], Training Loss: 0.019570531755951897, Validation Loss: 0.011287829089172542\n",
      "Epoch [3506/20000], Training Loss: 0.011193166287349803, Validation Loss: 0.009172682667018048\n",
      "Epoch [3507/20000], Training Loss: 0.008965427283588465, Validation Loss: 0.010639718354598569\n",
      "Epoch [3508/20000], Training Loss: 0.008753285610250064, Validation Loss: 0.008276924279925879\n",
      "Epoch [3509/20000], Training Loss: 0.007636075865515782, Validation Loss: 0.008342263045051368\n",
      "Epoch [3510/20000], Training Loss: 0.01381203651960407, Validation Loss: 0.005660500880237253\n",
      "Epoch [3511/20000], Training Loss: 0.03223388287005946, Validation Loss: 0.02614068398328787\n",
      "Epoch [3512/20000], Training Loss: 0.014339062863395416, Validation Loss: 0.009986634672519099\n",
      "Epoch [3513/20000], Training Loss: 0.008745427571869056, Validation Loss: 0.005893185830375712\n",
      "Epoch [3514/20000], Training Loss: 0.007596152181836909, Validation Loss: 0.015922374737605032\n",
      "Epoch [3515/20000], Training Loss: 0.012000125242463713, Validation Loss: 0.010078963816015727\n",
      "Epoch [3516/20000], Training Loss: 0.008721180964909894, Validation Loss: 0.005058864747785005\n",
      "Epoch [3517/20000], Training Loss: 0.008385461742623843, Validation Loss: 0.018055088084890616\n",
      "Epoch [3518/20000], Training Loss: 0.011182323347644083, Validation Loss: 0.00366285690886668\n",
      "Epoch [3519/20000], Training Loss: 0.009757841862405517, Validation Loss: 0.004323830286239172\n",
      "Epoch [3520/20000], Training Loss: 0.02700903601960038, Validation Loss: 0.015509675165325072\n",
      "Epoch [3521/20000], Training Loss: 0.01223337790205343, Validation Loss: 0.007900845510716539\n",
      "Epoch [3522/20000], Training Loss: 0.008712242473848164, Validation Loss: 0.01026089334344858\n",
      "Epoch [3523/20000], Training Loss: 0.011940591835549899, Validation Loss: 0.01039426956772108\n",
      "Epoch [3524/20000], Training Loss: 0.013559784615478878, Validation Loss: 0.02944381591831708\n",
      "Epoch [3525/20000], Training Loss: 0.01372186777630954, Validation Loss: 0.006778010445227015\n",
      "Epoch [3526/20000], Training Loss: 0.005855695653735893, Validation Loss: 0.01539664331586518\n",
      "Epoch [3527/20000], Training Loss: 0.01434922413318418, Validation Loss: 0.00432341238378139\n",
      "Epoch [3528/20000], Training Loss: 0.012715013387995506, Validation Loss: 0.016902472731609514\n",
      "Epoch [3529/20000], Training Loss: 0.010198944915568322, Validation Loss: 0.00544385645035358\n",
      "Epoch [3530/20000], Training Loss: 0.010917191189946607, Validation Loss: 0.008274471224703055\n",
      "Epoch [3531/20000], Training Loss: 0.0070365725883415765, Validation Loss: 0.004997247370665393\n",
      "Epoch [3532/20000], Training Loss: 0.004965905757022223, Validation Loss: 0.0034968537201751004\n",
      "Epoch [3533/20000], Training Loss: 0.0051450297947407565, Validation Loss: 0.0034874431359120145\n",
      "Epoch [3534/20000], Training Loss: 0.005920356680560092, Validation Loss: 0.004599193804360928\n",
      "Epoch [3535/20000], Training Loss: 0.007046429037083206, Validation Loss: 0.0032598824095962137\n",
      "Epoch [3536/20000], Training Loss: 0.00710096796059848, Validation Loss: 0.005177249563156953\n",
      "Epoch [3537/20000], Training Loss: 0.018917038222882963, Validation Loss: 0.01469369285538161\n",
      "Epoch [3538/20000], Training Loss: 0.005082938028083715, Validation Loss: 0.01641230421637831\n",
      "Epoch [3539/20000], Training Loss: 0.011102497730624756, Validation Loss: 0.0037812364451253544\n",
      "Epoch [3540/20000], Training Loss: 0.012933280310660069, Validation Loss: 0.006362264925038679\n",
      "Epoch [3541/20000], Training Loss: 0.012087215363862924, Validation Loss: 0.010695420734137429\n",
      "Epoch [3542/20000], Training Loss: 0.024839305618245686, Validation Loss: 0.007015628923129823\n",
      "Epoch [3543/20000], Training Loss: 0.01279962035394939, Validation Loss: 0.006362993403594633\n",
      "Epoch [3544/20000], Training Loss: 0.006608569731594928, Validation Loss: 0.010100994402916643\n",
      "Epoch [3545/20000], Training Loss: 0.006285167010251566, Validation Loss: 0.012713946105167549\n",
      "Epoch [3546/20000], Training Loss: 0.018187357249969085, Validation Loss: 0.02408208147596997\n",
      "Epoch [3547/20000], Training Loss: 0.03127894970072832, Validation Loss: 0.026363079086877406\n",
      "Epoch [3548/20000], Training Loss: 0.02703737306209015, Validation Loss: 0.011380153186174928\n",
      "Epoch [3549/20000], Training Loss: 0.02047484508407901, Validation Loss: 0.014271892847643488\n",
      "Epoch [3550/20000], Training Loss: 0.016235863180814443, Validation Loss: 0.006888455636986431\n",
      "Epoch [3551/20000], Training Loss: 0.011684992525260895, Validation Loss: 0.005388138468593492\n",
      "Epoch [3552/20000], Training Loss: 0.01150393965092787, Validation Loss: 0.010085681891723652\n",
      "Epoch [3553/20000], Training Loss: 0.010526523106299075, Validation Loss: 0.0051472340809206275\n",
      "Epoch [3554/20000], Training Loss: 0.012418752516038824, Validation Loss: 0.004921588712152568\n",
      "Epoch [3555/20000], Training Loss: 0.007220262826519631, Validation Loss: 0.025821658999302142\n",
      "Epoch [3556/20000], Training Loss: 0.0146449998132344, Validation Loss: 0.0069957386579225\n",
      "Epoch [3557/20000], Training Loss: 0.009864201864859621, Validation Loss: 0.01763019949102532\n",
      "Epoch [3558/20000], Training Loss: 0.014076008533655633, Validation Loss: 0.016390831390055687\n",
      "Epoch [3559/20000], Training Loss: 0.008903865548096032, Validation Loss: 0.00737720363083229\n",
      "Epoch [3560/20000], Training Loss: 0.010898698734048853, Validation Loss: 0.013001974981307285\n",
      "Epoch [3561/20000], Training Loss: 0.014784886445600673, Validation Loss: 0.010364108955140441\n",
      "Epoch [3562/20000], Training Loss: 0.01343969500937549, Validation Loss: 0.004373566584490618\n",
      "Epoch [3563/20000], Training Loss: 0.01648604197232219, Validation Loss: 0.005721483495173223\n",
      "Epoch [3564/20000], Training Loss: 0.010212367345437607, Validation Loss: 0.008842623838599397\n",
      "Epoch [3565/20000], Training Loss: 0.018351630561353107, Validation Loss: 0.014559881879648628\n",
      "Epoch [3566/20000], Training Loss: 0.0248647528372073, Validation Loss: 0.031894156434607926\n",
      "Epoch [3567/20000], Training Loss: 0.012395090425603226, Validation Loss: 0.005920494207722575\n",
      "Epoch [3568/20000], Training Loss: 0.017957755030823006, Validation Loss: 0.010243053657859491\n",
      "Epoch [3569/20000], Training Loss: 0.008753869067212301, Validation Loss: 0.010345227556793393\n",
      "Epoch [3570/20000], Training Loss: 0.012686484842561185, Validation Loss: 0.017751831356939616\n",
      "Epoch [3571/20000], Training Loss: 0.014395492804136925, Validation Loss: 0.008740215773432933\n",
      "Epoch [3572/20000], Training Loss: 0.0166026784603933, Validation Loss: 0.07099676137174718\n",
      "Epoch [3573/20000], Training Loss: 0.027733057953550348, Validation Loss: 0.009161672982112836\n",
      "Epoch [3574/20000], Training Loss: 0.009784746873525105, Validation Loss: 0.0067770840848554925\n",
      "Epoch [3575/20000], Training Loss: 0.007814406712506232, Validation Loss: 0.004338731521524356\n",
      "Epoch [3576/20000], Training Loss: 0.008015178998383428, Validation Loss: 0.010827055504445784\n",
      "Epoch [3577/20000], Training Loss: 0.012079460442432069, Validation Loss: 0.01383316736402256\n",
      "Epoch [3578/20000], Training Loss: 0.011810511290345207, Validation Loss: 0.005282023506489557\n",
      "Epoch [3579/20000], Training Loss: 0.007493942164728651, Validation Loss: 0.02089395161424688\n",
      "Epoch [3580/20000], Training Loss: 0.011335104615552285, Validation Loss: 0.02911916966444445\n",
      "Epoch [3581/20000], Training Loss: 0.020016407661647202, Validation Loss: 0.01113281969513277\n",
      "Epoch [3582/20000], Training Loss: 0.0133641637565224, Validation Loss: 0.009772376206303843\n",
      "Epoch [3583/20000], Training Loss: 0.012619557869454314, Validation Loss: 0.004502460452239835\n",
      "Epoch [3584/20000], Training Loss: 0.011515551608421706, Validation Loss: 0.004356856908218886\n",
      "Epoch [3585/20000], Training Loss: 0.00882083217246483, Validation Loss: 0.023642853330979312\n",
      "Epoch [3586/20000], Training Loss: 0.012686730350001849, Validation Loss: 0.008477404447538512\n",
      "Epoch [3587/20000], Training Loss: 0.00831143582529746, Validation Loss: 0.005263501336311752\n",
      "Epoch [3588/20000], Training Loss: 0.015841366246864448, Validation Loss: 0.015383109637826107\n",
      "Epoch [3589/20000], Training Loss: 0.024509710912492925, Validation Loss: 0.02722156580128138\n",
      "Epoch [3590/20000], Training Loss: 0.04938356632607922, Validation Loss: 0.017212243380616395\n",
      "Epoch [3591/20000], Training Loss: 0.049696276343248816, Validation Loss: 0.06930841553323526\n",
      "Epoch [3592/20000], Training Loss: 0.04219729099921616, Validation Loss: 0.05807931522213407\n",
      "Epoch [3593/20000], Training Loss: 0.018421518419927452, Validation Loss: 0.009899388224896808\n",
      "Epoch [3594/20000], Training Loss: 0.009901921979949943, Validation Loss: 0.013475988618405868\n",
      "Epoch [3595/20000], Training Loss: 0.01021569553995505, Validation Loss: 0.010672435013345551\n",
      "Epoch [3596/20000], Training Loss: 0.00837213926882084, Validation Loss: 0.007265349492107427\n",
      "Epoch [3597/20000], Training Loss: 0.007128594939786126, Validation Loss: 0.0054455375263517325\n",
      "Epoch [3598/20000], Training Loss: 0.0059575608216359145, Validation Loss: 0.004883699485065206\n",
      "Epoch [3599/20000], Training Loss: 0.005682938497297333, Validation Loss: 0.010059949526868601\n",
      "Epoch [3600/20000], Training Loss: 0.012853946453625602, Validation Loss: 0.006815689628410837\n",
      "Epoch [3601/20000], Training Loss: 0.007663068215785123, Validation Loss: 0.0066290392962044765\n",
      "Epoch [3602/20000], Training Loss: 0.00652678468135751, Validation Loss: 0.008197927880618319\n",
      "Epoch [3603/20000], Training Loss: 0.009346801751235034, Validation Loss: 0.0036089778313095977\n",
      "Epoch [3604/20000], Training Loss: 0.011752501205462198, Validation Loss: 0.007977934899283159\n",
      "Epoch [3605/20000], Training Loss: 0.019675855763385437, Validation Loss: 0.007812828193774126\n",
      "Epoch [3606/20000], Training Loss: 0.010094987382347296, Validation Loss: 0.009272856563516793\n",
      "Epoch [3607/20000], Training Loss: 0.00924033885865226, Validation Loss: 0.010249545471294499\n",
      "Epoch [3608/20000], Training Loss: 0.020327223065188655, Validation Loss: 0.01944470867265777\n",
      "Epoch [3609/20000], Training Loss: 0.01657426416958125, Validation Loss: 0.004163415387698218\n",
      "Epoch [3610/20000], Training Loss: 0.0066077584779122844, Validation Loss: 0.004131792999548419\n",
      "Epoch [3611/20000], Training Loss: 0.005338601162130934, Validation Loss: 0.011079477269278064\n",
      "Epoch [3612/20000], Training Loss: 0.006210190647314968, Validation Loss: 0.006031542511907771\n",
      "Epoch [3613/20000], Training Loss: 0.008599874904445772, Validation Loss: 0.005625348044369665\n",
      "Epoch [3614/20000], Training Loss: 0.009605170073752691, Validation Loss: 0.013614093655520385\n",
      "Epoch [3615/20000], Training Loss: 0.0075092186515186244, Validation Loss: 0.003660723817183964\n",
      "Epoch [3616/20000], Training Loss: 0.009660505562871029, Validation Loss: 0.010074985081758112\n",
      "Epoch [3617/20000], Training Loss: 0.00828630279076086, Validation Loss: 0.003902190501642596\n",
      "Epoch [3618/20000], Training Loss: 0.010755245234966944, Validation Loss: 0.0030132178284065147\n",
      "Epoch [3619/20000], Training Loss: 0.007911316344429256, Validation Loss: 0.005221231132713936\n",
      "Epoch [3620/20000], Training Loss: 0.010730625166615937, Validation Loss: 0.00588716881406981\n",
      "Epoch [3621/20000], Training Loss: 0.007274218030423591, Validation Loss: 0.006617819683587608\n",
      "Epoch [3622/20000], Training Loss: 0.022668098773075535, Validation Loss: 0.0031580183436956994\n",
      "Epoch [3623/20000], Training Loss: 0.005620955663679135, Validation Loss: 0.006510775265488584\n",
      "Epoch [3624/20000], Training Loss: 0.004514017590119173, Validation Loss: 0.007852592069436273\n",
      "Epoch [3625/20000], Training Loss: 0.006367628786132887, Validation Loss: 0.0031171769605888694\n",
      "Epoch [3626/20000], Training Loss: 0.009346306031797471, Validation Loss: 0.02122191304074642\n",
      "Epoch [3627/20000], Training Loss: 0.035365888175355006, Validation Loss: 0.06692764801638726\n",
      "Epoch [3628/20000], Training Loss: 0.05314491539528327, Validation Loss: 0.027591189719195035\n",
      "Epoch [3629/20000], Training Loss: 0.025369328862455274, Validation Loss: 0.018226656294635695\n",
      "Epoch [3630/20000], Training Loss: 0.018672613743027405, Validation Loss: 0.024658737741577043\n",
      "Epoch [3631/20000], Training Loss: 0.016457669087685645, Validation Loss: 0.026172182578842955\n",
      "Epoch [3632/20000], Training Loss: 0.0343246771538231, Validation Loss: 0.08539162887666\n",
      "Epoch [3633/20000], Training Loss: 0.037016627145931125, Validation Loss: 0.05547497632921607\n",
      "Epoch [3634/20000], Training Loss: 0.035557804945191104, Validation Loss: 0.0291726101173221\n",
      "Epoch [3635/20000], Training Loss: 0.018555192039847106, Validation Loss: 0.010641438629266173\n",
      "Epoch [3636/20000], Training Loss: 0.010534602795295151, Validation Loss: 0.005659296649517793\n",
      "Epoch [3637/20000], Training Loss: 0.0048334213253318535, Validation Loss: 0.007191706420907654\n",
      "Epoch [3638/20000], Training Loss: 0.0086587362893137, Validation Loss: 0.002668903837705524\n",
      "Epoch [3639/20000], Training Loss: 0.024176460022837482, Validation Loss: 0.008765874367270499\n",
      "Epoch [3640/20000], Training Loss: 0.02241708706307171, Validation Loss: 0.008443117365199478\n",
      "Epoch [3641/20000], Training Loss: 0.02701371007632198, Validation Loss: 0.008378785872732176\n",
      "Epoch [3642/20000], Training Loss: 0.01693132429707995, Validation Loss: 0.012091539628272599\n",
      "Epoch [3643/20000], Training Loss: 0.010342622853516201, Validation Loss: 0.019470471983238116\n",
      "Epoch [3644/20000], Training Loss: 0.013341274569808905, Validation Loss: 0.009489107161536075\n",
      "Epoch [3645/20000], Training Loss: 0.007432593248917588, Validation Loss: 0.009437103049744369\n",
      "Epoch [3646/20000], Training Loss: 0.013014269181959597, Validation Loss: 0.01421941612465259\n",
      "Epoch [3647/20000], Training Loss: 0.010821246583805778, Validation Loss: 0.01569580905264242\n",
      "Epoch [3648/20000], Training Loss: 0.008455658090367382, Validation Loss: 0.006604907442936435\n",
      "Epoch [3649/20000], Training Loss: 0.010022157540203937, Validation Loss: 0.0035645784871413916\n",
      "Epoch [3650/20000], Training Loss: 0.006828197951628161, Validation Loss: 0.003806312924910214\n",
      "Epoch [3651/20000], Training Loss: 0.013139126299003172, Validation Loss: 0.004804795247959451\n",
      "Epoch [3652/20000], Training Loss: 0.013322895001692814, Validation Loss: 0.0560604994413682\n",
      "Epoch [3653/20000], Training Loss: 0.028753946339033325, Validation Loss: 0.01586478682656889\n",
      "Epoch [3654/20000], Training Loss: 0.013614698916041692, Validation Loss: 0.008044801967197859\n",
      "Epoch [3655/20000], Training Loss: 0.005973523027413259, Validation Loss: 0.0068503490300843494\n",
      "Epoch [3656/20000], Training Loss: 0.008032944909879396, Validation Loss: 0.00534974813049465\n",
      "Epoch [3657/20000], Training Loss: 0.014347895320887412, Validation Loss: 0.026983342976717863\n",
      "Epoch [3658/20000], Training Loss: 0.01664767147407734, Validation Loss: 0.02772250451796156\n",
      "Epoch [3659/20000], Training Loss: 0.01765398636104822, Validation Loss: 0.02566560580505341\n",
      "Epoch [3660/20000], Training Loss: 0.04442123787833095, Validation Loss: 0.01313676935179891\n",
      "Epoch [3661/20000], Training Loss: 0.03678787669299969, Validation Loss: 0.05522340808851571\n",
      "Epoch [3662/20000], Training Loss: 0.024812673551163504, Validation Loss: 0.018295574723846852\n",
      "Epoch [3663/20000], Training Loss: 0.019792359255786453, Validation Loss: 0.017669004446361217\n",
      "Epoch [3664/20000], Training Loss: 0.015105833293741202, Validation Loss: 0.010568266043784596\n",
      "Epoch [3665/20000], Training Loss: 0.012818776312542468, Validation Loss: 0.006820748815373788\n",
      "Epoch [3666/20000], Training Loss: 0.0071790668880566955, Validation Loss: 0.0050553509701773336\n",
      "Epoch [3667/20000], Training Loss: 0.007779300256515853, Validation Loss: 0.015229054852562587\n",
      "Epoch [3668/20000], Training Loss: 0.02110625587686497, Validation Loss: 0.01013814874980439\n",
      "Epoch [3669/20000], Training Loss: 0.031161592994716818, Validation Loss: 0.010838327732382627\n",
      "Epoch [3670/20000], Training Loss: 0.01048068397462235, Validation Loss: 0.012825226454521856\n",
      "Epoch [3671/20000], Training Loss: 0.009500165915377628, Validation Loss: 0.006439273189025739\n",
      "Epoch [3672/20000], Training Loss: 0.013652069773408064, Validation Loss: 0.006672077813202496\n",
      "Epoch [3673/20000], Training Loss: 0.0069984084527407375, Validation Loss: 0.00423574364949835\n",
      "Epoch [3674/20000], Training Loss: 0.008431300005863054, Validation Loss: 0.004959639563898983\n",
      "Epoch [3675/20000], Training Loss: 0.009223330883417345, Validation Loss: 0.006552319870043074\n",
      "Epoch [3676/20000], Training Loss: 0.016218458909341798, Validation Loss: 0.008114568165809086\n",
      "Epoch [3677/20000], Training Loss: 0.009343183220413007, Validation Loss: 0.006310930751020294\n",
      "Epoch [3678/20000], Training Loss: 0.007992238876145816, Validation Loss: 0.01724221420169338\n",
      "Epoch [3679/20000], Training Loss: 0.01170899177564674, Validation Loss: 0.012994688945997217\n",
      "Epoch [3680/20000], Training Loss: 0.006129542548608567, Validation Loss: 0.023479810917003328\n",
      "Epoch [3681/20000], Training Loss: 0.01667974416442064, Validation Loss: 0.09710185442653478\n",
      "Epoch [3682/20000], Training Loss: 0.04081005885797952, Validation Loss: 0.08189904499638657\n",
      "Epoch [3683/20000], Training Loss: 0.02387436625680753, Validation Loss: 0.02430222225588555\n",
      "Epoch [3684/20000], Training Loss: 0.01849967276211828, Validation Loss: 0.03329487125821678\n",
      "Epoch [3685/20000], Training Loss: 0.023723255052963004, Validation Loss: 0.020527504473908005\n",
      "Epoch [3686/20000], Training Loss: 0.0219247470002821, Validation Loss: 0.010213803883743822\n",
      "Epoch [3687/20000], Training Loss: 0.012868651542313663, Validation Loss: 0.010471170901139106\n",
      "Epoch [3688/20000], Training Loss: 0.008789614659950271, Validation Loss: 0.006989841317720545\n",
      "Epoch [3689/20000], Training Loss: 0.008392501148461764, Validation Loss: 0.005891189967899534\n",
      "Epoch [3690/20000], Training Loss: 0.009695166131709445, Validation Loss: 0.006473243123577633\n",
      "Epoch [3691/20000], Training Loss: 0.011002098151948303, Validation Loss: 0.013018120684526886\n",
      "Epoch [3692/20000], Training Loss: 0.00787241053798685, Validation Loss: 0.017882060664642756\n",
      "Epoch [3693/20000], Training Loss: 0.01659125934072238, Validation Loss: 0.020218818596912738\n",
      "Epoch [3694/20000], Training Loss: 0.007767150553783202, Validation Loss: 0.007282911822721084\n",
      "Epoch [3695/20000], Training Loss: 0.008112427096680872, Validation Loss: 0.008285631302746716\n",
      "Epoch [3696/20000], Training Loss: 0.012468395141435653, Validation Loss: 0.011977605256861677\n",
      "Epoch [3697/20000], Training Loss: 0.014418548606175006, Validation Loss: 0.09190278080619205\n",
      "Epoch [3698/20000], Training Loss: 0.033471655801154805, Validation Loss: 0.013337631595147417\n",
      "Epoch [3699/20000], Training Loss: 0.012771115486040279, Validation Loss: 0.011277654955002618\n",
      "Epoch [3700/20000], Training Loss: 0.015469973262432697, Validation Loss: 0.024976105031458\n",
      "Epoch [3701/20000], Training Loss: 0.021808383360816412, Validation Loss: 0.036625373044305895\n",
      "Epoch [3702/20000], Training Loss: 0.041811982119855075, Validation Loss: 0.025680536855317855\n",
      "Epoch [3703/20000], Training Loss: 0.0729943229816854, Validation Loss: 0.020764395761294248\n",
      "Epoch [3704/20000], Training Loss: 0.035603822308725545, Validation Loss: 0.02919529600434805\n",
      "Epoch [3705/20000], Training Loss: 0.022280452639928887, Validation Loss: 0.01956909345551618\n",
      "Epoch [3706/20000], Training Loss: 0.014528259003002728, Validation Loss: 0.011526090974387313\n",
      "Epoch [3707/20000], Training Loss: 0.013961195646386062, Validation Loss: 0.009695713187933766\n",
      "Epoch [3708/20000], Training Loss: 0.010090929688885808, Validation Loss: 0.010546148686054429\n",
      "Epoch [3709/20000], Training Loss: 0.008646660367958248, Validation Loss: 0.009160292136584758\n",
      "Epoch [3710/20000], Training Loss: 0.009602036734577268, Validation Loss: 0.006163637689886465\n",
      "Epoch [3711/20000], Training Loss: 0.008296350299912904, Validation Loss: 0.006676000645061971\n",
      "Epoch [3712/20000], Training Loss: 0.006742155911134822, Validation Loss: 0.010422702877117931\n",
      "Epoch [3713/20000], Training Loss: 0.008897392763174139, Validation Loss: 0.0052245041822191396\n",
      "Epoch [3714/20000], Training Loss: 0.005604508966955889, Validation Loss: 0.004219756522161333\n",
      "Epoch [3715/20000], Training Loss: 0.005026896384411624, Validation Loss: 0.004490132448154327\n",
      "Epoch [3716/20000], Training Loss: 0.005368863419529849, Validation Loss: 0.014533605346564655\n",
      "Epoch [3717/20000], Training Loss: 0.0147458602358321, Validation Loss: 0.0050401832408429725\n",
      "Epoch [3718/20000], Training Loss: 0.013722194440301141, Validation Loss: 0.00684978609663237\n",
      "Epoch [3719/20000], Training Loss: 0.013739393036563083, Validation Loss: 0.008382290475050727\n",
      "Epoch [3720/20000], Training Loss: 0.0175679659119591, Validation Loss: 0.023785887458091435\n",
      "Epoch [3721/20000], Training Loss: 0.017149923044988618, Validation Loss: 0.029718750120602117\n",
      "Epoch [3722/20000], Training Loss: 0.0172869770966437, Validation Loss: 0.006346146319629042\n",
      "Epoch [3723/20000], Training Loss: 0.01711947307090408, Validation Loss: 0.009327178561306937\n",
      "Epoch [3724/20000], Training Loss: 0.0109605121282844, Validation Loss: 0.006114159467837713\n",
      "Epoch [3725/20000], Training Loss: 0.008682860269410802, Validation Loss: 0.004994207676369504\n",
      "Epoch [3726/20000], Training Loss: 0.005386290073926959, Validation Loss: 0.005699678810064565\n",
      "Epoch [3727/20000], Training Loss: 0.007291104070775743, Validation Loss: 0.0036035122797914354\n",
      "Epoch [3728/20000], Training Loss: 0.0057681015127205425, Validation Loss: 0.01064431345040465\n",
      "Epoch [3729/20000], Training Loss: 0.008177809866278299, Validation Loss: 0.009008293703207724\n",
      "Epoch [3730/20000], Training Loss: 0.014377292891899833, Validation Loss: 0.021271987632338556\n",
      "Epoch [3731/20000], Training Loss: 0.03212057912190046, Validation Loss: 0.05121610127649918\n",
      "Epoch [3732/20000], Training Loss: 0.030927930002300336, Validation Loss: 0.01517083179697821\n",
      "Epoch [3733/20000], Training Loss: 0.016805914885480888, Validation Loss: 0.014366038871114317\n",
      "Epoch [3734/20000], Training Loss: 0.010941997236971344, Validation Loss: 0.009905380140870486\n",
      "Epoch [3735/20000], Training Loss: 0.0072660342297500135, Validation Loss: 0.005262967141236784\n",
      "Epoch [3736/20000], Training Loss: 0.007896265633462463, Validation Loss: 0.01601291017770179\n",
      "Epoch [3737/20000], Training Loss: 0.009761508657123028, Validation Loss: 0.01500688058068722\n",
      "Epoch [3738/20000], Training Loss: 0.014551421022458402, Validation Loss: 0.0133679471365582\n",
      "Epoch [3739/20000], Training Loss: 0.00724267748280129, Validation Loss: 0.04759312643039136\n",
      "Epoch [3740/20000], Training Loss: 0.023403626191014024, Validation Loss: 0.006066801487902142\n",
      "Epoch [3741/20000], Training Loss: 0.04480704768294735, Validation Loss: 0.005937531007865411\n",
      "Epoch [3742/20000], Training Loss: 0.04294375077006407, Validation Loss: 0.08751177574910647\n",
      "Epoch [3743/20000], Training Loss: 0.03829505481865324, Validation Loss: 0.04148589840527604\n",
      "Epoch [3744/20000], Training Loss: 0.024091752225233774, Validation Loss: 0.009598139376456467\n",
      "Epoch [3745/20000], Training Loss: 0.011297852815394955, Validation Loss: 0.007614592990648816\n",
      "Epoch [3746/20000], Training Loss: 0.006346831460340192, Validation Loss: 0.005624809854075303\n",
      "Epoch [3747/20000], Training Loss: 0.00935977573629186, Validation Loss: 0.017281259083209273\n",
      "Epoch [3748/20000], Training Loss: 0.027008607589128326, Validation Loss: 0.026160171398185383\n",
      "Epoch [3749/20000], Training Loss: 0.013128820648749493, Validation Loss: 0.012002759365974012\n",
      "Epoch [3750/20000], Training Loss: 0.009621918899938464, Validation Loss: 0.025329284324210936\n",
      "Epoch [3751/20000], Training Loss: 0.016234103349103992, Validation Loss: 0.006982287560536601\n",
      "Epoch [3752/20000], Training Loss: 0.008679129191607769, Validation Loss: 0.020700208067805228\n",
      "Epoch [3753/20000], Training Loss: 0.013909047116093072, Validation Loss: 0.011033621855057454\n",
      "Epoch [3754/20000], Training Loss: 0.009388567638000365, Validation Loss: 0.005248434186772296\n",
      "Epoch [3755/20000], Training Loss: 0.008537559695209243, Validation Loss: 0.007350543927746652\n",
      "Epoch [3756/20000], Training Loss: 0.014608488045009185, Validation Loss: 0.028449109621533353\n",
      "Epoch [3757/20000], Training Loss: 0.024765502412005196, Validation Loss: 0.011804373837159281\n",
      "Epoch [3758/20000], Training Loss: 0.011526655105366703, Validation Loss: 0.009118189306020343\n",
      "Epoch [3759/20000], Training Loss: 0.008459907653429712, Validation Loss: 0.00555640801349\n",
      "Epoch [3760/20000], Training Loss: 0.00962992525143948, Validation Loss: 0.01405175658455907\n",
      "Epoch [3761/20000], Training Loss: 0.015468155129513304, Validation Loss: 0.00918541653425312\n",
      "Epoch [3762/20000], Training Loss: 0.009488963524095848, Validation Loss: 0.006304626147717889\n",
      "Epoch [3763/20000], Training Loss: 0.005409918777477937, Validation Loss: 0.004850278882892261\n",
      "Epoch [3764/20000], Training Loss: 0.008061816191652074, Validation Loss: 0.0036503328527714984\n",
      "Epoch [3765/20000], Training Loss: 0.005434304709654368, Validation Loss: 0.0051968652267240555\n",
      "Epoch [3766/20000], Training Loss: 0.006642484257879134, Validation Loss: 0.0056136530316186805\n",
      "Epoch [3767/20000], Training Loss: 0.008420111511065491, Validation Loss: 0.010030753478937575\n",
      "Epoch [3768/20000], Training Loss: 0.03246846795497861, Validation Loss: 0.07049536684852294\n",
      "Epoch [3769/20000], Training Loss: 0.06244428295342784, Validation Loss: 0.005539106636563572\n",
      "Epoch [3770/20000], Training Loss: 0.017628871482364566, Validation Loss: 0.022635728653072423\n",
      "Epoch [3771/20000], Training Loss: 0.016374820296602723, Validation Loss: 0.023598182930072906\n",
      "Epoch [3772/20000], Training Loss: 0.009724626571239372, Validation Loss: 0.00690868181517804\n",
      "Epoch [3773/20000], Training Loss: 0.006759019930281543, Validation Loss: 0.007073706036326907\n",
      "Epoch [3774/20000], Training Loss: 0.008744573045987636, Validation Loss: 0.006915459005605433\n",
      "Epoch [3775/20000], Training Loss: 0.011464613202926037, Validation Loss: 0.005135924583376956\n",
      "Epoch [3776/20000], Training Loss: 0.01036207184578026, Validation Loss: 0.005500789196223163\n",
      "Epoch [3777/20000], Training Loss: 0.006495694596586483, Validation Loss: 0.006378696868106358\n",
      "Epoch [3778/20000], Training Loss: 0.005305318602560354, Validation Loss: 0.0036741846605293305\n",
      "Epoch [3779/20000], Training Loss: 0.00526204578324853, Validation Loss: 0.005594521789443466\n",
      "Epoch [3780/20000], Training Loss: 0.010900362584054944, Validation Loss: 0.003837374705727013\n",
      "Epoch [3781/20000], Training Loss: 0.006552660134080465, Validation Loss: 0.006934527612250615\n",
      "Epoch [3782/20000], Training Loss: 0.007368085859655237, Validation Loss: 0.0029197266660056747\n",
      "Epoch [3783/20000], Training Loss: 0.008839640816274499, Validation Loss: 0.07974361096109976\n",
      "Epoch [3784/20000], Training Loss: 0.056309082029786496, Validation Loss: 0.06492978973048176\n",
      "Epoch [3785/20000], Training Loss: 0.030817741760984063, Validation Loss: 0.01612168493355506\n",
      "Epoch [3786/20000], Training Loss: 0.007900987140601501, Validation Loss: 0.005542117210422605\n",
      "Epoch [3787/20000], Training Loss: 0.006174666790424713, Validation Loss: 0.004925820954602906\n",
      "Epoch [3788/20000], Training Loss: 0.006291628769758972, Validation Loss: 0.004621413152319421\n",
      "Epoch [3789/20000], Training Loss: 0.008855811517800507, Validation Loss: 0.005123917838533641\n",
      "Epoch [3790/20000], Training Loss: 0.009529942162251765, Validation Loss: 0.01929914883635136\n",
      "Epoch [3791/20000], Training Loss: 0.01550953948753886, Validation Loss: 0.0172139510114209\n",
      "Epoch [3792/20000], Training Loss: 0.00563764462380537, Validation Loss: 0.004769304672208818\n",
      "Epoch [3793/20000], Training Loss: 0.006041610029309855, Validation Loss: 0.0045019837195598355\n",
      "Epoch [3794/20000], Training Loss: 0.009129207996662916, Validation Loss: 0.010997347607891632\n",
      "Epoch [3795/20000], Training Loss: 0.035401025670580566, Validation Loss: 0.008888361858773968\n",
      "Epoch [3796/20000], Training Loss: 0.023789470774188106, Validation Loss: 0.005712633815367748\n",
      "Epoch [3797/20000], Training Loss: 0.03838586669215666, Validation Loss: 0.007305145244182703\n",
      "Epoch [3798/20000], Training Loss: 0.015064521380866478, Validation Loss: 0.018265394364300445\n",
      "Epoch [3799/20000], Training Loss: 0.0204113026624587, Validation Loss: 0.006018278048194199\n",
      "Epoch [3800/20000], Training Loss: 0.013567417926554168, Validation Loss: 0.006764773919883282\n",
      "Epoch [3801/20000], Training Loss: 0.012545119834241194, Validation Loss: 0.02341169807654556\n",
      "Epoch [3802/20000], Training Loss: 0.021432181596407775, Validation Loss: 0.03023709684930509\n",
      "Epoch [3803/20000], Training Loss: 0.017389837399605312, Validation Loss: 0.011865002877503425\n",
      "Epoch [3804/20000], Training Loss: 0.013626536882059424, Validation Loss: 0.006270629678905745\n",
      "Epoch [3805/20000], Training Loss: 0.008285865827929229, Validation Loss: 0.007920771246087302\n",
      "Epoch [3806/20000], Training Loss: 0.009501060537461723, Validation Loss: 0.006058287059927027\n",
      "Epoch [3807/20000], Training Loss: 0.006062528060283512, Validation Loss: 0.007480807604319235\n",
      "Epoch [3808/20000], Training Loss: 0.008157063973028147, Validation Loss: 0.012511557449036965\n",
      "Epoch [3809/20000], Training Loss: 0.010411407578171097, Validation Loss: 0.007693854066648116\n",
      "Epoch [3810/20000], Training Loss: 0.01136978144807342, Validation Loss: 0.004472776052336457\n",
      "Epoch [3811/20000], Training Loss: 0.014592005599947047, Validation Loss: 0.04378643051952987\n",
      "Epoch [3812/20000], Training Loss: 0.03737288335105404, Validation Loss: 0.007473621744964021\n",
      "Epoch [3813/20000], Training Loss: 0.010796216708383457, Validation Loss: 0.006869279160826279\n",
      "Epoch [3814/20000], Training Loss: 0.007776195372571237, Validation Loss: 0.00500721428345839\n",
      "Epoch [3815/20000], Training Loss: 0.0068983021566444746, Validation Loss: 0.013427806071544226\n",
      "Epoch [3816/20000], Training Loss: 0.0073097723236839685, Validation Loss: 0.0056001974830905965\n",
      "Epoch [3817/20000], Training Loss: 0.006605899460347635, Validation Loss: 0.005331149249546537\n",
      "Epoch [3818/20000], Training Loss: 0.006966753072837102, Validation Loss: 0.004959295278760172\n",
      "Epoch [3819/20000], Training Loss: 0.00881325434810216, Validation Loss: 0.008442207342406098\n",
      "Epoch [3820/20000], Training Loss: 0.01114530284290335, Validation Loss: 0.015350632284285634\n",
      "Epoch [3821/20000], Training Loss: 0.008820782631768711, Validation Loss: 0.011385726974586414\n",
      "Epoch [3822/20000], Training Loss: 0.006863921633859198, Validation Loss: 0.013344746819646909\n",
      "Epoch [3823/20000], Training Loss: 0.017740871633577626, Validation Loss: 0.006030516330284286\n",
      "Epoch [3824/20000], Training Loss: 0.038451928404226364, Validation Loss: 0.01510404083011012\n",
      "Epoch [3825/20000], Training Loss: 0.02563113396588181, Validation Loss: 0.010638200763196437\n",
      "Epoch [3826/20000], Training Loss: 0.010588379493648452, Validation Loss: 0.005344435763358238\n",
      "Epoch [3827/20000], Training Loss: 0.006477916209925232, Validation Loss: 0.009698265584123471\n",
      "Epoch [3828/20000], Training Loss: 0.01033891555666092, Validation Loss: 0.01937811396699011\n",
      "Epoch [3829/20000], Training Loss: 0.0061062172011735905, Validation Loss: 0.013815490847004444\n",
      "Epoch [3830/20000], Training Loss: 0.026446418652735053, Validation Loss: 0.005971235586555069\n",
      "Epoch [3831/20000], Training Loss: 0.038035387539171746, Validation Loss: 0.011287060762697365\n",
      "Epoch [3832/20000], Training Loss: 0.04332222655180625, Validation Loss: 0.03302312751787083\n",
      "Epoch [3833/20000], Training Loss: 0.022730644603367774, Validation Loss: 0.011825172590241502\n",
      "Epoch [3834/20000], Training Loss: 0.02466589235700667, Validation Loss: 0.04321301286984204\n",
      "Epoch [3835/20000], Training Loss: 0.013124513043440597, Validation Loss: 0.00896013121694393\n",
      "Epoch [3836/20000], Training Loss: 0.00935295644947993, Validation Loss: 0.008062364182218775\n",
      "Epoch [3837/20000], Training Loss: 0.007571819121949375, Validation Loss: 0.005627678970215909\n",
      "Epoch [3838/20000], Training Loss: 0.006851536027430224, Validation Loss: 0.004937561997004585\n",
      "Epoch [3839/20000], Training Loss: 0.005615282210393551, Validation Loss: 0.009952622137234357\n",
      "Epoch [3840/20000], Training Loss: 0.012429151085338421, Validation Loss: 0.013617588293239382\n",
      "Epoch [3841/20000], Training Loss: 0.00994703174884697, Validation Loss: 0.007154487010503934\n",
      "Epoch [3842/20000], Training Loss: 0.007325463287997991, Validation Loss: 0.006502610993395917\n",
      "Epoch [3843/20000], Training Loss: 0.009871280136784273, Validation Loss: 0.0037870356448235854\n",
      "Epoch [3844/20000], Training Loss: 0.017716021407977678, Validation Loss: 0.03841517091495916\n",
      "Epoch [3845/20000], Training Loss: 0.03706395385337861, Validation Loss: 0.06853612834571063\n",
      "Epoch [3846/20000], Training Loss: 0.04456607500157718, Validation Loss: 0.043125353379374634\n",
      "Epoch [3847/20000], Training Loss: 0.022884874696111574, Validation Loss: 0.006589000476586599\n",
      "Epoch [3848/20000], Training Loss: 0.009379653073015757, Validation Loss: 0.010110786995418104\n",
      "Epoch [3849/20000], Training Loss: 0.01775584034801016, Validation Loss: 0.018951977938727875\n",
      "Epoch [3850/20000], Training Loss: 0.017530634500352398, Validation Loss: 0.008037665624678019\n",
      "Epoch [3851/20000], Training Loss: 0.012826273633566285, Validation Loss: 0.0125425287302018\n",
      "Epoch [3852/20000], Training Loss: 0.018249723977143213, Validation Loss: 0.007135984286492203\n",
      "Epoch [3853/20000], Training Loss: 0.010962641039181367, Validation Loss: 0.011187110237366531\n",
      "Epoch [3854/20000], Training Loss: 0.020042269465713098, Validation Loss: 0.016319995127296188\n",
      "Epoch [3855/20000], Training Loss: 0.023931695951042848, Validation Loss: 0.010729928712732675\n",
      "Epoch [3856/20000], Training Loss: 0.025690725166766373, Validation Loss: 0.009184751289629796\n",
      "Epoch [3857/20000], Training Loss: 0.02356511228884171, Validation Loss: 0.008454020948842558\n",
      "Epoch [3858/20000], Training Loss: 0.012773525657913913, Validation Loss: 0.008471437482772412\n",
      "Epoch [3859/20000], Training Loss: 0.0076947741888164145, Validation Loss: 0.005333384427022533\n",
      "Epoch [3860/20000], Training Loss: 0.00769649409838686, Validation Loss: 0.0057897560730322895\n",
      "Epoch [3861/20000], Training Loss: 0.005199259275936389, Validation Loss: 0.005301315336389223\n",
      "Epoch [3862/20000], Training Loss: 0.005196708934363414, Validation Loss: 0.020583541507221526\n",
      "Epoch [3863/20000], Training Loss: 0.009719297940526823, Validation Loss: 0.006099976916110401\n",
      "Epoch [3864/20000], Training Loss: 0.00750630849922475, Validation Loss: 0.0038950561578790905\n",
      "Epoch [3865/20000], Training Loss: 0.00816228324505833, Validation Loss: 0.017892380466946634\n",
      "Epoch [3866/20000], Training Loss: 0.013549265471738181, Validation Loss: 0.01025728657939839\n",
      "Epoch [3867/20000], Training Loss: 0.011784203974197485, Validation Loss: 0.006556334050385558\n",
      "Epoch [3868/20000], Training Loss: 0.0057050595205510035, Validation Loss: 0.0047341610875832885\n",
      "Epoch [3869/20000], Training Loss: 0.010744449887299976, Validation Loss: 0.005782522368057081\n",
      "Epoch [3870/20000], Training Loss: 0.00893580423559927, Validation Loss: 0.01413302552386345\n",
      "Epoch [3871/20000], Training Loss: 0.022726590458281782, Validation Loss: 0.1263605046422153\n",
      "Epoch [3872/20000], Training Loss: 0.05737189054915299, Validation Loss: 0.006434558773760202\n",
      "Epoch [3873/20000], Training Loss: 0.0316081696089116, Validation Loss: 0.07711125271660942\n",
      "Epoch [3874/20000], Training Loss: 0.035246256622485816, Validation Loss: 0.020893093995912944\n",
      "Epoch [3875/20000], Training Loss: 0.028637543935993954, Validation Loss: 0.014668624382085176\n",
      "Epoch [3876/20000], Training Loss: 0.0390776719515478, Validation Loss: 0.0243632003186039\n",
      "Epoch [3877/20000], Training Loss: 0.01239362295018509, Validation Loss: 0.007737810114381968\n",
      "Epoch [3878/20000], Training Loss: 0.00971838802485893, Validation Loss: 0.007286648187021326\n",
      "Epoch [3879/20000], Training Loss: 0.010311969572545163, Validation Loss: 0.007241179524758731\n",
      "Epoch [3880/20000], Training Loss: 0.006864692894201393, Validation Loss: 0.00572533316049625\n",
      "Epoch [3881/20000], Training Loss: 0.009501773335289596, Validation Loss: 0.00603945395635362\n",
      "Epoch [3882/20000], Training Loss: 0.011898830840696714, Validation Loss: 0.007231009991746719\n",
      "Epoch [3883/20000], Training Loss: 0.013715924113057554, Validation Loss: 0.010428265014338134\n",
      "Epoch [3884/20000], Training Loss: 0.013878401842833097, Validation Loss: 0.03697782972716422\n",
      "Epoch [3885/20000], Training Loss: 0.01588307928925912, Validation Loss: 0.009305434999271738\n",
      "Epoch [3886/20000], Training Loss: 0.007496007117359633, Validation Loss: 0.007117668389972616\n",
      "Epoch [3887/20000], Training Loss: 0.007179705050865388, Validation Loss: 0.005893089731086677\n",
      "Epoch [3888/20000], Training Loss: 0.009447064298521062, Validation Loss: 0.008682573737860569\n",
      "Epoch [3889/20000], Training Loss: 0.005617899757843199, Validation Loss: 0.006019389529888944\n",
      "Epoch [3890/20000], Training Loss: 0.005216733978678738, Validation Loss: 0.0041701881403111785\n",
      "Epoch [3891/20000], Training Loss: 0.006823523024130347, Validation Loss: 0.0029759249820407213\n",
      "Epoch [3892/20000], Training Loss: 0.01075162613311217, Validation Loss: 0.007050784659571334\n",
      "Epoch [3893/20000], Training Loss: 0.013514515453217817, Validation Loss: 0.012804537201213504\n",
      "Epoch [3894/20000], Training Loss: 0.00998249741465429, Validation Loss: 0.01887517870491489\n",
      "Epoch [3895/20000], Training Loss: 0.010805455090511324, Validation Loss: 0.015886776282336278\n",
      "Epoch [3896/20000], Training Loss: 0.01814083918257633, Validation Loss: 0.018767688667083653\n",
      "Epoch [3897/20000], Training Loss: 0.040467493527103215, Validation Loss: 0.08130253425666265\n",
      "Epoch [3898/20000], Training Loss: 0.05221151900644015, Validation Loss: 0.01981199254881716\n",
      "Epoch [3899/20000], Training Loss: 0.02484455766222839, Validation Loss: 0.00561838195582303\n",
      "Epoch [3900/20000], Training Loss: 0.018602376521032835, Validation Loss: 0.022144786477188712\n",
      "Epoch [3901/20000], Training Loss: 0.010621609090906401, Validation Loss: 0.011956567098011866\n",
      "Epoch [3902/20000], Training Loss: 0.009708545969000884, Validation Loss: 0.012660847328448572\n",
      "Epoch [3903/20000], Training Loss: 0.013137020108323278, Validation Loss: 0.0056788227014164605\n",
      "Epoch [3904/20000], Training Loss: 0.008541054968580803, Validation Loss: 0.005840145994885201\n",
      "Epoch [3905/20000], Training Loss: 0.008549207806936465, Validation Loss: 0.006152123815678375\n",
      "Epoch [3906/20000], Training Loss: 0.005611928836775145, Validation Loss: 0.00415616371239074\n",
      "Epoch [3907/20000], Training Loss: 0.006670407045800987, Validation Loss: 0.009635968078441918\n",
      "Epoch [3908/20000], Training Loss: 0.008539467143626618, Validation Loss: 0.021198622361893602\n",
      "Epoch [3909/20000], Training Loss: 0.013432887723735933, Validation Loss: 0.011504442564373473\n",
      "Epoch [3910/20000], Training Loss: 0.039550204042045935, Validation Loss: 0.018693556578779993\n",
      "Epoch [3911/20000], Training Loss: 0.05598395121549921, Validation Loss: 0.042268851318762506\n",
      "Epoch [3912/20000], Training Loss: 0.049997790375657915, Validation Loss: 0.01854071914151843\n",
      "Epoch [3913/20000], Training Loss: 0.021752257615194788, Validation Loss: 0.01493320575017216\n",
      "Epoch [3914/20000], Training Loss: 0.0122953303590683, Validation Loss: 0.009466460939230663\n",
      "Epoch [3915/20000], Training Loss: 0.009893868451139756, Validation Loss: 0.006982833286331568\n",
      "Epoch [3916/20000], Training Loss: 0.007789861901463675, Validation Loss: 0.005970707136189698\n",
      "Epoch [3917/20000], Training Loss: 0.005933366162935272, Validation Loss: 0.005877332289652973\n",
      "Epoch [3918/20000], Training Loss: 0.005471853575727437, Validation Loss: 0.002816847606506404\n",
      "Epoch [3919/20000], Training Loss: 0.00846801564992867, Validation Loss: 0.0017417798803376985\n",
      "Epoch [3920/20000], Training Loss: 0.011392765416011181, Validation Loss: 0.0026813702619575353\n",
      "Epoch [3921/20000], Training Loss: 0.010363227193010971, Validation Loss: 0.004749353715646569\n",
      "Epoch [3922/20000], Training Loss: 0.005299924172504689, Validation Loss: 0.0053647431809297685\n",
      "Epoch [3923/20000], Training Loss: 0.004706982351698181, Validation Loss: 0.0037288666260276875\n",
      "Epoch [3924/20000], Training Loss: 0.01601358058526005, Validation Loss: 0.010783437896602013\n",
      "Epoch [3925/20000], Training Loss: 0.02219926757139287, Validation Loss: 0.013925087317845808\n",
      "Epoch [3926/20000], Training Loss: 0.013036297565642079, Validation Loss: 0.028883641291551094\n",
      "Epoch [3927/20000], Training Loss: 0.024270725749374833, Validation Loss: 0.05857605002009742\n",
      "Epoch [3928/20000], Training Loss: 0.04177502025517502, Validation Loss: 0.008906765402831038\n",
      "Epoch [3929/20000], Training Loss: 0.022474188199599406, Validation Loss: 0.009547274856491381\n",
      "Epoch [3930/20000], Training Loss: 0.009877131154228534, Validation Loss: 0.01416643462380307\n",
      "Epoch [3931/20000], Training Loss: 0.01190524349554575, Validation Loss: 0.005312407172661031\n",
      "Epoch [3932/20000], Training Loss: 0.009518961079135937, Validation Loss: 0.005293023685862279\n",
      "Epoch [3933/20000], Training Loss: 0.022379029353307076, Validation Loss: 0.01596093680174298\n",
      "Epoch [3934/20000], Training Loss: 0.016947042363296663, Validation Loss: 0.0071466250681169186\n",
      "Epoch [3935/20000], Training Loss: 0.011256366444285959, Validation Loss: 0.00809103278283325\n",
      "Epoch [3936/20000], Training Loss: 0.007648526142085237, Validation Loss: 0.008601076497539242\n",
      "Epoch [3937/20000], Training Loss: 0.00979648034055052, Validation Loss: 0.005732426512523148\n",
      "Epoch [3938/20000], Training Loss: 0.012996347257285379, Validation Loss: 0.007539520323979231\n",
      "Epoch [3939/20000], Training Loss: 0.018199349884525873, Validation Loss: 0.010607953875351532\n",
      "Epoch [3940/20000], Training Loss: 0.014275769441155717, Validation Loss: 0.004433625955178806\n",
      "Epoch [3941/20000], Training Loss: 0.011142300009461386, Validation Loss: 0.005065039667199146\n",
      "Epoch [3942/20000], Training Loss: 0.007923724605591684, Validation Loss: 0.005120948889795228\n",
      "Epoch [3943/20000], Training Loss: 0.007094599730667791, Validation Loss: 0.0030954785624755588\n",
      "Epoch [3944/20000], Training Loss: 0.009713221362159987, Validation Loss: 0.007188870761655356\n",
      "Epoch [3945/20000], Training Loss: 0.007357831309491303, Validation Loss: 0.006697517114710878\n",
      "Epoch [3946/20000], Training Loss: 0.006281610025877983, Validation Loss: 0.003300501763637672\n",
      "Epoch [3947/20000], Training Loss: 0.01502529672818907, Validation Loss: 0.01240486050531737\n",
      "Epoch [3948/20000], Training Loss: 0.025002262725268207, Validation Loss: 0.003914285690189193\n",
      "Epoch [3949/20000], Training Loss: 0.07990304248905236, Validation Loss: 0.15883889868147413\n",
      "Epoch [3950/20000], Training Loss: 0.04936770007147321, Validation Loss: 0.021865648345348394\n",
      "Epoch [3951/20000], Training Loss: 0.01910779091745748, Validation Loss: 0.010588099766102185\n",
      "Epoch [3952/20000], Training Loss: 0.01826602942050418, Validation Loss: 0.010620765782687603\n",
      "Epoch [3953/20000], Training Loss: 0.008682343936665607, Validation Loss: 0.006055816044871476\n",
      "Epoch [3954/20000], Training Loss: 0.008278669922479562, Validation Loss: 0.009820158411374501\n",
      "Epoch [3955/20000], Training Loss: 0.022316875381095867, Validation Loss: 0.0037909690708302977\n",
      "Epoch [3956/20000], Training Loss: 0.008603700509411283, Validation Loss: 0.009111920369133318\n",
      "Epoch [3957/20000], Training Loss: 0.009442202021351218, Validation Loss: 0.010394340320050545\n",
      "Epoch [3958/20000], Training Loss: 0.008063017024791666, Validation Loss: 0.013006938408117966\n",
      "Epoch [3959/20000], Training Loss: 0.0314535503530351, Validation Loss: 0.023306331676906687\n",
      "Epoch [3960/20000], Training Loss: 0.02210285349568559, Validation Loss: 0.015340872151868414\n",
      "Epoch [3961/20000], Training Loss: 0.015194294807900275, Validation Loss: 0.010667551283025089\n",
      "Epoch [3962/20000], Training Loss: 0.010516585440866168, Validation Loss: 0.00522964688595149\n",
      "Epoch [3963/20000], Training Loss: 0.004540290599834407, Validation Loss: 0.011360007902284796\n",
      "Epoch [3964/20000], Training Loss: 0.006383067053580557, Validation Loss: 0.019971033824357182\n",
      "Epoch [3965/20000], Training Loss: 0.0131698368135663, Validation Loss: 0.013229111750233155\n",
      "Epoch [3966/20000], Training Loss: 0.013393939377416635, Validation Loss: 0.007048336457842583\n",
      "Epoch [3967/20000], Training Loss: 0.036006187794425513, Validation Loss: 0.014096884796413323\n",
      "Epoch [3968/20000], Training Loss: 0.030351239389606884, Validation Loss: 0.03280118040317426\n",
      "Epoch [3969/20000], Training Loss: 0.035339837704002876, Validation Loss: 0.1173661494861726\n",
      "Epoch [3970/20000], Training Loss: 0.0674059378714966, Validation Loss: 0.018683768159679086\n",
      "Epoch [3971/20000], Training Loss: 0.01750141741441829, Validation Loss: 0.006417435709529355\n",
      "Epoch [3972/20000], Training Loss: 0.013626023749696157, Validation Loss: 0.0028604098885354656\n",
      "Epoch [3973/20000], Training Loss: 0.009471988276345655, Validation Loss: 0.008010298793503266\n",
      "Epoch [3974/20000], Training Loss: 0.009276508149923757, Validation Loss: 0.007962691609933069\n",
      "Epoch [3975/20000], Training Loss: 0.020450841609480058, Validation Loss: 0.02431531848047947\n",
      "Epoch [3976/20000], Training Loss: 0.019834218613271202, Validation Loss: 0.010096078427348816\n",
      "Epoch [3977/20000], Training Loss: 0.01486612791528127, Validation Loss: 0.005937254778420291\n",
      "Epoch [3978/20000], Training Loss: 0.014105008130173806, Validation Loss: 0.01758805801257574\n",
      "Epoch [3979/20000], Training Loss: 0.02821901965113024, Validation Loss: 0.015027391488889874\n",
      "Epoch [3980/20000], Training Loss: 0.018438933307152183, Validation Loss: 0.004948578417088219\n",
      "Epoch [3981/20000], Training Loss: 0.01686415051205716, Validation Loss: 0.014207131891425749\n",
      "Epoch [3982/20000], Training Loss: 0.02044225344876135, Validation Loss: 0.06167227897565878\n",
      "Epoch [3983/20000], Training Loss: 0.02763884951675775, Validation Loss: 0.021536995567801785\n",
      "Epoch [3984/20000], Training Loss: 0.016981071439139277, Validation Loss: 0.013568912048736285\n",
      "Epoch [3985/20000], Training Loss: 0.014255310284040337, Validation Loss: 0.005511892874598437\n",
      "Epoch [3986/20000], Training Loss: 0.008158369652457103, Validation Loss: 0.009498532363239507\n",
      "Epoch [3987/20000], Training Loss: 0.006584808502728785, Validation Loss: 0.005922499509574459\n",
      "Epoch [3988/20000], Training Loss: 0.013053486658463953, Validation Loss: 0.04584069010266555\n",
      "Epoch [3989/20000], Training Loss: 0.024744227997611494, Validation Loss: 0.007208112098343198\n",
      "Epoch [3990/20000], Training Loss: 0.013694994397935392, Validation Loss: 0.010895192631582321\n",
      "Epoch [3991/20000], Training Loss: 0.008214466524285464, Validation Loss: 0.026596839292292052\n",
      "Epoch [3992/20000], Training Loss: 0.012566302798014866, Validation Loss: 0.0058572107587205515\n",
      "Epoch [3993/20000], Training Loss: 0.0071892516404789475, Validation Loss: 0.00327587825373805\n",
      "Epoch [3994/20000], Training Loss: 0.012144197796130487, Validation Loss: 0.01841112830428639\n",
      "Epoch [3995/20000], Training Loss: 0.04285675542944643, Validation Loss: 0.009677622412891622\n",
      "Epoch [3996/20000], Training Loss: 0.029340745149446384, Validation Loss: 0.018208540104523514\n",
      "Epoch [3997/20000], Training Loss: 0.02037175841230367, Validation Loss: 0.020827566487315492\n",
      "Epoch [3998/20000], Training Loss: 0.025500482813056027, Validation Loss: 0.02158006519174782\n",
      "Epoch [3999/20000], Training Loss: 0.01974835408951289, Validation Loss: 0.008635988352055375\n",
      "Epoch [4000/20000], Training Loss: 0.011676555039311356, Validation Loss: 0.005649808566540346\n",
      "Epoch [4001/20000], Training Loss: 0.0067905320153970805, Validation Loss: 0.0061003747976349654\n",
      "Epoch [4002/20000], Training Loss: 0.008950692873830641, Validation Loss: 0.012498587442266632\n",
      "Epoch [4003/20000], Training Loss: 0.019975430744645046, Validation Loss: 0.020786217652272693\n",
      "Epoch [4004/20000], Training Loss: 0.03305571592708085, Validation Loss: 0.056003133423146174\n",
      "Epoch [4005/20000], Training Loss: 0.05741815602440121, Validation Loss: 0.06027824322072206\n",
      "Epoch [4006/20000], Training Loss: 0.03373726395823594, Validation Loss: 0.01738127221212579\n",
      "Epoch [4007/20000], Training Loss: 0.016195687182646776, Validation Loss: 0.008490690096568979\n",
      "Epoch [4008/20000], Training Loss: 0.02417330338825455, Validation Loss: 0.027927518965176985\n",
      "Epoch [4009/20000], Training Loss: 0.04493855171105159, Validation Loss: 0.01501513311608516\n",
      "Epoch [4010/20000], Training Loss: 0.033002934767864645, Validation Loss: 0.04021568875186167\n",
      "Epoch [4011/20000], Training Loss: 0.02299283375032246, Validation Loss: 0.013851384478454992\n",
      "Epoch [4012/20000], Training Loss: 0.016430330828630497, Validation Loss: 0.008876746814394211\n",
      "Epoch [4013/20000], Training Loss: 0.00829766770974467, Validation Loss: 0.008395063437935057\n",
      "Epoch [4014/20000], Training Loss: 0.01179868814220494, Validation Loss: 0.008349583663885262\n",
      "Epoch [4015/20000], Training Loss: 0.018045373039058177, Validation Loss: 0.0067018197397438984\n",
      "Epoch [4016/20000], Training Loss: 0.017974174034731862, Validation Loss: 0.007921491420022785\n",
      "Epoch [4017/20000], Training Loss: 0.0065491073054632564, Validation Loss: 0.005745617729529736\n",
      "Epoch [4018/20000], Training Loss: 0.005813108978795104, Validation Loss: 0.015638366390234394\n",
      "Epoch [4019/20000], Training Loss: 0.009740469885790455, Validation Loss: 0.008888570841451286\n",
      "Epoch [4020/20000], Training Loss: 0.012109018621101444, Validation Loss: 0.023304320084674916\n",
      "Epoch [4021/20000], Training Loss: 0.014365470025430633, Validation Loss: 0.007894486617782298\n",
      "Epoch [4022/20000], Training Loss: 0.013839940787095526, Validation Loss: 0.00859019144912412\n",
      "Epoch [4023/20000], Training Loss: 0.025925837342323836, Validation Loss: 0.016592255867830693\n",
      "Epoch [4024/20000], Training Loss: 0.012689307830961687, Validation Loss: 0.008592131814111812\n",
      "Epoch [4025/20000], Training Loss: 0.008997646774202752, Validation Loss: 0.006492957396744716\n",
      "Epoch [4026/20000], Training Loss: 0.013993210464832373, Validation Loss: 0.011102276115630211\n",
      "Epoch [4027/20000], Training Loss: 0.01627242211491518, Validation Loss: 0.016364859084422038\n",
      "Epoch [4028/20000], Training Loss: 0.00872602151918857, Validation Loss: 0.004613423870584457\n",
      "Epoch [4029/20000], Training Loss: 0.012480588734823479, Validation Loss: 0.017865959810462434\n",
      "Epoch [4030/20000], Training Loss: 0.007986161912932792, Validation Loss: 0.005826168053918732\n",
      "Epoch [4031/20000], Training Loss: 0.00931193846554379, Validation Loss: 0.012648147366810494\n",
      "Epoch [4032/20000], Training Loss: 0.008296126303321216, Validation Loss: 0.007478353173064635\n",
      "Epoch [4033/20000], Training Loss: 0.02468710762851905, Validation Loss: 0.008907032803084119\n",
      "Epoch [4034/20000], Training Loss: 0.021548021117788658, Validation Loss: 0.009542649573153166\n",
      "Epoch [4035/20000], Training Loss: 0.01502344533946598, Validation Loss: 0.006278673115387366\n",
      "Epoch [4036/20000], Training Loss: 0.01519857237040664, Validation Loss: 0.00495662732297711\n",
      "Epoch [4037/20000], Training Loss: 0.01835818268591538, Validation Loss: 0.0068296016414260505\n",
      "Epoch [4038/20000], Training Loss: 0.0118860345563527, Validation Loss: 0.007021495807517409\n",
      "Epoch [4039/20000], Training Loss: 0.00607554878375335, Validation Loss: 0.006384704108382635\n",
      "Epoch [4040/20000], Training Loss: 0.012025819979628847, Validation Loss: 0.017150212858103014\n",
      "Epoch [4041/20000], Training Loss: 0.012257696227607084, Validation Loss: 0.009394768987443212\n",
      "Epoch [4042/20000], Training Loss: 0.014691738537554297, Validation Loss: 0.011574730577659629\n",
      "Epoch [4043/20000], Training Loss: 0.009986719210108472, Validation Loss: 0.00500197753855706\n",
      "Epoch [4044/20000], Training Loss: 0.008547623349711233, Validation Loss: 0.0039444369485782576\n",
      "Epoch [4045/20000], Training Loss: 0.004931196038212095, Validation Loss: 0.003371102224154552\n",
      "Epoch [4046/20000], Training Loss: 0.005396715695702626, Validation Loss: 0.002602964539646178\n",
      "Epoch [4047/20000], Training Loss: 0.008366973222499447, Validation Loss: 0.01400433239466484\n",
      "Epoch [4048/20000], Training Loss: 0.021835088657098822, Validation Loss: 0.0049536964045162\n",
      "Epoch [4049/20000], Training Loss: 0.008295728075089366, Validation Loss: 0.003576463717454972\n",
      "Epoch [4050/20000], Training Loss: 0.009260017183935685, Validation Loss: 0.018816685098759733\n",
      "Epoch [4051/20000], Training Loss: 0.03197349516475307, Validation Loss: 0.025710626405533184\n",
      "Epoch [4052/20000], Training Loss: 0.016892889148688743, Validation Loss: 0.009990015331174476\n",
      "Epoch [4053/20000], Training Loss: 0.010367872509440141, Validation Loss: 0.009012637036773605\n",
      "Epoch [4054/20000], Training Loss: 0.010291073032671452, Validation Loss: 0.006650321767665452\n",
      "Epoch [4055/20000], Training Loss: 0.00783629585514843, Validation Loss: 0.005584171256097242\n",
      "Epoch [4056/20000], Training Loss: 0.00842338687341128, Validation Loss: 0.006290551247080397\n",
      "Epoch [4057/20000], Training Loss: 0.007787152567678797, Validation Loss: 0.005934890794865818\n",
      "Epoch [4058/20000], Training Loss: 0.005033590621938596, Validation Loss: 0.006116517181875939\n",
      "Epoch [4059/20000], Training Loss: 0.006975629840926472, Validation Loss: 0.008673349042445417\n",
      "Epoch [4060/20000], Training Loss: 0.010458235298106697, Validation Loss: 0.008519016941038769\n",
      "Epoch [4061/20000], Training Loss: 0.009508599067755443, Validation Loss: 0.006652391770309407\n",
      "Epoch [4062/20000], Training Loss: 0.004943033356019961, Validation Loss: 0.004352785549201599\n",
      "Epoch [4063/20000], Training Loss: 0.006789693653835067, Validation Loss: 0.003631606932588381\n",
      "Epoch [4064/20000], Training Loss: 0.008924328549255733, Validation Loss: 0.008778715584605936\n",
      "Epoch [4065/20000], Training Loss: 0.034536407765342246, Validation Loss: 0.07532049076995827\n",
      "Epoch [4066/20000], Training Loss: 0.03155612847334461, Validation Loss: 0.016889385113615724\n",
      "Epoch [4067/20000], Training Loss: 0.018260469354572706, Validation Loss: 0.015217260808153046\n",
      "Epoch [4068/20000], Training Loss: 0.018386871125715385, Validation Loss: 0.013280267380001274\n",
      "Epoch [4069/20000], Training Loss: 0.012332398361260337, Validation Loss: 0.01546091243004152\n",
      "Epoch [4070/20000], Training Loss: 0.015568268634004718, Validation Loss: 0.006456720761694409\n",
      "Epoch [4071/20000], Training Loss: 0.010060661318001556, Validation Loss: 0.00921339033281551\n",
      "Epoch [4072/20000], Training Loss: 0.006406238985489056, Validation Loss: 0.0061278027855483485\n",
      "Epoch [4073/20000], Training Loss: 0.01031350006815046, Validation Loss: 0.00520987324820713\n",
      "Epoch [4074/20000], Training Loss: 0.010438299532065034, Validation Loss: 0.011465529515781725\n",
      "Epoch [4075/20000], Training Loss: 0.006900810642816525, Validation Loss: 0.014090322107429236\n",
      "Epoch [4076/20000], Training Loss: 0.018988590783140222, Validation Loss: 0.005946654705548481\n",
      "Epoch [4077/20000], Training Loss: 0.007778285959424076, Validation Loss: 0.031240995849517006\n",
      "Epoch [4078/20000], Training Loss: 0.009846048760144705, Validation Loss: 0.004700718536321956\n",
      "Epoch [4079/20000], Training Loss: 0.007875821472013596, Validation Loss: 0.016969685130154216\n",
      "Epoch [4080/20000], Training Loss: 0.015791952119282575, Validation Loss: 0.011784592459335259\n",
      "Epoch [4081/20000], Training Loss: 0.02288152341913831, Validation Loss: 0.01639858744711756\n",
      "Epoch [4082/20000], Training Loss: 0.020799613216870023, Validation Loss: 0.008074685810324158\n",
      "Epoch [4083/20000], Training Loss: 0.010226455332096, Validation Loss: 0.006801693384172073\n",
      "Epoch [4084/20000], Training Loss: 0.008959671810901324, Validation Loss: 0.006395218982447659\n",
      "Epoch [4085/20000], Training Loss: 0.02195976816126079, Validation Loss: 0.007027472127512218\n",
      "Epoch [4086/20000], Training Loss: 0.013389705526476194, Validation Loss: 0.00848180338230772\n",
      "Epoch [4087/20000], Training Loss: 0.013695616728260316, Validation Loss: 0.019450620938347946\n",
      "Epoch [4088/20000], Training Loss: 0.0077474593459295905, Validation Loss: 0.005332043267401332\n",
      "Epoch [4089/20000], Training Loss: 0.008109919901471585, Validation Loss: 0.007547841240345069\n",
      "Epoch [4090/20000], Training Loss: 0.008440674931210066, Validation Loss: 0.005254873083723156\n",
      "Epoch [4091/20000], Training Loss: 0.0082261274884721, Validation Loss: 0.005453975800966404\n",
      "Epoch [4092/20000], Training Loss: 0.01292047679557332, Validation Loss: 0.007442073779407294\n",
      "Epoch [4093/20000], Training Loss: 0.0197078996925159, Validation Loss: 0.005819889684135303\n",
      "Epoch [4094/20000], Training Loss: 0.03119541953492444, Validation Loss: 0.011318724294288327\n",
      "Epoch [4095/20000], Training Loss: 0.024665900884428993, Validation Loss: 0.0076418123015693186\n",
      "Epoch [4096/20000], Training Loss: 0.016219110519159585, Validation Loss: 0.007217415490945963\n",
      "Epoch [4097/20000], Training Loss: 0.007172776572718119, Validation Loss: 0.006542627098635226\n",
      "Epoch [4098/20000], Training Loss: 0.008875407133018598, Validation Loss: 0.007332165076344869\n",
      "Epoch [4099/20000], Training Loss: 0.007234807465075781, Validation Loss: 0.00955259682651948\n",
      "Epoch [4100/20000], Training Loss: 0.00968538923189044, Validation Loss: 0.005629038761696847\n",
      "Epoch [4101/20000], Training Loss: 0.0070783422758852665, Validation Loss: 0.008859990111555202\n",
      "Epoch [4102/20000], Training Loss: 0.004139326205144503, Validation Loss: 0.011319175625284774\n",
      "Epoch [4103/20000], Training Loss: 0.01356570893715668, Validation Loss: 0.026716358972992436\n",
      "Epoch [4104/20000], Training Loss: 0.014442666756037528, Validation Loss: 0.01565491385984214\n",
      "Epoch [4105/20000], Training Loss: 0.014876680237869446, Validation Loss: 0.03620614063720901\n",
      "Epoch [4106/20000], Training Loss: 0.03348180764741926, Validation Loss: 0.02651621424174014\n",
      "Epoch [4107/20000], Training Loss: 0.010568794269309885, Validation Loss: 0.025380733564231508\n",
      "Epoch [4108/20000], Training Loss: 0.014214892950673987, Validation Loss: 0.008184797939650577\n",
      "Epoch [4109/20000], Training Loss: 0.007591492538007775, Validation Loss: 0.004344698089750766\n",
      "Epoch [4110/20000], Training Loss: 0.01664057985180989, Validation Loss: 0.010672802779871355\n",
      "Epoch [4111/20000], Training Loss: 0.014434466272697526, Validation Loss: 0.005929271153425881\n",
      "Epoch [4112/20000], Training Loss: 0.022412952777553334, Validation Loss: 0.008121988019801003\n",
      "Epoch [4113/20000], Training Loss: 0.02937509263366727, Validation Loss: 0.013155797072353639\n",
      "Epoch [4114/20000], Training Loss: 0.019595059197018014, Validation Loss: 0.01006026087270584\n",
      "Epoch [4115/20000], Training Loss: 0.012432918006587508, Validation Loss: 0.014280210323187856\n",
      "Epoch [4116/20000], Training Loss: 0.01173257478304939, Validation Loss: 0.006816361663498226\n",
      "Epoch [4117/20000], Training Loss: 0.008429895109397225, Validation Loss: 0.02298756818890979\n",
      "Epoch [4118/20000], Training Loss: 0.008507934415579907, Validation Loss: 0.008290268187741796\n",
      "Epoch [4119/20000], Training Loss: 0.007365503244467878, Validation Loss: 0.006771530657910522\n",
      "Epoch [4120/20000], Training Loss: 0.008945842128014192, Validation Loss: 0.00416858859167795\n",
      "Epoch [4121/20000], Training Loss: 0.006633670705403867, Validation Loss: 0.0036535262993148926\n",
      "Epoch [4122/20000], Training Loss: 0.005823058496210771, Validation Loss: 0.010972158078657859\n",
      "Epoch [4123/20000], Training Loss: 0.007393012721357601, Validation Loss: 0.012564952551151578\n",
      "Epoch [4124/20000], Training Loss: 0.008578914284173931, Validation Loss: 0.005037610931383695\n",
      "Epoch [4125/20000], Training Loss: 0.007942832748994988, Validation Loss: 0.006615378300697979\n",
      "Epoch [4126/20000], Training Loss: 0.008772776721993327, Validation Loss: 0.0030559267892483278\n",
      "Epoch [4127/20000], Training Loss: 0.008087226815405302, Validation Loss: 0.010115715728001564\n",
      "Epoch [4128/20000], Training Loss: 0.014606438506396964, Validation Loss: 0.0031768202321213073\n",
      "Epoch [4129/20000], Training Loss: 0.012071414110063674, Validation Loss: 0.005272157469400034\n",
      "Epoch [4130/20000], Training Loss: 0.01892009905999917, Validation Loss: 0.010975461786861374\n",
      "Epoch [4131/20000], Training Loss: 0.023774672685509098, Validation Loss: 0.030323303523399656\n",
      "Epoch [4132/20000], Training Loss: 0.01709643706064006, Validation Loss: 0.028328222025003975\n",
      "Epoch [4133/20000], Training Loss: 0.01969975945290311, Validation Loss: 0.008967180166929925\n",
      "Epoch [4134/20000], Training Loss: 0.012994536298681478, Validation Loss: 0.014035494043246746\n",
      "Epoch [4135/20000], Training Loss: 0.007643125165486708, Validation Loss: 0.008097616121241507\n",
      "Epoch [4136/20000], Training Loss: 0.014456253781515573, Validation Loss: 0.006472826648027714\n",
      "Epoch [4137/20000], Training Loss: 0.00870467123812497, Validation Loss: 0.00505215395644752\n",
      "Epoch [4138/20000], Training Loss: 0.005020438691644813, Validation Loss: 0.0036876244955042887\n",
      "Epoch [4139/20000], Training Loss: 0.004387198788338408, Validation Loss: 0.010164015088599447\n",
      "Epoch [4140/20000], Training Loss: 0.005909485601867865, Validation Loss: 0.0027021740585106663\n",
      "Epoch [4141/20000], Training Loss: 0.011307368043264075, Validation Loss: 0.006505786950372437\n",
      "Epoch [4142/20000], Training Loss: 0.012040055275438786, Validation Loss: 0.004621021928138751\n",
      "Epoch [4143/20000], Training Loss: 0.01756937666661023, Validation Loss: 0.004954782476488617\n",
      "Epoch [4144/20000], Training Loss: 0.025714902100405976, Validation Loss: 0.009931659263501607\n",
      "Epoch [4145/20000], Training Loss: 0.010296918870697678, Validation Loss: 0.005396472210999751\n",
      "Epoch [4146/20000], Training Loss: 0.010562854103877075, Validation Loss: 0.004300191304699703\n",
      "Epoch [4147/20000], Training Loss: 0.009636892770816172, Validation Loss: 0.00965004585071126\n",
      "Epoch [4148/20000], Training Loss: 0.00817256817494386, Validation Loss: 0.003456237777235011\n",
      "Epoch [4149/20000], Training Loss: 0.007908230508162108, Validation Loss: 0.007069897147890255\n",
      "Epoch [4150/20000], Training Loss: 0.01089008352054017, Validation Loss: 0.006631500321028787\n",
      "Epoch [4151/20000], Training Loss: 0.005850198716091525, Validation Loss: 0.0093609490146004\n",
      "Epoch [4152/20000], Training Loss: 0.014239107817827192, Validation Loss: 0.0262591096468116\n",
      "Epoch [4153/20000], Training Loss: 0.040760455877066955, Validation Loss: 0.017386624431570193\n",
      "Epoch [4154/20000], Training Loss: 0.024848977241033156, Validation Loss: 0.023902663216826596\n",
      "Epoch [4155/20000], Training Loss: 0.0209813299630436, Validation Loss: 0.027854765941031416\n",
      "Epoch [4156/20000], Training Loss: 0.019241393070842605, Validation Loss: 0.017797271046479855\n",
      "Epoch [4157/20000], Training Loss: 0.01599922790358375, Validation Loss: 0.033265639127681164\n",
      "Epoch [4158/20000], Training Loss: 0.017944473498833498, Validation Loss: 0.007955814613004417\n",
      "Epoch [4159/20000], Training Loss: 0.015505263868752601, Validation Loss: 0.019410182029137753\n",
      "Epoch [4160/20000], Training Loss: 0.020752707645962282, Validation Loss: 0.025368373084347695\n",
      "Epoch [4161/20000], Training Loss: 0.010568287998986696, Validation Loss: 0.01558127387917401\n",
      "Epoch [4162/20000], Training Loss: 0.014233540724879796, Validation Loss: 0.010950624916793978\n",
      "Epoch [4163/20000], Training Loss: 0.011917031608131115, Validation Loss: 0.0050489699886903895\n",
      "Epoch [4164/20000], Training Loss: 0.00870638680810641, Validation Loss: 0.004305114966233045\n",
      "Epoch [4165/20000], Training Loss: 0.007603982538317463, Validation Loss: 0.004111924485057118\n",
      "Epoch [4166/20000], Training Loss: 0.005883125027009685, Validation Loss: 0.007493516552423378\n",
      "Epoch [4167/20000], Training Loss: 0.00590196845587343, Validation Loss: 0.005925534468198228\n",
      "Epoch [4168/20000], Training Loss: 0.013048901330746179, Validation Loss: 0.014088906231892549\n",
      "Epoch [4169/20000], Training Loss: 0.007907812739566518, Validation Loss: 0.003281279199135107\n",
      "Epoch [4170/20000], Training Loss: 0.007673820023358401, Validation Loss: 0.0043230459882254635\n",
      "Epoch [4171/20000], Training Loss: 0.0147076447226157, Validation Loss: 0.016603999170066736\n",
      "Epoch [4172/20000], Training Loss: 0.025386200660640106, Validation Loss: 0.02630508627440317\n",
      "Epoch [4173/20000], Training Loss: 0.019511883805015322, Validation Loss: 0.028287837448194457\n",
      "Epoch [4174/20000], Training Loss: 0.028113234576137205, Validation Loss: 0.046619759118751096\n",
      "Epoch [4175/20000], Training Loss: 0.030493986984116157, Validation Loss: 0.021472296180555923\n",
      "Epoch [4176/20000], Training Loss: 0.013645893180052684, Validation Loss: 0.011462643547149907\n",
      "Epoch [4177/20000], Training Loss: 0.012424582016787358, Validation Loss: 0.007801372518120583\n",
      "Epoch [4178/20000], Training Loss: 0.008833445744780224, Validation Loss: 0.007138163181869979\n",
      "Epoch [4179/20000], Training Loss: 0.007464028262932386, Validation Loss: 0.0059283201745919155\n",
      "Epoch [4180/20000], Training Loss: 0.00782706170243078, Validation Loss: 0.008017706560037498\n",
      "Epoch [4181/20000], Training Loss: 0.012077497108423683, Validation Loss: 0.0058976411011307005\n",
      "Epoch [4182/20000], Training Loss: 0.009073094878528667, Validation Loss: 0.006383091075284548\n",
      "Epoch [4183/20000], Training Loss: 0.007410673383024654, Validation Loss: 0.0044188847745577475\n",
      "Epoch [4184/20000], Training Loss: 0.00771373326277366, Validation Loss: 0.007751061185997189\n",
      "Epoch [4185/20000], Training Loss: 0.009473156922467751, Validation Loss: 0.006736958008998174\n",
      "Epoch [4186/20000], Training Loss: 0.004418128395627718, Validation Loss: 0.005773234112459542\n",
      "Epoch [4187/20000], Training Loss: 0.006372408215871214, Validation Loss: 0.004139247600960516\n",
      "Epoch [4188/20000], Training Loss: 0.008065868473051654, Validation Loss: 0.005915902762386135\n",
      "Epoch [4189/20000], Training Loss: 0.011838996894864198, Validation Loss: 0.008545797661489425\n",
      "Epoch [4190/20000], Training Loss: 0.011529695757157501, Validation Loss: 0.004065954187678601\n",
      "Epoch [4191/20000], Training Loss: 0.005956559976246873, Validation Loss: 0.0071513958295911706\n",
      "Epoch [4192/20000], Training Loss: 0.012357265487870401, Validation Loss: 0.004909734530680713\n",
      "Epoch [4193/20000], Training Loss: 0.004803762544075393, Validation Loss: 0.007848846122750988\n",
      "Epoch [4194/20000], Training Loss: 0.009060839064269592, Validation Loss: 0.0022723490312565836\n",
      "Epoch [4195/20000], Training Loss: 0.016132627494499223, Validation Loss: 0.020955539366955885\n",
      "Epoch [4196/20000], Training Loss: 0.022432588760076606, Validation Loss: 0.016448190407887\n",
      "Epoch [4197/20000], Training Loss: 0.023297440385379432, Validation Loss: 0.010638636837419264\n",
      "Epoch [4198/20000], Training Loss: 0.048864026254575164, Validation Loss: 0.007904153544326878\n",
      "Epoch [4199/20000], Training Loss: 0.07081171083596668, Validation Loss: 0.07666345798368164\n",
      "Epoch [4200/20000], Training Loss: 0.06556649971753359, Validation Loss: 0.0686155979478197\n",
      "Epoch [4201/20000], Training Loss: 0.04090620713707592, Validation Loss: 0.028317154653791716\n",
      "Epoch [4202/20000], Training Loss: 0.02157258497235099, Validation Loss: 0.013654202015204524\n",
      "Epoch [4203/20000], Training Loss: 0.014701711679143565, Validation Loss: 0.016823046252070078\n",
      "Epoch [4204/20000], Training Loss: 0.013021825546664851, Validation Loss: 0.010043507838775787\n",
      "Epoch [4205/20000], Training Loss: 0.009278516023186967, Validation Loss: 0.010190959514537619\n",
      "Epoch [4206/20000], Training Loss: 0.012449118312880663, Validation Loss: 0.006821367375924615\n",
      "Epoch [4207/20000], Training Loss: 0.015764224555044035, Validation Loss: 0.007232509068152315\n",
      "Epoch [4208/20000], Training Loss: 0.009457356587518007, Validation Loss: 0.007367645569121554\n",
      "Epoch [4209/20000], Training Loss: 0.014624310622041645, Validation Loss: 0.016259856081148296\n",
      "Epoch [4210/20000], Training Loss: 0.014069711880957974, Validation Loss: 0.01590098884449279\n",
      "Epoch [4211/20000], Training Loss: 0.011004881247312628, Validation Loss: 0.014035006105608245\n",
      "Epoch [4212/20000], Training Loss: 0.008181524113751948, Validation Loss: 0.00648706410143112\n",
      "Epoch [4213/20000], Training Loss: 0.006663157749114491, Validation Loss: 0.005047853911700072\n",
      "Epoch [4214/20000], Training Loss: 0.006000563897292263, Validation Loss: 0.005498804511327242\n",
      "Epoch [4215/20000], Training Loss: 0.006564689151543591, Validation Loss: 0.005974676738584301\n",
      "Epoch [4216/20000], Training Loss: 0.006397807334516463, Validation Loss: 0.0037305927658530485\n",
      "Epoch [4217/20000], Training Loss: 0.00702393967367243, Validation Loss: 0.006841201399080621\n",
      "Epoch [4218/20000], Training Loss: 0.009051243198752803, Validation Loss: 0.0069145809565982165\n",
      "Epoch [4219/20000], Training Loss: 0.01405216957313574, Validation Loss: 0.013270558207416818\n",
      "Epoch [4220/20000], Training Loss: 0.013812847112213993, Validation Loss: 0.00967186993456659\n",
      "Epoch [4221/20000], Training Loss: 0.008779596807601462, Validation Loss: 0.004167133185934598\n",
      "Epoch [4222/20000], Training Loss: 0.005699520916095935, Validation Loss: 0.005163784562441265\n",
      "Epoch [4223/20000], Training Loss: 0.005079867535511896, Validation Loss: 0.0030975467942379203\n",
      "Epoch [4224/20000], Training Loss: 0.005133416470926022, Validation Loss: 0.005260898976145155\n",
      "Epoch [4225/20000], Training Loss: 0.010861349446973432, Validation Loss: 0.00706871517404203\n",
      "Epoch [4226/20000], Training Loss: 0.007427552421375668, Validation Loss: 0.005906995989527835\n",
      "Epoch [4227/20000], Training Loss: 0.019972000824054703, Validation Loss: 0.008689297013868419\n",
      "Epoch [4228/20000], Training Loss: 0.008052544182906527, Validation Loss: 0.004681654447372198\n",
      "Epoch [4229/20000], Training Loss: 0.006782712712135565, Validation Loss: 0.015230163464020539\n",
      "Epoch [4230/20000], Training Loss: 0.009567367171257501, Validation Loss: 0.00633096934882506\n",
      "Epoch [4231/20000], Training Loss: 0.008011983017370636, Validation Loss: 0.004308557142872514\n",
      "Epoch [4232/20000], Training Loss: 0.006243421407426857, Validation Loss: 0.006014346288701566\n",
      "Epoch [4233/20000], Training Loss: 0.01509099874238018, Validation Loss: 0.025410938479472484\n",
      "Epoch [4234/20000], Training Loss: 0.03314858763665792, Validation Loss: 0.04831208294862973\n",
      "Epoch [4235/20000], Training Loss: 0.014292784268036485, Validation Loss: 0.009123713638652069\n",
      "Epoch [4236/20000], Training Loss: 0.010420717496474805, Validation Loss: 0.01654884525714806\n",
      "Epoch [4237/20000], Training Loss: 0.04039940786164412, Validation Loss: 0.018319601198267525\n",
      "Epoch [4238/20000], Training Loss: 0.025894824278241555, Validation Loss: 0.013829337612800632\n",
      "Epoch [4239/20000], Training Loss: 0.013470202069064336, Validation Loss: 0.026014954639582095\n",
      "Epoch [4240/20000], Training Loss: 0.030274285194796642, Validation Loss: 0.012835298303148846\n",
      "Epoch [4241/20000], Training Loss: 0.012396883447133331, Validation Loss: 0.011507624864472226\n",
      "Epoch [4242/20000], Training Loss: 0.00764946620412437, Validation Loss: 0.010911337862775585\n",
      "Epoch [4243/20000], Training Loss: 0.00949469572099458, Validation Loss: 0.007715634567700492\n",
      "Epoch [4244/20000], Training Loss: 0.008558496600016952, Validation Loss: 0.0040907827447621715\n",
      "Epoch [4245/20000], Training Loss: 0.013361671632862584, Validation Loss: 0.008744104736511613\n",
      "Epoch [4246/20000], Training Loss: 0.0075905690734673825, Validation Loss: 0.008803664913904592\n",
      "Epoch [4247/20000], Training Loss: 0.005075175821210516, Validation Loss: 0.0066184342883518965\n",
      "Epoch [4248/20000], Training Loss: 0.004973438932211138, Validation Loss: 0.005893807521009161\n",
      "Epoch [4249/20000], Training Loss: 0.006414964849162581, Validation Loss: 0.014633654829270069\n",
      "Epoch [4250/20000], Training Loss: 0.026899154027757635, Validation Loss: 0.008958104384360257\n",
      "Epoch [4251/20000], Training Loss: 0.026897579564579895, Validation Loss: 0.011193896554427088\n",
      "Epoch [4252/20000], Training Loss: 0.021157902870202503, Validation Loss: 0.0051441441755741835\n",
      "Epoch [4253/20000], Training Loss: 0.006537721916434488, Validation Loss: 0.010434094160184973\n",
      "Epoch [4254/20000], Training Loss: 0.0082668251587685, Validation Loss: 0.005124373557984134\n",
      "Epoch [4255/20000], Training Loss: 0.01007490127813071, Validation Loss: 0.0056796443890212754\n",
      "Epoch [4256/20000], Training Loss: 0.008996615227910556, Validation Loss: 0.004512428690564515\n",
      "Epoch [4257/20000], Training Loss: 0.007445639860634401, Validation Loss: 0.0036113210026006265\n",
      "Epoch [4258/20000], Training Loss: 0.008617643960211094, Validation Loss: 0.004526413491147098\n",
      "Epoch [4259/20000], Training Loss: 0.017520580547300466, Validation Loss: 0.004863590441378359\n",
      "Epoch [4260/20000], Training Loss: 0.013018977113720862, Validation Loss: 0.010464129004474718\n",
      "Epoch [4261/20000], Training Loss: 0.007251505997763681, Validation Loss: 0.003811993899416523\n",
      "Epoch [4262/20000], Training Loss: 0.010745609769531126, Validation Loss: 0.009980400914055767\n",
      "Epoch [4263/20000], Training Loss: 0.007652372433637668, Validation Loss: 0.0029237571402437163\n",
      "Epoch [4264/20000], Training Loss: 0.005349148685387003, Validation Loss: 0.004052034314014463\n",
      "Epoch [4265/20000], Training Loss: 0.021723089490218887, Validation Loss: 0.06319720887898127\n",
      "Epoch [4266/20000], Training Loss: 0.05447994129540997, Validation Loss: 0.022957283853819326\n",
      "Epoch [4267/20000], Training Loss: 0.02721656404901296, Validation Loss: 0.013435479561200035\n",
      "Epoch [4268/20000], Training Loss: 0.01058468995116917, Validation Loss: 0.009004637173347912\n",
      "Epoch [4269/20000], Training Loss: 0.006891037097933658, Validation Loss: 0.012287236186174206\n",
      "Epoch [4270/20000], Training Loss: 0.008445172479176628, Validation Loss: 0.007103477564669447\n",
      "Epoch [4271/20000], Training Loss: 0.00625801512173244, Validation Loss: 0.00419229172211999\n",
      "Epoch [4272/20000], Training Loss: 0.005314617168291339, Validation Loss: 0.012411754972796578\n",
      "Epoch [4273/20000], Training Loss: 0.006411486949738381, Validation Loss: 0.007284500874202341\n",
      "Epoch [4274/20000], Training Loss: 0.018160331424561655, Validation Loss: 0.012722966678536198\n",
      "Epoch [4275/20000], Training Loss: 0.00968408034532331, Validation Loss: 0.013582261041030108\n",
      "Epoch [4276/20000], Training Loss: 0.007757245812432042, Validation Loss: 0.006314873660552881\n",
      "Epoch [4277/20000], Training Loss: 0.013083610602604625, Validation Loss: 0.010226376122359073\n",
      "Epoch [4278/20000], Training Loss: 0.020550925212903946, Validation Loss: 0.015108301241100313\n",
      "Epoch [4279/20000], Training Loss: 0.02255772208575633, Validation Loss: 0.008076532391701871\n",
      "Epoch [4280/20000], Training Loss: 0.011745124273667378, Validation Loss: 0.006125012170481828\n",
      "Epoch [4281/20000], Training Loss: 0.013831369503999926, Validation Loss: 0.005836408348191721\n",
      "Epoch [4282/20000], Training Loss: 0.01394045355846174, Validation Loss: 0.004825717236859158\n",
      "Epoch [4283/20000], Training Loss: 0.011816803790030203, Validation Loss: 0.010962721268892242\n",
      "Epoch [4284/20000], Training Loss: 0.016186384070481705, Validation Loss: 0.016540487600390667\n",
      "Epoch [4285/20000], Training Loss: 0.012407184445432254, Validation Loss: 0.008436951114163094\n",
      "Epoch [4286/20000], Training Loss: 0.006462948218021276, Validation Loss: 0.014932775918672243\n",
      "Epoch [4287/20000], Training Loss: 0.007073335016944579, Validation Loss: 0.004164727307495143\n",
      "Epoch [4288/20000], Training Loss: 0.006288743777466672, Validation Loss: 0.0121771394933603\n",
      "Epoch [4289/20000], Training Loss: 0.0075180419059636605, Validation Loss: 0.005792310781414146\n",
      "Epoch [4290/20000], Training Loss: 0.007276069177480947, Validation Loss: 0.004014301097153553\n",
      "Epoch [4291/20000], Training Loss: 0.008144704410085524, Validation Loss: 0.011111070868342463\n",
      "Epoch [4292/20000], Training Loss: 0.019668408107203765, Validation Loss: 0.07760243439458593\n",
      "Epoch [4293/20000], Training Loss: 0.06974794476978527, Validation Loss: 0.07319856142151236\n",
      "Epoch [4294/20000], Training Loss: 0.044314354907588234, Validation Loss: 0.007222119064425095\n",
      "Epoch [4295/20000], Training Loss: 0.022973996566309194, Validation Loss: 0.021411704087509382\n",
      "Epoch [4296/20000], Training Loss: 0.015203575021587312, Validation Loss: 0.008757511030125897\n",
      "Epoch [4297/20000], Training Loss: 0.008381000719964504, Validation Loss: 0.00711342012203074\n",
      "Epoch [4298/20000], Training Loss: 0.007412116259469518, Validation Loss: 0.005222652574177898\n",
      "Epoch [4299/20000], Training Loss: 0.005376996428822167, Validation Loss: 0.01125395454768961\n",
      "Epoch [4300/20000], Training Loss: 0.01150309322110843, Validation Loss: 0.04180465773903062\n",
      "Epoch [4301/20000], Training Loss: 0.028152696957736874, Validation Loss: 0.02724836182453949\n",
      "Epoch [4302/20000], Training Loss: 0.03194733192503918, Validation Loss: 0.05914903344881671\n",
      "Epoch [4303/20000], Training Loss: 0.02448571567323857, Validation Loss: 0.02736797369269845\n",
      "Epoch [4304/20000], Training Loss: 0.01354444248967671, Validation Loss: 0.007166880216962976\n",
      "Epoch [4305/20000], Training Loss: 0.005708346750387656, Validation Loss: 0.006281731652701897\n",
      "Epoch [4306/20000], Training Loss: 0.008508883283606597, Validation Loss: 0.009111272183352932\n",
      "Epoch [4307/20000], Training Loss: 0.007549427258449474, Validation Loss: 0.005237436761035497\n",
      "Epoch [4308/20000], Training Loss: 0.007690136782392594, Validation Loss: 0.005709617378507963\n",
      "Epoch [4309/20000], Training Loss: 0.01207019026956654, Validation Loss: 0.0070713797085285735\n",
      "Epoch [4310/20000], Training Loss: 0.02005758740233838, Validation Loss: 0.0050385521076350415\n",
      "Epoch [4311/20000], Training Loss: 0.0076600076946696, Validation Loss: 0.006858551082672193\n",
      "Epoch [4312/20000], Training Loss: 0.006231709747610685, Validation Loss: 0.009565167618256558\n",
      "Epoch [4313/20000], Training Loss: 0.012269014189119585, Validation Loss: 0.014198085066043729\n",
      "Epoch [4314/20000], Training Loss: 0.012178707257671053, Validation Loss: 0.004269828899802113\n",
      "Epoch [4315/20000], Training Loss: 0.003592399442693152, Validation Loss: 0.010073518700331923\n",
      "Epoch [4316/20000], Training Loss: 0.00848337953461201, Validation Loss: 0.005213426283195385\n",
      "Epoch [4317/20000], Training Loss: 0.0082718703966488, Validation Loss: 0.022217816663604544\n",
      "Epoch [4318/20000], Training Loss: 0.016614620659960826, Validation Loss: 0.008991983321501655\n",
      "Epoch [4319/20000], Training Loss: 0.007691460241663403, Validation Loss: 0.003553740357135991\n",
      "Epoch [4320/20000], Training Loss: 0.007493702842371671, Validation Loss: 0.006705812011124935\n",
      "Epoch [4321/20000], Training Loss: 0.007013038898419056, Validation Loss: 0.005141880091729588\n",
      "Epoch [4322/20000], Training Loss: 0.008122431584134964, Validation Loss: 0.004679719211513397\n",
      "Epoch [4323/20000], Training Loss: 0.006982135634253999, Validation Loss: 0.0031774834262543144\n",
      "Epoch [4324/20000], Training Loss: 0.004256688937727761, Validation Loss: 0.008103614449695382\n",
      "Epoch [4325/20000], Training Loss: 0.018423811496177223, Validation Loss: 0.0062489330361716255\n",
      "Epoch [4326/20000], Training Loss: 0.011945257104733693, Validation Loss: 0.006809538159131989\n",
      "Epoch [4327/20000], Training Loss: 0.0702094378378076, Validation Loss: 0.02864962419594771\n",
      "Epoch [4328/20000], Training Loss: 0.04504543425615078, Validation Loss: 0.04566581707363661\n",
      "Epoch [4329/20000], Training Loss: 0.02929542355039822, Validation Loss: 0.030950734594877294\n",
      "Epoch [4330/20000], Training Loss: 0.025310837497402514, Validation Loss: 0.010039234731867531\n",
      "Epoch [4331/20000], Training Loss: 0.015088545690689768, Validation Loss: 0.018343323729433387\n",
      "Epoch [4332/20000], Training Loss: 0.012433834357320197, Validation Loss: 0.009299942315854943\n",
      "Epoch [4333/20000], Training Loss: 0.0081273094178219, Validation Loss: 0.011058850011815835\n",
      "Epoch [4334/20000], Training Loss: 0.009354984903308963, Validation Loss: 0.013807932192998125\n",
      "Epoch [4335/20000], Training Loss: 0.013297362264503525, Validation Loss: 0.004436182016279234\n",
      "Epoch [4336/20000], Training Loss: 0.014572059717725747, Validation Loss: 0.004899779443184522\n",
      "Epoch [4337/20000], Training Loss: 0.0068132866986811026, Validation Loss: 0.009962164870265562\n",
      "Epoch [4338/20000], Training Loss: 0.006567809040591653, Validation Loss: 0.005120993153349573\n",
      "Epoch [4339/20000], Training Loss: 0.008102487273780363, Validation Loss: 0.008497169636556683\n",
      "Epoch [4340/20000], Training Loss: 0.009910937377104087, Validation Loss: 0.07088801477762825\n",
      "Epoch [4341/20000], Training Loss: 0.04428245435701683, Validation Loss: 0.034329098787531556\n",
      "Epoch [4342/20000], Training Loss: 0.02634238586428442, Validation Loss: 0.03265018843020925\n",
      "Epoch [4343/20000], Training Loss: 0.013926362134433086, Validation Loss: 0.008673946778701844\n",
      "Epoch [4344/20000], Training Loss: 0.009708249084984086, Validation Loss: 0.010727836115880305\n",
      "Epoch [4345/20000], Training Loss: 0.011231865126839173, Validation Loss: 0.021349097370289893\n",
      "Epoch [4346/20000], Training Loss: 0.012091804898643334, Validation Loss: 0.007134354540669717\n",
      "Epoch [4347/20000], Training Loss: 0.014345984606604491, Validation Loss: 0.008413386337711668\n",
      "Epoch [4348/20000], Training Loss: 0.017640125035987757, Validation Loss: 0.014093213359362469\n",
      "Epoch [4349/20000], Training Loss: 0.01545884569973818, Validation Loss: 0.010406747205330191\n",
      "Epoch [4350/20000], Training Loss: 0.010113557187911024, Validation Loss: 0.006451901828049651\n",
      "Epoch [4351/20000], Training Loss: 0.006338384822795108, Validation Loss: 0.006280021545774973\n",
      "Epoch [4352/20000], Training Loss: 0.009862193746292698, Validation Loss: 0.004952413680769041\n",
      "Epoch [4353/20000], Training Loss: 0.01596563125661175, Validation Loss: 0.07306821180931625\n",
      "Epoch [4354/20000], Training Loss: 0.03322159170472462, Validation Loss: 0.00830381036939798\n",
      "Epoch [4355/20000], Training Loss: 0.008880692251425768, Validation Loss: 0.019894207969684274\n",
      "Epoch [4356/20000], Training Loss: 0.011821705818874761, Validation Loss: 0.008425900783794507\n",
      "Epoch [4357/20000], Training Loss: 0.011176668438045973, Validation Loss: 0.0060533904804235105\n",
      "Epoch [4358/20000], Training Loss: 0.015000101580994851, Validation Loss: 0.014007250851802513\n",
      "Epoch [4359/20000], Training Loss: 0.030983451812624286, Validation Loss: 0.03809284794156805\n",
      "Epoch [4360/20000], Training Loss: 0.04757529099879321, Validation Loss: 0.014058650211414513\n",
      "Epoch [4361/20000], Training Loss: 0.01793860110254692, Validation Loss: 0.00892280098716119\n",
      "Epoch [4362/20000], Training Loss: 0.02021638022935284, Validation Loss: 0.011476878033136127\n",
      "Epoch [4363/20000], Training Loss: 0.017237913262631212, Validation Loss: 0.007565590204779164\n",
      "Epoch [4364/20000], Training Loss: 0.009407401068269141, Validation Loss: 0.006821837884282498\n",
      "Epoch [4365/20000], Training Loss: 0.007498992062340092, Validation Loss: 0.007628320104101608\n",
      "Epoch [4366/20000], Training Loss: 0.007408509554807097, Validation Loss: 0.015647089708425353\n",
      "Epoch [4367/20000], Training Loss: 0.010698172527814418, Validation Loss: 0.022861346926439246\n",
      "Epoch [4368/20000], Training Loss: 0.016869348812698654, Validation Loss: 0.015439024925009497\n",
      "Epoch [4369/20000], Training Loss: 0.024718927268362938, Validation Loss: 0.014582858085174979\n",
      "Epoch [4370/20000], Training Loss: 0.009326333670677351, Validation Loss: 0.0053109859999105015\n",
      "Epoch [4371/20000], Training Loss: 0.007288712151681206, Validation Loss: 0.0073761097282191\n",
      "Epoch [4372/20000], Training Loss: 0.013788830659385505, Validation Loss: 0.014670701231417687\n",
      "Epoch [4373/20000], Training Loss: 0.022141988081524948, Validation Loss: 0.026366177676079263\n",
      "Epoch [4374/20000], Training Loss: 0.01552651816528591, Validation Loss: 0.009944828504582606\n",
      "Epoch [4375/20000], Training Loss: 0.010924751336070975, Validation Loss: 0.0069570676633442675\n",
      "Epoch [4376/20000], Training Loss: 0.012841154767167089, Validation Loss: 0.0071338029296725836\n",
      "Epoch [4377/20000], Training Loss: 0.010601324141524466, Validation Loss: 0.006074878877858022\n",
      "Epoch [4378/20000], Training Loss: 0.007570031568840412, Validation Loss: 0.0119195454595261\n",
      "Epoch [4379/20000], Training Loss: 0.0075974185134068, Validation Loss: 0.005488008833903483\n",
      "Epoch [4380/20000], Training Loss: 0.009491982566292531, Validation Loss: 0.004456754029563724\n",
      "Epoch [4381/20000], Training Loss: 0.010947169228269818, Validation Loss: 0.011374410094731502\n",
      "Epoch [4382/20000], Training Loss: 0.019267149117825153, Validation Loss: 0.008813119068495976\n",
      "Epoch [4383/20000], Training Loss: 0.015197701358244688, Validation Loss: 0.010703944989424079\n",
      "Epoch [4384/20000], Training Loss: 0.029631742306134714, Validation Loss: 0.02558086185953406\n",
      "Epoch [4385/20000], Training Loss: 0.03685390593818738, Validation Loss: 0.06557530372837002\n",
      "Epoch [4386/20000], Training Loss: 0.05283936572128108, Validation Loss: 0.04635919045259556\n",
      "Epoch [4387/20000], Training Loss: 0.03719574326117124, Validation Loss: 0.023131712604640038\n",
      "Epoch [4388/20000], Training Loss: 0.01603824131390346, Validation Loss: 0.011644416685157921\n",
      "Epoch [4389/20000], Training Loss: 0.013108455667471779, Validation Loss: 0.0072383010827772\n",
      "Epoch [4390/20000], Training Loss: 0.008799843107616263, Validation Loss: 0.007501531250227345\n",
      "Epoch [4391/20000], Training Loss: 0.00902365256167416, Validation Loss: 0.007242010129013059\n",
      "Epoch [4392/20000], Training Loss: 0.007966581989551611, Validation Loss: 0.0054003417331906734\n",
      "Epoch [4393/20000], Training Loss: 0.006350482564552554, Validation Loss: 0.006308219414839706\n",
      "Epoch [4394/20000], Training Loss: 0.005797593912575394, Validation Loss: 0.004649627717372239\n",
      "Epoch [4395/20000], Training Loss: 0.004983574617654085, Validation Loss: 0.007816027632832703\n",
      "Epoch [4396/20000], Training Loss: 0.007050291514263206, Validation Loss: 0.004547189940355777\n",
      "Epoch [4397/20000], Training Loss: 0.007555472377654431, Validation Loss: 0.004913083077570134\n",
      "Epoch [4398/20000], Training Loss: 0.006642121601284349, Validation Loss: 0.0050356263625173725\n",
      "Epoch [4399/20000], Training Loss: 0.03343956767691582, Validation Loss: 0.007636913762717816\n",
      "Epoch [4400/20000], Training Loss: 0.0185944478933899, Validation Loss: 0.007099122265838024\n",
      "Epoch [4401/20000], Training Loss: 0.015681841876357794, Validation Loss: 0.004689205770505721\n",
      "Epoch [4402/20000], Training Loss: 0.008820345415317985, Validation Loss: 0.008600735486413871\n",
      "Epoch [4403/20000], Training Loss: 0.015760742587060252, Validation Loss: 0.005718857893469931\n",
      "Epoch [4404/20000], Training Loss: 0.009497670361140211, Validation Loss: 0.009963079601301874\n",
      "Epoch [4405/20000], Training Loss: 0.018619696394515422, Validation Loss: 0.006500398123876039\n",
      "Epoch [4406/20000], Training Loss: 0.013868437613252484, Validation Loss: 0.008061302849301098\n",
      "Epoch [4407/20000], Training Loss: 0.012519148636751456, Validation Loss: 0.005850266862050242\n",
      "Epoch [4408/20000], Training Loss: 0.00785891077040495, Validation Loss: 0.01604764567824546\n",
      "Epoch [4409/20000], Training Loss: 0.008747326530283317, Validation Loss: 0.009390675847828442\n",
      "Epoch [4410/20000], Training Loss: 0.005700789452280982, Validation Loss: 0.007007636209302559\n",
      "Epoch [4411/20000], Training Loss: 0.005481291921569209, Validation Loss: 0.009810239139588768\n",
      "Epoch [4412/20000], Training Loss: 0.007169777016055637, Validation Loss: 0.007439271264014684\n",
      "Epoch [4413/20000], Training Loss: 0.0166273542633592, Validation Loss: 0.0025686233589828434\n",
      "Epoch [4414/20000], Training Loss: 0.011059882281415412, Validation Loss: 0.021696799054211482\n",
      "Epoch [4415/20000], Training Loss: 0.01885835520287011, Validation Loss: 0.016668222393304923\n",
      "Epoch [4416/20000], Training Loss: 0.01586675359534898, Validation Loss: 0.021865015322011176\n",
      "Epoch [4417/20000], Training Loss: 0.015772622936180727, Validation Loss: 0.04670002497733424\n",
      "Epoch [4418/20000], Training Loss: 0.014363738392213625, Validation Loss: 0.0043917291054447515\n",
      "Epoch [4419/20000], Training Loss: 0.008344238970104405, Validation Loss: 0.004643763298231208\n",
      "Epoch [4420/20000], Training Loss: 0.0049125510013254825, Validation Loss: 0.005964618930717995\n",
      "Epoch [4421/20000], Training Loss: 0.0053882841228707025, Validation Loss: 0.004672080577610213\n",
      "Epoch [4422/20000], Training Loss: 0.006117922114624109, Validation Loss: 0.013152332456050837\n",
      "Epoch [4423/20000], Training Loss: 0.011862254953095024, Validation Loss: 0.004757762527396078\n",
      "Epoch [4424/20000], Training Loss: 0.016731228096302533, Validation Loss: 0.004268975411546297\n",
      "Epoch [4425/20000], Training Loss: 0.012818595075389436, Validation Loss: 0.009450215102415425\n",
      "Epoch [4426/20000], Training Loss: 0.006969354488059513, Validation Loss: 0.004538253789003299\n",
      "Epoch [4427/20000], Training Loss: 0.0036188421363476664, Validation Loss: 0.0026804334284597736\n",
      "Epoch [4428/20000], Training Loss: 0.007095089436396458, Validation Loss: 0.025108086616153207\n",
      "Epoch [4429/20000], Training Loss: 0.01653972063484486, Validation Loss: 0.03708632470215483\n",
      "Epoch [4430/20000], Training Loss: 0.038841896486701444, Validation Loss: 0.010581011733652954\n",
      "Epoch [4431/20000], Training Loss: 0.009150004661933053, Validation Loss: 0.013694846315001888\n",
      "Epoch [4432/20000], Training Loss: 0.007570872736096135, Validation Loss: 0.006022199583307598\n",
      "Epoch [4433/20000], Training Loss: 0.00800682440032168, Validation Loss: 0.0060026097286942\n",
      "Epoch [4434/20000], Training Loss: 0.010669971338107384, Validation Loss: 0.013564691808688625\n",
      "Epoch [4435/20000], Training Loss: 0.011038436353763765, Validation Loss: 0.005974243172274288\n",
      "Epoch [4436/20000], Training Loss: 0.00906642762960733, Validation Loss: 0.0078956648135947\n",
      "Epoch [4437/20000], Training Loss: 0.005144531123473176, Validation Loss: 0.003025283681060666\n",
      "Epoch [4438/20000], Training Loss: 0.01476693710355903, Validation Loss: 0.007225769485563559\n",
      "Epoch [4439/20000], Training Loss: 0.018099854118190706, Validation Loss: 0.005435519291592884\n",
      "Epoch [4440/20000], Training Loss: 0.015742514486191794, Validation Loss: 0.0035969712333019693\n",
      "Epoch [4441/20000], Training Loss: 0.01657118773333163, Validation Loss: 0.01915225140131887\n",
      "Epoch [4442/20000], Training Loss: 0.023703430727826862, Validation Loss: 0.01639060930665437\n",
      "Epoch [4443/20000], Training Loss: 0.028323702578615797, Validation Loss: 0.01710104151314949\n",
      "Epoch [4444/20000], Training Loss: 0.0146050469567334, Validation Loss: 0.009015370893235643\n",
      "Epoch [4445/20000], Training Loss: 0.009041246276215784, Validation Loss: 0.02263367408821607\n",
      "Epoch [4446/20000], Training Loss: 0.016660043009324, Validation Loss: 0.009577964283104836\n",
      "Epoch [4447/20000], Training Loss: 0.011126343971617254, Validation Loss: 0.005372764973458288\n",
      "Epoch [4448/20000], Training Loss: 0.00840574870900517, Validation Loss: 0.012122246323445258\n",
      "Epoch [4449/20000], Training Loss: 0.011281738211567114, Validation Loss: 0.01823365604768899\n",
      "Epoch [4450/20000], Training Loss: 0.01602765434446545, Validation Loss: 0.009784343563699473\n",
      "Epoch [4451/20000], Training Loss: 0.06152453434957091, Validation Loss: 0.0129799197541394\n",
      "Epoch [4452/20000], Training Loss: 0.04619570419059268, Validation Loss: 0.01095387917088481\n",
      "Epoch [4453/20000], Training Loss: 0.013975340827268415, Validation Loss: 0.006925979615262893\n",
      "Epoch [4454/20000], Training Loss: 0.006968099524133972, Validation Loss: 0.007566677549371392\n",
      "Epoch [4455/20000], Training Loss: 0.006048737384844571, Validation Loss: 0.006404679649060877\n",
      "Epoch [4456/20000], Training Loss: 0.007965779340338486, Validation Loss: 0.003604094933637543\n",
      "Epoch [4457/20000], Training Loss: 0.02826683043947144, Validation Loss: 0.005509523744292581\n",
      "Epoch [4458/20000], Training Loss: 0.02420799884878631, Validation Loss: 0.008132157658440001\n",
      "Epoch [4459/20000], Training Loss: 0.01278483315504023, Validation Loss: 0.005334359750200247\n",
      "Epoch [4460/20000], Training Loss: 0.008255237705140774, Validation Loss: 0.0054587693145748606\n",
      "Epoch [4461/20000], Training Loss: 0.009946492483972438, Validation Loss: 0.004934298519662002\n",
      "Epoch [4462/20000], Training Loss: 0.006537854339382777, Validation Loss: 0.00362949739506547\n",
      "Epoch [4463/20000], Training Loss: 0.004329628388014888, Validation Loss: 0.009549355808636457\n",
      "Epoch [4464/20000], Training Loss: 0.007163424910881336, Validation Loss: 0.007462559581199457\n",
      "Epoch [4465/20000], Training Loss: 0.0069058196009531714, Validation Loss: 0.006273508113996026\n",
      "Epoch [4466/20000], Training Loss: 0.005621908932750687, Validation Loss: 0.016074126993085462\n",
      "Epoch [4467/20000], Training Loss: 0.006596157214517007, Validation Loss: 0.032209247035449175\n",
      "Epoch [4468/20000], Training Loss: 0.04075470839974774, Validation Loss: 0.028444931459668105\n",
      "Epoch [4469/20000], Training Loss: 0.024202600744112197, Validation Loss: 0.014057752297304696\n",
      "Epoch [4470/20000], Training Loss: 0.016710272965221002, Validation Loss: 0.024694113345033013\n",
      "Epoch [4471/20000], Training Loss: 0.014022096960047747, Validation Loss: 0.03293724176736497\n",
      "Epoch [4472/20000], Training Loss: 0.018182704998512884, Validation Loss: 0.006607480109388869\n",
      "Epoch [4473/20000], Training Loss: 0.013978079116023017, Validation Loss: 0.0068628939387575276\n",
      "Epoch [4474/20000], Training Loss: 0.007129428993461521, Validation Loss: 0.005621501543893359\n",
      "Epoch [4475/20000], Training Loss: 0.008135886316137788, Validation Loss: 0.004515559639035678\n",
      "Epoch [4476/20000], Training Loss: 0.009011448930583097, Validation Loss: 0.004677064307705746\n",
      "Epoch [4477/20000], Training Loss: 0.005852026330103399, Validation Loss: 0.006872246251451284\n",
      "Epoch [4478/20000], Training Loss: 0.01982857907257442, Validation Loss: 0.005514174960028738\n",
      "Epoch [4479/20000], Training Loss: 0.019582569121017253, Validation Loss: 0.0110371091285251\n",
      "Epoch [4480/20000], Training Loss: 0.007081648623820261, Validation Loss: 0.022735109354860988\n",
      "Epoch [4481/20000], Training Loss: 0.02030877986024799, Validation Loss: 0.017997269863631767\n",
      "Epoch [4482/20000], Training Loss: 0.013749964557064231, Validation Loss: 0.008863038084980093\n",
      "Epoch [4483/20000], Training Loss: 0.04058772439566383, Validation Loss: 0.04164853912412323\n",
      "Epoch [4484/20000], Training Loss: 0.048346864425444176, Validation Loss: 0.012006575617273823\n",
      "Epoch [4485/20000], Training Loss: 0.012636029510758817, Validation Loss: 0.019443367686124646\n",
      "Epoch [4486/20000], Training Loss: 0.012409908083749801, Validation Loss: 0.011040046463287065\n",
      "Epoch [4487/20000], Training Loss: 0.00947291317528912, Validation Loss: 0.006740822184448396\n",
      "Epoch [4488/20000], Training Loss: 0.009702578303404152, Validation Loss: 0.006535469281564994\n",
      "Epoch [4489/20000], Training Loss: 0.008235307684766926, Validation Loss: 0.004967560306909036\n",
      "Epoch [4490/20000], Training Loss: 0.00833572685119829, Validation Loss: 0.00838695117869191\n",
      "Epoch [4491/20000], Training Loss: 0.006544510065785809, Validation Loss: 0.0049762283248711014\n",
      "Epoch [4492/20000], Training Loss: 0.005906806610125516, Validation Loss: 0.005991674958829256\n",
      "Epoch [4493/20000], Training Loss: 0.009188312737884579, Validation Loss: 0.009308283392707855\n",
      "Epoch [4494/20000], Training Loss: 0.011587661470652424, Validation Loss: 0.003967688357371062\n",
      "Epoch [4495/20000], Training Loss: 0.021742518646143644, Validation Loss: 0.03387941952297963\n",
      "Epoch [4496/20000], Training Loss: 0.041605066823519464, Validation Loss: 0.06810679948319733\n",
      "Epoch [4497/20000], Training Loss: 0.04245044858128365, Validation Loss: 0.0264624999531933\n",
      "Epoch [4498/20000], Training Loss: 0.01735277335891234, Validation Loss: 0.03152634493699027\n",
      "Epoch [4499/20000], Training Loss: 0.01822033662548555, Validation Loss: 0.012167748848862214\n",
      "Epoch [4500/20000], Training Loss: 0.013467390755457538, Validation Loss: 0.008902353920628646\n",
      "Epoch [4501/20000], Training Loss: 0.012337130764665614, Validation Loss: 0.008165057125943103\n",
      "Epoch [4502/20000], Training Loss: 0.008802682563587691, Validation Loss: 0.014863537532554427\n",
      "Epoch [4503/20000], Training Loss: 0.009491203471822831, Validation Loss: 0.00669308876734119\n",
      "Epoch [4504/20000], Training Loss: 0.006453275389503688, Validation Loss: 0.006156863247659723\n",
      "Epoch [4505/20000], Training Loss: 0.006991662322044638, Validation Loss: 0.010442051831741342\n",
      "Epoch [4506/20000], Training Loss: 0.009231113849507113, Validation Loss: 0.014236265564834736\n",
      "Epoch [4507/20000], Training Loss: 0.010013606045894059, Validation Loss: 0.004556606864103482\n",
      "Epoch [4508/20000], Training Loss: 0.008726966687910525, Validation Loss: 0.011437796058633662\n",
      "Epoch [4509/20000], Training Loss: 0.010053802011368265, Validation Loss: 0.004892177977126266\n",
      "Epoch [4510/20000], Training Loss: 0.01016835432743132, Validation Loss: 0.008599036888358771\n",
      "Epoch [4511/20000], Training Loss: 0.0071269786234812015, Validation Loss: 0.006223091438105907\n",
      "Epoch [4512/20000], Training Loss: 0.006703277540509589, Validation Loss: 0.007735918188440597\n",
      "Epoch [4513/20000], Training Loss: 0.009452648053411394, Validation Loss: 0.005028720966233128\n",
      "Epoch [4514/20000], Training Loss: 0.01372619326243044, Validation Loss: 0.003864250901610181\n",
      "Epoch [4515/20000], Training Loss: 0.009976919068971515, Validation Loss: 0.004884267963071969\n",
      "Epoch [4516/20000], Training Loss: 0.008002459564262867, Validation Loss: 0.018105362026269795\n",
      "Epoch [4517/20000], Training Loss: 0.011715352564351633, Validation Loss: 0.019186043106831696\n",
      "Epoch [4518/20000], Training Loss: 0.01524673752049109, Validation Loss: 0.017698284026293747\n",
      "Epoch [4519/20000], Training Loss: 0.013780028989588442, Validation Loss: 0.013512221125344987\n",
      "Epoch [4520/20000], Training Loss: 0.00523050880084546, Validation Loss: 0.00907675578900832\n",
      "Epoch [4521/20000], Training Loss: 0.008309608626145095, Validation Loss: 0.00734323239359063\n",
      "Epoch [4522/20000], Training Loss: 0.004933188500997078, Validation Loss: 0.01789570385633172\n",
      "Epoch [4523/20000], Training Loss: 0.008742147649343224, Validation Loss: 0.01181599952428566\n",
      "Epoch [4524/20000], Training Loss: 0.01964310352715464, Validation Loss: 0.010415833050678154\n",
      "Epoch [4525/20000], Training Loss: 0.013260629930430565, Validation Loss: 0.0052430611481863055\n",
      "Epoch [4526/20000], Training Loss: 0.008308006684379572, Validation Loss: 0.003420693647854023\n",
      "Epoch [4527/20000], Training Loss: 0.011594150187322936, Validation Loss: 0.00526701346052505\n",
      "Epoch [4528/20000], Training Loss: 0.01005256106561449, Validation Loss: 0.003776107686137818\n",
      "Epoch [4529/20000], Training Loss: 0.009386215055676992, Validation Loss: 0.007761128899281695\n",
      "Epoch [4530/20000], Training Loss: 0.0072089368602194425, Validation Loss: 0.010164787164995914\n",
      "Epoch [4531/20000], Training Loss: 0.01086845955744918, Validation Loss: 0.014694459755991167\n",
      "Epoch [4532/20000], Training Loss: 0.009441504658233109, Validation Loss: 0.014060411824812848\n",
      "Epoch [4533/20000], Training Loss: 0.016457240483889888, Validation Loss: 0.005577919093630003\n",
      "Epoch [4534/20000], Training Loss: 0.010368679186545446, Validation Loss: 0.004835006726474538\n",
      "Epoch [4535/20000], Training Loss: 0.015530642417226253, Validation Loss: 0.006514427585179712\n",
      "Epoch [4536/20000], Training Loss: 0.004720749790846769, Validation Loss: 0.00386316129575042\n",
      "Epoch [4537/20000], Training Loss: 0.007761782627702425, Validation Loss: 0.009453828497355639\n",
      "Epoch [4538/20000], Training Loss: 0.007321560031933976, Validation Loss: 0.0034942540254405685\n",
      "Epoch [4539/20000], Training Loss: 0.009586705784645997, Validation Loss: 0.004515735126866665\n",
      "Epoch [4540/20000], Training Loss: 0.010331981965074582, Validation Loss: 0.04066397090099659\n",
      "Epoch [4541/20000], Training Loss: 0.015158562773389608, Validation Loss: 0.00978795395145988\n",
      "Epoch [4542/20000], Training Loss: 0.007964338903548196, Validation Loss: 0.005745324873640101\n",
      "Epoch [4543/20000], Training Loss: 0.012222504716711617, Validation Loss: 0.0080604646923982\n",
      "Epoch [4544/20000], Training Loss: 0.014328016232086935, Validation Loss: 0.004713679071755905\n",
      "Epoch [4545/20000], Training Loss: 0.01182487179747633, Validation Loss: 0.007314332581466536\n",
      "Epoch [4546/20000], Training Loss: 0.013971426803306843, Validation Loss: 0.010535807418494057\n",
      "Epoch [4547/20000], Training Loss: 0.021760221506996556, Validation Loss: 0.0192489580059315\n",
      "Epoch [4548/20000], Training Loss: 0.03904058400283767, Validation Loss: 0.03852602933870312\n",
      "Epoch [4549/20000], Training Loss: 0.019994019410140545, Validation Loss: 0.03342507088382263\n",
      "Epoch [4550/20000], Training Loss: 0.015004013170255348, Validation Loss: 0.034654173776548954\n",
      "Epoch [4551/20000], Training Loss: 0.04074808078179818, Validation Loss: 0.015414548336203942\n",
      "Epoch [4552/20000], Training Loss: 0.03850938026360901, Validation Loss: 0.013810254889124605\n",
      "Epoch [4553/20000], Training Loss: 0.022375604283297434, Validation Loss: 0.04067958489408241\n",
      "Epoch [4554/20000], Training Loss: 0.02318849735560694, Validation Loss: 0.010177042173381778\n",
      "Epoch [4555/20000], Training Loss: 0.012576920174719166, Validation Loss: 0.018361376635896574\n",
      "Epoch [4556/20000], Training Loss: 0.014214051055854984, Validation Loss: 0.0067606294990625315\n",
      "Epoch [4557/20000], Training Loss: 0.006926019663556612, Validation Loss: 0.0044308839109378695\n",
      "Epoch [4558/20000], Training Loss: 0.012580545618139436, Validation Loss: 0.007332722659829903\n",
      "Epoch [4559/20000], Training Loss: 0.012928293365153618, Validation Loss: 0.006596053941180584\n",
      "Epoch [4560/20000], Training Loss: 0.0696305736616653, Validation Loss: 0.0961913695299107\n",
      "Epoch [4561/20000], Training Loss: 0.060003058970323764, Validation Loss: 0.04475430559250536\n",
      "Epoch [4562/20000], Training Loss: 0.021345729517634027, Validation Loss: 0.015913721104344906\n",
      "Epoch [4563/20000], Training Loss: 0.01336985481404034, Validation Loss: 0.012297049960768826\n",
      "Epoch [4564/20000], Training Loss: 0.006098059332413998, Validation Loss: 0.0017470171941179394\n",
      "Epoch [4565/20000], Training Loss: 0.05576654592031056, Validation Loss: 0.025609464829488258\n",
      "Epoch [4566/20000], Training Loss: 0.07116703727349107, Validation Loss: 0.03875828687077506\n",
      "Epoch [4567/20000], Training Loss: 0.023138671161307554, Validation Loss: 0.010613905173300547\n",
      "Epoch [4568/20000], Training Loss: 0.009640889929739518, Validation Loss: 0.010940011302724378\n",
      "Epoch [4569/20000], Training Loss: 0.04614751604718289, Validation Loss: 0.024535169560404385\n",
      "Epoch [4570/20000], Training Loss: 0.044215024931223264, Validation Loss: 0.03901444565727884\n",
      "Epoch [4571/20000], Training Loss: 0.02242349480677928, Validation Loss: 0.014506062115963005\n",
      "Epoch [4572/20000], Training Loss: 0.01741713165704693, Validation Loss: 0.008407902059731293\n",
      "Epoch [4573/20000], Training Loss: 0.00994660563758641, Validation Loss: 0.005654706083670918\n",
      "Epoch [4574/20000], Training Loss: 0.009734670091347652, Validation Loss: 0.010575977517800915\n",
      "Epoch [4575/20000], Training Loss: 0.012272197422654634, Validation Loss: 0.006828871424858108\n",
      "Epoch [4576/20000], Training Loss: 0.012647984653345443, Validation Loss: 0.0034409162446450902\n",
      "Epoch [4577/20000], Training Loss: 0.02022943307331713, Validation Loss: 0.006427371156853468\n",
      "Epoch [4578/20000], Training Loss: 0.010346943450193586, Validation Loss: 0.010372189348864991\n",
      "Epoch [4579/20000], Training Loss: 0.009314478467526246, Validation Loss: 0.00781147630605515\n",
      "Epoch [4580/20000], Training Loss: 0.011452684140489769, Validation Loss: 0.016794516199947145\n",
      "Epoch [4581/20000], Training Loss: 0.014389018474113462, Validation Loss: 0.004774732594650505\n",
      "Epoch [4582/20000], Training Loss: 0.010302664662179137, Validation Loss: 0.016766708834296665\n",
      "Epoch [4583/20000], Training Loss: 0.011942570147636746, Validation Loss: 0.00522372136513079\n",
      "Epoch [4584/20000], Training Loss: 0.005889043299248442, Validation Loss: 0.006021553448874788\n",
      "Epoch [4585/20000], Training Loss: 0.010039692102249578, Validation Loss: 0.005537717373930005\n",
      "Epoch [4586/20000], Training Loss: 0.01093024877627613, Validation Loss: 0.010086933790431272\n",
      "Epoch [4587/20000], Training Loss: 0.020236218298772916, Validation Loss: 0.005930096053205096\n",
      "Epoch [4588/20000], Training Loss: 0.03181531829094248, Validation Loss: 0.010568287887443724\n",
      "Epoch [4589/20000], Training Loss: 0.02744357125733846, Validation Loss: 0.051127663027728656\n",
      "Epoch [4590/20000], Training Loss: 0.04256001710226493, Validation Loss: 0.021826105169440668\n",
      "Epoch [4591/20000], Training Loss: 0.017268293015409393, Validation Loss: 0.014377705925398932\n",
      "Epoch [4592/20000], Training Loss: 0.011423597744266902, Validation Loss: 0.009070171196072183\n",
      "Epoch [4593/20000], Training Loss: 0.00932067893778107, Validation Loss: 0.008457017014474606\n",
      "Epoch [4594/20000], Training Loss: 0.007812612270104833, Validation Loss: 0.005494287431832033\n",
      "Epoch [4595/20000], Training Loss: 0.006281794454480405, Validation Loss: 0.006155863940086144\n",
      "Epoch [4596/20000], Training Loss: 0.005794937468765836, Validation Loss: 0.009462122235658186\n",
      "Epoch [4597/20000], Training Loss: 0.009149301104802785, Validation Loss: 0.011740331232383685\n",
      "Epoch [4598/20000], Training Loss: 0.008673301808552683, Validation Loss: 0.00518291940331326\n",
      "Epoch [4599/20000], Training Loss: 0.007313226433585182, Validation Loss: 0.013119027818051117\n",
      "Epoch [4600/20000], Training Loss: 0.010389365041451779, Validation Loss: 0.007757083180963411\n",
      "Epoch [4601/20000], Training Loss: 0.02321783386975897, Validation Loss: 0.05153890384023626\n",
      "Epoch [4602/20000], Training Loss: 0.020281346606290235, Validation Loss: 0.022504966861130692\n",
      "Epoch [4603/20000], Training Loss: 0.01838726413448707, Validation Loss: 0.00853017950979559\n",
      "Epoch [4604/20000], Training Loss: 0.01602611072511146, Validation Loss: 0.013015650370236045\n",
      "Epoch [4605/20000], Training Loss: 0.008078916827798821, Validation Loss: 0.006112124794464689\n",
      "Epoch [4606/20000], Training Loss: 0.0081561025415015, Validation Loss: 0.004704192874545932\n",
      "Epoch [4607/20000], Training Loss: 0.009242695513031711, Validation Loss: 0.007394795168620151\n",
      "Epoch [4608/20000], Training Loss: 0.007948650936928712, Validation Loss: 0.004180918838084072\n",
      "Epoch [4609/20000], Training Loss: 0.008917301980545744, Validation Loss: 0.005901776840092314\n",
      "Epoch [4610/20000], Training Loss: 0.00793319358698292, Validation Loss: 0.014996347153987679\n",
      "Epoch [4611/20000], Training Loss: 0.010912552711878172, Validation Loss: 0.0030546163130359565\n",
      "Epoch [4612/20000], Training Loss: 0.00973253244384458, Validation Loss: 0.00561336946391786\n",
      "Epoch [4613/20000], Training Loss: 0.015445232811561627, Validation Loss: 0.015531479775832977\n",
      "Epoch [4614/20000], Training Loss: 0.030987710068334957, Validation Loss: 0.01227910346897281\n",
      "Epoch [4615/20000], Training Loss: 0.014725207517455732, Validation Loss: 0.005266189932756661\n",
      "Epoch [4616/20000], Training Loss: 0.00935069688629093, Validation Loss: 0.0077164362892615055\n",
      "Epoch [4617/20000], Training Loss: 0.0070865616996473235, Validation Loss: 0.009656628450665648\n",
      "Epoch [4618/20000], Training Loss: 0.0055505988587226185, Validation Loss: 0.004109656464476886\n",
      "Epoch [4619/20000], Training Loss: 0.005759748503416111, Validation Loss: 0.0035217540572684163\n",
      "Epoch [4620/20000], Training Loss: 0.00815189911476669, Validation Loss: 0.011356290414652168\n",
      "Epoch [4621/20000], Training Loss: 0.010899579517042315, Validation Loss: 0.006537786369760783\n",
      "Epoch [4622/20000], Training Loss: 0.009454548318379758, Validation Loss: 0.007333244193250036\n",
      "Epoch [4623/20000], Training Loss: 0.009262314489335819, Validation Loss: 0.018725602400918157\n",
      "Epoch [4624/20000], Training Loss: 0.008035662375342716, Validation Loss: 0.006661357919099942\n",
      "Epoch [4625/20000], Training Loss: 0.007886139248488864, Validation Loss: 0.018170599477831013\n",
      "Epoch [4626/20000], Training Loss: 0.008448511561157699, Validation Loss: 0.0062993516313963405\n",
      "Epoch [4627/20000], Training Loss: 0.005903339433383995, Validation Loss: 0.009063518486259739\n",
      "Epoch [4628/20000], Training Loss: 0.009481890808923968, Validation Loss: 0.0031537663339894655\n",
      "Epoch [4629/20000], Training Loss: 0.010894451814237982, Validation Loss: 0.013163493503896864\n",
      "Epoch [4630/20000], Training Loss: 0.024663760464006503, Validation Loss: 0.015556597806183094\n",
      "Epoch [4631/20000], Training Loss: 0.01820851763477549, Validation Loss: 0.011406980808746121\n",
      "Epoch [4632/20000], Training Loss: 0.021513619472638572, Validation Loss: 0.07986727044347308\n",
      "Epoch [4633/20000], Training Loss: 0.02976620468897246, Validation Loss: 0.04677547811372408\n",
      "Epoch [4634/20000], Training Loss: 0.022070914335407515, Validation Loss: 0.016327663021261995\n",
      "Epoch [4635/20000], Training Loss: 0.017549172908599888, Validation Loss: 0.01036339704712321\n",
      "Epoch [4636/20000], Training Loss: 0.012392430840658821, Validation Loss: 0.007382823773826074\n",
      "Epoch [4637/20000], Training Loss: 0.011244212374939317, Validation Loss: 0.00854759292782806\n",
      "Epoch [4638/20000], Training Loss: 0.010515369919760684, Validation Loss: 0.0037555774523363133\n",
      "Epoch [4639/20000], Training Loss: 0.005941521242285879, Validation Loss: 0.006508091346404399\n",
      "Epoch [4640/20000], Training Loss: 0.006327487883806627, Validation Loss: 0.004556051419204518\n",
      "Epoch [4641/20000], Training Loss: 0.006905494667340203, Validation Loss: 0.00581968271408024\n",
      "Epoch [4642/20000], Training Loss: 0.0201663964825067, Validation Loss: 0.030203649777177186\n",
      "Epoch [4643/20000], Training Loss: 0.009634179433175762, Validation Loss: 0.009658585864339589\n",
      "Epoch [4644/20000], Training Loss: 0.009215068217599764, Validation Loss: 0.015262128435499076\n",
      "Epoch [4645/20000], Training Loss: 0.03701457752945966, Validation Loss: 0.006218346075261901\n",
      "Epoch [4646/20000], Training Loss: 0.05158543005485886, Validation Loss: 0.034200775211713705\n",
      "Epoch [4647/20000], Training Loss: 0.020123258532424058, Validation Loss: 0.012655245780479187\n",
      "Epoch [4648/20000], Training Loss: 0.01609870680009148, Validation Loss: 0.011140401665865543\n",
      "Epoch [4649/20000], Training Loss: 0.008905310150501984, Validation Loss: 0.008547860624925565\n",
      "Epoch [4650/20000], Training Loss: 0.010347278601589746, Validation Loss: 0.006218743842509866\n",
      "Epoch [4651/20000], Training Loss: 0.016266890252674266, Validation Loss: 0.024065756723660212\n",
      "Epoch [4652/20000], Training Loss: 0.011116281606226559, Validation Loss: 0.007259667037430972\n",
      "Epoch [4653/20000], Training Loss: 0.012357877791925733, Validation Loss: 0.006928469099816782\n",
      "Epoch [4654/20000], Training Loss: 0.01109200926813563, Validation Loss: 0.011165708450399896\n",
      "Epoch [4655/20000], Training Loss: 0.011630602970918906, Validation Loss: 0.00549719618446\n",
      "Epoch [4656/20000], Training Loss: 0.005782898698401239, Validation Loss: 0.009155317476249713\n",
      "Epoch [4657/20000], Training Loss: 0.004614165354886219, Validation Loss: 0.01769351041210553\n",
      "Epoch [4658/20000], Training Loss: 0.009586829277265809, Validation Loss: 0.0035444628696814627\n",
      "Epoch [4659/20000], Training Loss: 0.008284309768766564, Validation Loss: 0.011843474848235207\n",
      "Epoch [4660/20000], Training Loss: 0.023808047437341884, Validation Loss: 0.04092529416204146\n",
      "Epoch [4661/20000], Training Loss: 0.0179388320275718, Validation Loss: 0.032187313583290744\n",
      "Epoch [4662/20000], Training Loss: 0.01930653237754346, Validation Loss: 0.008059118678959618\n",
      "Epoch [4663/20000], Training Loss: 0.009015864276859378, Validation Loss: 0.008003818421847038\n",
      "Epoch [4664/20000], Training Loss: 0.00875554874905252, Validation Loss: 0.0069830961414092075\n",
      "Epoch [4665/20000], Training Loss: 0.011057506847594465, Validation Loss: 0.005688317443757894\n",
      "Epoch [4666/20000], Training Loss: 0.006879789872357378, Validation Loss: 0.008246848986146915\n",
      "Epoch [4667/20000], Training Loss: 0.0064662670754062545, Validation Loss: 0.006423125855462786\n",
      "Epoch [4668/20000], Training Loss: 0.007570636733102479, Validation Loss: 0.008968189244582854\n",
      "Epoch [4669/20000], Training Loss: 0.005649774299984399, Validation Loss: 0.019082766025348974\n",
      "Epoch [4670/20000], Training Loss: 0.014616933522380091, Validation Loss: 0.008113197649647321\n",
      "Epoch [4671/20000], Training Loss: 0.006805807676365865, Validation Loss: 0.00488258381238893\n",
      "Epoch [4672/20000], Training Loss: 0.005785653819787383, Validation Loss: 0.006210348135105649\n",
      "Epoch [4673/20000], Training Loss: 0.01041900807051986, Validation Loss: 0.03831503966155617\n",
      "Epoch [4674/20000], Training Loss: 0.014869146434648428, Validation Loss: 0.008518502725468073\n",
      "Epoch [4675/20000], Training Loss: 0.008575537992978102, Validation Loss: 0.013041492892265718\n",
      "Epoch [4676/20000], Training Loss: 0.012615656819044878, Validation Loss: 0.02433272399682521\n",
      "Epoch [4677/20000], Training Loss: 0.014213741501277712, Validation Loss: 0.021390124379777935\n",
      "Epoch [4678/20000], Training Loss: 0.016850014828378335, Validation Loss: 0.005252647893712492\n",
      "Epoch [4679/20000], Training Loss: 0.009645578635952137, Validation Loss: 0.013522150201294738\n",
      "Epoch [4680/20000], Training Loss: 0.009301073244094888, Validation Loss: 0.005003466240631431\n",
      "Epoch [4681/20000], Training Loss: 0.011413191422596876, Validation Loss: 0.004723960616425009\n",
      "Epoch [4682/20000], Training Loss: 0.03466418996270347, Validation Loss: 0.11251001650489785\n",
      "Epoch [4683/20000], Training Loss: 0.08758858848003521, Validation Loss: 0.030547003968552845\n",
      "Epoch [4684/20000], Training Loss: 0.023975816671996393, Validation Loss: 0.010299114402337366\n",
      "Epoch [4685/20000], Training Loss: 0.013814354398553925, Validation Loss: 0.008475104238787877\n",
      "Epoch [4686/20000], Training Loss: 0.01134910251546119, Validation Loss: 0.007396792244566244\n",
      "Epoch [4687/20000], Training Loss: 0.011285935428791813, Validation Loss: 0.007266189819867313\n",
      "Epoch [4688/20000], Training Loss: 0.010352914562515383, Validation Loss: 0.0068644396212351345\n",
      "Epoch [4689/20000], Training Loss: 0.007129128664798502, Validation Loss: 0.0075088119525129875\n",
      "Epoch [4690/20000], Training Loss: 0.012359106372709252, Validation Loss: 0.009628261831364528\n",
      "Epoch [4691/20000], Training Loss: 0.005387448994692282, Validation Loss: 0.013521216682940056\n",
      "Epoch [4692/20000], Training Loss: 0.01084912134267922, Validation Loss: 0.010159560029153778\n",
      "Epoch [4693/20000], Training Loss: 0.0071978953305266, Validation Loss: 0.00742997104472514\n",
      "Epoch [4694/20000], Training Loss: 0.009270101543474343, Validation Loss: 0.0103119711576645\n",
      "Epoch [4695/20000], Training Loss: 0.00933612620445144, Validation Loss: 0.006148397473485342\n",
      "Epoch [4696/20000], Training Loss: 0.008050721903730716, Validation Loss: 0.006180032420421152\n",
      "Epoch [4697/20000], Training Loss: 0.010801561207959562, Validation Loss: 0.004683086758762199\n",
      "Epoch [4698/20000], Training Loss: 0.008772395979446759, Validation Loss: 0.006262491932180377\n",
      "Epoch [4699/20000], Training Loss: 0.011740636683368524, Validation Loss: 0.009495993216530354\n",
      "Epoch [4700/20000], Training Loss: 0.009517374057655357, Validation Loss: 0.016640794855317966\n",
      "Epoch [4701/20000], Training Loss: 0.019364112697076052, Validation Loss: 0.04605099606266906\n",
      "Epoch [4702/20000], Training Loss: 0.01730789325042029, Validation Loss: 0.01030145194753069\n",
      "Epoch [4703/20000], Training Loss: 0.0064410042936547795, Validation Loss: 0.004201391353750036\n",
      "Epoch [4704/20000], Training Loss: 0.0060213609524453204, Validation Loss: 0.00572091314071648\n",
      "Epoch [4705/20000], Training Loss: 0.008243562887270985, Validation Loss: 0.01988661442062169\n",
      "Epoch [4706/20000], Training Loss: 0.013594645503742089, Validation Loss: 0.0068359187436206935\n",
      "Epoch [4707/20000], Training Loss: 0.010286952327338181, Validation Loss: 0.005170643255041796\n",
      "Epoch [4708/20000], Training Loss: 0.005022814601293901, Validation Loss: 0.009510303590494524\n",
      "Epoch [4709/20000], Training Loss: 0.01713376405564304, Validation Loss: 0.009569998043714025\n",
      "Epoch [4710/20000], Training Loss: 0.007919229989056475, Validation Loss: 0.02274644385449522\n",
      "Epoch [4711/20000], Training Loss: 0.016284969563587635, Validation Loss: 0.01707696636601343\n",
      "Epoch [4712/20000], Training Loss: 0.016584020271173876, Validation Loss: 0.025879697880944668\n",
      "Epoch [4713/20000], Training Loss: 0.022832694380278035, Validation Loss: 0.015854658984770658\n",
      "Epoch [4714/20000], Training Loss: 0.046130373200867325, Validation Loss: 0.0784227902624508\n",
      "Epoch [4715/20000], Training Loss: 0.044312358767326386, Validation Loss: 0.05081554229114512\n",
      "Epoch [4716/20000], Training Loss: 0.08475085607746483, Validation Loss: 0.055735847222552246\n",
      "Epoch [4717/20000], Training Loss: 0.035394859400444796, Validation Loss: 0.039832098168387495\n",
      "Epoch [4718/20000], Training Loss: 0.021172395283688923, Validation Loss: 0.010278764721338778\n",
      "Epoch [4719/20000], Training Loss: 0.013229852820846386, Validation Loss: 0.019246502034158248\n",
      "Epoch [4720/20000], Training Loss: 0.010060518264903553, Validation Loss: 0.009726725653828388\n",
      "Epoch [4721/20000], Training Loss: 0.009856934060475655, Validation Loss: 0.007353419587681336\n",
      "Epoch [4722/20000], Training Loss: 0.008228191182882125, Validation Loss: 0.00888075796787494\n",
      "Epoch [4723/20000], Training Loss: 0.007935267997839088, Validation Loss: 0.0071145438336576005\n",
      "Epoch [4724/20000], Training Loss: 0.007189650737148311, Validation Loss: 0.006367845289267799\n",
      "Epoch [4725/20000], Training Loss: 0.004990420531872327, Validation Loss: 0.013149462243485946\n",
      "Epoch [4726/20000], Training Loss: 0.008693142589306393, Validation Loss: 0.008050567505831299\n",
      "Epoch [4727/20000], Training Loss: 0.008245325975752036, Validation Loss: 0.00826950244299951\n",
      "Epoch [4728/20000], Training Loss: 0.008427368340822536, Validation Loss: 0.005023858669769002\n",
      "Epoch [4729/20000], Training Loss: 0.02162269625945815, Validation Loss: 0.007899188323010873\n",
      "Epoch [4730/20000], Training Loss: 0.019893649242086604, Validation Loss: 0.009873995787789422\n",
      "Epoch [4731/20000], Training Loss: 0.00729719519793954, Validation Loss: 0.007601923725555285\n",
      "Epoch [4732/20000], Training Loss: 0.0064782299866367665, Validation Loss: 0.0043645786238682505\n",
      "Epoch [4733/20000], Training Loss: 0.007509022167401521, Validation Loss: 0.004297639854095385\n",
      "Epoch [4734/20000], Training Loss: 0.005454973258955371, Validation Loss: 0.007133456670479583\n",
      "Epoch [4735/20000], Training Loss: 0.005814768037193322, Validation Loss: 0.003989606713275862\n",
      "Epoch [4736/20000], Training Loss: 0.004381200599445785, Validation Loss: 0.01737553456721181\n",
      "Epoch [4737/20000], Training Loss: 0.01534810809373864, Validation Loss: 0.008252494028933402\n",
      "Epoch [4738/20000], Training Loss: 0.008093958608826921, Validation Loss: 0.011751554646066\n",
      "Epoch [4739/20000], Training Loss: 0.01210075694533381, Validation Loss: 0.012466450796020061\n",
      "Epoch [4740/20000], Training Loss: 0.013647328357494968, Validation Loss: 0.011931843378127789\n",
      "Epoch [4741/20000], Training Loss: 0.02499310694734699, Validation Loss: 0.009711935667707752\n",
      "Epoch [4742/20000], Training Loss: 0.019525697464164944, Validation Loss: 0.016033182190052946\n",
      "Epoch [4743/20000], Training Loss: 0.030762029093498962, Validation Loss: 0.014762505914274828\n",
      "Epoch [4744/20000], Training Loss: 0.02443983158030148, Validation Loss: 0.014712905471761977\n",
      "Epoch [4745/20000], Training Loss: 0.016774735480014766, Validation Loss: 0.006099995023500274\n",
      "Epoch [4746/20000], Training Loss: 0.01877724879886955, Validation Loss: 0.009681876186472696\n",
      "Epoch [4747/20000], Training Loss: 0.019543185223093524, Validation Loss: 0.012750416237914968\n",
      "Epoch [4748/20000], Training Loss: 0.022028183481389924, Validation Loss: 0.017913504286817834\n",
      "Epoch [4749/20000], Training Loss: 0.011461980857088097, Validation Loss: 0.009607852766618765\n",
      "Epoch [4750/20000], Training Loss: 0.0071459458434089485, Validation Loss: 0.0060028955488503144\n",
      "Epoch [4751/20000], Training Loss: 0.009368264669319615, Validation Loss: 0.008617880337364372\n",
      "Epoch [4752/20000], Training Loss: 0.008958249011941786, Validation Loss: 0.004799606562820762\n",
      "Epoch [4753/20000], Training Loss: 0.014671330860957304, Validation Loss: 0.02771394342221356\n",
      "Epoch [4754/20000], Training Loss: 0.013316845546276974, Validation Loss: 0.012753830734124807\n",
      "Epoch [4755/20000], Training Loss: 0.013050292071836469, Validation Loss: 0.00715618448754997\n",
      "Epoch [4756/20000], Training Loss: 0.01180684075058837, Validation Loss: 0.005526530486999296\n",
      "Epoch [4757/20000], Training Loss: 0.005980255784899262, Validation Loss: 0.006302492598868932\n",
      "Epoch [4758/20000], Training Loss: 0.005897855849720405, Validation Loss: 0.006673386536671647\n",
      "Epoch [4759/20000], Training Loss: 0.010590175532602839, Validation Loss: 0.00495552277724138\n",
      "Epoch [4760/20000], Training Loss: 0.01048390963114798, Validation Loss: 0.00430689631356732\n",
      "Epoch [4761/20000], Training Loss: 0.006567554001646515, Validation Loss: 0.00517467261469002\n",
      "Epoch [4762/20000], Training Loss: 0.010850708666631752, Validation Loss: 0.020584249939227157\n",
      "Epoch [4763/20000], Training Loss: 0.008011244295630604, Validation Loss: 0.007136357947079627\n",
      "Epoch [4764/20000], Training Loss: 0.015269067261085314, Validation Loss: 0.004968141368554565\n",
      "Epoch [4765/20000], Training Loss: 0.019448380927445084, Validation Loss: 0.013169123204339226\n",
      "Epoch [4766/20000], Training Loss: 0.023162511662771328, Validation Loss: 0.013979804020274449\n",
      "Epoch [4767/20000], Training Loss: 0.007913081821633179, Validation Loss: 0.007236146421616597\n",
      "Epoch [4768/20000], Training Loss: 0.0058479328415290055, Validation Loss: 0.0039211922068846305\n",
      "Epoch [4769/20000], Training Loss: 0.006944443751958066, Validation Loss: 0.02082919465073831\n",
      "Epoch [4770/20000], Training Loss: 0.01527965544456882, Validation Loss: 0.016883907466721432\n",
      "Epoch [4771/20000], Training Loss: 0.008561251319146581, Validation Loss: 0.003802407318029769\n",
      "Epoch [4772/20000], Training Loss: 0.006441844517602087, Validation Loss: 0.008007955173476538\n",
      "Epoch [4773/20000], Training Loss: 0.011908278648963997, Validation Loss: 0.008423285293507174\n",
      "Epoch [4774/20000], Training Loss: 0.019744070962977794, Validation Loss: 0.011133506569876772\n",
      "Epoch [4775/20000], Training Loss: 0.020285089960972464, Validation Loss: 0.009448287333486047\n",
      "Epoch [4776/20000], Training Loss: 0.039194383095621434, Validation Loss: 0.02295362417446865\n",
      "Epoch [4777/20000], Training Loss: 0.018662189665649618, Validation Loss: 0.011555208864073003\n",
      "Epoch [4778/20000], Training Loss: 0.01490816569587748, Validation Loss: 0.007467116450892978\n",
      "Epoch [4779/20000], Training Loss: 0.012943980113569913, Validation Loss: 0.005090537395257603\n",
      "Epoch [4780/20000], Training Loss: 0.011954294625736241, Validation Loss: 0.015154322870005541\n",
      "Epoch [4781/20000], Training Loss: 0.010009499851418826, Validation Loss: 0.018734445257905463\n",
      "Epoch [4782/20000], Training Loss: 0.030492226680507883, Validation Loss: 0.052781411230276784\n",
      "Epoch [4783/20000], Training Loss: 0.033992523310839066, Validation Loss: 0.006821324291325125\n",
      "Epoch [4784/20000], Training Loss: 0.021295376695759063, Validation Loss: 0.015911446545942454\n",
      "Epoch [4785/20000], Training Loss: 0.037168878521437625, Validation Loss: 0.015604091299922272\n",
      "Epoch [4786/20000], Training Loss: 0.017485383091427917, Validation Loss: 0.008677055065033659\n",
      "Epoch [4787/20000], Training Loss: 0.00966740023224182, Validation Loss: 0.006879001547401588\n",
      "Epoch [4788/20000], Training Loss: 0.012460632486701278, Validation Loss: 0.014433867675896701\n",
      "Epoch [4789/20000], Training Loss: 0.00982924799401579, Validation Loss: 0.009994464072795901\n",
      "Epoch [4790/20000], Training Loss: 0.00836615140516577, Validation Loss: 0.006690991550190769\n",
      "Epoch [4791/20000], Training Loss: 0.00618517620854878, Validation Loss: 0.005835701777544143\n",
      "Epoch [4792/20000], Training Loss: 0.004714889027906922, Validation Loss: 0.014250327179476179\n",
      "Epoch [4793/20000], Training Loss: 0.0096662028299761, Validation Loss: 0.005982573221006403\n",
      "Epoch [4794/20000], Training Loss: 0.007024121990460637, Validation Loss: 0.005839947383979701\n",
      "Epoch [4795/20000], Training Loss: 0.006226928441070153, Validation Loss: 0.0048297872511804075\n",
      "Epoch [4796/20000], Training Loss: 0.006549491655148033, Validation Loss: 0.0060698094766524135\n",
      "Epoch [4797/20000], Training Loss: 0.005818277160788706, Validation Loss: 0.004060294647047158\n",
      "Epoch [4798/20000], Training Loss: 0.005137915512542739, Validation Loss: 0.0098889564378177\n",
      "Epoch [4799/20000], Training Loss: 0.006670827349807951, Validation Loss: 0.015273683057692214\n",
      "Epoch [4800/20000], Training Loss: 0.01398851485958273, Validation Loss: 0.023642717294163975\n",
      "Epoch [4801/20000], Training Loss: 0.0110981935406796, Validation Loss: 0.012151883417203635\n",
      "Epoch [4802/20000], Training Loss: 0.00826514118099096, Validation Loss: 0.003475405013058922\n",
      "Epoch [4803/20000], Training Loss: 0.0047646478039113606, Validation Loss: 0.014057730310097685\n",
      "Epoch [4804/20000], Training Loss: 0.010151827257816746, Validation Loss: 0.0078102925494079045\n",
      "Epoch [4805/20000], Training Loss: 0.0113938288402226, Validation Loss: 0.007418044001056501\n",
      "Epoch [4806/20000], Training Loss: 0.012067493428211102, Validation Loss: 0.010952751762382766\n",
      "Epoch [4807/20000], Training Loss: 0.015845468061992767, Validation Loss: 0.02036652417960941\n",
      "Epoch [4808/20000], Training Loss: 0.013553622064916584, Validation Loss: 0.010919701581810517\n",
      "Epoch [4809/20000], Training Loss: 0.009456074607093872, Validation Loss: 0.022681320865298664\n",
      "Epoch [4810/20000], Training Loss: 0.014856527806841768, Validation Loss: 0.012283823777774419\n",
      "Epoch [4811/20000], Training Loss: 0.008232626393172333, Validation Loss: 0.004823278294842177\n",
      "Epoch [4812/20000], Training Loss: 0.008583505492424592, Validation Loss: 0.01044525486482988\n",
      "Epoch [4813/20000], Training Loss: 0.010716559458939758, Validation Loss: 0.005961024146180145\n",
      "Epoch [4814/20000], Training Loss: 0.006051384511270693, Validation Loss: 0.010752910543455332\n",
      "Epoch [4815/20000], Training Loss: 0.010223151483972157, Validation Loss: 0.029889609198349376\n",
      "Epoch [4816/20000], Training Loss: 0.01310349967921606, Validation Loss: 0.004387293300851525\n",
      "Epoch [4817/20000], Training Loss: 0.00575899715892904, Validation Loss: 0.0033562948612403227\n",
      "Epoch [4818/20000], Training Loss: 0.005024015866053689, Validation Loss: 0.006564391054837415\n",
      "Epoch [4819/20000], Training Loss: 0.005377844903912988, Validation Loss: 0.0029833173271396974\n",
      "Epoch [4820/20000], Training Loss: 0.006365400961450567, Validation Loss: 0.0054777104089274485\n",
      "Epoch [4821/20000], Training Loss: 0.00809010028439973, Validation Loss: 0.01530798891631743\n",
      "Epoch [4822/20000], Training Loss: 0.011187302971718185, Validation Loss: 0.002589643312912163\n",
      "Epoch [4823/20000], Training Loss: 0.008618115597138447, Validation Loss: 0.00833422583653893\n",
      "Epoch [4824/20000], Training Loss: 0.019055074429031395, Validation Loss: 0.07674212690004165\n",
      "Epoch [4825/20000], Training Loss: 0.04472762438880246, Validation Loss: 0.03813513956616329\n",
      "Epoch [4826/20000], Training Loss: 0.01941978396209265, Validation Loss: 0.027561380812726806\n",
      "Epoch [4827/20000], Training Loss: 0.018942202614458177, Validation Loss: 0.03628843551609124\n",
      "Epoch [4828/20000], Training Loss: 0.011273629822036517, Validation Loss: 0.025606673929107288\n",
      "Epoch [4829/20000], Training Loss: 0.022271338288971623, Validation Loss: 0.056708133481163145\n",
      "Epoch [4830/20000], Training Loss: 0.040685153898916075, Validation Loss: 0.015447524476010836\n",
      "Epoch [4831/20000], Training Loss: 0.023233725905551443, Validation Loss: 0.012278008226922665\n",
      "Epoch [4832/20000], Training Loss: 0.010137221376810755, Validation Loss: 0.005427754640031283\n",
      "Epoch [4833/20000], Training Loss: 0.009898688939366755, Validation Loss: 0.007289156994375254\n",
      "Epoch [4834/20000], Training Loss: 0.00851423710940123, Validation Loss: 0.004108876586542515\n",
      "Epoch [4835/20000], Training Loss: 0.008455937965793834, Validation Loss: 0.004961289605749444\n",
      "Epoch [4836/20000], Training Loss: 0.0056813904385697766, Validation Loss: 0.005317780909016232\n",
      "Epoch [4837/20000], Training Loss: 0.009187061372878296, Validation Loss: 0.005553077085226377\n",
      "Epoch [4838/20000], Training Loss: 0.01037432600631811, Validation Loss: 0.011851853991694432\n",
      "Epoch [4839/20000], Training Loss: 0.027171081726139943, Validation Loss: 0.006697414133148933\n",
      "Epoch [4840/20000], Training Loss: 0.011227785709447094, Validation Loss: 0.006469540630600283\n",
      "Epoch [4841/20000], Training Loss: 0.008586474590369366, Validation Loss: 0.008675667266288658\n",
      "Epoch [4842/20000], Training Loss: 0.009527756957270737, Validation Loss: 0.005297960280326412\n",
      "Epoch [4843/20000], Training Loss: 0.015621611565750624, Validation Loss: 0.007365194600034946\n",
      "Epoch [4844/20000], Training Loss: 0.009676206261376916, Validation Loss: 0.0048216549466815195\n",
      "Epoch [4845/20000], Training Loss: 0.007806812654182847, Validation Loss: 0.0072951382805643205\n",
      "Epoch [4846/20000], Training Loss: 0.013146930324312831, Validation Loss: 0.004008744556553471\n",
      "Epoch [4847/20000], Training Loss: 0.007349490585641304, Validation Loss: 0.006377018755431761\n",
      "Epoch [4848/20000], Training Loss: 0.011071898446451607, Validation Loss: 0.018561637358353828\n",
      "Epoch [4849/20000], Training Loss: 0.012652999272436969, Validation Loss: 0.039090589347633146\n",
      "Epoch [4850/20000], Training Loss: 0.012666017731784709, Validation Loss: 0.005128441237502557\n",
      "Epoch [4851/20000], Training Loss: 0.015673587385598303, Validation Loss: 0.007498492271581979\n",
      "Epoch [4852/20000], Training Loss: 0.035891756146156695, Validation Loss: 0.013520672821999011\n",
      "Epoch [4853/20000], Training Loss: 0.035916624341293106, Validation Loss: 0.03162867665841202\n",
      "Epoch [4854/20000], Training Loss: 0.019086874403780842, Validation Loss: 0.015563066086802533\n",
      "Epoch [4855/20000], Training Loss: 0.020218229835986028, Validation Loss: 0.01175755717460041\n",
      "Epoch [4856/20000], Training Loss: 0.011738753145826715, Validation Loss: 0.015271832901687179\n",
      "Epoch [4857/20000], Training Loss: 0.013200688292272389, Validation Loss: 0.00852622382431686\n",
      "Epoch [4858/20000], Training Loss: 0.006775395381347542, Validation Loss: 0.009852970543891258\n",
      "Epoch [4859/20000], Training Loss: 0.007515117921034938, Validation Loss: 0.00745363280397474\n",
      "Epoch [4860/20000], Training Loss: 0.007341184162734342, Validation Loss: 0.013004538474303575\n",
      "Epoch [4861/20000], Training Loss: 0.00955769420917412, Validation Loss: 0.00413810488056957\n",
      "Epoch [4862/20000], Training Loss: 0.00986571833657633, Validation Loss: 0.004906437320122288\n",
      "Epoch [4863/20000], Training Loss: 0.017827218203560084, Validation Loss: 0.004979826873750426\n",
      "Epoch [4864/20000], Training Loss: 0.019401382819653788, Validation Loss: 0.009047327218273235\n",
      "Epoch [4865/20000], Training Loss: 0.01233504075831401, Validation Loss: 0.019148012995226997\n",
      "Epoch [4866/20000], Training Loss: 0.007359187670122732, Validation Loss: 0.018253171132788078\n",
      "Epoch [4867/20000], Training Loss: 0.017216097237126502, Validation Loss: 0.015530886245999551\n",
      "Epoch [4868/20000], Training Loss: 0.027697700171691913, Validation Loss: 0.02218025201909768\n",
      "Epoch [4869/20000], Training Loss: 0.018313022736817532, Validation Loss: 0.008547852566087906\n",
      "Epoch [4870/20000], Training Loss: 0.010296055356905396, Validation Loss: 0.006192394283841297\n",
      "Epoch [4871/20000], Training Loss: 0.010334425560098939, Validation Loss: 0.0046288719000325985\n",
      "Epoch [4872/20000], Training Loss: 0.007116175823690745, Validation Loss: 0.011642363249945577\n",
      "Epoch [4873/20000], Training Loss: 0.007768242619931698, Validation Loss: 0.00503171838740205\n",
      "Epoch [4874/20000], Training Loss: 0.011880557261715044, Validation Loss: 0.0038638551168982565\n",
      "Epoch [4875/20000], Training Loss: 0.03851234528919382, Validation Loss: 0.023512819216323675\n",
      "Epoch [4876/20000], Training Loss: 0.02780800311172373, Validation Loss: 0.030261277031968348\n",
      "Epoch [4877/20000], Training Loss: 0.041369580358150415, Validation Loss: 0.012958310930116568\n",
      "Epoch [4878/20000], Training Loss: 0.024225016590207815, Validation Loss: 0.01495958359373617\n",
      "Epoch [4879/20000], Training Loss: 0.010195520756367062, Validation Loss: 0.01399037229566602\n",
      "Epoch [4880/20000], Training Loss: 0.012612305781138795, Validation Loss: 0.00712924451831246\n",
      "Epoch [4881/20000], Training Loss: 0.007136907827641282, Validation Loss: 0.009542390652508661\n",
      "Epoch [4882/20000], Training Loss: 0.007888835168809496, Validation Loss: 0.020558136242470937\n",
      "Epoch [4883/20000], Training Loss: 0.012884545314591378, Validation Loss: 0.008637710757934005\n",
      "Epoch [4884/20000], Training Loss: 0.011238915689176565, Validation Loss: 0.010376519520962444\n",
      "Epoch [4885/20000], Training Loss: 0.009569047850423626, Validation Loss: 0.006618538353287866\n",
      "Epoch [4886/20000], Training Loss: 0.005847820608843384, Validation Loss: 0.004285561270275399\n",
      "Epoch [4887/20000], Training Loss: 0.00580057768619164, Validation Loss: 0.005239589906490697\n",
      "Epoch [4888/20000], Training Loss: 0.0044571557660414585, Validation Loss: 0.010593159838793196\n",
      "Epoch [4889/20000], Training Loss: 0.015263490195918297, Validation Loss: 0.008373408109296829\n",
      "Epoch [4890/20000], Training Loss: 0.02676455167758312, Validation Loss: 0.008035240236495156\n",
      "Epoch [4891/20000], Training Loss: 0.013205609084772212, Validation Loss: 0.008374136489757282\n",
      "Epoch [4892/20000], Training Loss: 0.005848832374405382, Validation Loss: 0.005018727976740982\n",
      "Epoch [4893/20000], Training Loss: 0.011801462167308532, Validation Loss: 0.006762729958706716\n",
      "Epoch [4894/20000], Training Loss: 0.006658980916004761, Validation Loss: 0.005246227620805907\n",
      "Epoch [4895/20000], Training Loss: 0.007376381318733495, Validation Loss: 0.014477280597017536\n",
      "Epoch [4896/20000], Training Loss: 0.010544780352997154, Validation Loss: 0.005396732944323698\n",
      "Epoch [4897/20000], Training Loss: 0.0065959325820585946, Validation Loss: 0.011177875979326148\n",
      "Epoch [4898/20000], Training Loss: 0.008048328016359716, Validation Loss: 0.005438808134109812\n",
      "Epoch [4899/20000], Training Loss: 0.010013439604725656, Validation Loss: 0.004109420095379132\n",
      "Epoch [4900/20000], Training Loss: 0.008140316586702414, Validation Loss: 0.00838860610153753\n",
      "Epoch [4901/20000], Training Loss: 0.008190271515299432, Validation Loss: 0.0061134996841799094\n",
      "Epoch [4902/20000], Training Loss: 0.008120874915350993, Validation Loss: 0.006114500045141215\n",
      "Epoch [4903/20000], Training Loss: 0.009035217421894361, Validation Loss: 0.00803338180539624\n",
      "Epoch [4904/20000], Training Loss: 0.010877588961063469, Validation Loss: 0.0034601216424822007\n",
      "Epoch [4905/20000], Training Loss: 0.009703914119329835, Validation Loss: 0.0050567196237401345\n",
      "Epoch [4906/20000], Training Loss: 0.02467121671829123, Validation Loss: 0.011562986766821024\n",
      "Epoch [4907/20000], Training Loss: 0.012343763581676674, Validation Loss: 0.027284382100804448\n",
      "Epoch [4908/20000], Training Loss: 0.014306464519384983, Validation Loss: 0.022760247650560523\n",
      "Epoch [4909/20000], Training Loss: 0.020938462956110016, Validation Loss: 0.046870309782625394\n",
      "Epoch [4910/20000], Training Loss: 0.02397115296896248, Validation Loss: 0.009766832919435078\n",
      "Epoch [4911/20000], Training Loss: 0.009673731526293392, Validation Loss: 0.005512918412250656\n",
      "Epoch [4912/20000], Training Loss: 0.007085148802226675, Validation Loss: 0.00905872582102672\n",
      "Epoch [4913/20000], Training Loss: 0.0058240468871580174, Validation Loss: 0.0038726218834069315\n",
      "Epoch [4914/20000], Training Loss: 0.006616544078237244, Validation Loss: 0.005663886836113566\n",
      "Epoch [4915/20000], Training Loss: 0.009549744435519512, Validation Loss: 0.012186759592420262\n",
      "Epoch [4916/20000], Training Loss: 0.00445085864131605, Validation Loss: 0.009083269921510302\n",
      "Epoch [4917/20000], Training Loss: 0.010857362167549971, Validation Loss: 0.008300983497357655\n",
      "Epoch [4918/20000], Training Loss: 0.011307888265166963, Validation Loss: 0.005633063427727133\n",
      "Epoch [4919/20000], Training Loss: 0.00892372209948787, Validation Loss: 0.005887817736038414\n",
      "Epoch [4920/20000], Training Loss: 0.006689680898229459, Validation Loss: 0.009632782883569137\n",
      "Epoch [4921/20000], Training Loss: 0.01341668758790807, Validation Loss: 0.020210884969541456\n",
      "Epoch [4922/20000], Training Loss: 0.006511910614140756, Validation Loss: 0.006062056574344459\n",
      "Epoch [4923/20000], Training Loss: 0.006864133082997016, Validation Loss: 0.008951091611826152\n",
      "Epoch [4924/20000], Training Loss: 0.0094987276845911, Validation Loss: 0.0042163260950717684\n",
      "Epoch [4925/20000], Training Loss: 0.00819687732044778, Validation Loss: 0.012237177944809576\n",
      "Epoch [4926/20000], Training Loss: 0.03143438154074829, Validation Loss: 0.018799755564604732\n",
      "Epoch [4927/20000], Training Loss: 0.03494174757153295, Validation Loss: 0.006521367556680551\n",
      "Epoch [4928/20000], Training Loss: 0.03476589326081531, Validation Loss: 0.017327731997543845\n",
      "Epoch [4929/20000], Training Loss: 0.025655227737193594, Validation Loss: 0.023199435852542853\n",
      "Epoch [4930/20000], Training Loss: 0.015339974147666777, Validation Loss: 0.013902185027421052\n",
      "Epoch [4931/20000], Training Loss: 0.015087020277340863, Validation Loss: 0.007946876347497371\n",
      "Epoch [4932/20000], Training Loss: 0.038418026229399924, Validation Loss: 0.05589435891102669\n",
      "Epoch [4933/20000], Training Loss: 0.03564325405750424, Validation Loss: 0.0379660314045915\n",
      "Epoch [4934/20000], Training Loss: 0.018939557572593912, Validation Loss: 0.028553000189796358\n",
      "Epoch [4935/20000], Training Loss: 0.012691124250912773, Validation Loss: 0.009918917269325382\n",
      "Epoch [4936/20000], Training Loss: 0.00984075709285597, Validation Loss: 0.006425919736492298\n",
      "Epoch [4937/20000], Training Loss: 0.024497063514510437, Validation Loss: 0.011669617419231924\n",
      "Epoch [4938/20000], Training Loss: 0.022831504877623438, Validation Loss: 0.03532456551891577\n",
      "Epoch [4939/20000], Training Loss: 0.019263271308903183, Validation Loss: 0.030728760174111715\n",
      "Epoch [4940/20000], Training Loss: 0.02164616700195308, Validation Loss: 0.007925806514841887\n",
      "Epoch [4941/20000], Training Loss: 0.01006400291641642, Validation Loss: 0.007771232056256849\n",
      "Epoch [4942/20000], Training Loss: 0.00813295985010752, Validation Loss: 0.009903804897411257\n",
      "Epoch [4943/20000], Training Loss: 0.008220031063371738, Validation Loss: 0.009813303484926499\n",
      "Epoch [4944/20000], Training Loss: 0.008669914324335488, Validation Loss: 0.00702865908806416\n",
      "Epoch [4945/20000], Training Loss: 0.006462294134377901, Validation Loss: 0.010744083772351587\n",
      "Epoch [4946/20000], Training Loss: 0.007036641593523589, Validation Loss: 0.012436002616336086\n",
      "Epoch [4947/20000], Training Loss: 0.012916229511444857, Validation Loss: 0.010997444888354429\n",
      "Epoch [4948/20000], Training Loss: 0.01084784156734843, Validation Loss: 0.006119771221165138\n",
      "Epoch [4949/20000], Training Loss: 0.008986585274604815, Validation Loss: 0.01402087176747838\n",
      "Epoch [4950/20000], Training Loss: 0.019399244537843124, Validation Loss: 0.006510399850380314\n",
      "Epoch [4951/20000], Training Loss: 0.02064905337257577, Validation Loss: 0.0057700413939788375\n",
      "Epoch [4952/20000], Training Loss: 0.02685951431963726, Validation Loss: 0.009867633825738917\n",
      "Epoch [4953/20000], Training Loss: 0.05877811493285533, Validation Loss: 0.0159585442597745\n",
      "Epoch [4954/20000], Training Loss: 0.02225699375516602, Validation Loss: 0.008089008137176279\n",
      "Epoch [4955/20000], Training Loss: 0.016399522000158737, Validation Loss: 0.010894348555406655\n",
      "Epoch [4956/20000], Training Loss: 0.023815067218882695, Validation Loss: 0.020932530975967114\n",
      "Epoch [4957/20000], Training Loss: 0.011681163629483697, Validation Loss: 0.011617278871524053\n",
      "Epoch [4958/20000], Training Loss: 0.011394702334655449, Validation Loss: 0.00913903443298685\n",
      "Epoch [4959/20000], Training Loss: 0.011165140142631052, Validation Loss: 0.007959315461318641\n",
      "Epoch [4960/20000], Training Loss: 0.01110211355678205, Validation Loss: 0.009942984903675876\n",
      "Epoch [4961/20000], Training Loss: 0.011117389131478765, Validation Loss: 0.005573754936449404\n",
      "Epoch [4962/20000], Training Loss: 0.006332081162586941, Validation Loss: 0.01135314648334835\n",
      "Epoch [4963/20000], Training Loss: 0.008961279094884438, Validation Loss: 0.009252360674590168\n",
      "Epoch [4964/20000], Training Loss: 0.00886561431858419, Validation Loss: 0.011708635057690995\n",
      "Epoch [4965/20000], Training Loss: 0.005826538234292197, Validation Loss: 0.007517654259962521\n",
      "Epoch [4966/20000], Training Loss: 0.011286295900000758, Validation Loss: 0.008585136536826863\n",
      "Epoch [4967/20000], Training Loss: 0.007093027477822034, Validation Loss: 0.006281891569333961\n",
      "Epoch [4968/20000], Training Loss: 0.007515336714797221, Validation Loss: 0.008452466220666517\n",
      "Epoch [4969/20000], Training Loss: 0.009131213808099605, Validation Loss: 0.006661206827455349\n",
      "Epoch [4970/20000], Training Loss: 0.02161881320561016, Validation Loss: 0.012396285383277115\n",
      "Epoch [4971/20000], Training Loss: 0.011918988180696033, Validation Loss: 0.04906687778145776\n",
      "Epoch [4972/20000], Training Loss: 0.024323791386060684, Validation Loss: 0.005658778311304299\n",
      "Epoch [4973/20000], Training Loss: 0.021678242581531646, Validation Loss: 0.02008103536094755\n",
      "Epoch [4974/20000], Training Loss: 0.02849286819608616, Validation Loss: 0.02139842107135337\n",
      "Epoch [4975/20000], Training Loss: 0.01791088893410883, Validation Loss: 0.013484288163454039\n",
      "Epoch [4976/20000], Training Loss: 0.019236082477228984, Validation Loss: 0.011973374678502426\n",
      "Epoch [4977/20000], Training Loss: 0.009394220276070493, Validation Loss: 0.006564926153357321\n",
      "Epoch [4978/20000], Training Loss: 0.009320517902129464, Validation Loss: 0.005513608860253173\n",
      "Epoch [4979/20000], Training Loss: 0.006477802596886509, Validation Loss: 0.013454622510445395\n",
      "Epoch [4980/20000], Training Loss: 0.010390728955306778, Validation Loss: 0.01029865726548808\n",
      "Epoch [4981/20000], Training Loss: 0.00956161207743987, Validation Loss: 0.008885289350229999\n",
      "Epoch [4982/20000], Training Loss: 0.01650466819670068, Validation Loss: 0.020717364308491786\n",
      "Epoch [4983/20000], Training Loss: 0.025839130392601613, Validation Loss: 0.008337823987858428\n",
      "Epoch [4984/20000], Training Loss: 0.02154795453783923, Validation Loss: 0.009782621445732989\n",
      "Epoch [4985/20000], Training Loss: 0.019975893840897437, Validation Loss: 0.013143337843303016\n",
      "Epoch [4986/20000], Training Loss: 0.01772956576782495, Validation Loss: 0.03350376358772727\n",
      "Epoch [4987/20000], Training Loss: 0.01993595887740542, Validation Loss: 0.01314220927250582\n",
      "Epoch [4988/20000], Training Loss: 0.007355511635458762, Validation Loss: 0.012796349160063463\n",
      "Epoch [4989/20000], Training Loss: 0.011963658026486104, Validation Loss: 0.006570169437249466\n",
      "Epoch [4990/20000], Training Loss: 0.010785470982747418, Validation Loss: 0.012744677885556095\n",
      "Epoch [4991/20000], Training Loss: 0.009489445839270567, Validation Loss: 0.012983984492068754\n",
      "Epoch [4992/20000], Training Loss: 0.00817085541452148, Validation Loss: 0.006938556944792903\n",
      "Epoch [4993/20000], Training Loss: 0.006746430174514119, Validation Loss: 0.008976395397309846\n",
      "Epoch [4994/20000], Training Loss: 0.011072757703784322, Validation Loss: 0.015675071159681205\n",
      "Epoch [4995/20000], Training Loss: 0.012347368120182571, Validation Loss: 0.007843358629543218\n",
      "Epoch [4996/20000], Training Loss: 0.006157066186590653, Validation Loss: 0.004706390885855528\n",
      "Epoch [4997/20000], Training Loss: 0.00956191867382066, Validation Loss: 0.004142802049792798\n",
      "Epoch [4998/20000], Training Loss: 0.013482427432401372, Validation Loss: 0.0065565160390893395\n",
      "Epoch [4999/20000], Training Loss: 0.006965466887257727, Validation Loss: 0.004238674659518438\n",
      "Epoch [5000/20000], Training Loss: 0.008465507215337962, Validation Loss: 0.0040014149616191775\n",
      "Epoch [5001/20000], Training Loss: 0.0058655047328751864, Validation Loss: 0.006263911640417064\n",
      "Epoch [5002/20000], Training Loss: 0.012886924615096567, Validation Loss: 0.003706511501377463\n",
      "Epoch [5003/20000], Training Loss: 0.013105978989614187, Validation Loss: 0.02556454239012445\n",
      "Epoch [5004/20000], Training Loss: 0.030143052046956394, Validation Loss: 0.01008748164986173\n",
      "Epoch [5005/20000], Training Loss: 0.026102235025194074, Validation Loss: 0.013905337347695073\n",
      "Epoch [5006/20000], Training Loss: 0.013572868372388516, Validation Loss: 0.008195708463192983\n",
      "Epoch [5007/20000], Training Loss: 0.015368720603873953, Validation Loss: 0.011567959286756897\n",
      "Epoch [5008/20000], Training Loss: 0.008148140219938276, Validation Loss: 0.005632520653307438\n",
      "Epoch [5009/20000], Training Loss: 0.0072103220279262, Validation Loss: 0.004300949184263507\n",
      "Epoch [5010/20000], Training Loss: 0.006162609928326544, Validation Loss: 0.010641700341182059\n",
      "Epoch [5011/20000], Training Loss: 0.005978492591696393, Validation Loss: 0.0114281242890034\n",
      "Epoch [5012/20000], Training Loss: 0.00808456496477967, Validation Loss: 0.0044621501679752585\n",
      "Epoch [5013/20000], Training Loss: 0.006491513814710613, Validation Loss: 0.0053801060595820415\n",
      "Epoch [5014/20000], Training Loss: 0.007610521592661306, Validation Loss: 0.006186569382293783\n",
      "Epoch [5015/20000], Training Loss: 0.006912941890602399, Validation Loss: 0.005207659879585792\n",
      "Epoch [5016/20000], Training Loss: 0.00542039461366325, Validation Loss: 0.008425208346076423\n",
      "Epoch [5017/20000], Training Loss: 0.006951427424480373, Validation Loss: 0.0035024067454969554\n",
      "Epoch [5018/20000], Training Loss: 0.007198735980117428, Validation Loss: 0.004572359394225194\n",
      "Epoch [5019/20000], Training Loss: 0.010326805056787893, Validation Loss: 0.01415397825322933\n",
      "Epoch [5020/20000], Training Loss: 0.017001824100849, Validation Loss: 0.025478851036154992\n",
      "Epoch [5021/20000], Training Loss: 0.017296698193604243, Validation Loss: 0.010066470692891016\n",
      "Epoch [5022/20000], Training Loss: 0.017616459596735825, Validation Loss: 0.01722409224729355\n",
      "Epoch [5023/20000], Training Loss: 0.018916677272175417, Validation Loss: 0.03589045994838865\n",
      "Epoch [5024/20000], Training Loss: 0.02140504513954511, Validation Loss: 0.03958970341461411\n",
      "Epoch [5025/20000], Training Loss: 0.03911951297777705, Validation Loss: 0.006515127470915364\n",
      "Epoch [5026/20000], Training Loss: 0.024256813132004545, Validation Loss: 0.024421068519326843\n",
      "Epoch [5027/20000], Training Loss: 0.028815644835162857, Validation Loss: 0.009127834004175384\n",
      "Epoch [5028/20000], Training Loss: 0.018152800812588663, Validation Loss: 0.010987411608356621\n",
      "Epoch [5029/20000], Training Loss: 0.008918766723581939, Validation Loss: 0.010511937766750634\n",
      "Epoch [5030/20000], Training Loss: 0.015203749935608357, Validation Loss: 0.011717593237443273\n",
      "Epoch [5031/20000], Training Loss: 0.01758271974020837, Validation Loss: 0.02027141695750093\n",
      "Epoch [5032/20000], Training Loss: 0.02485482590938253, Validation Loss: 0.021042427752208832\n",
      "Epoch [5033/20000], Training Loss: 0.022354726696253886, Validation Loss: 0.009645384596491826\n",
      "Epoch [5034/20000], Training Loss: 0.014800928416661918, Validation Loss: 0.006088271562605639\n",
      "Epoch [5035/20000], Training Loss: 0.00605270203336009, Validation Loss: 0.009266466205994155\n",
      "Epoch [5036/20000], Training Loss: 0.008172323370282746, Validation Loss: 0.006363020995812414\n",
      "Epoch [5037/20000], Training Loss: 0.008643910779418158, Validation Loss: 0.005696231587519053\n",
      "Epoch [5038/20000], Training Loss: 0.006570043316709676, Validation Loss: 0.004894501949917997\n",
      "Epoch [5039/20000], Training Loss: 0.006142147862452215, Validation Loss: 0.004493741367956058\n",
      "Epoch [5040/20000], Training Loss: 0.005204694146024329, Validation Loss: 0.0051983235176261845\n",
      "Epoch [5041/20000], Training Loss: 0.00903693439613562, Validation Loss: 0.01025383288444053\n",
      "Epoch [5042/20000], Training Loss: 0.011420251050107513, Validation Loss: 0.02210046847501092\n",
      "Epoch [5043/20000], Training Loss: 0.024372876466389113, Validation Loss: 0.008318971004622913\n",
      "Epoch [5044/20000], Training Loss: 0.03542951092822477, Validation Loss: 0.006948732612564916\n",
      "Epoch [5045/20000], Training Loss: 0.01896079193102196, Validation Loss: 0.011505556808227116\n",
      "Epoch [5046/20000], Training Loss: 0.010275595258333072, Validation Loss: 0.004854332367098583\n",
      "Epoch [5047/20000], Training Loss: 0.00645800621714443, Validation Loss: 0.009392113842315959\n",
      "Epoch [5048/20000], Training Loss: 0.0105774300172925, Validation Loss: 0.005001829108834954\n",
      "Epoch [5049/20000], Training Loss: 0.007805378077526777, Validation Loss: 0.02549857167270956\n",
      "Epoch [5050/20000], Training Loss: 0.009051358376642124, Validation Loss: 0.01974714621345746\n",
      "Epoch [5051/20000], Training Loss: 0.011960398734247844, Validation Loss: 0.006246179241925022\n",
      "Epoch [5052/20000], Training Loss: 0.023068228918418754, Validation Loss: 0.018457371080899065\n",
      "Epoch [5053/20000], Training Loss: 0.03460816894325295, Validation Loss: 0.00989579369214815\n",
      "Epoch [5054/20000], Training Loss: 0.012757071676397962, Validation Loss: 0.008288123796579125\n",
      "Epoch [5055/20000], Training Loss: 0.008859740716746143, Validation Loss: 0.0074908828355729285\n",
      "Epoch [5056/20000], Training Loss: 0.007023699764561441, Validation Loss: 0.005561571056830417\n",
      "Epoch [5057/20000], Training Loss: 0.006998601271854048, Validation Loss: 0.004839491321653051\n",
      "Epoch [5058/20000], Training Loss: 0.00829171179793775, Validation Loss: 0.007915287953564538\n",
      "Epoch [5059/20000], Training Loss: 0.006509350322416095, Validation Loss: 0.018202652406955046\n",
      "Epoch [5060/20000], Training Loss: 0.012937915120606444, Validation Loss: 0.0055674205907406205\n",
      "Epoch [5061/20000], Training Loss: 0.00986280597239134, Validation Loss: 0.004620034310040718\n",
      "Epoch [5062/20000], Training Loss: 0.008185216391471581, Validation Loss: 0.003357988530412724\n",
      "Epoch [5063/20000], Training Loss: 0.008726000470086806, Validation Loss: 0.005003064858209021\n",
      "Epoch [5064/20000], Training Loss: 0.006507842953266975, Validation Loss: 0.005142147432219778\n",
      "Epoch [5065/20000], Training Loss: 0.00537540944447252, Validation Loss: 0.003917288709437068\n",
      "Epoch [5066/20000], Training Loss: 0.0056867033261564105, Validation Loss: 0.002498755299834343\n",
      "Epoch [5067/20000], Training Loss: 0.00667195827064201, Validation Loss: 0.010237265251221697\n",
      "Epoch [5068/20000], Training Loss: 0.00887146225016165, Validation Loss: 0.009189521378046942\n",
      "Epoch [5069/20000], Training Loss: 0.01009042321987051, Validation Loss: 0.012973829170506221\n",
      "Epoch [5070/20000], Training Loss: 0.00785509524637616, Validation Loss: 0.012891549092132021\n",
      "Epoch [5071/20000], Training Loss: 0.008913813833454956, Validation Loss: 0.002839979875199461\n",
      "Epoch [5072/20000], Training Loss: 0.004239915329110643, Validation Loss: 0.004106122861383454\n",
      "Epoch [5073/20000], Training Loss: 0.003887494348288913, Validation Loss: 0.01250469126457945\n",
      "Epoch [5074/20000], Training Loss: 0.024137233640431077, Validation Loss: 0.011151190814680646\n",
      "Epoch [5075/20000], Training Loss: 0.06163044060979571, Validation Loss: 0.018987133748753746\n",
      "Epoch [5076/20000], Training Loss: 0.03166091764952788, Validation Loss: 0.018015288677093264\n",
      "Epoch [5077/20000], Training Loss: 0.015577788429384651, Validation Loss: 0.007129998053382484\n",
      "Epoch [5078/20000], Training Loss: 0.010226187720296107, Validation Loss: 0.0058959407454364865\n",
      "Epoch [5079/20000], Training Loss: 0.007914228065471565, Validation Loss: 0.008054307520848982\n",
      "Epoch [5080/20000], Training Loss: 0.007234313501352777, Validation Loss: 0.008475435856260967\n",
      "Epoch [5081/20000], Training Loss: 0.007240933840096529, Validation Loss: 0.006658758086684559\n",
      "Epoch [5082/20000], Training Loss: 0.006376061535515224, Validation Loss: 0.0045234493674278\n",
      "Epoch [5083/20000], Training Loss: 0.01269856431255383, Validation Loss: 0.009334259825664049\n",
      "Epoch [5084/20000], Training Loss: 0.009193573009854714, Validation Loss: 0.009690814849818707\n",
      "Epoch [5085/20000], Training Loss: 0.006670129705785907, Validation Loss: 0.011501551048078389\n",
      "Epoch [5086/20000], Training Loss: 0.005620741886689627, Validation Loss: 0.018856628220614118\n",
      "Epoch [5087/20000], Training Loss: 0.025642354861442333, Validation Loss: 0.005932923169471646\n",
      "Epoch [5088/20000], Training Loss: 0.0494321096801806, Validation Loss: 0.008730220166139125\n",
      "Epoch [5089/20000], Training Loss: 0.02503101810413812, Validation Loss: 0.008763357517247234\n",
      "Epoch [5090/20000], Training Loss: 0.022265992441264513, Validation Loss: 0.013714409604422842\n",
      "Epoch [5091/20000], Training Loss: 0.010832996831075954, Validation Loss: 0.00768303642709159\n",
      "Epoch [5092/20000], Training Loss: 0.012104539175717426, Validation Loss: 0.010992518120670472\n",
      "Epoch [5093/20000], Training Loss: 0.010806089771254588, Validation Loss: 0.012143344824705636\n",
      "Epoch [5094/20000], Training Loss: 0.01023307436012796, Validation Loss: 0.005295195137949011\n",
      "Epoch [5095/20000], Training Loss: 0.014560418195538142, Validation Loss: 0.004176912838090684\n",
      "Epoch [5096/20000], Training Loss: 0.011992976435327105, Validation Loss: 0.0074698764764226785\n",
      "Epoch [5097/20000], Training Loss: 0.010678192325940472, Validation Loss: 0.011103583837723873\n",
      "Epoch [5098/20000], Training Loss: 0.005562797738613361, Validation Loss: 0.023028871653880203\n",
      "Epoch [5099/20000], Training Loss: 0.045095393708574454, Validation Loss: 0.04703365159711298\n",
      "Epoch [5100/20000], Training Loss: 0.04081651542635102, Validation Loss: 0.05728353635549936\n",
      "Epoch [5101/20000], Training Loss: 0.031054160198047093, Validation Loss: 0.038932112931470635\n",
      "Epoch [5102/20000], Training Loss: 0.024516195740683804, Validation Loss: 0.009038919543984647\n",
      "Epoch [5103/20000], Training Loss: 0.00772813414265069, Validation Loss: 0.01037913728424503\n",
      "Epoch [5104/20000], Training Loss: 0.011007261741302241, Validation Loss: 0.03207868911359161\n",
      "Epoch [5105/20000], Training Loss: 0.03289136201731578, Validation Loss: 0.005591607882020558\n",
      "Epoch [5106/20000], Training Loss: 0.010790397284186579, Validation Loss: 0.007964151190488373\n",
      "Epoch [5107/20000], Training Loss: 0.03672373288204004, Validation Loss: 0.00647095806490537\n",
      "Epoch [5108/20000], Training Loss: 0.029195247280378162, Validation Loss: 0.03249199314450677\n",
      "Epoch [5109/20000], Training Loss: 0.02036102559525586, Validation Loss: 0.013525887530185026\n",
      "Epoch [5110/20000], Training Loss: 0.013594826165769649, Validation Loss: 0.013657408686722305\n",
      "Epoch [5111/20000], Training Loss: 0.009854667607994218, Validation Loss: 0.00781796089192507\n",
      "Epoch [5112/20000], Training Loss: 0.008571601722256414, Validation Loss: 0.004385036361276613\n",
      "Epoch [5113/20000], Training Loss: 0.01075031776729572, Validation Loss: 0.005193389303908954\n",
      "Epoch [5114/20000], Training Loss: 0.009152463506325148, Validation Loss: 0.008340016188607484\n",
      "Epoch [5115/20000], Training Loss: 0.01333576073830045, Validation Loss: 0.018396229076042152\n",
      "Epoch [5116/20000], Training Loss: 0.022936259154968348, Validation Loss: 0.01407273768510566\n",
      "Epoch [5117/20000], Training Loss: 0.008873209190954055, Validation Loss: 0.00439720234086611\n",
      "Epoch [5118/20000], Training Loss: 0.005457637662142848, Validation Loss: 0.00796541194965757\n",
      "Epoch [5119/20000], Training Loss: 0.007907526733885919, Validation Loss: 0.01918652438819232\n",
      "Epoch [5120/20000], Training Loss: 0.016476357879582793, Validation Loss: 0.014422886979453224\n",
      "Epoch [5121/20000], Training Loss: 0.013738540356696052, Validation Loss: 0.02514963258203975\n",
      "Epoch [5122/20000], Training Loss: 0.014373049368649455, Validation Loss: 0.008907561722474458\n",
      "Epoch [5123/20000], Training Loss: 0.011864188732683292, Validation Loss: 0.004000853956115163\n",
      "Epoch [5124/20000], Training Loss: 0.013234147505759861, Validation Loss: 0.0104482917700688\n",
      "Epoch [5125/20000], Training Loss: 0.014158357851556502, Validation Loss: 0.023681195086610245\n",
      "Epoch [5126/20000], Training Loss: 0.012950711220452962, Validation Loss: 0.00507520549422688\n",
      "Epoch [5127/20000], Training Loss: 0.011772619136276521, Validation Loss: 0.004475804132020254\n",
      "Epoch [5128/20000], Training Loss: 0.011853571964560874, Validation Loss: 0.009288777511946922\n",
      "Epoch [5129/20000], Training Loss: 0.007682782493897581, Validation Loss: 0.011019151699774257\n",
      "Epoch [5130/20000], Training Loss: 0.01182415404868412, Validation Loss: 0.016783671819764225\n",
      "Epoch [5131/20000], Training Loss: 0.014781412782180528, Validation Loss: 0.00429173895885731\n",
      "Epoch [5132/20000], Training Loss: 0.013174474318345477, Validation Loss: 0.004952110310800408\n",
      "Epoch [5133/20000], Training Loss: 0.012066459100294327, Validation Loss: 0.007291935189694448\n",
      "Epoch [5134/20000], Training Loss: 0.0069007468732057275, Validation Loss: 0.01428156849692844\n",
      "Epoch [5135/20000], Training Loss: 0.009067757886364396, Validation Loss: 0.01049168135398913\n",
      "Epoch [5136/20000], Training Loss: 0.007035902782393221, Validation Loss: 0.004333306310432558\n",
      "Epoch [5137/20000], Training Loss: 0.018108543029354354, Validation Loss: 0.004892346145230216\n",
      "Epoch [5138/20000], Training Loss: 0.010491393138571376, Validation Loss: 0.012125127363853091\n",
      "Epoch [5139/20000], Training Loss: 0.013719023845624179, Validation Loss: 0.015851040822262\n",
      "Epoch [5140/20000], Training Loss: 0.008983390152155022, Validation Loss: 0.00721981649348899\n",
      "Epoch [5141/20000], Training Loss: 0.007187239952535103, Validation Loss: 0.011170601205500535\n",
      "Epoch [5142/20000], Training Loss: 0.01363009119294085, Validation Loss: 0.00935355036956156\n",
      "Epoch [5143/20000], Training Loss: 0.025528158797117482, Validation Loss: 0.011522666835540079\n",
      "Epoch [5144/20000], Training Loss: 0.03872394357858866, Validation Loss: 0.03989579770697322\n",
      "Epoch [5145/20000], Training Loss: 0.028739609928119796, Validation Loss: 0.014022125467243957\n",
      "Epoch [5146/20000], Training Loss: 0.014603765826905146, Validation Loss: 0.008350622115155418\n",
      "Epoch [5147/20000], Training Loss: 0.008273000666771882, Validation Loss: 0.005646497938941357\n",
      "Epoch [5148/20000], Training Loss: 0.009033878685191407, Validation Loss: 0.003906489223540374\n",
      "Epoch [5149/20000], Training Loss: 0.01890004922045461, Validation Loss: 0.006332394574050136\n",
      "Epoch [5150/20000], Training Loss: 0.009860739477777056, Validation Loss: 0.00474643292028721\n",
      "Epoch [5151/20000], Training Loss: 0.012129519310422308, Validation Loss: 0.006270216999703759\n",
      "Epoch [5152/20000], Training Loss: 0.02082848417506154, Validation Loss: 0.020428934562979912\n",
      "Epoch [5153/20000], Training Loss: 0.020560982836676494, Validation Loss: 0.013286391690298391\n",
      "Epoch [5154/20000], Training Loss: 0.008984692860394716, Validation Loss: 0.00907854592001524\n",
      "Epoch [5155/20000], Training Loss: 0.007858051976654679, Validation Loss: 0.00730304129234557\n",
      "Epoch [5156/20000], Training Loss: 0.013386463860894895, Validation Loss: 0.021325656908483585\n",
      "Epoch [5157/20000], Training Loss: 0.012777949356989535, Validation Loss: 0.005649443281095147\n",
      "Epoch [5158/20000], Training Loss: 0.013085642839411906, Validation Loss: 0.008649995279007274\n",
      "Epoch [5159/20000], Training Loss: 0.009077592486586841, Validation Loss: 0.012337678416216564\n",
      "Epoch [5160/20000], Training Loss: 0.00820266819530957, Validation Loss: 0.005830786077717397\n",
      "Epoch [5161/20000], Training Loss: 0.004117069825692202, Validation Loss: 0.005183634642910796\n",
      "Epoch [5162/20000], Training Loss: 0.01245044252495323, Validation Loss: 0.012861014221714059\n",
      "Epoch [5163/20000], Training Loss: 0.008675605719742765, Validation Loss: 0.004945930812996142\n",
      "Epoch [5164/20000], Training Loss: 0.010612794380904234, Validation Loss: 0.003265555627692215\n",
      "Epoch [5165/20000], Training Loss: 0.0215040718716669, Validation Loss: 0.005922404782102042\n",
      "Epoch [5166/20000], Training Loss: 0.009934891394680432, Validation Loss: 0.0033322825389058763\n",
      "Epoch [5167/20000], Training Loss: 0.005663835730534369, Validation Loss: 0.009938963080323049\n",
      "Epoch [5168/20000], Training Loss: 0.019296630909853514, Validation Loss: 0.008215868045364592\n",
      "Epoch [5169/20000], Training Loss: 0.00988062132923265, Validation Loss: 0.006883465230874971\n",
      "Epoch [5170/20000], Training Loss: 0.005411265475101702, Validation Loss: 0.003357671458045632\n",
      "Epoch [5171/20000], Training Loss: 0.007523142736317823, Validation Loss: 0.01013474730009786\n",
      "Epoch [5172/20000], Training Loss: 0.022782062025466336, Validation Loss: 0.004798215116731492\n",
      "Epoch [5173/20000], Training Loss: 0.02324437273741101, Validation Loss: 0.012647638691760221\n",
      "Epoch [5174/20000], Training Loss: 0.02747429210493075, Validation Loss: 0.006262621689724528\n",
      "Epoch [5175/20000], Training Loss: 0.01898745042242516, Validation Loss: 0.03589697550537187\n",
      "Epoch [5176/20000], Training Loss: 0.019938097880055596, Validation Loss: 0.05156694532286806\n",
      "Epoch [5177/20000], Training Loss: 0.023674992199500724, Validation Loss: 0.024258497308624682\n",
      "Epoch [5178/20000], Training Loss: 0.021251368441880913, Validation Loss: 0.010135778827127203\n",
      "Epoch [5179/20000], Training Loss: 0.01067765848493948, Validation Loss: 0.007325144728838526\n",
      "Epoch [5180/20000], Training Loss: 0.006631792924183953, Validation Loss: 0.02224465094376973\n",
      "Epoch [5181/20000], Training Loss: 0.01214279321512939, Validation Loss: 0.004643526410869165\n",
      "Epoch [5182/20000], Training Loss: 0.006965569224640993, Validation Loss: 0.004508159483645168\n",
      "Epoch [5183/20000], Training Loss: 0.017134394767189014, Validation Loss: 0.006761558707382186\n",
      "Epoch [5184/20000], Training Loss: 0.02601295562947468, Validation Loss: 0.01417053302278859\n",
      "Epoch [5185/20000], Training Loss: 0.014653925143647939, Validation Loss: 0.006829517625553864\n",
      "Epoch [5186/20000], Training Loss: 0.007944193660348122, Validation Loss: 0.0083258083474409\n",
      "Epoch [5187/20000], Training Loss: 0.008420579656006209, Validation Loss: 0.012342934748383392\n",
      "Epoch [5188/20000], Training Loss: 0.014739336591446772, Validation Loss: 0.026769430697976413\n",
      "Epoch [5189/20000], Training Loss: 0.02240921224958064, Validation Loss: 0.02225094761557822\n",
      "Epoch [5190/20000], Training Loss: 0.024066532624211896, Validation Loss: 0.03255689885103203\n",
      "Epoch [5191/20000], Training Loss: 0.020752588706922585, Validation Loss: 0.01571624054448204\n",
      "Epoch [5192/20000], Training Loss: 0.016050637982386564, Validation Loss: 0.010949803674794981\n",
      "Epoch [5193/20000], Training Loss: 0.010923052447781498, Validation Loss: 0.01578631238113401\n",
      "Epoch [5194/20000], Training Loss: 0.017848334474755184, Validation Loss: 0.008914359960033978\n",
      "Epoch [5195/20000], Training Loss: 0.017751062646441693, Validation Loss: 0.007280046193274431\n",
      "Epoch [5196/20000], Training Loss: 0.01661629805208317, Validation Loss: 0.006062974612405329\n",
      "Epoch [5197/20000], Training Loss: 0.014053551995727633, Validation Loss: 0.024857468896829\n",
      "Epoch [5198/20000], Training Loss: 0.013113271682024268, Validation Loss: 0.007838818483483997\n",
      "Epoch [5199/20000], Training Loss: 0.010202626192559754, Validation Loss: 0.024637148756362835\n",
      "Epoch [5200/20000], Training Loss: 0.009543972647017134, Validation Loss: 0.005498310097531244\n",
      "Epoch [5201/20000], Training Loss: 0.0060274908776461545, Validation Loss: 0.008863386838379839\n",
      "Epoch [5202/20000], Training Loss: 0.010685572280116113, Validation Loss: 0.012067759731275465\n",
      "Epoch [5203/20000], Training Loss: 0.00954609757121716, Validation Loss: 0.012801407224767303\n",
      "Epoch [5204/20000], Training Loss: 0.01479427189042326, Validation Loss: 0.0039033059322589126\n",
      "Epoch [5205/20000], Training Loss: 0.00920666303552155, Validation Loss: 0.004019270290721292\n",
      "Epoch [5206/20000], Training Loss: 0.0035702343240180717, Validation Loss: 0.00644530972230084\n",
      "Epoch [5207/20000], Training Loss: 0.007915350223551545, Validation Loss: 0.008650362133209506\n",
      "Epoch [5208/20000], Training Loss: 0.012501547844570788, Validation Loss: 0.015153570263817886\n",
      "Epoch [5209/20000], Training Loss: 0.00736561036371443, Validation Loss: 0.0034395741254765583\n",
      "Epoch [5210/20000], Training Loss: 0.006349037288081101, Validation Loss: 0.029325281402911556\n",
      "Epoch [5211/20000], Training Loss: 0.011170915533203438, Validation Loss: 0.004131333817606934\n",
      "Epoch [5212/20000], Training Loss: 0.008626855070360762, Validation Loss: 0.015738646633388198\n",
      "Epoch [5213/20000], Training Loss: 0.010101456970149922, Validation Loss: 0.004904647906551318\n",
      "Epoch [5214/20000], Training Loss: 0.015436867459876729, Validation Loss: 0.008977554813215907\n",
      "Epoch [5215/20000], Training Loss: 0.012781773429326546, Validation Loss: 0.022022790155821998\n",
      "Epoch [5216/20000], Training Loss: 0.017671727037980287, Validation Loss: 0.005685957062088219\n",
      "Epoch [5217/20000], Training Loss: 0.0063316450272059245, Validation Loss: 0.00868726279742736\n",
      "Epoch [5218/20000], Training Loss: 0.007423501962745961, Validation Loss: 0.026568564461163315\n",
      "Epoch [5219/20000], Training Loss: 0.013004184213929968, Validation Loss: 0.005007028597349393\n",
      "Epoch [5220/20000], Training Loss: 0.009070681968556269, Validation Loss: 0.004138438057149228\n",
      "Epoch [5221/20000], Training Loss: 0.008885552958547902, Validation Loss: 0.004508061186662867\n",
      "Epoch [5222/20000], Training Loss: 0.006404460308067834, Validation Loss: 0.004117452563579078\n",
      "Epoch [5223/20000], Training Loss: 0.006290810438671889, Validation Loss: 0.005282853017988539\n",
      "Epoch [5224/20000], Training Loss: 0.004977347782666389, Validation Loss: 0.003542899352472884\n",
      "Epoch [5225/20000], Training Loss: 0.010856682187295519, Validation Loss: 0.015014072018958451\n",
      "Epoch [5226/20000], Training Loss: 0.02752496785485294, Validation Loss: 0.052676909204040445\n",
      "Epoch [5227/20000], Training Loss: 0.016028771954422285, Validation Loss: 0.017386783448260553\n",
      "Epoch [5228/20000], Training Loss: 0.022967709151479148, Validation Loss: 0.010478309680592459\n",
      "Epoch [5229/20000], Training Loss: 0.02217249879114596, Validation Loss: 0.010469604235514194\n",
      "Epoch [5230/20000], Training Loss: 0.0303128901702751, Validation Loss: 0.013108636273910028\n",
      "Epoch [5231/20000], Training Loss: 0.014032280635676995, Validation Loss: 0.01028016700771214\n",
      "Epoch [5232/20000], Training Loss: 0.012604033595770099, Validation Loss: 0.006570518752076688\n",
      "Epoch [5233/20000], Training Loss: 0.011313434210023843, Validation Loss: 0.008014263796342545\n",
      "Epoch [5234/20000], Training Loss: 0.00789687261151682, Validation Loss: 0.005365290262992468\n",
      "Epoch [5235/20000], Training Loss: 0.0068142607092990404, Validation Loss: 0.007258034773366363\n",
      "Epoch [5236/20000], Training Loss: 0.00591342441319804, Validation Loss: 0.008707241573932021\n",
      "Epoch [5237/20000], Training Loss: 0.012168884094405388, Validation Loss: 0.00452713864379969\n",
      "Epoch [5238/20000], Training Loss: 0.038230324197294455, Validation Loss: 0.006252290274322344\n",
      "Epoch [5239/20000], Training Loss: 0.01728452929611584, Validation Loss: 0.005738892069692676\n",
      "Epoch [5240/20000], Training Loss: 0.007579352183632133, Validation Loss: 0.016197502020938108\n",
      "Epoch [5241/20000], Training Loss: 0.01047648000339255, Validation Loss: 0.015439678288691892\n",
      "Epoch [5242/20000], Training Loss: 0.011995220980939589, Validation Loss: 0.0065699976024851435\n",
      "Epoch [5243/20000], Training Loss: 0.006481687492591196, Validation Loss: 0.00661766373494694\n",
      "Epoch [5244/20000], Training Loss: 0.0073148723479659694, Validation Loss: 0.005416094524432781\n",
      "Epoch [5245/20000], Training Loss: 0.0062532832047769, Validation Loss: 0.004433524885424082\n",
      "Epoch [5246/20000], Training Loss: 0.0064712738234707756, Validation Loss: 0.007761526563302011\n",
      "Epoch [5247/20000], Training Loss: 0.020981962730827166, Validation Loss: 0.025199709347296966\n",
      "Epoch [5248/20000], Training Loss: 0.02223361112692146, Validation Loss: 0.015276740220412779\n",
      "Epoch [5249/20000], Training Loss: 0.019975045938768483, Validation Loss: 0.005354979711093222\n",
      "Epoch [5250/20000], Training Loss: 0.008979886461539925, Validation Loss: 0.008359201095449418\n",
      "Epoch [5251/20000], Training Loss: 0.007278844780687775, Validation Loss: 0.007542951307607896\n",
      "Epoch [5252/20000], Training Loss: 0.01779944731164765, Validation Loss: 0.034800755126646175\n",
      "Epoch [5253/20000], Training Loss: 0.018794508300613546, Validation Loss: 0.02299492912669122\n",
      "Epoch [5254/20000], Training Loss: 0.017030730071253077, Validation Loss: 0.004139890793005918\n",
      "Epoch [5255/20000], Training Loss: 0.006937934787336937, Validation Loss: 0.00644257282978298\n",
      "Epoch [5256/20000], Training Loss: 0.00595370901802588, Validation Loss: 0.0029668677005538053\n",
      "Epoch [5257/20000], Training Loss: 0.007919517806840304, Validation Loss: 0.005862211578694455\n",
      "Epoch [5258/20000], Training Loss: 0.007320529115661755, Validation Loss: 0.005353996187903094\n",
      "Epoch [5259/20000], Training Loss: 0.003534458326872222, Validation Loss: 0.010733643815802791\n",
      "Epoch [5260/20000], Training Loss: 0.011137505913126742, Validation Loss: 0.02284913137638351\n",
      "Epoch [5261/20000], Training Loss: 0.01901845739173171, Validation Loss: 0.04096624361929765\n",
      "Epoch [5262/20000], Training Loss: 0.021067585432579757, Validation Loss: 0.01214414369073223\n",
      "Epoch [5263/20000], Training Loss: 0.009893027649793242, Validation Loss: 0.0045631723144562595\n",
      "Epoch [5264/20000], Training Loss: 0.005358256427306125, Validation Loss: 0.00505802294268116\n",
      "Epoch [5265/20000], Training Loss: 0.005640407398069718, Validation Loss: 0.005626069808633767\n",
      "Epoch [5266/20000], Training Loss: 0.006179553531442902, Validation Loss: 0.005982471007437239\n",
      "Epoch [5267/20000], Training Loss: 0.005722208078493297, Validation Loss: 0.0037957506262316087\n",
      "Epoch [5268/20000], Training Loss: 0.004659601643132711, Validation Loss: 0.016233336655493043\n",
      "Epoch [5269/20000], Training Loss: 0.023469326000736146, Validation Loss: 0.008593437825490064\n",
      "Epoch [5270/20000], Training Loss: 0.021772065934783313, Validation Loss: 0.07323028888069195\n",
      "Epoch [5271/20000], Training Loss: 0.04512154860848178, Validation Loss: 0.0398918788170239\n",
      "Epoch [5272/20000], Training Loss: 0.028254748245152377, Validation Loss: 0.05909196300075111\n",
      "Epoch [5273/20000], Training Loss: 0.01802590912640361, Validation Loss: 0.010944170954766932\n",
      "Epoch [5274/20000], Training Loss: 0.009241847354652626, Validation Loss: 0.006769177772558481\n",
      "Epoch [5275/20000], Training Loss: 0.006919533084978217, Validation Loss: 0.006528622169124059\n",
      "Epoch [5276/20000], Training Loss: 0.0066445623420544764, Validation Loss: 0.006626370487436735\n",
      "Epoch [5277/20000], Training Loss: 0.005667248521474123, Validation Loss: 0.00860378814201927\n",
      "Epoch [5278/20000], Training Loss: 0.009730794362569992, Validation Loss: 0.003952981618068314\n",
      "Epoch [5279/20000], Training Loss: 0.005579716760881378, Validation Loss: 0.00651049425639881\n",
      "Epoch [5280/20000], Training Loss: 0.007221766781835218, Validation Loss: 0.004536213713306481\n",
      "Epoch [5281/20000], Training Loss: 0.02991000857686491, Validation Loss: 0.005540179520681122\n",
      "Epoch [5282/20000], Training Loss: 0.016849948517379483, Validation Loss: 0.006556747386960234\n",
      "Epoch [5283/20000], Training Loss: 0.008932672956559275, Validation Loss: 0.005947738143606607\n",
      "Epoch [5284/20000], Training Loss: 0.008665164713290454, Validation Loss: 0.005668020802606375\n",
      "Epoch [5285/20000], Training Loss: 0.015032191412631488, Validation Loss: 0.020208223497266475\n",
      "Epoch [5286/20000], Training Loss: 0.010112165851101704, Validation Loss: 0.006035719766689596\n",
      "Epoch [5287/20000], Training Loss: 0.008200393185688881, Validation Loss: 0.007806837906712613\n",
      "Epoch [5288/20000], Training Loss: 0.010749862710196924, Validation Loss: 0.006913740348398534\n",
      "Epoch [5289/20000], Training Loss: 0.005862785400037548, Validation Loss: 0.03139562372928926\n",
      "Epoch [5290/20000], Training Loss: 0.015392055374832125, Validation Loss: 0.036263560623817206\n",
      "Epoch [5291/20000], Training Loss: 0.0219387666741438, Validation Loss: 0.015225510975001213\n",
      "Epoch [5292/20000], Training Loss: 0.01990591747952359, Validation Loss: 0.037487082254925204\n",
      "Epoch [5293/20000], Training Loss: 0.024681930588225702, Validation Loss: 0.010290728122116224\n",
      "Epoch [5294/20000], Training Loss: 0.0366610226940663, Validation Loss: 0.03528032815250177\n",
      "Epoch [5295/20000], Training Loss: 0.016566784477747985, Validation Loss: 0.008553513068493793\n",
      "Epoch [5296/20000], Training Loss: 0.011815551734928573, Validation Loss: 0.008544120124822393\n",
      "Epoch [5297/20000], Training Loss: 0.010393822100013494, Validation Loss: 0.023302244966529977\n",
      "Epoch [5298/20000], Training Loss: 0.008939441312187617, Validation Loss: 0.006075089463172034\n",
      "Epoch [5299/20000], Training Loss: 0.012419116899504192, Validation Loss: 0.008470478931282546\n",
      "Epoch [5300/20000], Training Loss: 0.008123413700169684, Validation Loss: 0.005920150433179775\n",
      "Epoch [5301/20000], Training Loss: 0.009281035538736302, Validation Loss: 0.013814846739933273\n",
      "Epoch [5302/20000], Training Loss: 0.01738367715032447, Validation Loss: 0.006888306293343963\n",
      "Epoch [5303/20000], Training Loss: 0.014681217501804764, Validation Loss: 0.01657536646753702\n",
      "Epoch [5304/20000], Training Loss: 0.01701822891644968, Validation Loss: 0.008091047204018003\n",
      "Epoch [5305/20000], Training Loss: 0.014278041597987925, Validation Loss: 0.00461249748463339\n",
      "Epoch [5306/20000], Training Loss: 0.008996689847533292, Validation Loss: 0.013072947219963913\n",
      "Epoch [5307/20000], Training Loss: 0.005504654111953412, Validation Loss: 0.003192273106235583\n",
      "Epoch [5308/20000], Training Loss: 0.0064108155156564605, Validation Loss: 0.004396923041924009\n",
      "Epoch [5309/20000], Training Loss: 0.006707171802970281, Validation Loss: 0.011059845083035416\n",
      "Epoch [5310/20000], Training Loss: 0.012120715700023408, Validation Loss: 0.01146165120358741\n",
      "Epoch [5311/20000], Training Loss: 0.010811224546549576, Validation Loss: 0.006127651794119525\n",
      "Epoch [5312/20000], Training Loss: 0.005490174865371955, Validation Loss: 0.023606173161978973\n",
      "Epoch [5313/20000], Training Loss: 0.020114259663127347, Validation Loss: 0.00832614813225214\n",
      "Epoch [5314/20000], Training Loss: 0.014833848807029426, Validation Loss: 0.007439261672872328\n",
      "Epoch [5315/20000], Training Loss: 0.012274487538434187, Validation Loss: 0.00593149193596787\n",
      "Epoch [5316/20000], Training Loss: 0.03344189563540567, Validation Loss: 0.020558672793007254\n",
      "Epoch [5317/20000], Training Loss: 0.015887368455634845, Validation Loss: 0.014371782522861531\n",
      "Epoch [5318/20000], Training Loss: 0.00961297786644926, Validation Loss: 0.010412764585388106\n",
      "Epoch [5319/20000], Training Loss: 0.010711597162298858, Validation Loss: 0.012692208534288176\n",
      "Epoch [5320/20000], Training Loss: 0.012563365634767771, Validation Loss: 0.020979561811770457\n",
      "Epoch [5321/20000], Training Loss: 0.010805251492846375, Validation Loss: 0.008729660961453842\n",
      "Epoch [5322/20000], Training Loss: 0.010046553719023774, Validation Loss: 0.0088897172800476\n",
      "Epoch [5323/20000], Training Loss: 0.008604706589332116, Validation Loss: 0.0050463397713299074\n",
      "Epoch [5324/20000], Training Loss: 0.007274406820735229, Validation Loss: 0.005280939976791339\n",
      "Epoch [5325/20000], Training Loss: 0.006307100600679405, Validation Loss: 0.005336059691372709\n",
      "Epoch [5326/20000], Training Loss: 0.005840277549785762, Validation Loss: 0.0031538642681308965\n",
      "Epoch [5327/20000], Training Loss: 0.0076995785526118455, Validation Loss: 0.0038762857366521075\n",
      "Epoch [5328/20000], Training Loss: 0.007361784298804456, Validation Loss: 0.0038372265700065294\n",
      "Epoch [5329/20000], Training Loss: 0.012380875846637147, Validation Loss: 0.004181127112087417\n",
      "Epoch [5330/20000], Training Loss: 0.005156856420009197, Validation Loss: 0.0039720054234305534\n",
      "Epoch [5331/20000], Training Loss: 0.00772570190643559, Validation Loss: 0.016228376754843416\n",
      "Epoch [5332/20000], Training Loss: 0.008781257118763668, Validation Loss: 0.009287853593985634\n",
      "Epoch [5333/20000], Training Loss: 0.007345623621534157, Validation Loss: 0.005648614906905189\n",
      "Epoch [5334/20000], Training Loss: 0.005611909858738987, Validation Loss: 0.011193497968918109\n",
      "Epoch [5335/20000], Training Loss: 0.012488139809907548, Validation Loss: 0.0121002970266101\n",
      "Epoch [5336/20000], Training Loss: 0.03385968504354813, Validation Loss: 0.02812195835019793\n",
      "Epoch [5337/20000], Training Loss: 0.03242925391357078, Validation Loss: 0.025263704625632072\n",
      "Epoch [5338/20000], Training Loss: 0.025596120935266038, Validation Loss: 0.017885858867318496\n",
      "Epoch [5339/20000], Training Loss: 0.0076813079857467005, Validation Loss: 0.005858338437462862\n",
      "Epoch [5340/20000], Training Loss: 0.0067681998396957556, Validation Loss: 0.005471469780246707\n",
      "Epoch [5341/20000], Training Loss: 0.00525572995476458, Validation Loss: 0.011833009316462957\n",
      "Epoch [5342/20000], Training Loss: 0.007296791522484065, Validation Loss: 0.009070975908461574\n",
      "Epoch [5343/20000], Training Loss: 0.0070222548425330645, Validation Loss: 0.003985829443840723\n",
      "Epoch [5344/20000], Training Loss: 0.010866750030345429, Validation Loss: 0.00426283535457309\n",
      "Epoch [5345/20000], Training Loss: 0.011201381387893759, Validation Loss: 0.018605021759883662\n",
      "Epoch [5346/20000], Training Loss: 0.0086579740628777, Validation Loss: 0.005151315850010258\n",
      "Epoch [5347/20000], Training Loss: 0.011765824850174664, Validation Loss: 0.018224500651898123\n",
      "Epoch [5348/20000], Training Loss: 0.007687489748994787, Validation Loss: 0.007008119553326686\n",
      "Epoch [5349/20000], Training Loss: 0.005028278260490586, Validation Loss: 0.0040072170973518345\n",
      "Epoch [5350/20000], Training Loss: 0.005502187765029832, Validation Loss: 0.004187935997589256\n",
      "Epoch [5351/20000], Training Loss: 0.006832651413528116, Validation Loss: 0.004893967083738575\n",
      "Epoch [5352/20000], Training Loss: 0.009940175467428551, Validation Loss: 0.01078378648630943\n",
      "Epoch [5353/20000], Training Loss: 0.007538298075814964, Validation Loss: 0.004213817887251686\n",
      "Epoch [5354/20000], Training Loss: 0.010681228093484865, Validation Loss: 0.05799591541290287\n",
      "Epoch [5355/20000], Training Loss: 0.021420016741363464, Validation Loss: 0.01763092673253453\n",
      "Epoch [5356/20000], Training Loss: 0.035294404077441346, Validation Loss: 0.03703024435630401\n",
      "Epoch [5357/20000], Training Loss: 0.0796240542923832, Validation Loss: 0.0948313482896366\n",
      "Epoch [5358/20000], Training Loss: 0.03611082957026416, Validation Loss: 0.013321298299097933\n",
      "Epoch [5359/20000], Training Loss: 0.021987014732855772, Validation Loss: 0.011860137431679699\n",
      "Epoch [5360/20000], Training Loss: 0.031211793223129853, Validation Loss: 0.015134089763578771\n",
      "Epoch [5361/20000], Training Loss: 0.009955134292665337, Validation Loss: 0.009579308838250913\n",
      "Epoch [5362/20000], Training Loss: 0.00866840295826218, Validation Loss: 0.015842329582304804\n",
      "Epoch [5363/20000], Training Loss: 0.008203557226806879, Validation Loss: 0.0067207902814386345\n",
      "Epoch [5364/20000], Training Loss: 0.006144848456772577, Validation Loss: 0.008464406634435104\n",
      "Epoch [5365/20000], Training Loss: 0.00546435293758155, Validation Loss: 0.010484422586809419\n",
      "Epoch [5366/20000], Training Loss: 0.01224143268440717, Validation Loss: 0.006726808353383669\n",
      "Epoch [5367/20000], Training Loss: 0.00733056577363251, Validation Loss: 0.0034457859423550224\n",
      "Epoch [5368/20000], Training Loss: 0.00485850015372437, Validation Loss: 0.006579302267506851\n",
      "Epoch [5369/20000], Training Loss: 0.010152246352455612, Validation Loss: 0.020490490613935566\n",
      "Epoch [5370/20000], Training Loss: 0.01628979032143921, Validation Loss: 0.010319718917819338\n",
      "Epoch [5371/20000], Training Loss: 0.009961781971339536, Validation Loss: 0.008083929043453151\n",
      "Epoch [5372/20000], Training Loss: 0.014775030994704659, Validation Loss: 0.004149917621418808\n",
      "Epoch [5373/20000], Training Loss: 0.020087452126192926, Validation Loss: 0.03779265443388015\n",
      "Epoch [5374/20000], Training Loss: 0.028066187764384916, Validation Loss: 0.0255415427007912\n",
      "Epoch [5375/20000], Training Loss: 0.018339136785860837, Validation Loss: 0.020147293281947247\n",
      "Epoch [5376/20000], Training Loss: 0.01300080567878987, Validation Loss: 0.008213235153172198\n",
      "Epoch [5377/20000], Training Loss: 0.009547473536388549, Validation Loss: 0.01208104465753682\n",
      "Epoch [5378/20000], Training Loss: 0.00998739134229254, Validation Loss: 0.00785605810818067\n",
      "Epoch [5379/20000], Training Loss: 0.006368044410399827, Validation Loss: 0.00524729448060108\n",
      "Epoch [5380/20000], Training Loss: 0.006369899438660858, Validation Loss: 0.016709060083875165\n",
      "Epoch [5381/20000], Training Loss: 0.013642809967028111, Validation Loss: 0.0053105210915548795\n",
      "Epoch [5382/20000], Training Loss: 0.010255717527927897, Validation Loss: 0.005637686486547214\n",
      "Epoch [5383/20000], Training Loss: 0.0085150915775947, Validation Loss: 0.00830775127292327\n",
      "Epoch [5384/20000], Training Loss: 0.012798401285960739, Validation Loss: 0.0047936117419859715\n",
      "Epoch [5385/20000], Training Loss: 0.005585702821138382, Validation Loss: 0.008515942016219111\n",
      "Epoch [5386/20000], Training Loss: 0.006472686625784263, Validation Loss: 0.004140932519313927\n",
      "Epoch [5387/20000], Training Loss: 0.005408070885875661, Validation Loss: 0.004209966338196734\n",
      "Epoch [5388/20000], Training Loss: 0.00819395884172991, Validation Loss: 0.008590065290259677\n",
      "Epoch [5389/20000], Training Loss: 0.01112491249140086, Validation Loss: 0.012920545031925634\n",
      "Epoch [5390/20000], Training Loss: 0.010594541720528079, Validation Loss: 0.022226538272053835\n",
      "Epoch [5391/20000], Training Loss: 0.021116342363321956, Validation Loss: 0.013374907005364938\n",
      "Epoch [5392/20000], Training Loss: 0.01248550834965759, Validation Loss: 0.01619387461406368\n",
      "Epoch [5393/20000], Training Loss: 0.008112284901601794, Validation Loss: 0.005251309353285803\n",
      "Epoch [5394/20000], Training Loss: 0.018854230696368695, Validation Loss: 0.010551734585602546\n",
      "Epoch [5395/20000], Training Loss: 0.01842310550273396, Validation Loss: 0.007350858616228655\n",
      "Epoch [5396/20000], Training Loss: 0.021815698656220257, Validation Loss: 0.010554940223909983\n",
      "Epoch [5397/20000], Training Loss: 0.015570000248642048, Validation Loss: 0.014276837980968285\n",
      "Epoch [5398/20000], Training Loss: 0.013456306137543703, Validation Loss: 0.02771895870621068\n",
      "Epoch [5399/20000], Training Loss: 0.01966081716402966, Validation Loss: 0.023833026556287353\n",
      "Epoch [5400/20000], Training Loss: 0.016547109657299837, Validation Loss: 0.008645280195902874\n",
      "Epoch [5401/20000], Training Loss: 0.01265791553721231, Validation Loss: 0.026808585242240963\n",
      "Epoch [5402/20000], Training Loss: 0.018486288567406535, Validation Loss: 0.029690001167651744\n",
      "Epoch [5403/20000], Training Loss: 0.015579513901944406, Validation Loss: 0.00950586896483076\n",
      "Epoch [5404/20000], Training Loss: 0.015609904840987707, Validation Loss: 0.010578888772280541\n",
      "Epoch [5405/20000], Training Loss: 0.014131967541678543, Validation Loss: 0.008357763677366847\n",
      "Epoch [5406/20000], Training Loss: 0.008220125736410669, Validation Loss: 0.003692713129495928\n",
      "Epoch [5407/20000], Training Loss: 0.005853033440611658, Validation Loss: 0.004941737341985711\n",
      "Epoch [5408/20000], Training Loss: 0.0057204316555247, Validation Loss: 0.009937020624391835\n",
      "Epoch [5409/20000], Training Loss: 0.010002601138304661, Validation Loss: 0.020370726297695034\n",
      "Epoch [5410/20000], Training Loss: 0.01169619632336045, Validation Loss: 0.004756343262176951\n",
      "Epoch [5411/20000], Training Loss: 0.008340702090728363, Validation Loss: 0.020905843176580317\n",
      "Epoch [5412/20000], Training Loss: 0.01180511433787095, Validation Loss: 0.004690983014653126\n",
      "Epoch [5413/20000], Training Loss: 0.008112995488253156, Validation Loss: 0.004804308547798516\n",
      "Epoch [5414/20000], Training Loss: 0.011100841968852495, Validation Loss: 0.003698812626541021\n",
      "Epoch [5415/20000], Training Loss: 0.012452823244530009, Validation Loss: 0.006036014551121127\n",
      "Epoch [5416/20000], Training Loss: 0.006822947177819775, Validation Loss: 0.0040561865045976985\n",
      "Epoch [5417/20000], Training Loss: 0.006859146555923091, Validation Loss: 0.006645461666032938\n",
      "Epoch [5418/20000], Training Loss: 0.007043839507137558, Validation Loss: 0.005568296724444873\n",
      "Epoch [5419/20000], Training Loss: 0.033023719287094924, Validation Loss: 0.006633210090812065\n",
      "Epoch [5420/20000], Training Loss: 0.06164156461006444, Validation Loss: 0.01159050669593853\n",
      "Epoch [5421/20000], Training Loss: 0.021947412669922933, Validation Loss: 0.00710726623323613\n",
      "Epoch [5422/20000], Training Loss: 0.011981358909646847, Validation Loss: 0.013834530533391185\n",
      "Epoch [5423/20000], Training Loss: 0.013158166689598667, Validation Loss: 0.007575008323848513\n",
      "Epoch [5424/20000], Training Loss: 0.009486123620133315, Validation Loss: 0.00839600145575657\n",
      "Epoch [5425/20000], Training Loss: 0.006651232195768638, Validation Loss: 0.006286810453665177\n",
      "Epoch [5426/20000], Training Loss: 0.005895172807088654, Validation Loss: 0.005633587128873582\n",
      "Epoch [5427/20000], Training Loss: 0.007900933599947686, Validation Loss: 0.008675675579330086\n",
      "Epoch [5428/20000], Training Loss: 0.02155074409633276, Validation Loss: 0.017703830237483738\n",
      "Epoch [5429/20000], Training Loss: 0.021279839624185115, Validation Loss: 0.013239844988025393\n",
      "Epoch [5430/20000], Training Loss: 0.014272842668495806, Validation Loss: 0.011734454577630718\n",
      "Epoch [5431/20000], Training Loss: 0.01206481414348153, Validation Loss: 0.005406486696821438\n",
      "Epoch [5432/20000], Training Loss: 0.008953631581643893, Validation Loss: 0.005511256226912208\n",
      "Epoch [5433/20000], Training Loss: 0.007360761626581994, Validation Loss: 0.0052951959566728745\n",
      "Epoch [5434/20000], Training Loss: 0.005793892285769938, Validation Loss: 0.0046120334996625646\n",
      "Epoch [5435/20000], Training Loss: 0.005120099127192849, Validation Loss: 0.0046039087839225045\n",
      "Epoch [5436/20000], Training Loss: 0.006575794320919418, Validation Loss: 0.003961413322810715\n",
      "Epoch [5437/20000], Training Loss: 0.005286345407772127, Validation Loss: 0.003948121992185146\n",
      "Epoch [5438/20000], Training Loss: 0.01248244653522436, Validation Loss: 0.0040454548813743585\n",
      "Epoch [5439/20000], Training Loss: 0.011744723842379503, Validation Loss: 0.00606918005515971\n",
      "Epoch [5440/20000], Training Loss: 0.005422856928946983, Validation Loss: 0.027070101444353765\n",
      "Epoch [5441/20000], Training Loss: 0.009708625372565751, Validation Loss: 0.004603064791548864\n",
      "Epoch [5442/20000], Training Loss: 0.007318452765632953, Validation Loss: 0.004854647119957788\n",
      "Epoch [5443/20000], Training Loss: 0.005966047504046216, Validation Loss: 0.0035036396891103194\n",
      "Epoch [5444/20000], Training Loss: 0.005766520913052123, Validation Loss: 0.003635571467124724\n",
      "Epoch [5445/20000], Training Loss: 0.005890326934084962, Validation Loss: 0.0037825076875118873\n",
      "Epoch [5446/20000], Training Loss: 0.006434250298687922, Validation Loss: 0.0169031336180134\n",
      "Epoch [5447/20000], Training Loss: 0.012456234893241864, Validation Loss: 0.009339953880823617\n",
      "Epoch [5448/20000], Training Loss: 0.019623842227572044, Validation Loss: 0.0100307534773622\n",
      "Epoch [5449/20000], Training Loss: 0.012663709565198846, Validation Loss: 0.004452008633961668\n",
      "Epoch [5450/20000], Training Loss: 0.016120784324552266, Validation Loss: 0.024532271026412256\n",
      "Epoch [5451/20000], Training Loss: 0.00999789479822409, Validation Loss: 0.009650015626968005\n",
      "Epoch [5452/20000], Training Loss: 0.010171686384377867, Validation Loss: 0.0095688536324425\n",
      "Epoch [5453/20000], Training Loss: 0.008866965404844709, Validation Loss: 0.008923865092063872\n",
      "Epoch [5454/20000], Training Loss: 0.006618521336349659, Validation Loss: 0.005613854220168183\n",
      "Epoch [5455/20000], Training Loss: 0.008479398593148548, Validation Loss: 0.0035918062151592623\n",
      "Epoch [5456/20000], Training Loss: 0.009663947891177875, Validation Loss: 0.0031437478508970734\n",
      "Epoch [5457/20000], Training Loss: 0.005445400338170917, Validation Loss: 0.017117957705530835\n",
      "Epoch [5458/20000], Training Loss: 0.015167392137560196, Validation Loss: 0.022986093919396718\n",
      "Epoch [5459/20000], Training Loss: 0.00716346622207098, Validation Loss: 0.0033574705531853916\n",
      "Epoch [5460/20000], Training Loss: 0.011775795160376998, Validation Loss: 0.04792130525698641\n",
      "Epoch [5461/20000], Training Loss: 0.052281916219206845, Validation Loss: 0.0920655709788077\n",
      "Epoch [5462/20000], Training Loss: 0.036024957836031844, Validation Loss: 0.005450069531434565\n",
      "Epoch [5463/20000], Training Loss: 0.011932751496455498, Validation Loss: 0.012621293688425794\n",
      "Epoch [5464/20000], Training Loss: 0.027069465947403972, Validation Loss: 0.041461758274822715\n",
      "Epoch [5465/20000], Training Loss: 0.02708539256959089, Validation Loss: 0.012136920493763423\n",
      "Epoch [5466/20000], Training Loss: 0.017298369922043224, Validation Loss: 0.03612156501136659\n",
      "Epoch [5467/20000], Training Loss: 0.014053835774705346, Validation Loss: 0.009973160672026487\n",
      "Epoch [5468/20000], Training Loss: 0.00828010908350864, Validation Loss: 0.006965163048984583\n",
      "Epoch [5469/20000], Training Loss: 0.00866098334725913, Validation Loss: 0.0070929880473288775\n",
      "Epoch [5470/20000], Training Loss: 0.007274480091707248, Validation Loss: 0.007930527729123631\n",
      "Epoch [5471/20000], Training Loss: 0.0088167591394657, Validation Loss: 0.005571634048205202\n",
      "Epoch [5472/20000], Training Loss: 0.006160053777940837, Validation Loss: 0.005677562111876406\n",
      "Epoch [5473/20000], Training Loss: 0.007054984390768888, Validation Loss: 0.004253745486494154\n",
      "Epoch [5474/20000], Training Loss: 0.005327056419836091, Validation Loss: 0.006561746503004022\n",
      "Epoch [5475/20000], Training Loss: 0.006087215687979811, Validation Loss: 0.0048881200308252505\n",
      "Epoch [5476/20000], Training Loss: 0.009935416295775212, Validation Loss: 0.00562009303368021\n",
      "Epoch [5477/20000], Training Loss: 0.0209532906550781, Validation Loss: 0.06597145427544351\n",
      "Epoch [5478/20000], Training Loss: 0.031358706968603656, Validation Loss: 0.013570688909957036\n",
      "Epoch [5479/20000], Training Loss: 0.008701643187253336, Validation Loss: 0.005778064140551058\n",
      "Epoch [5480/20000], Training Loss: 0.013589437360808785, Validation Loss: 0.009471850926274783\n",
      "Epoch [5481/20000], Training Loss: 0.010105980996740982, Validation Loss: 0.010429101563581915\n",
      "Epoch [5482/20000], Training Loss: 0.010504899226361886, Validation Loss: 0.006012118875267853\n",
      "Epoch [5483/20000], Training Loss: 0.012342275463327366, Validation Loss: 0.009315794570803908\n",
      "Epoch [5484/20000], Training Loss: 0.005410344628866629, Validation Loss: 0.0062391807550089395\n",
      "Epoch [5485/20000], Training Loss: 0.006698678964832132, Validation Loss: 0.0126769434562805\n",
      "Epoch [5486/20000], Training Loss: 0.008073954036392803, Validation Loss: 0.005489223195776763\n",
      "Epoch [5487/20000], Training Loss: 0.005123678165806008, Validation Loss: 0.011637471071334662\n",
      "Epoch [5488/20000], Training Loss: 0.007480043986756105, Validation Loss: 0.0031504353802509805\n",
      "Epoch [5489/20000], Training Loss: 0.009732576122457561, Validation Loss: 0.024660082149629097\n",
      "Epoch [5490/20000], Training Loss: 0.01678753544131593, Validation Loss: 0.015509263745895754\n",
      "Epoch [5491/20000], Training Loss: 0.019713153222775354, Validation Loss: 0.008608586430974745\n",
      "Epoch [5492/20000], Training Loss: 0.016321102593792602, Validation Loss: 0.014978425500852868\n",
      "Epoch [5493/20000], Training Loss: 0.006700821167864238, Validation Loss: 0.0078521014308665\n",
      "Epoch [5494/20000], Training Loss: 0.006401922961945924, Validation Loss: 0.011235603617838106\n",
      "Epoch [5495/20000], Training Loss: 0.014577892060125512, Validation Loss: 0.05663798321194845\n",
      "Epoch [5496/20000], Training Loss: 0.05010721759494378, Validation Loss: 0.10140162280627667\n",
      "Epoch [5497/20000], Training Loss: 0.051983514368267994, Validation Loss: 0.015000151883279094\n",
      "Epoch [5498/20000], Training Loss: 0.007689693177651081, Validation Loss: 0.007172496906735303\n",
      "Epoch [5499/20000], Training Loss: 0.007855532387623312, Validation Loss: 0.006422984724330101\n",
      "Epoch [5500/20000], Training Loss: 0.008428864819247142, Validation Loss: 0.007388010252010905\n",
      "Epoch [5501/20000], Training Loss: 0.0071489035472041, Validation Loss: 0.008383021999179618\n",
      "Epoch [5502/20000], Training Loss: 0.00503855225958562, Validation Loss: 0.006520530021069655\n",
      "Epoch [5503/20000], Training Loss: 0.009665893307620925, Validation Loss: 0.005102955403929498\n",
      "Epoch [5504/20000], Training Loss: 0.0060943178271242815, Validation Loss: 0.00805464402877273\n",
      "Epoch [5505/20000], Training Loss: 0.008088997136967788, Validation Loss: 0.006764233643220255\n",
      "Epoch [5506/20000], Training Loss: 0.007702770522363218, Validation Loss: 0.011617105684893301\n",
      "Epoch [5507/20000], Training Loss: 0.019403972171338473, Validation Loss: 0.004586693578208464\n",
      "Epoch [5508/20000], Training Loss: 0.0262039012626961, Validation Loss: 0.015283809180004937\n",
      "Epoch [5509/20000], Training Loss: 0.02279064742781754, Validation Loss: 0.015795376884077634\n",
      "Epoch [5510/20000], Training Loss: 0.016981473067841892, Validation Loss: 0.009809113250672194\n",
      "Epoch [5511/20000], Training Loss: 0.010879715090335853, Validation Loss: 0.00806351535227025\n",
      "Epoch [5512/20000], Training Loss: 0.006060258022833815, Validation Loss: 0.00770090132695194\n",
      "Epoch [5513/20000], Training Loss: 0.006469830554643912, Validation Loss: 0.003900222254450065\n",
      "Epoch [5514/20000], Training Loss: 0.00593794611505213, Validation Loss: 0.004703821470278464\n",
      "Epoch [5515/20000], Training Loss: 0.007818779578623694, Validation Loss: 0.0071404411992261885\n",
      "Epoch [5516/20000], Training Loss: 0.010991021252266364, Validation Loss: 0.014922443937133063\n",
      "Epoch [5517/20000], Training Loss: 0.00827791160970394, Validation Loss: 0.004650362240811644\n",
      "Epoch [5518/20000], Training Loss: 0.0072731033557959434, Validation Loss: 0.008183311689111048\n",
      "Epoch [5519/20000], Training Loss: 0.006591582379769534, Validation Loss: 0.004322738775726975\n",
      "Epoch [5520/20000], Training Loss: 0.014012444683300731, Validation Loss: 0.010619312069479747\n",
      "Epoch [5521/20000], Training Loss: 0.014138690994643963, Validation Loss: 0.008632065474427821\n",
      "Epoch [5522/20000], Training Loss: 0.007379603413158163, Validation Loss: 0.003002311304304125\n",
      "Epoch [5523/20000], Training Loss: 0.004093016127756398, Validation Loss: 0.0061714312448129605\n",
      "Epoch [5524/20000], Training Loss: 0.008743334750761278, Validation Loss: 0.003282746102805894\n",
      "Epoch [5525/20000], Training Loss: 0.007490580532963317, Validation Loss: 0.022827610905697026\n",
      "Epoch [5526/20000], Training Loss: 0.014580361201037053, Validation Loss: 0.007939127049212402\n",
      "Epoch [5527/20000], Training Loss: 0.024528755496573598, Validation Loss: 0.006095194871576837\n",
      "Epoch [5528/20000], Training Loss: 0.008199375231924932, Validation Loss: 0.033951219565226766\n",
      "Epoch [5529/20000], Training Loss: 0.03919827822989776, Validation Loss: 0.07058374323917503\n",
      "Epoch [5530/20000], Training Loss: 0.030168358486012688, Validation Loss: 0.024947573716060236\n",
      "Epoch [5531/20000], Training Loss: 0.017808447466710016, Validation Loss: 0.007285832055328813\n",
      "Epoch [5532/20000], Training Loss: 0.007793550350210613, Validation Loss: 0.009631710898247547\n",
      "Epoch [5533/20000], Training Loss: 0.02123431544792506, Validation Loss: 0.0651748280791123\n",
      "Epoch [5534/20000], Training Loss: 0.052290395413625186, Validation Loss: 0.009963747405501116\n",
      "Epoch [5535/20000], Training Loss: 0.017065070182850053, Validation Loss: 0.05918768777649218\n",
      "Epoch [5536/20000], Training Loss: 0.06544129594320632, Validation Loss: 0.053336264490748624\n",
      "Epoch [5537/20000], Training Loss: 0.02679168406757526, Validation Loss: 0.01036268997107291\n",
      "Epoch [5538/20000], Training Loss: 0.01144991779334045, Validation Loss: 0.00934437611431349\n",
      "Epoch [5539/20000], Training Loss: 0.009927405044436455, Validation Loss: 0.008668667471283698\n",
      "Epoch [5540/20000], Training Loss: 0.009535916399077646, Validation Loss: 0.0057310196290408\n",
      "Epoch [5541/20000], Training Loss: 0.008507457185519993, Validation Loss: 0.007137196899163557\n",
      "Epoch [5542/20000], Training Loss: 0.028677373843700695, Validation Loss: 0.024733122282859326\n",
      "Epoch [5543/20000], Training Loss: 0.014635435854350882, Validation Loss: 0.019732549544814376\n",
      "Epoch [5544/20000], Training Loss: 0.015780684129692548, Validation Loss: 0.01619030610787117\n",
      "Epoch [5545/20000], Training Loss: 0.009455841267481446, Validation Loss: 0.019644298788015085\n",
      "Epoch [5546/20000], Training Loss: 0.013060440941314613, Validation Loss: 0.008088994794759077\n",
      "Epoch [5547/20000], Training Loss: 0.009058482570773256, Validation Loss: 0.012792382417273984\n",
      "Epoch [5548/20000], Training Loss: 0.008968213273744498, Validation Loss: 0.003716621483103414\n",
      "Epoch [5549/20000], Training Loss: 0.007111397536001667, Validation Loss: 0.005594220506671783\n",
      "Epoch [5550/20000], Training Loss: 0.004775175864064555, Validation Loss: 0.004360602543264187\n",
      "Epoch [5551/20000], Training Loss: 0.004536955097656963, Validation Loss: 0.004391286756864636\n",
      "Epoch [5552/20000], Training Loss: 0.007051284430157726, Validation Loss: 0.0036506510573417472\n",
      "Epoch [5553/20000], Training Loss: 0.008711168072685333, Validation Loss: 0.0031841904574970875\n",
      "Epoch [5554/20000], Training Loss: 0.010145405844793589, Validation Loss: 0.0038750759632161396\n",
      "Epoch [5555/20000], Training Loss: 0.008058939602051396, Validation Loss: 0.015094963977600148\n",
      "Epoch [5556/20000], Training Loss: 0.010585376811962175, Validation Loss: 0.006500908580993122\n",
      "Epoch [5557/20000], Training Loss: 0.006940258419555383, Validation Loss: 0.0035720969565643046\n",
      "Epoch [5558/20000], Training Loss: 0.0072080078867397136, Validation Loss: 0.0028626613704350995\n",
      "Epoch [5559/20000], Training Loss: 0.005364615783816719, Validation Loss: 0.005050024626663279\n",
      "Epoch [5560/20000], Training Loss: 0.007315443922185035, Validation Loss: 0.011513384482181468\n",
      "Epoch [5561/20000], Training Loss: 0.015387823215860408, Validation Loss: 0.014306273973488715\n",
      "Epoch [5562/20000], Training Loss: 0.017708473265104527, Validation Loss: 0.005588605223197517\n",
      "Epoch [5563/20000], Training Loss: 0.018728529300590577, Validation Loss: 0.05845851983415871\n",
      "Epoch [5564/20000], Training Loss: 0.04431744918526549, Validation Loss: 0.029736257846308383\n",
      "Epoch [5565/20000], Training Loss: 0.03521324495419061, Validation Loss: 0.045861376789356915\n",
      "Epoch [5566/20000], Training Loss: 0.031389980001287246, Validation Loss: 0.013643260166385776\n",
      "Epoch [5567/20000], Training Loss: 0.011215126129432715, Validation Loss: 0.010120279641081911\n",
      "Epoch [5568/20000], Training Loss: 0.013824600767942943, Validation Loss: 0.01261580130468636\n",
      "Epoch [5569/20000], Training Loss: 0.012968267343239859, Validation Loss: 0.009028001852129819\n",
      "Epoch [5570/20000], Training Loss: 0.01221139338407998, Validation Loss: 0.009584975328315133\n",
      "Epoch [5571/20000], Training Loss: 0.010907652521772044, Validation Loss: 0.008143222848316003\n",
      "Epoch [5572/20000], Training Loss: 0.010910910931150803, Validation Loss: 0.008242814292251361\n",
      "Epoch [5573/20000], Training Loss: 0.00897897974105685, Validation Loss: 0.006221332213889192\n",
      "Epoch [5574/20000], Training Loss: 0.005864451204875617, Validation Loss: 0.004531581830373658\n",
      "Epoch [5575/20000], Training Loss: 0.009712164681072213, Validation Loss: 0.007704336001048822\n",
      "Epoch [5576/20000], Training Loss: 0.014307514297667825, Validation Loss: 0.007273347796009537\n",
      "Epoch [5577/20000], Training Loss: 0.009117000891170133, Validation Loss: 0.005636582163301033\n",
      "Epoch [5578/20000], Training Loss: 0.007160893338940306, Validation Loss: 0.007340146137201893\n",
      "Epoch [5579/20000], Training Loss: 0.007354437471803976, Validation Loss: 0.007817336324526383\n",
      "Epoch [5580/20000], Training Loss: 0.008423681516433135, Validation Loss: 0.006791776051077899\n",
      "Epoch [5581/20000], Training Loss: 0.0070514619926273425, Validation Loss: 0.003596597511594674\n",
      "Epoch [5582/20000], Training Loss: 0.006442614209455704, Validation Loss: 0.008257450736242131\n",
      "Epoch [5583/20000], Training Loss: 0.010308170891769513, Validation Loss: 0.006250412076756869\n",
      "Epoch [5584/20000], Training Loss: 0.022758318265135, Validation Loss: 0.01327157622609862\n",
      "Epoch [5585/20000], Training Loss: 0.01813234614902675, Validation Loss: 0.011778536272189442\n",
      "Epoch [5586/20000], Training Loss: 0.011242961452808231, Validation Loss: 0.008293863176082336\n",
      "Epoch [5587/20000], Training Loss: 0.004412010380162558, Validation Loss: 0.0071573069228959086\n",
      "Epoch [5588/20000], Training Loss: 0.0065789978013656636, Validation Loss: 0.004519871374993402\n",
      "Epoch [5589/20000], Training Loss: 0.006502957566616325, Validation Loss: 0.004528416456391174\n",
      "Epoch [5590/20000], Training Loss: 0.005466602828944035, Validation Loss: 0.005260835265780932\n",
      "Epoch [5591/20000], Training Loss: 0.009335067299876496, Validation Loss: 0.00445914414384723\n",
      "Epoch [5592/20000], Training Loss: 0.007351913896854967, Validation Loss: 0.0029346198824506636\n",
      "Epoch [5593/20000], Training Loss: 0.0052215628841492745, Validation Loss: 0.003771069916183478\n",
      "Epoch [5594/20000], Training Loss: 0.0050564341056867435, Validation Loss: 0.016545066544495057\n",
      "Epoch [5595/20000], Training Loss: 0.01730816891566584, Validation Loss: 0.010524295369447958\n",
      "Epoch [5596/20000], Training Loss: 0.006611395123253975, Validation Loss: 0.009720471651351101\n",
      "Epoch [5597/20000], Training Loss: 0.014285351546537381, Validation Loss: 0.004393871727407779\n",
      "Epoch [5598/20000], Training Loss: 0.005873268684289152, Validation Loss: 0.014749074090092637\n",
      "Epoch [5599/20000], Training Loss: 0.005855911783458266, Validation Loss: 0.0035223757018911783\n",
      "Epoch [5600/20000], Training Loss: 0.008614193675124884, Validation Loss: 0.011353832190934554\n",
      "Epoch [5601/20000], Training Loss: 0.022021341982541083, Validation Loss: 0.02262920749996868\n",
      "Epoch [5602/20000], Training Loss: 0.047105040834529585, Validation Loss: 0.030496987912946288\n",
      "Epoch [5603/20000], Training Loss: 0.029587305978306437, Validation Loss: 0.0192235584919607\n",
      "Epoch [5604/20000], Training Loss: 0.02480581059353426, Validation Loss: 0.05108006612395651\n",
      "Epoch [5605/20000], Training Loss: 0.03176233692127945, Validation Loss: 0.012466794356961535\n",
      "Epoch [5606/20000], Training Loss: 0.009421546897751145, Validation Loss: 0.009935998374541859\n",
      "Epoch [5607/20000], Training Loss: 0.008051271787345675, Validation Loss: 0.021871729883596962\n",
      "Epoch [5608/20000], Training Loss: 0.011951669254423385, Validation Loss: 0.007251498286952897\n",
      "Epoch [5609/20000], Training Loss: 0.008560950805466356, Validation Loss: 0.005433777113272039\n",
      "Epoch [5610/20000], Training Loss: 0.006429482633913202, Validation Loss: 0.006414843327677643\n",
      "Epoch [5611/20000], Training Loss: 0.008701367690394233, Validation Loss: 0.011127633143060084\n",
      "Epoch [5612/20000], Training Loss: 0.017252039037495188, Validation Loss: 0.023070596166603667\n",
      "Epoch [5613/20000], Training Loss: 0.025844751175359955, Validation Loss: 0.01667625716488403\n",
      "Epoch [5614/20000], Training Loss: 0.018857545175706036, Validation Loss: 0.008417847782019245\n",
      "Epoch [5615/20000], Training Loss: 0.01777921908069402, Validation Loss: 0.005850265796287398\n",
      "Epoch [5616/20000], Training Loss: 0.009516871958372317, Validation Loss: 0.011232429120113957\n",
      "Epoch [5617/20000], Training Loss: 0.006289479130048546, Validation Loss: 0.0065310354979633955\n",
      "Epoch [5618/20000], Training Loss: 0.007460063065601779, Validation Loss: 0.004312801360161497\n",
      "Epoch [5619/20000], Training Loss: 0.005350468064339761, Validation Loss: 0.005630032941443882\n",
      "Epoch [5620/20000], Training Loss: 0.006242693044311766, Validation Loss: 0.011990765149322766\n",
      "Epoch [5621/20000], Training Loss: 0.011231368305743672, Validation Loss: 0.004932512623067201\n",
      "Epoch [5622/20000], Training Loss: 0.00513945916035092, Validation Loss: 0.0034326182900526548\n",
      "Epoch [5623/20000], Training Loss: 0.0047210583226322865, Validation Loss: 0.005929989761052619\n",
      "Epoch [5624/20000], Training Loss: 0.007708973858305919, Validation Loss: 0.006720722731583972\n",
      "Epoch [5625/20000], Training Loss: 0.007352678328970796, Validation Loss: 0.004692762326158181\n",
      "Epoch [5626/20000], Training Loss: 0.005499973597661015, Validation Loss: 0.003282225529306158\n",
      "Epoch [5627/20000], Training Loss: 0.01173093288837533, Validation Loss: 0.008254186555892115\n",
      "Epoch [5628/20000], Training Loss: 0.024756249707674476, Validation Loss: 0.07838693261146545\n",
      "Epoch [5629/20000], Training Loss: 0.021477950655805347, Validation Loss: 0.02935830290833185\n",
      "Epoch [5630/20000], Training Loss: 0.012247525591582027, Validation Loss: 0.0081268322680051\n",
      "Epoch [5631/20000], Training Loss: 0.005793470717662785, Validation Loss: 0.005899052094928184\n",
      "Epoch [5632/20000], Training Loss: 0.009142922528553754, Validation Loss: 0.010901445782581374\n",
      "Epoch [5633/20000], Training Loss: 0.015197967143779221, Validation Loss: 0.00807399179472666\n",
      "Epoch [5634/20000], Training Loss: 0.010017305905681237, Validation Loss: 0.014229934130401486\n",
      "Epoch [5635/20000], Training Loss: 0.00964662930346094, Validation Loss: 0.006948131004117688\n",
      "Epoch [5636/20000], Training Loss: 0.0075884959868354985, Validation Loss: 0.03610493052864123\n",
      "Epoch [5637/20000], Training Loss: 0.013778505447979634, Validation Loss: 0.0080749860070425\n",
      "Epoch [5638/20000], Training Loss: 0.03536474117572652, Validation Loss: 0.09785175134434862\n",
      "Epoch [5639/20000], Training Loss: 0.03394637301763786, Validation Loss: 0.007506617757895762\n",
      "Epoch [5640/20000], Training Loss: 0.007579663498160828, Validation Loss: 0.005229437089516763\n",
      "Epoch [5641/20000], Training Loss: 0.006345124267422112, Validation Loss: 0.004971308739075507\n",
      "Epoch [5642/20000], Training Loss: 0.0049260923364532316, Validation Loss: 0.010639393777729846\n",
      "Epoch [5643/20000], Training Loss: 0.00789993364118605, Validation Loss: 0.005907230784416372\n",
      "Epoch [5644/20000], Training Loss: 0.008233128408651933, Validation Loss: 0.0053795111022098075\n",
      "Epoch [5645/20000], Training Loss: 0.007815139953371337, Validation Loss: 0.003540570566946296\n",
      "Epoch [5646/20000], Training Loss: 0.009166348602645615, Validation Loss: 0.04964079707860947\n",
      "Epoch [5647/20000], Training Loss: 0.015651566697800132, Validation Loss: 0.015438051894310453\n",
      "Epoch [5648/20000], Training Loss: 0.011412448162705655, Validation Loss: 0.014532838016937019\n",
      "Epoch [5649/20000], Training Loss: 0.00671373923770651, Validation Loss: 0.011281148548569828\n",
      "Epoch [5650/20000], Training Loss: 0.016254939928850427, Validation Loss: 0.030170255748745132\n",
      "Epoch [5651/20000], Training Loss: 0.014314443421816187, Validation Loss: 0.0043868894111336\n",
      "Epoch [5652/20000], Training Loss: 0.012975532986129, Validation Loss: 0.010964063528394399\n",
      "Epoch [5653/20000], Training Loss: 0.007245375026416566, Validation Loss: 0.008042051927922221\n",
      "Epoch [5654/20000], Training Loss: 0.009534870565403253, Validation Loss: 0.0063652002091775595\n",
      "Epoch [5655/20000], Training Loss: 0.006437061666344691, Validation Loss: 0.0048902670550237115\n",
      "Epoch [5656/20000], Training Loss: 0.006772529069816561, Validation Loss: 0.00497918763450413\n",
      "Epoch [5657/20000], Training Loss: 0.008527938378392719, Validation Loss: 0.0037477781060115995\n",
      "Epoch [5658/20000], Training Loss: 0.010747906929412108, Validation Loss: 0.005346880090908073\n",
      "Epoch [5659/20000], Training Loss: 0.011855091183211439, Validation Loss: 0.029096323369641044\n",
      "Epoch [5660/20000], Training Loss: 0.020006658986517323, Validation Loss: 0.006348754388224417\n",
      "Epoch [5661/20000], Training Loss: 0.013853106818195167, Validation Loss: 0.006964614149222107\n",
      "Epoch [5662/20000], Training Loss: 0.007156385402335478, Validation Loss: 0.007402506658402182\n",
      "Epoch [5663/20000], Training Loss: 0.0058070351530789465, Validation Loss: 0.02946482880455343\n",
      "Epoch [5664/20000], Training Loss: 0.01163875111732133, Validation Loss: 0.003531981166738823\n",
      "Epoch [5665/20000], Training Loss: 0.007075543272159328, Validation Loss: 0.011131060442768435\n",
      "Epoch [5666/20000], Training Loss: 0.00742986337718321, Validation Loss: 0.008146519839266888\n",
      "Epoch [5667/20000], Training Loss: 0.013846161073680767, Validation Loss: 0.006502550960150185\n",
      "Epoch [5668/20000], Training Loss: 0.015554225349930562, Validation Loss: 0.008472643031229759\n",
      "Epoch [5669/20000], Training Loss: 0.019422168414166663, Validation Loss: 0.009470101578979238\n",
      "Epoch [5670/20000], Training Loss: 0.006975643798276516, Validation Loss: 0.007867086973967779\n",
      "Epoch [5671/20000], Training Loss: 0.005244236601616389, Validation Loss: 0.010173861237754878\n",
      "Epoch [5672/20000], Training Loss: 0.006241589740966447, Validation Loss: 0.004747264708783934\n",
      "Epoch [5673/20000], Training Loss: 0.009448769128149641, Validation Loss: 0.008382238714050477\n",
      "Epoch [5674/20000], Training Loss: 0.008427980883750155, Validation Loss: 0.030997725469725478\n",
      "Epoch [5675/20000], Training Loss: 0.016749892659455718, Validation Loss: 0.02365768168857174\n",
      "Epoch [5676/20000], Training Loss: 0.01382979974628792, Validation Loss: 0.015502632729070693\n",
      "Epoch [5677/20000], Training Loss: 0.012081347650981757, Validation Loss: 0.0067019090552081495\n",
      "Epoch [5678/20000], Training Loss: 0.007096886732532377, Validation Loss: 0.015657416411808713\n",
      "Epoch [5679/20000], Training Loss: 0.010006286889880098, Validation Loss: 0.007722765539938098\n",
      "Epoch [5680/20000], Training Loss: 0.004932762379342291, Validation Loss: 0.008390640022680402\n",
      "Epoch [5681/20000], Training Loss: 0.01390118074965179, Validation Loss: 0.007657088759060571\n",
      "Epoch [5682/20000], Training Loss: 0.0633724151910948, Validation Loss: 0.08842670518580624\n",
      "Epoch [5683/20000], Training Loss: 0.10041148477674662, Validation Loss: 0.11033648008417783\n",
      "Epoch [5684/20000], Training Loss: 0.04419740155156303, Validation Loss: 0.020991109465240115\n",
      "Epoch [5685/20000], Training Loss: 0.026092779327882454, Validation Loss: 0.053442609371683956\n",
      "Epoch [5686/20000], Training Loss: 0.021854649181477726, Validation Loss: 0.014209773250013871\n",
      "Epoch [5687/20000], Training Loss: 0.01344760603803609, Validation Loss: 0.008889390357532365\n",
      "Epoch [5688/20000], Training Loss: 0.01021948952360877, Validation Loss: 0.007445861860885218\n",
      "Epoch [5689/20000], Training Loss: 0.006336413265671581, Validation Loss: 0.005986085405441243\n",
      "Epoch [5690/20000], Training Loss: 0.006374762892456991, Validation Loss: 0.00569707172384426\n",
      "Epoch [5691/20000], Training Loss: 0.005799174676358884, Validation Loss: 0.003973265755088765\n",
      "Epoch [5692/20000], Training Loss: 0.005547678971197456, Validation Loss: 0.008034390492508552\n",
      "Epoch [5693/20000], Training Loss: 0.007671414854745048, Validation Loss: 0.004731101801228631\n",
      "Epoch [5694/20000], Training Loss: 0.008916237023575897, Validation Loss: 0.0034089160929064682\n",
      "Epoch [5695/20000], Training Loss: 0.008418012244971709, Validation Loss: 0.008699588462028578\n",
      "Epoch [5696/20000], Training Loss: 0.025351273914566264, Validation Loss: 0.02364369747062613\n",
      "Epoch [5697/20000], Training Loss: 0.0835640018007585, Validation Loss: 0.024183891510671986\n",
      "Epoch [5698/20000], Training Loss: 0.03289389292850891, Validation Loss: 0.05239976938220763\n",
      "Epoch [5699/20000], Training Loss: 0.027386803787002072, Validation Loss: 0.01441875629409815\n",
      "Epoch [5700/20000], Training Loss: 0.012695466450947736, Validation Loss: 0.008364462680349714\n",
      "Epoch [5701/20000], Training Loss: 0.01815723623024366, Validation Loss: 0.011881173889768044\n",
      "Epoch [5702/20000], Training Loss: 0.00979818472374713, Validation Loss: 0.011960621080236575\n",
      "Epoch [5703/20000], Training Loss: 0.00859637968824245, Validation Loss: 0.005800063625169319\n",
      "Epoch [5704/20000], Training Loss: 0.006289144321012178, Validation Loss: 0.004800774156007849\n",
      "Epoch [5705/20000], Training Loss: 0.005382034245745412, Validation Loss: 0.010513118469784135\n",
      "Epoch [5706/20000], Training Loss: 0.008796173293376341, Validation Loss: 0.005476585168383151\n",
      "Epoch [5707/20000], Training Loss: 0.009187807578460447, Validation Loss: 0.013015744352872258\n",
      "Epoch [5708/20000], Training Loss: 0.005927875729477299, Validation Loss: 0.011394089027037677\n",
      "Epoch [5709/20000], Training Loss: 0.014409830336392458, Validation Loss: 0.004507142277361709\n",
      "Epoch [5710/20000], Training Loss: 0.019500298296666836, Validation Loss: 0.006347401212094285\n",
      "Epoch [5711/20000], Training Loss: 0.012433753663052423, Validation Loss: 0.014748342038469022\n",
      "Epoch [5712/20000], Training Loss: 0.014861794492130034, Validation Loss: 0.006417952146875905\n",
      "Epoch [5713/20000], Training Loss: 0.017922077814416428, Validation Loss: 0.005738715912489819\n",
      "Epoch [5714/20000], Training Loss: 0.03764363923671356, Validation Loss: 0.04112181780510582\n",
      "Epoch [5715/20000], Training Loss: 0.03141487296670675, Validation Loss: 0.016839118643214794\n",
      "Epoch [5716/20000], Training Loss: 0.036486240708784735, Validation Loss: 0.0070496579428436235\n",
      "Epoch [5717/20000], Training Loss: 0.03709436160612053, Validation Loss: 0.014458115454666702\n",
      "Epoch [5718/20000], Training Loss: 0.017504314571851864, Validation Loss: 0.009431109046220496\n",
      "Epoch [5719/20000], Training Loss: 0.010038239101829407, Validation Loss: 0.011308991069459548\n",
      "Epoch [5720/20000], Training Loss: 0.02189490418615086, Validation Loss: 0.009843185572598096\n",
      "Epoch [5721/20000], Training Loss: 0.01646618391636626, Validation Loss: 0.012127353556154827\n",
      "Epoch [5722/20000], Training Loss: 0.013640841358989877, Validation Loss: 0.007821016476685014\n",
      "Epoch [5723/20000], Training Loss: 0.009827113327836352, Validation Loss: 0.007660337639078664\n",
      "Epoch [5724/20000], Training Loss: 0.007554215603574578, Validation Loss: 0.006520783956927646\n",
      "Epoch [5725/20000], Training Loss: 0.008480667818470724, Validation Loss: 0.010438611199268573\n",
      "Epoch [5726/20000], Training Loss: 0.01018163637490943, Validation Loss: 0.011676788618582836\n",
      "Epoch [5727/20000], Training Loss: 0.011516239086631685, Validation Loss: 0.010919514644293875\n",
      "Epoch [5728/20000], Training Loss: 0.013805638048905016, Validation Loss: 0.007325936164826479\n",
      "Epoch [5729/20000], Training Loss: 0.009707175419732397, Validation Loss: 0.009147210908024712\n",
      "Epoch [5730/20000], Training Loss: 0.008760023722840873, Validation Loss: 0.0056980232560631195\n",
      "Epoch [5731/20000], Training Loss: 0.005806840115318275, Validation Loss: 0.0037982309757142374\n",
      "Epoch [5732/20000], Training Loss: 0.013768455595709383, Validation Loss: 0.004148522907290239\n",
      "Epoch [5733/20000], Training Loss: 0.03541793079706882, Validation Loss: 0.007099758564436343\n",
      "Epoch [5734/20000], Training Loss: 0.025140703813771585, Validation Loss: 0.009315879832554077\n",
      "Epoch [5735/20000], Training Loss: 0.013228284020442516, Validation Loss: 0.016881530446491006\n",
      "Epoch [5736/20000], Training Loss: 0.01836072170798226, Validation Loss: 0.03024755118531175\n",
      "Epoch [5737/20000], Training Loss: 0.03344101704091632, Validation Loss: 0.02056972778635619\n",
      "Epoch [5738/20000], Training Loss: 0.03974252733002816, Validation Loss: 0.09006891176817129\n",
      "Epoch [5739/20000], Training Loss: 0.036149738497832526, Validation Loss: 0.029203440364053028\n",
      "Epoch [5740/20000], Training Loss: 0.01848008484479838, Validation Loss: 0.003082230655889719\n",
      "Epoch [5741/20000], Training Loss: 0.009184986714119856, Validation Loss: 0.018827267992666714\n",
      "Epoch [5742/20000], Training Loss: 0.008390792312050215, Validation Loss: 0.0033884031795875353\n",
      "Epoch [5743/20000], Training Loss: 0.005932701576966792, Validation Loss: 0.0031179720429896918\n",
      "Epoch [5744/20000], Training Loss: 0.01739104540579969, Validation Loss: 0.07936241196434171\n",
      "Epoch [5745/20000], Training Loss: 0.028141164499434775, Validation Loss: 0.0318122341197659\n",
      "Epoch [5746/20000], Training Loss: 0.012985184745048173, Validation Loss: 0.00631754306001702\n",
      "Epoch [5747/20000], Training Loss: 0.007142379428842105, Validation Loss: 0.00603249974366398\n",
      "Epoch [5748/20000], Training Loss: 0.010028753130297576, Validation Loss: 0.00620476067226556\n",
      "Epoch [5749/20000], Training Loss: 0.009945723889229287, Validation Loss: 0.00801105436979859\n",
      "Epoch [5750/20000], Training Loss: 0.005899827702835735, Validation Loss: 0.008593588221745765\n",
      "Epoch [5751/20000], Training Loss: 0.004182044441124942, Validation Loss: 0.01154561024117535\n",
      "Epoch [5752/20000], Training Loss: 0.010870855236654669, Validation Loss: 0.0069784881841101765\n",
      "Epoch [5753/20000], Training Loss: 0.016667901849245936, Validation Loss: 0.01810209465371924\n",
      "Epoch [5754/20000], Training Loss: 0.01228191703090228, Validation Loss: 0.008836983067843513\n",
      "Epoch [5755/20000], Training Loss: 0.01176164556610664, Validation Loss: 0.004885259136252833\n",
      "Epoch [5756/20000], Training Loss: 0.008023186823785571, Validation Loss: 0.0044983684601455865\n",
      "Epoch [5757/20000], Training Loss: 0.0060615536817100035, Validation Loss: 0.004420265377157355\n",
      "Epoch [5758/20000], Training Loss: 0.005189330843027814, Validation Loss: 0.003069615102792308\n",
      "Epoch [5759/20000], Training Loss: 0.00540866328655909, Validation Loss: 0.0039030913092505315\n",
      "Epoch [5760/20000], Training Loss: 0.005930361318990306, Validation Loss: 0.004779722875439469\n",
      "Epoch [5761/20000], Training Loss: 0.016190754958578118, Validation Loss: 0.0073744143624697245\n",
      "Epoch [5762/20000], Training Loss: 0.0190526481757323, Validation Loss: 0.0042159862825477444\n",
      "Epoch [5763/20000], Training Loss: 0.008826756031859466, Validation Loss: 0.00670564193047036\n",
      "Epoch [5764/20000], Training Loss: 0.0062718092605272046, Validation Loss: 0.006424516137917026\n",
      "Epoch [5765/20000], Training Loss: 0.004967215664302265, Validation Loss: 0.007559779906078542\n",
      "Epoch [5766/20000], Training Loss: 0.014808258591074264, Validation Loss: 0.009111090130407515\n",
      "Epoch [5767/20000], Training Loss: 0.005058339276177192, Validation Loss: 0.004512537280663967\n",
      "Epoch [5768/20000], Training Loss: 0.005379402687789739, Validation Loss: 0.005299942654835276\n",
      "Epoch [5769/20000], Training Loss: 0.012301162824379779, Validation Loss: 0.1060922358717425\n",
      "Epoch [5770/20000], Training Loss: 0.07508131620124914, Validation Loss: 0.02478844782958456\n",
      "Epoch [5771/20000], Training Loss: 0.024068998234465004, Validation Loss: 0.019824289550992295\n",
      "Epoch [5772/20000], Training Loss: 0.010321615729480982, Validation Loss: 0.010642114469581954\n",
      "Epoch [5773/20000], Training Loss: 0.009215865699973489, Validation Loss: 0.010011785636639356\n",
      "Epoch [5774/20000], Training Loss: 0.00888188582445894, Validation Loss: 0.008441236347997412\n",
      "Epoch [5775/20000], Training Loss: 0.011694648111837782, Validation Loss: 0.008283425464599401\n",
      "Epoch [5776/20000], Training Loss: 0.008799483288644947, Validation Loss: 0.009074013402693839\n",
      "Epoch [5777/20000], Training Loss: 0.010176248490876918, Validation Loss: 0.008404963164918757\n",
      "Epoch [5778/20000], Training Loss: 0.013967599318545711, Validation Loss: 0.011119115633619003\n",
      "Epoch [5779/20000], Training Loss: 0.008834142761770636, Validation Loss: 0.0068488991488655825\n",
      "Epoch [5780/20000], Training Loss: 0.009256437803352517, Validation Loss: 0.006362502309002593\n",
      "Epoch [5781/20000], Training Loss: 0.006903714001444834, Validation Loss: 0.0053047585718754165\n",
      "Epoch [5782/20000], Training Loss: 0.005904835812024041, Validation Loss: 0.005041480187855996\n",
      "Epoch [5783/20000], Training Loss: 0.010777022244708081, Validation Loss: 0.005653463437965911\n",
      "Epoch [5784/20000], Training Loss: 0.021750419534198175, Validation Loss: 0.013904105722736355\n",
      "Epoch [5785/20000], Training Loss: 0.007168449665186927, Validation Loss: 0.005240971232556798\n",
      "Epoch [5786/20000], Training Loss: 0.005971873206103088, Validation Loss: 0.0064313293885009315\n",
      "Epoch [5787/20000], Training Loss: 0.008527754215590124, Validation Loss: 0.00690610173499197\n",
      "Epoch [5788/20000], Training Loss: 0.009014845958777837, Validation Loss: 0.017633833683901496\n",
      "Epoch [5789/20000], Training Loss: 0.01236523543372771, Validation Loss: 0.015365466116634252\n",
      "Epoch [5790/20000], Training Loss: 0.009512415459019914, Validation Loss: 0.005539346913792542\n",
      "Epoch [5791/20000], Training Loss: 0.005667273204640618, Validation Loss: 0.005941425760555928\n",
      "Epoch [5792/20000], Training Loss: 0.005235018870215364, Validation Loss: 0.0040479514362752\n",
      "Epoch [5793/20000], Training Loss: 0.006180311436764896, Validation Loss: 0.0049832764285722775\n",
      "Epoch [5794/20000], Training Loss: 0.00873841179457356, Validation Loss: 0.010861920287502309\n",
      "Epoch [5795/20000], Training Loss: 0.007440719383469384, Validation Loss: 0.006764853925078386\n",
      "Epoch [5796/20000], Training Loss: 0.01070645386763707, Validation Loss: 0.00884587477056036\n",
      "Epoch [5797/20000], Training Loss: 0.009900598309676363, Validation Loss: 0.010777152954103388\n",
      "Epoch [5798/20000], Training Loss: 0.010642568483100539, Validation Loss: 0.006700558410001024\n",
      "Epoch [5799/20000], Training Loss: 0.007964171431012801, Validation Loss: 0.005394518122063882\n",
      "Epoch [5800/20000], Training Loss: 0.004309063944674563, Validation Loss: 0.013924088165996582\n",
      "Epoch [5801/20000], Training Loss: 0.006927282321782968, Validation Loss: 0.028016636418047396\n",
      "Epoch [5802/20000], Training Loss: 0.013728203752959547, Validation Loss: 0.0032591441208182914\n",
      "Epoch [5803/20000], Training Loss: 0.010928693446463253, Validation Loss: 0.004088989898817833\n",
      "Epoch [5804/20000], Training Loss: 0.0063798498972573725, Validation Loss: 0.010563398217495237\n",
      "Epoch [5805/20000], Training Loss: 0.013247272234626248, Validation Loss: 0.05222486483788608\n",
      "Epoch [5806/20000], Training Loss: 0.03924565374960106, Validation Loss: 0.01205914215119362\n",
      "Epoch [5807/20000], Training Loss: 0.01844605957532102, Validation Loss: 0.025245975584011506\n",
      "Epoch [5808/20000], Training Loss: 0.03265958575398794, Validation Loss: 0.0331537835332386\n",
      "Epoch [5809/20000], Training Loss: 0.023855131267087666, Validation Loss: 0.03479277450064241\n",
      "Epoch [5810/20000], Training Loss: 0.025791462577347244, Validation Loss: 0.008193219290754021\n",
      "Epoch [5811/20000], Training Loss: 0.012238636564201702, Validation Loss: 0.007080183081760489\n",
      "Epoch [5812/20000], Training Loss: 0.012114044212337052, Validation Loss: 0.009579154826993326\n",
      "Epoch [5813/20000], Training Loss: 0.008633026171342604, Validation Loss: 0.01412200270341454\n",
      "Epoch [5814/20000], Training Loss: 0.006759898940799758, Validation Loss: 0.00853939266445585\n",
      "Epoch [5815/20000], Training Loss: 0.010008272286670814, Validation Loss: 0.016068314004834456\n",
      "Epoch [5816/20000], Training Loss: 0.02131881804299545, Validation Loss: 0.051532720853824036\n",
      "Epoch [5817/20000], Training Loss: 0.04631879546234684, Validation Loss: 0.018871235600922125\n",
      "Epoch [5818/20000], Training Loss: 0.023517386942070777, Validation Loss: 0.010470473777112537\n",
      "Epoch [5819/20000], Training Loss: 0.007368113024085687, Validation Loss: 0.005844136682972996\n",
      "Epoch [5820/20000], Training Loss: 0.007584925537230447, Validation Loss: 0.005286756274797101\n",
      "Epoch [5821/20000], Training Loss: 0.008009524566919677, Validation Loss: 0.00804228376656673\n",
      "Epoch [5822/20000], Training Loss: 0.009204013555842851, Validation Loss: 0.005380440421557101\n",
      "Epoch [5823/20000], Training Loss: 0.007956445624586195, Validation Loss: 0.005995573649532356\n",
      "Epoch [5824/20000], Training Loss: 0.013728291216206603, Validation Loss: 0.013007763357724808\n",
      "Epoch [5825/20000], Training Loss: 0.008641007235123002, Validation Loss: 0.018820109212796587\n",
      "Epoch [5826/20000], Training Loss: 0.010814408194521872, Validation Loss: 0.011687893829162022\n",
      "Epoch [5827/20000], Training Loss: 0.010735816750509133, Validation Loss: 0.010375661797878268\n",
      "Epoch [5828/20000], Training Loss: 0.014849858067464083, Validation Loss: 0.006663756237547399\n",
      "Epoch [5829/20000], Training Loss: 0.012498241463098176, Validation Loss: 0.0036851746618233555\n",
      "Epoch [5830/20000], Training Loss: 0.005425540881706469, Validation Loss: 0.009400870049186895\n",
      "Epoch [5831/20000], Training Loss: 0.004874505479425092, Validation Loss: 0.004119178949183931\n",
      "Epoch [5832/20000], Training Loss: 0.006642909105201917, Validation Loss: 0.005343219517863064\n",
      "Epoch [5833/20000], Training Loss: 0.006125182192980512, Validation Loss: 0.009824203740541536\n",
      "Epoch [5834/20000], Training Loss: 0.014293487969553098, Validation Loss: 0.039336679769413845\n",
      "Epoch [5835/20000], Training Loss: 0.009400482256231564, Validation Loss: 0.026643685996547865\n",
      "Epoch [5836/20000], Training Loss: 0.023524787850224778, Validation Loss: 0.00357904290093972\n",
      "Epoch [5837/20000], Training Loss: 0.02077871937269395, Validation Loss: 0.008375828636152034\n",
      "Epoch [5838/20000], Training Loss: 0.013100375021167565, Validation Loss: 0.05328732569302831\n",
      "Epoch [5839/20000], Training Loss: 0.019786344637395814, Validation Loss: 0.004834178416527113\n",
      "Epoch [5840/20000], Training Loss: 0.012131768547890585, Validation Loss: 0.005285821134113863\n",
      "Epoch [5841/20000], Training Loss: 0.009391456583086568, Validation Loss: 0.005830306097347915\n",
      "Epoch [5842/20000], Training Loss: 0.014239007514074078, Validation Loss: 0.008862607838321759\n",
      "Epoch [5843/20000], Training Loss: 0.03741985095049521, Validation Loss: 0.01047564147933196\n",
      "Epoch [5844/20000], Training Loss: 0.03106296794222934, Validation Loss: 0.011830320575156745\n",
      "Epoch [5845/20000], Training Loss: 0.03285017486528626, Validation Loss: 0.00486993715822034\n",
      "Epoch [5846/20000], Training Loss: 0.03247306816878596, Validation Loss: 0.02094890709442404\n",
      "Epoch [5847/20000], Training Loss: 0.00831515884159931, Validation Loss: 0.0069783665431866525\n",
      "Epoch [5848/20000], Training Loss: 0.00843553431955765, Validation Loss: 0.015679092035457285\n",
      "Epoch [5849/20000], Training Loss: 0.01722329914004409, Validation Loss: 0.011108541867624158\n",
      "Epoch [5850/20000], Training Loss: 0.01096188513994483, Validation Loss: 0.006565447711750304\n",
      "Epoch [5851/20000], Training Loss: 0.007107129958837634, Validation Loss: 0.00987616249078363\n",
      "Epoch [5852/20000], Training Loss: 0.005747907783058638, Validation Loss: 0.005086323667860755\n",
      "Epoch [5853/20000], Training Loss: 0.005502726948049842, Validation Loss: 0.003879254538743176\n",
      "Epoch [5854/20000], Training Loss: 0.005081725415168746, Validation Loss: 0.003735701652129521\n",
      "Epoch [5855/20000], Training Loss: 0.012114903957516228, Validation Loss: 0.015927794655636946\n",
      "Epoch [5856/20000], Training Loss: 0.016487742806020833, Validation Loss: 0.026484363313275854\n",
      "Epoch [5857/20000], Training Loss: 0.019723356535126055, Validation Loss: 0.016728064839390284\n",
      "Epoch [5858/20000], Training Loss: 0.007581084205802264, Validation Loss: 0.010508238853330345\n",
      "Epoch [5859/20000], Training Loss: 0.006325692890511293, Validation Loss: 0.020440876307377818\n",
      "Epoch [5860/20000], Training Loss: 0.012873493468597579, Validation Loss: 0.005361334869608535\n",
      "Epoch [5861/20000], Training Loss: 0.005513033011344045, Validation Loss: 0.0036914721420708285\n",
      "Epoch [5862/20000], Training Loss: 0.010437441465251749, Validation Loss: 0.01030747433528933\n",
      "Epoch [5863/20000], Training Loss: 0.004181872521127973, Validation Loss: 0.004371936330011016\n",
      "Epoch [5864/20000], Training Loss: 0.008320532752837348, Validation Loss: 0.01291136363859738\n",
      "Epoch [5865/20000], Training Loss: 0.00873321430741011, Validation Loss: 0.0028257005662051644\n",
      "Epoch [5866/20000], Training Loss: 0.007966347568851364, Validation Loss: 0.0028652139835508373\n",
      "Epoch [5867/20000], Training Loss: 0.023155795345311554, Validation Loss: 0.002330173669016\n",
      "Epoch [5868/20000], Training Loss: 0.032295867714050734, Validation Loss: 0.03912247980283371\n",
      "Epoch [5869/20000], Training Loss: 0.02139109090889438, Validation Loss: 0.008119207052065544\n",
      "Epoch [5870/20000], Training Loss: 0.016235452685837766, Validation Loss: 0.014580714656793654\n",
      "Epoch [5871/20000], Training Loss: 0.02030761808674291, Validation Loss: 0.011992487267710723\n",
      "Epoch [5872/20000], Training Loss: 0.010150723886389252, Validation Loss: 0.0060332677394967504\n",
      "Epoch [5873/20000], Training Loss: 0.007226320677935811, Validation Loss: 0.00728454323398101\n",
      "Epoch [5874/20000], Training Loss: 0.0077103301134359625, Validation Loss: 0.007917992371247351\n",
      "Epoch [5875/20000], Training Loss: 0.0057492869922758216, Validation Loss: 0.004654717235000915\n",
      "Epoch [5876/20000], Training Loss: 0.0052592834628220385, Validation Loss: 0.006566203540131964\n",
      "Epoch [5877/20000], Training Loss: 0.007695824714444045, Validation Loss: 0.00410952821697005\n",
      "Epoch [5878/20000], Training Loss: 0.0062161682492712445, Validation Loss: 0.005015058866626581\n",
      "Epoch [5879/20000], Training Loss: 0.0063446610999692765, Validation Loss: 0.004643992120978625\n",
      "Epoch [5880/20000], Training Loss: 0.00960863458125719, Validation Loss: 0.00443560253507783\n",
      "Epoch [5881/20000], Training Loss: 0.009246890385319213, Validation Loss: 0.016452932198534435\n",
      "Epoch [5882/20000], Training Loss: 0.008230731627138863, Validation Loss: 0.0030472870036604783\n",
      "Epoch [5883/20000], Training Loss: 0.02931708351908518, Validation Loss: 0.022358511192493134\n",
      "Epoch [5884/20000], Training Loss: 0.03469140659990444, Validation Loss: 0.037666632074368475\n",
      "Epoch [5885/20000], Training Loss: 0.027223301635655974, Validation Loss: 0.016820840475182713\n",
      "Epoch [5886/20000], Training Loss: 0.009414811705937609, Validation Loss: 0.012034415674601664\n",
      "Epoch [5887/20000], Training Loss: 0.007047709842611637, Validation Loss: 0.005303737830087941\n",
      "Epoch [5888/20000], Training Loss: 0.007529418657733393, Validation Loss: 0.005406110351811678\n",
      "Epoch [5889/20000], Training Loss: 0.006444582757207432, Validation Loss: 0.01638946597107958\n",
      "Epoch [5890/20000], Training Loss: 0.014570591099202699, Validation Loss: 0.006294818355822177\n",
      "Epoch [5891/20000], Training Loss: 0.010802570988224553, Validation Loss: 0.00459644751153324\n",
      "Epoch [5892/20000], Training Loss: 0.01052917078725711, Validation Loss: 0.010830159124464802\n",
      "Epoch [5893/20000], Training Loss: 0.007074310315407014, Validation Loss: 0.004682341963026764\n",
      "Epoch [5894/20000], Training Loss: 0.007387813161975438, Validation Loss: 0.005016367546707963\n",
      "Epoch [5895/20000], Training Loss: 0.006444480170362762, Validation Loss: 0.004897949054955981\n",
      "Epoch [5896/20000], Training Loss: 0.007698950477268747, Validation Loss: 0.006268740543536718\n",
      "Epoch [5897/20000], Training Loss: 0.005009978869436184, Validation Loss: 0.016193570729683415\n",
      "Epoch [5898/20000], Training Loss: 0.006151903525668396, Validation Loss: 0.0161366314924285\n",
      "Epoch [5899/20000], Training Loss: 0.010512671957777846, Validation Loss: 0.0031968838361373336\n",
      "Epoch [5900/20000], Training Loss: 0.01346860585025362, Validation Loss: 0.014014372726892686\n",
      "Epoch [5901/20000], Training Loss: 0.011307454936156449, Validation Loss: 0.004068048170332921\n",
      "Epoch [5902/20000], Training Loss: 0.00959240660137896, Validation Loss: 0.005505218765760235\n",
      "Epoch [5903/20000], Training Loss: 0.008636382770159148, Validation Loss: 0.008845511161943527\n",
      "Epoch [5904/20000], Training Loss: 0.013809565503574308, Validation Loss: 0.0856308149439968\n",
      "Epoch [5905/20000], Training Loss: 0.05384138941631785, Validation Loss: 0.03236679660190995\n",
      "Epoch [5906/20000], Training Loss: 0.045976400178395646, Validation Loss: 0.008608982412722201\n",
      "Epoch [5907/20000], Training Loss: 0.034667033061850816, Validation Loss: 0.044405274508258534\n",
      "Epoch [5908/20000], Training Loss: 0.011488554450157349, Validation Loss: 0.010059134773590423\n",
      "Epoch [5909/20000], Training Loss: 0.00944455819704412, Validation Loss: 0.009233043655902813\n",
      "Epoch [5910/20000], Training Loss: 0.008594267590004685, Validation Loss: 0.006142083265591022\n",
      "Epoch [5911/20000], Training Loss: 0.007583364528337759, Validation Loss: 0.005285581920101582\n",
      "Epoch [5912/20000], Training Loss: 0.007720250210175956, Validation Loss: 0.0052784609366166736\n",
      "Epoch [5913/20000], Training Loss: 0.006049233293327104, Validation Loss: 0.005450888578902128\n",
      "Epoch [5914/20000], Training Loss: 0.0058849458832160705, Validation Loss: 0.011788808948917253\n",
      "Epoch [5915/20000], Training Loss: 0.0061623984081182115, Validation Loss: 0.004661262414612922\n",
      "Epoch [5916/20000], Training Loss: 0.00516546979893714, Validation Loss: 0.003953177342769517\n",
      "Epoch [5917/20000], Training Loss: 0.005617219811808484, Validation Loss: 0.005781696436181554\n",
      "Epoch [5918/20000], Training Loss: 0.011892571772339775, Validation Loss: 0.005251768220245724\n",
      "Epoch [5919/20000], Training Loss: 0.007471411646942475, Validation Loss: 0.004171252898085446\n",
      "Epoch [5920/20000], Training Loss: 0.007754890537528679, Validation Loss: 0.004181139693547006\n",
      "Epoch [5921/20000], Training Loss: 0.030730943684050414, Validation Loss: 0.03380169094796045\n",
      "Epoch [5922/20000], Training Loss: 0.06494422545490254, Validation Loss: 0.11136726844886388\n",
      "Epoch [5923/20000], Training Loss: 0.04070293574094969, Validation Loss: 0.0688196412961903\n",
      "Epoch [5924/20000], Training Loss: 0.03304786896998329, Validation Loss: 0.010970069991947867\n",
      "Epoch [5925/20000], Training Loss: 0.015294598119258549, Validation Loss: 0.012495994147945635\n",
      "Epoch [5926/20000], Training Loss: 0.012009680653656167, Validation Loss: 0.006947138795762387\n",
      "Epoch [5927/20000], Training Loss: 0.00769610668482658, Validation Loss: 0.008713067336040192\n",
      "Epoch [5928/20000], Training Loss: 0.007305419710389417, Validation Loss: 0.005439658313532297\n",
      "Epoch [5929/20000], Training Loss: 0.007497158760088496, Validation Loss: 0.0061003578039974735\n",
      "Epoch [5930/20000], Training Loss: 0.007107173812897827, Validation Loss: 0.004617089753066596\n",
      "Epoch [5931/20000], Training Loss: 0.006228535203263164, Validation Loss: 0.006542588681968807\n",
      "Epoch [5932/20000], Training Loss: 0.010782903434509146, Validation Loss: 0.004234414625037581\n",
      "Epoch [5933/20000], Training Loss: 0.009537861657528473, Validation Loss: 0.006438047644160909\n",
      "Epoch [5934/20000], Training Loss: 0.009958666968616723, Validation Loss: 0.006194606450434824\n",
      "Epoch [5935/20000], Training Loss: 0.019136872829757783, Validation Loss: 0.02566501718735838\n",
      "Epoch [5936/20000], Training Loss: 0.03273840825048475, Validation Loss: 0.00660104778632692\n",
      "Epoch [5937/20000], Training Loss: 0.014275747076940857, Validation Loss: 0.008101367238852348\n",
      "Epoch [5938/20000], Training Loss: 0.007471343616007938, Validation Loss: 0.012068349905349168\n",
      "Epoch [5939/20000], Training Loss: 0.010335640143759, Validation Loss: 0.005932477061183794\n",
      "Epoch [5940/20000], Training Loss: 0.01053015022314087, Validation Loss: 0.009756140523157828\n",
      "Epoch [5941/20000], Training Loss: 0.013118685331262116, Validation Loss: 0.016633822143765953\n",
      "Epoch [5942/20000], Training Loss: 0.014441466416298811, Validation Loss: 0.004692911371258951\n",
      "Epoch [5943/20000], Training Loss: 0.007636341696654979, Validation Loss: 0.0036732480224515034\n",
      "Epoch [5944/20000], Training Loss: 0.0042742053457394446, Validation Loss: 0.003906456780190938\n",
      "Epoch [5945/20000], Training Loss: 0.013684703345851241, Validation Loss: 0.012452281855205801\n",
      "Epoch [5946/20000], Training Loss: 0.012551582684474332, Validation Loss: 0.03676647564848954\n",
      "Epoch [5947/20000], Training Loss: 0.018504352047914705, Validation Loss: 0.005297463508800465\n",
      "Epoch [5948/20000], Training Loss: 0.006401939232944187, Validation Loss: 0.023657676212595923\n",
      "Epoch [5949/20000], Training Loss: 0.014467937439414007, Validation Loss: 0.009943917756757195\n",
      "Epoch [5950/20000], Training Loss: 0.007727162140716766, Validation Loss: 0.014621298793973697\n",
      "Epoch [5951/20000], Training Loss: 0.009973694552326964, Validation Loss: 0.010239985942046429\n",
      "Epoch [5952/20000], Training Loss: 0.017729935686345146, Validation Loss: 0.008782968892459983\n",
      "Epoch [5953/20000], Training Loss: 0.017142263873081123, Validation Loss: 0.007831237244795251\n",
      "Epoch [5954/20000], Training Loss: 0.01326046634598502, Validation Loss: 0.022789077559835093\n",
      "Epoch [5955/20000], Training Loss: 0.010820197564897731, Validation Loss: 0.018578745498610327\n",
      "Epoch [5956/20000], Training Loss: 0.010767851594859426, Validation Loss: 0.03424751383796228\n",
      "Epoch [5957/20000], Training Loss: 0.021546233389796856, Validation Loss: 0.006582359644008418\n",
      "Epoch [5958/20000], Training Loss: 0.014135687262723755, Validation Loss: 0.012241488885652182\n",
      "Epoch [5959/20000], Training Loss: 0.020550059360955077, Validation Loss: 0.006609878279441087\n",
      "Epoch [5960/20000], Training Loss: 0.04674115646464218, Validation Loss: 0.02746577698729638\n",
      "Epoch [5961/20000], Training Loss: 0.024643899689960693, Validation Loss: 0.00878401369981367\n",
      "Epoch [5962/20000], Training Loss: 0.007934890629258007, Validation Loss: 0.008590145164042166\n",
      "Epoch [5963/20000], Training Loss: 0.008905856516711148, Validation Loss: 0.008323713378951353\n",
      "Epoch [5964/20000], Training Loss: 0.006455850375849488, Validation Loss: 0.009310681796902956\n",
      "Epoch [5965/20000], Training Loss: 0.007019709689302545, Validation Loss: 0.004861532895900038\n",
      "Epoch [5966/20000], Training Loss: 0.00657534206818257, Validation Loss: 0.005460738685607494\n",
      "Epoch [5967/20000], Training Loss: 0.007164115998810823, Validation Loss: 0.004169305424706539\n",
      "Epoch [5968/20000], Training Loss: 0.011965405023147468, Validation Loss: 0.019598190072144646\n",
      "Epoch [5969/20000], Training Loss: 0.011009514805794294, Validation Loss: 0.017564705240926924\n",
      "Epoch [5970/20000], Training Loss: 0.02039768175122195, Validation Loss: 0.00797276031762199\n",
      "Epoch [5971/20000], Training Loss: 0.007897647255699016, Validation Loss: 0.005494001507502503\n",
      "Epoch [5972/20000], Training Loss: 0.011025322198422925, Validation Loss: 0.00795527761400398\n",
      "Epoch [5973/20000], Training Loss: 0.01275652599204997, Validation Loss: 0.009309377922933374\n",
      "Epoch [5974/20000], Training Loss: 0.018804785184329376, Validation Loss: 0.015264001662639467\n",
      "Epoch [5975/20000], Training Loss: 0.016220611994088228, Validation Loss: 0.022049042007956578\n",
      "Epoch [5976/20000], Training Loss: 0.020896929773568575, Validation Loss: 0.021088944223057744\n",
      "Epoch [5977/20000], Training Loss: 0.01237469185774184, Validation Loss: 0.010146438037841864\n",
      "Epoch [5978/20000], Training Loss: 0.004572011463876281, Validation Loss: 0.004395897130929639\n",
      "Epoch [5979/20000], Training Loss: 0.009347198018856164, Validation Loss: 0.010343222485192077\n",
      "Epoch [5980/20000], Training Loss: 0.005912334632189304, Validation Loss: 0.006527659965091259\n",
      "Epoch [5981/20000], Training Loss: 0.007513746420623794, Validation Loss: 0.00446244784735761\n",
      "Epoch [5982/20000], Training Loss: 0.005166715239787274, Validation Loss: 0.004542547764076842\n",
      "Epoch [5983/20000], Training Loss: 0.006428551274958798, Validation Loss: 0.0034302233901552124\n",
      "Epoch [5984/20000], Training Loss: 0.006091790005614582, Validation Loss: 0.004598516110206674\n",
      "Epoch [5985/20000], Training Loss: 0.023712784633452038, Validation Loss: 0.026054594690701878\n",
      "Epoch [5986/20000], Training Loss: 0.010988474557442325, Validation Loss: 0.00396992139957827\n",
      "Epoch [5987/20000], Training Loss: 0.003992897116404492, Validation Loss: 0.003987480280427899\n",
      "Epoch [5988/20000], Training Loss: 0.004383209348556453, Validation Loss: 0.003218957346148394\n",
      "Epoch [5989/20000], Training Loss: 0.009093802626011893, Validation Loss: 0.010591177951890381\n",
      "Epoch [5990/20000], Training Loss: 0.006841992978089755, Validation Loss: 0.002445736601610119\n",
      "Epoch [5991/20000], Training Loss: 0.008809360538309972, Validation Loss: 0.005590798765684349\n",
      "Epoch [5992/20000], Training Loss: 0.03138606592209011, Validation Loss: 0.012614868991958444\n",
      "Epoch [5993/20000], Training Loss: 0.018367297751995335, Validation Loss: 0.013235547190995131\n",
      "Epoch [5994/20000], Training Loss: 0.015920586906917476, Validation Loss: 0.008486332582283042\n",
      "Epoch [5995/20000], Training Loss: 0.008909528998109246, Validation Loss: 0.007987796557176885\n",
      "Epoch [5996/20000], Training Loss: 0.008720887457522102, Validation Loss: 0.014831596373273945\n",
      "Epoch [5997/20000], Training Loss: 0.011436800262474987, Validation Loss: 0.0035846137847000505\n",
      "Epoch [5998/20000], Training Loss: 0.009490218354455595, Validation Loss: 0.00514261623188071\n",
      "Epoch [5999/20000], Training Loss: 0.005575528944193918, Validation Loss: 0.011618567168107006\n",
      "Epoch [6000/20000], Training Loss: 0.009205276011406178, Validation Loss: 0.003506490419203447\n",
      "Epoch [6001/20000], Training Loss: 0.004869524460186118, Validation Loss: 0.003752750135235905\n",
      "Epoch [6002/20000], Training Loss: 0.0037606027188823127, Validation Loss: 0.014681187706959318\n",
      "Epoch [6003/20000], Training Loss: 0.015150194572405391, Validation Loss: 0.006741395872881061\n",
      "Epoch [6004/20000], Training Loss: 0.016504133961136955, Validation Loss: 0.06531001622384533\n",
      "Epoch [6005/20000], Training Loss: 0.04884466210413458, Validation Loss: 0.04684038794099691\n",
      "Epoch [6006/20000], Training Loss: 0.04119777315853363, Validation Loss: 0.013054186539080064\n",
      "Epoch [6007/20000], Training Loss: 0.02548086128912733, Validation Loss: 0.017892144688610032\n",
      "Epoch [6008/20000], Training Loss: 0.014780631691511432, Validation Loss: 0.046707451444250446\n",
      "Epoch [6009/20000], Training Loss: 0.03420573835527258, Validation Loss: 0.02394095180580204\n",
      "Epoch [6010/20000], Training Loss: 0.013125153769838757, Validation Loss: 0.007425546074283379\n",
      "Epoch [6011/20000], Training Loss: 0.014847982762928171, Validation Loss: 0.006852134022796119\n",
      "Epoch [6012/20000], Training Loss: 0.012447336378110256, Validation Loss: 0.011572183926806562\n",
      "Epoch [6013/20000], Training Loss: 0.010207247704134456, Validation Loss: 0.006955359081205515\n",
      "Epoch [6014/20000], Training Loss: 0.007935114298431602, Validation Loss: 0.007775894899688345\n",
      "Epoch [6015/20000], Training Loss: 0.010533482809218444, Validation Loss: 0.00572326245648063\n",
      "Epoch [6016/20000], Training Loss: 0.009559847056932216, Validation Loss: 0.004233867582765275\n",
      "Epoch [6017/20000], Training Loss: 0.005832746101077646, Validation Loss: 0.004166712314827237\n",
      "Epoch [6018/20000], Training Loss: 0.006224470360653608, Validation Loss: 0.003546355451297651\n",
      "Epoch [6019/20000], Training Loss: 0.004109951403474302, Validation Loss: 0.014287230504335557\n",
      "Epoch [6020/20000], Training Loss: 0.014658455313110608, Validation Loss: 0.015563637396240464\n",
      "Epoch [6021/20000], Training Loss: 0.030888844601577148, Validation Loss: 0.015255595983693939\n",
      "Epoch [6022/20000], Training Loss: 0.055250510683565936, Validation Loss: 0.02027762447762273\n",
      "Epoch [6023/20000], Training Loss: 0.031607636903312857, Validation Loss: 0.016798380333284988\n",
      "Epoch [6024/20000], Training Loss: 0.017170632440995957, Validation Loss: 0.007609896262514515\n",
      "Epoch [6025/20000], Training Loss: 0.01080708169112248, Validation Loss: 0.010568592267061052\n",
      "Epoch [6026/20000], Training Loss: 0.013205052247420619, Validation Loss: 0.0158642431550626\n",
      "Epoch [6027/20000], Training Loss: 0.01885326449770933, Validation Loss: 0.005813092257345228\n",
      "Epoch [6028/20000], Training Loss: 0.009247651502456782, Validation Loss: 0.01680879489258935\n",
      "Epoch [6029/20000], Training Loss: 0.007275137385087354, Validation Loss: 0.005611128158177182\n",
      "Epoch [6030/20000], Training Loss: 0.006149542675952294, Validation Loss: 0.0075988919786271125\n",
      "Epoch [6031/20000], Training Loss: 0.0062548289674201184, Validation Loss: 0.004878209759356521\n",
      "Epoch [6032/20000], Training Loss: 0.0058411635109223425, Validation Loss: 0.00447833286216337\n",
      "Epoch [6033/20000], Training Loss: 0.007219855112323005, Validation Loss: 0.010725137727247005\n",
      "Epoch [6034/20000], Training Loss: 0.012610447060848986, Validation Loss: 0.005618688089400946\n",
      "Epoch [6035/20000], Training Loss: 0.01052276075019368, Validation Loss: 0.007732790519980881\n",
      "Epoch [6036/20000], Training Loss: 0.006441087564391117, Validation Loss: 0.004636843630773261\n",
      "Epoch [6037/20000], Training Loss: 0.005820542985540149, Validation Loss: 0.006434702774527068\n",
      "Epoch [6038/20000], Training Loss: 0.006373395885540438, Validation Loss: 0.003827526183158625\n",
      "Epoch [6039/20000], Training Loss: 0.006954516850653038, Validation Loss: 0.010265730917256275\n",
      "Epoch [6040/20000], Training Loss: 0.017726245863221784, Validation Loss: 0.005660629652896075\n",
      "Epoch [6041/20000], Training Loss: 0.006824001052466754, Validation Loss: 0.0061801813769436975\n",
      "Epoch [6042/20000], Training Loss: 0.016411465550066038, Validation Loss: 0.003936262325623829\n",
      "Epoch [6043/20000], Training Loss: 0.037996028820219054, Validation Loss: 0.01724829466132048\n",
      "Epoch [6044/20000], Training Loss: 0.027783527367448966, Validation Loss: 0.017782242240303146\n",
      "Epoch [6045/20000], Training Loss: 0.02086903285401474, Validation Loss: 0.01831171008234378\n",
      "Epoch [6046/20000], Training Loss: 0.014384560381586198, Validation Loss: 0.008284749456087022\n",
      "Epoch [6047/20000], Training Loss: 0.012770270734368492, Validation Loss: 0.006988246923940876\n",
      "Epoch [6048/20000], Training Loss: 0.014108390996365674, Validation Loss: 0.019742494284297356\n",
      "Epoch [6049/20000], Training Loss: 0.015164601670611384, Validation Loss: 0.0394912001543616\n",
      "Epoch [6050/20000], Training Loss: 0.012874994971623113, Validation Loss: 0.006002322683221857\n",
      "Epoch [6051/20000], Training Loss: 0.006572168312102024, Validation Loss: 0.014769895272658167\n",
      "Epoch [6052/20000], Training Loss: 0.006151484061514826, Validation Loss: 0.010453941966802003\n",
      "Epoch [6053/20000], Training Loss: 0.010276798707699137, Validation Loss: 0.011217770675403114\n",
      "Epoch [6054/20000], Training Loss: 0.00961943285489854, Validation Loss: 0.007031605940807951\n",
      "Epoch [6055/20000], Training Loss: 0.021537185697395347, Validation Loss: 0.018862176997312678\n",
      "Epoch [6056/20000], Training Loss: 0.016969507747229988, Validation Loss: 0.005980282404315014\n",
      "Epoch [6057/20000], Training Loss: 0.006630130851192462, Validation Loss: 0.004279929865853254\n",
      "Epoch [6058/20000], Training Loss: 0.011283801752142608, Validation Loss: 0.00430534510542202\n",
      "Epoch [6059/20000], Training Loss: 0.007295350295472807, Validation Loss: 0.004696858765912891\n",
      "Epoch [6060/20000], Training Loss: 0.007907731400471059, Validation Loss: 0.004512785920171934\n",
      "Epoch [6061/20000], Training Loss: 0.0056887946240229735, Validation Loss: 0.004112414398800216\n",
      "Epoch [6062/20000], Training Loss: 0.009375016335980035, Validation Loss: 0.003895404255510065\n",
      "Epoch [6063/20000], Training Loss: 0.011982325776313831, Validation Loss: 0.00858677381378519\n",
      "Epoch [6064/20000], Training Loss: 0.004658737523381465, Validation Loss: 0.006333666660274113\n",
      "Epoch [6065/20000], Training Loss: 0.010235030718571838, Validation Loss: 0.011690196181786763\n",
      "Epoch [6066/20000], Training Loss: 0.01905623404828865, Validation Loss: 0.008833875020497674\n",
      "Epoch [6067/20000], Training Loss: 0.022059810868605773, Validation Loss: 0.004759456613750704\n",
      "Epoch [6068/20000], Training Loss: 0.018985995297823268, Validation Loss: 0.004977584237622068\n",
      "Epoch [6069/20000], Training Loss: 0.005572471349816104, Validation Loss: 0.004725355969640077\n",
      "Epoch [6070/20000], Training Loss: 0.005657845898115609, Validation Loss: 0.0033470535057469014\n",
      "Epoch [6071/20000], Training Loss: 0.006248989430397549, Validation Loss: 0.008537626247743901\n",
      "Epoch [6072/20000], Training Loss: 0.017132134868006688, Validation Loss: 0.0062656938818171125\n",
      "Epoch [6073/20000], Training Loss: 0.009853347303370745, Validation Loss: 0.0040260689037755325\n",
      "Epoch [6074/20000], Training Loss: 0.00990488374269002, Validation Loss: 0.00466040206362907\n",
      "Epoch [6075/20000], Training Loss: 0.004506906939111234, Validation Loss: 0.0060303381677450875\n",
      "Epoch [6076/20000], Training Loss: 0.007957851572427899, Validation Loss: 0.0036016558805321047\n",
      "Epoch [6077/20000], Training Loss: 0.004023640117208872, Validation Loss: 0.007926704731754544\n",
      "Epoch [6078/20000], Training Loss: 0.009033400792499638, Validation Loss: 0.0040386051707790626\n",
      "Epoch [6079/20000], Training Loss: 0.011152366084778416, Validation Loss: 0.0038480148504049794\n",
      "Epoch [6080/20000], Training Loss: 0.006091184392321988, Validation Loss: 0.00263133306572984\n",
      "Epoch [6081/20000], Training Loss: 0.003879942723350333, Validation Loss: 0.008864335952265736\n",
      "Epoch [6082/20000], Training Loss: 0.006683624021401816, Validation Loss: 0.0035981466958950087\n",
      "Epoch [6083/20000], Training Loss: 0.0039483843117652994, Validation Loss: 0.002755605131077183\n",
      "Epoch [6084/20000], Training Loss: 0.004705075699478455, Validation Loss: 0.006372300747057936\n",
      "Epoch [6085/20000], Training Loss: 0.01073903478494945, Validation Loss: 0.0098549514507033\n",
      "Epoch [6086/20000], Training Loss: 0.013842490196111612, Validation Loss: 0.02045211935806281\n",
      "Epoch [6087/20000], Training Loss: 0.01366256301948202, Validation Loss: 0.008910998142634139\n",
      "Epoch [6088/20000], Training Loss: 0.036357461025805345, Validation Loss: 0.009476229927843503\n",
      "Epoch [6089/20000], Training Loss: 0.060770426892434316, Validation Loss: 0.013312882718992114\n",
      "Epoch [6090/20000], Training Loss: 0.016088482632247696, Validation Loss: 0.007088547355067476\n",
      "Epoch [6091/20000], Training Loss: 0.008709246932994574, Validation Loss: 0.005899772345043987\n",
      "Epoch [6092/20000], Training Loss: 0.009485865015968946, Validation Loss: 0.011227303685440606\n",
      "Epoch [6093/20000], Training Loss: 0.012657558198303118, Validation Loss: 0.009025267446824208\n",
      "Epoch [6094/20000], Training Loss: 0.011089075739229364, Validation Loss: 0.007507617491232362\n",
      "Epoch [6095/20000], Training Loss: 0.0228997947831106, Validation Loss: 0.01251531841554814\n",
      "Epoch [6096/20000], Training Loss: 0.019695186020856324, Validation Loss: 0.009018795851846204\n",
      "Epoch [6097/20000], Training Loss: 0.01316291932848149, Validation Loss: 0.010294187279116938\n",
      "Epoch [6098/20000], Training Loss: 0.013219289423432201, Validation Loss: 0.00837085776846688\n",
      "Epoch [6099/20000], Training Loss: 0.014418663020341529, Validation Loss: 0.008950069646191747\n",
      "Epoch [6100/20000], Training Loss: 0.007383560739981476, Validation Loss: 0.00577842844391593\n",
      "Epoch [6101/20000], Training Loss: 0.0072761701220380405, Validation Loss: 0.0035120983059836624\n",
      "Epoch [6102/20000], Training Loss: 0.00608523489165285, Validation Loss: 0.0060863543802075325\n",
      "Epoch [6103/20000], Training Loss: 0.012999180897687828, Validation Loss: 0.005100240466033158\n",
      "Epoch [6104/20000], Training Loss: 0.005034694930405489, Validation Loss: 0.007995939009079831\n",
      "Epoch [6105/20000], Training Loss: 0.007254185435678144, Validation Loss: 0.007279758478018396\n",
      "Epoch [6106/20000], Training Loss: 0.011135672636945466, Validation Loss: 0.006910556454731217\n",
      "Epoch [6107/20000], Training Loss: 0.005992927739888338, Validation Loss: 0.03528330455771668\n",
      "Epoch [6108/20000], Training Loss: 0.01509841045533124, Validation Loss: 0.0038927227291196687\n",
      "Epoch [6109/20000], Training Loss: 0.016375363951997964, Validation Loss: 0.011191630746907322\n",
      "Epoch [6110/20000], Training Loss: 0.017814716666699888, Validation Loss: 0.018390140848733187\n",
      "Epoch [6111/20000], Training Loss: 0.011851780399281, Validation Loss: 0.005806939333784352\n",
      "Epoch [6112/20000], Training Loss: 0.00813312281747715, Validation Loss: 0.0047462638364907305\n",
      "Epoch [6113/20000], Training Loss: 0.0073510500671026035, Validation Loss: 0.0041155006173149945\n",
      "Epoch [6114/20000], Training Loss: 0.014222062218842828, Validation Loss: 0.003593145998073461\n",
      "Epoch [6115/20000], Training Loss: 0.013490371821847345, Validation Loss: 0.005758493893462661\n",
      "Epoch [6116/20000], Training Loss: 0.017603110009596485, Validation Loss: 0.007740994446911438\n",
      "Epoch [6117/20000], Training Loss: 0.01002642792887595, Validation Loss: 0.006689450157084918\n",
      "Epoch [6118/20000], Training Loss: 0.006679702647878523, Validation Loss: 0.004913536136752684\n",
      "Epoch [6119/20000], Training Loss: 0.006909698189182174, Validation Loss: 0.003728411061989131\n",
      "Epoch [6120/20000], Training Loss: 0.0052718330656976575, Validation Loss: 0.003247708047443406\n",
      "Epoch [6121/20000], Training Loss: 0.004809408260501057, Validation Loss: 0.01176178317210327\n",
      "Epoch [6122/20000], Training Loss: 0.010525118775798805, Validation Loss: 0.005607566759246245\n",
      "Epoch [6123/20000], Training Loss: 0.02431687404272712, Validation Loss: 0.010273295719512378\n",
      "Epoch [6124/20000], Training Loss: 0.03784872787842427, Validation Loss: 0.004469398530090984\n",
      "Epoch [6125/20000], Training Loss: 0.017646342801786626, Validation Loss: 0.0563426096216502\n",
      "Epoch [6126/20000], Training Loss: 0.023993561697092707, Validation Loss: 0.026187620456255445\n",
      "Epoch [6127/20000], Training Loss: 0.012786393637985125, Validation Loss: 0.004935255297043893\n",
      "Epoch [6128/20000], Training Loss: 0.013868460830833231, Validation Loss: 0.0062862920989833326\n",
      "Epoch [6129/20000], Training Loss: 0.007210309109983167, Validation Loss: 0.006201953049899852\n",
      "Epoch [6130/20000], Training Loss: 0.005738686736939209, Validation Loss: 0.0050341779855703605\n",
      "Epoch [6131/20000], Training Loss: 0.005258062750466966, Validation Loss: 0.007315004272121379\n",
      "Epoch [6132/20000], Training Loss: 0.006589245117668595, Validation Loss: 0.0036591442993605078\n",
      "Epoch [6133/20000], Training Loss: 0.007123903038778475, Validation Loss: 0.003648491987210686\n",
      "Epoch [6134/20000], Training Loss: 0.006205029588142809, Validation Loss: 0.01029086876923202\n",
      "Epoch [6135/20000], Training Loss: 0.011327183722252292, Validation Loss: 0.025449473636213225\n",
      "Epoch [6136/20000], Training Loss: 0.013023576273034061, Validation Loss: 0.02104911236240684\n",
      "Epoch [6137/20000], Training Loss: 0.02224172003739763, Validation Loss: 0.012784193158771424\n",
      "Epoch [6138/20000], Training Loss: 0.01251125241313795, Validation Loss: 0.035420980384539495\n",
      "Epoch [6139/20000], Training Loss: 0.036630299411107056, Validation Loss: 0.006858366256372912\n",
      "Epoch [6140/20000], Training Loss: 0.01905201360256927, Validation Loss: 0.014245033421372812\n",
      "Epoch [6141/20000], Training Loss: 0.017247985258239038, Validation Loss: 0.013395673806371633\n",
      "Epoch [6142/20000], Training Loss: 0.015483431264458756, Validation Loss: 0.0054808586880556375\n",
      "Epoch [6143/20000], Training Loss: 0.015165899838653527, Validation Loss: 0.0223475906864873\n",
      "Epoch [6144/20000], Training Loss: 0.022741330038115848, Validation Loss: 0.007373595453827875\n",
      "Epoch [6145/20000], Training Loss: 0.028655895678509426, Validation Loss: 0.006934359908265313\n",
      "Epoch [6146/20000], Training Loss: 0.022249708887622028, Validation Loss: 0.008666060418639583\n",
      "Epoch [6147/20000], Training Loss: 0.0077256680524442345, Validation Loss: 0.010417599549567415\n",
      "Epoch [6148/20000], Training Loss: 0.008958272648409807, Validation Loss: 0.006428827923108267\n",
      "Epoch [6149/20000], Training Loss: 0.014385125674639962, Validation Loss: 0.02345298772524527\n",
      "Epoch [6150/20000], Training Loss: 0.007102093426510692, Validation Loss: 0.0052468971072617675\n",
      "Epoch [6151/20000], Training Loss: 0.006377044429037986, Validation Loss: 0.004514772862454005\n",
      "Epoch [6152/20000], Training Loss: 0.006752074863665517, Validation Loss: 0.018363196566757063\n",
      "Epoch [6153/20000], Training Loss: 0.0104913625520047, Validation Loss: 0.004909310481187796\n",
      "Epoch [6154/20000], Training Loss: 0.006123405407249395, Validation Loss: 0.009648488621921279\n",
      "Epoch [6155/20000], Training Loss: 0.010012980843644723, Validation Loss: 0.006334669972492942\n",
      "Epoch [6156/20000], Training Loss: 0.007877061345165462, Validation Loss: 0.007724747437294914\n",
      "Epoch [6157/20000], Training Loss: 0.010477350821020082, Validation Loss: 0.004110408229880339\n",
      "Epoch [6158/20000], Training Loss: 0.0062820331461677726, Validation Loss: 0.005760988432687749\n",
      "Epoch [6159/20000], Training Loss: 0.005926861581558894, Validation Loss: 0.013117723807097763\n",
      "Epoch [6160/20000], Training Loss: 0.008403157906806362, Validation Loss: 0.01806539725344742\n",
      "Epoch [6161/20000], Training Loss: 0.01661911880052815, Validation Loss: 0.01411387043223518\n",
      "Epoch [6162/20000], Training Loss: 0.01686478615426625, Validation Loss: 0.003277655999675793\n",
      "Epoch [6163/20000], Training Loss: 0.018277603774289934, Validation Loss: 0.005117607059393744\n",
      "Epoch [6164/20000], Training Loss: 0.00856560136993981, Validation Loss: 0.010104932289907862\n",
      "Epoch [6165/20000], Training Loss: 0.015214864779928965, Validation Loss: 0.009448972794254067\n",
      "Epoch [6166/20000], Training Loss: 0.011327978479260179, Validation Loss: 0.007574979262228355\n",
      "Epoch [6167/20000], Training Loss: 0.008197102874484179, Validation Loss: 0.009964349347834047\n",
      "Epoch [6168/20000], Training Loss: 0.01960502430197916, Validation Loss: 0.0068186503879214145\n",
      "Epoch [6169/20000], Training Loss: 0.006905217004324575, Validation Loss: 0.00777218853661272\n",
      "Epoch [6170/20000], Training Loss: 0.008107575964718958, Validation Loss: 0.006365718910373376\n",
      "Epoch [6171/20000], Training Loss: 0.007727430102217373, Validation Loss: 0.004081310928506911\n",
      "Epoch [6172/20000], Training Loss: 0.014005576518164682, Validation Loss: 0.0038079967642222607\n",
      "Epoch [6173/20000], Training Loss: 0.018102264078541857, Validation Loss: 0.00791200802902178\n",
      "Epoch [6174/20000], Training Loss: 0.014323329881465594, Validation Loss: 0.006677927538572469\n",
      "Epoch [6175/20000], Training Loss: 0.013656249053643219, Validation Loss: 0.007987747550557296\n",
      "Epoch [6176/20000], Training Loss: 0.032780822618016306, Validation Loss: 0.043828903675152735\n",
      "Epoch [6177/20000], Training Loss: 0.07475706186752566, Validation Loss: 0.054925691130197914\n",
      "Epoch [6178/20000], Training Loss: 0.053789588198664466, Validation Loss: 0.12390405104282436\n",
      "Epoch [6179/20000], Training Loss: 0.04691660076579345, Validation Loss: 0.01900206213135139\n",
      "Epoch [6180/20000], Training Loss: 0.026257234600572183, Validation Loss: 0.020917237881803885\n",
      "Epoch [6181/20000], Training Loss: 0.01815596335966672, Validation Loss: 0.011397411738633829\n",
      "Epoch [6182/20000], Training Loss: 0.011724547000734933, Validation Loss: 0.009375029319812949\n",
      "Epoch [6183/20000], Training Loss: 0.009527850434616474, Validation Loss: 0.007088185202779381\n",
      "Epoch [6184/20000], Training Loss: 0.006791391434879708, Validation Loss: 0.0066821274544314035\n",
      "Epoch [6185/20000], Training Loss: 0.008714451346479888, Validation Loss: 0.0037757549670719265\n",
      "Epoch [6186/20000], Training Loss: 0.017644321868179498, Validation Loss: 0.005969492869602229\n",
      "Epoch [6187/20000], Training Loss: 0.027664244692589688, Validation Loss: 0.0878097504666936\n",
      "Epoch [6188/20000], Training Loss: 0.01681719199820821, Validation Loss: 0.06726629629083197\n",
      "Epoch [6189/20000], Training Loss: 0.02999440299858439, Validation Loss: 0.01022677218160035\n",
      "Epoch [6190/20000], Training Loss: 0.009248198646153989, Validation Loss: 0.008310947074798085\n",
      "Epoch [6191/20000], Training Loss: 0.011620324583158695, Validation Loss: 0.0043515737625788075\n",
      "Epoch [6192/20000], Training Loss: 0.0059095573352091636, Validation Loss: 0.00511974963612286\n",
      "Epoch [6193/20000], Training Loss: 0.005420817099677931, Validation Loss: 0.006904940603427738\n",
      "Epoch [6194/20000], Training Loss: 0.0064569365994040185, Validation Loss: 0.006127601874122439\n",
      "Epoch [6195/20000], Training Loss: 0.008028317920564274, Validation Loss: 0.0033891264648348495\n",
      "Epoch [6196/20000], Training Loss: 0.014099470160934808, Validation Loss: 0.005395135904341787\n",
      "Epoch [6197/20000], Training Loss: 0.015404937069044016, Validation Loss: 0.014532120505464263\n",
      "Epoch [6198/20000], Training Loss: 0.030407464057913915, Validation Loss: 0.006167191743305774\n",
      "Epoch [6199/20000], Training Loss: 0.017864810456688116, Validation Loss: 0.011409512030435767\n",
      "Epoch [6200/20000], Training Loss: 0.007076170021069369, Validation Loss: 0.00485956096322937\n",
      "Epoch [6201/20000], Training Loss: 0.008694096911183027, Validation Loss: 0.004662406717994656\n",
      "Epoch [6202/20000], Training Loss: 0.00777061140096131, Validation Loss: 0.0038420391035159374\n",
      "Epoch [6203/20000], Training Loss: 0.011839299625405277, Validation Loss: 0.0018860743233069482\n",
      "Epoch [6204/20000], Training Loss: 0.01023669765527302, Validation Loss: 0.012140297917851237\n",
      "Epoch [6205/20000], Training Loss: 0.033423544581767474, Validation Loss: 0.13354471769637838\n",
      "Epoch [6206/20000], Training Loss: 0.0625099648106178, Validation Loss: 0.04967365423424105\n",
      "Epoch [6207/20000], Training Loss: 0.0404448417601608, Validation Loss: 0.016083512486588476\n",
      "Epoch [6208/20000], Training Loss: 0.03151269279104391, Validation Loss: 0.014768834073007473\n",
      "Epoch [6209/20000], Training Loss: 0.021682002548394457, Validation Loss: 0.01197131736101474\n",
      "Epoch [6210/20000], Training Loss: 0.012513893689694149, Validation Loss: 0.010213416061382152\n",
      "Epoch [6211/20000], Training Loss: 0.009775680572991925, Validation Loss: 0.008533456820324707\n",
      "Epoch [6212/20000], Training Loss: 0.008960746594571642, Validation Loss: 0.006876783203681823\n",
      "Epoch [6213/20000], Training Loss: 0.007593147365176784, Validation Loss: 0.006239824661341637\n",
      "Epoch [6214/20000], Training Loss: 0.009000774335748116, Validation Loss: 0.005428644706301254\n",
      "Epoch [6215/20000], Training Loss: 0.005518041435816404, Validation Loss: 0.008191914954200083\n",
      "Epoch [6216/20000], Training Loss: 0.013505141895750836, Validation Loss: 0.015142690703522175\n",
      "Epoch [6217/20000], Training Loss: 0.00941707992335848, Validation Loss: 0.005466194755886785\n",
      "Epoch [6218/20000], Training Loss: 0.009441713212124472, Validation Loss: 0.012949235217612503\n",
      "Epoch [6219/20000], Training Loss: 0.01473130751401186, Validation Loss: 0.0056627840908082105\n",
      "Epoch [6220/20000], Training Loss: 0.013457702660161885, Validation Loss: 0.009991649005842516\n",
      "Epoch [6221/20000], Training Loss: 0.009329776765038591, Validation Loss: 0.009603437986957683\n",
      "Epoch [6222/20000], Training Loss: 0.026289529925894124, Validation Loss: 0.0076242282446920785\n",
      "Epoch [6223/20000], Training Loss: 0.009026741521665826, Validation Loss: 0.009254371962080248\n",
      "Epoch [6224/20000], Training Loss: 0.0063083014704586405, Validation Loss: 0.004851844916174741\n",
      "Epoch [6225/20000], Training Loss: 0.01067965854495664, Validation Loss: 0.0035905042371788503\n",
      "Epoch [6226/20000], Training Loss: 0.012224060904502818, Validation Loss: 0.004820067300775983\n",
      "Epoch [6227/20000], Training Loss: 0.006965069694810414, Validation Loss: 0.005627673801654964\n",
      "Epoch [6228/20000], Training Loss: 0.007594714252393585, Validation Loss: 0.005644555217730028\n",
      "Epoch [6229/20000], Training Loss: 0.0041962158076265564, Validation Loss: 0.003116247329752956\n",
      "Epoch [6230/20000], Training Loss: 0.0041004858196954175, Validation Loss: 0.0029846660105964213\n",
      "Epoch [6231/20000], Training Loss: 0.006001632155468022, Validation Loss: 0.014159705881831972\n",
      "Epoch [6232/20000], Training Loss: 0.012451789237405007, Validation Loss: 0.0053348493083927705\n",
      "Epoch [6233/20000], Training Loss: 0.008451367686835251, Validation Loss: 0.00809256755266168\n",
      "Epoch [6234/20000], Training Loss: 0.017101693979514363, Validation Loss: 0.007346802557571712\n",
      "Epoch [6235/20000], Training Loss: 0.01178598203348104, Validation Loss: 0.008352911935738965\n",
      "Epoch [6236/20000], Training Loss: 0.012842804487789294, Validation Loss: 0.03196820255986869\n",
      "Epoch [6237/20000], Training Loss: 0.03998409431161625, Validation Loss: 0.013742496490782028\n",
      "Epoch [6238/20000], Training Loss: 0.014138099432394873, Validation Loss: 0.008013290015307055\n",
      "Epoch [6239/20000], Training Loss: 0.006879509192783319, Validation Loss: 0.005697499591665941\n",
      "Epoch [6240/20000], Training Loss: 0.0075451423555413, Validation Loss: 0.005081720574837943\n",
      "Epoch [6241/20000], Training Loss: 0.01281309435476682, Validation Loss: 0.004742885143059539\n",
      "Epoch [6242/20000], Training Loss: 0.008503222350514403, Validation Loss: 0.012691698312068971\n",
      "Epoch [6243/20000], Training Loss: 0.00774539597581939, Validation Loss: 0.004103466723920844\n",
      "Epoch [6244/20000], Training Loss: 0.006175190456685543, Validation Loss: 0.005160993399710735\n",
      "Epoch [6245/20000], Training Loss: 0.010916233116794112, Validation Loss: 0.0033473955468902466\n",
      "Epoch [6246/20000], Training Loss: 0.015540855735058099, Validation Loss: 0.006152523297661868\n",
      "Epoch [6247/20000], Training Loss: 0.023442775760486256, Validation Loss: 0.007640195963826305\n",
      "Epoch [6248/20000], Training Loss: 0.05586821541094521, Validation Loss: 0.06969369331532757\n",
      "Epoch [6249/20000], Training Loss: 0.059362687429841444, Validation Loss: 0.01061231526741072\n",
      "Epoch [6250/20000], Training Loss: 0.011038708245905582, Validation Loss: 0.013578565662182232\n",
      "Epoch [6251/20000], Training Loss: 0.008252447471022606, Validation Loss: 0.007335642701426488\n",
      "Epoch [6252/20000], Training Loss: 0.0066012053970812955, Validation Loss: 0.008657872562090623\n",
      "Epoch [6253/20000], Training Loss: 0.007688768404477742, Validation Loss: 0.006389496607620718\n",
      "Epoch [6254/20000], Training Loss: 0.005525912191452724, Validation Loss: 0.006762216244737829\n",
      "Epoch [6255/20000], Training Loss: 0.005841796340454104, Validation Loss: 0.005696214514484511\n",
      "Epoch [6256/20000], Training Loss: 0.005277945860403374, Validation Loss: 0.004854867212218649\n",
      "Epoch [6257/20000], Training Loss: 0.005135976743069648, Validation Loss: 0.004886645562042499\n",
      "Epoch [6258/20000], Training Loss: 0.007499668156794671, Validation Loss: 0.003640302542960074\n",
      "Epoch [6259/20000], Training Loss: 0.01823471869075937, Validation Loss: 0.035707730304143245\n",
      "Epoch [6260/20000], Training Loss: 0.03051030750170217, Validation Loss: 0.012037364050615094\n",
      "Epoch [6261/20000], Training Loss: 0.028439563261729615, Validation Loss: 0.015063950331646774\n",
      "Epoch [6262/20000], Training Loss: 0.013146041709530567, Validation Loss: 0.013667705917896709\n",
      "Epoch [6263/20000], Training Loss: 0.009546467941878032, Validation Loss: 0.006166906402995664\n",
      "Epoch [6264/20000], Training Loss: 0.007766719903364512, Validation Loss: 0.00490722942213649\n",
      "Epoch [6265/20000], Training Loss: 0.004594500232834824, Validation Loss: 0.004437559695897367\n",
      "Epoch [6266/20000], Training Loss: 0.006240545169150989, Validation Loss: 0.005126469166734792\n",
      "Epoch [6267/20000], Training Loss: 0.005692437503187518, Validation Loss: 0.0035087149621598917\n",
      "Epoch [6268/20000], Training Loss: 0.003969760265005918, Validation Loss: 0.0073197801788053496\n",
      "Epoch [6269/20000], Training Loss: 0.004862578623163115, Validation Loss: 0.0037254816473770397\n",
      "Epoch [6270/20000], Training Loss: 0.0046755916726916825, Validation Loss: 0.0048173053777585805\n",
      "Epoch [6271/20000], Training Loss: 0.005510613164265773, Validation Loss: 0.003952080211758714\n",
      "Epoch [6272/20000], Training Loss: 0.006930087493466479, Validation Loss: 0.005359331965036874\n",
      "Epoch [6273/20000], Training Loss: 0.010247190083776201, Validation Loss: 0.00481079121566909\n",
      "Epoch [6274/20000], Training Loss: 0.006546997701661894, Validation Loss: 0.008395639958206231\n",
      "Epoch [6275/20000], Training Loss: 0.02335680263058748, Validation Loss: 0.009233864391327578\n",
      "Epoch [6276/20000], Training Loss: 0.022961516727394025, Validation Loss: 0.011131565760663542\n",
      "Epoch [6277/20000], Training Loss: 0.008834446123468556, Validation Loss: 0.015949591923344997\n",
      "Epoch [6278/20000], Training Loss: 0.010388715361062038, Validation Loss: 0.007572456316535993\n",
      "Epoch [6279/20000], Training Loss: 0.00712095648590808, Validation Loss: 0.006791253824433495\n",
      "Epoch [6280/20000], Training Loss: 0.007136168430276614, Validation Loss: 0.0038410724736032925\n",
      "Epoch [6281/20000], Training Loss: 0.017245598973074396, Validation Loss: 0.004641901244083003\n",
      "Epoch [6282/20000], Training Loss: 0.019609856512813297, Validation Loss: 0.005060774767506311\n",
      "Epoch [6283/20000], Training Loss: 0.015287322649133525, Validation Loss: 0.014243604633057723\n",
      "Epoch [6284/20000], Training Loss: 0.021066247337980064, Validation Loss: 0.005902200475621352\n",
      "Epoch [6285/20000], Training Loss: 0.008112552927091852, Validation Loss: 0.006139711716416839\n",
      "Epoch [6286/20000], Training Loss: 0.011351272568780197, Validation Loss: 0.012502059778595611\n",
      "Epoch [6287/20000], Training Loss: 0.017534954943195253, Validation Loss: 0.011728691609760691\n",
      "Epoch [6288/20000], Training Loss: 0.015210435740300454, Validation Loss: 0.019196260930779578\n",
      "Epoch [6289/20000], Training Loss: 0.047239809037169574, Validation Loss: 0.015306901187873987\n",
      "Epoch [6290/20000], Training Loss: 0.01688690551756216, Validation Loss: 0.01240928183428228\n",
      "Epoch [6291/20000], Training Loss: 0.007188420958950051, Validation Loss: 0.00547930659740814\n",
      "Epoch [6292/20000], Training Loss: 0.00675174581747205, Validation Loss: 0.005239770938195372\n",
      "Epoch [6293/20000], Training Loss: 0.007895828790164419, Validation Loss: 0.017193197932873567\n",
      "Epoch [6294/20000], Training Loss: 0.009414281703357119, Validation Loss: 0.00745093623809266\n",
      "Epoch [6295/20000], Training Loss: 0.005378469433318449, Validation Loss: 0.009452405700228286\n",
      "Epoch [6296/20000], Training Loss: 0.010030409528034008, Validation Loss: 0.004209119375202265\n",
      "Epoch [6297/20000], Training Loss: 0.005509752171097456, Validation Loss: 0.0031364514229932994\n",
      "Epoch [6298/20000], Training Loss: 0.004444282664605582, Validation Loss: 0.005700995556379341\n",
      "Epoch [6299/20000], Training Loss: 0.0066247203417983, Validation Loss: 0.0028325969861141076\n",
      "Epoch [6300/20000], Training Loss: 0.0073665534017501545, Validation Loss: 0.00425714431076228\n",
      "Epoch [6301/20000], Training Loss: 0.012512237451752688, Validation Loss: 0.00787347100205287\n",
      "Epoch [6302/20000], Training Loss: 0.011773012567053749, Validation Loss: 0.013295757493614653\n",
      "Epoch [6303/20000], Training Loss: 0.046317795087816194, Validation Loss: 0.01429110627471275\n",
      "Epoch [6304/20000], Training Loss: 0.022943128846236505, Validation Loss: 0.0034136813668348104\n",
      "Epoch [6305/20000], Training Loss: 0.009261121176158278, Validation Loss: 0.00778610056089032\n",
      "Epoch [6306/20000], Training Loss: 0.00624063699589377, Validation Loss: 0.007197270615145713\n",
      "Epoch [6307/20000], Training Loss: 0.00800509360020182, Validation Loss: 0.004795524373613651\n",
      "Epoch [6308/20000], Training Loss: 0.007817781156128538, Validation Loss: 0.00790542012068077\n",
      "Epoch [6309/20000], Training Loss: 0.009095632743472899, Validation Loss: 0.008603604714705528\n",
      "Epoch [6310/20000], Training Loss: 0.014011423754449684, Validation Loss: 0.015911009545431756\n",
      "Epoch [6311/20000], Training Loss: 0.0073633611266684185, Validation Loss: 0.009175195223686039\n",
      "Epoch [6312/20000], Training Loss: 0.006011670833686367, Validation Loss: 0.0029417218848461096\n",
      "Epoch [6313/20000], Training Loss: 0.004882103075423012, Validation Loss: 0.0035517010314184256\n",
      "Epoch [6314/20000], Training Loss: 0.004063678890815936, Validation Loss: 0.015467850122888507\n",
      "Epoch [6315/20000], Training Loss: 0.013573941392158824, Validation Loss: 0.03973040373699784\n",
      "Epoch [6316/20000], Training Loss: 0.022249758217542746, Validation Loss: 0.06819771230220807\n",
      "Epoch [6317/20000], Training Loss: 0.022554962760685675, Validation Loss: 0.031800121091134566\n",
      "Epoch [6318/20000], Training Loss: 0.02519303757747236, Validation Loss: 0.00771589998234999\n",
      "Epoch [6319/20000], Training Loss: 0.04880933138779905, Validation Loss: 0.028208608606039975\n",
      "Epoch [6320/20000], Training Loss: 0.01799113065603056, Validation Loss: 0.006103375387389016\n",
      "Epoch [6321/20000], Training Loss: 0.01579410592343525, Validation Loss: 0.0177157153385841\n",
      "Epoch [6322/20000], Training Loss: 0.03618581390203092, Validation Loss: 0.03911191439484745\n",
      "Epoch [6323/20000], Training Loss: 0.025741922579722347, Validation Loss: 0.011421931675223667\n",
      "Epoch [6324/20000], Training Loss: 0.009562420974751669, Validation Loss: 0.00902490623021939\n",
      "Epoch [6325/20000], Training Loss: 0.00973621266686158, Validation Loss: 0.009571328512988333\n",
      "Epoch [6326/20000], Training Loss: 0.009333378452408527, Validation Loss: 0.006985160413462056\n",
      "Epoch [6327/20000], Training Loss: 0.008061518788703584, Validation Loss: 0.004916627823669322\n",
      "Epoch [6328/20000], Training Loss: 0.007512643505054127, Validation Loss: 0.004964803500521546\n",
      "Epoch [6329/20000], Training Loss: 0.005200237858973976, Validation Loss: 0.006996256304488112\n",
      "Epoch [6330/20000], Training Loss: 0.006496872919212494, Validation Loss: 0.005349601615664922\n",
      "Epoch [6331/20000], Training Loss: 0.007286605742202872, Validation Loss: 0.004296880015674885\n",
      "Epoch [6332/20000], Training Loss: 0.00774074747044194, Validation Loss: 0.0045288611023132875\n",
      "Epoch [6333/20000], Training Loss: 0.008184977821656503, Validation Loss: 0.007895106312429638\n",
      "Epoch [6334/20000], Training Loss: 0.008249758031524834, Validation Loss: 0.0067675241966426315\n",
      "Epoch [6335/20000], Training Loss: 0.006772623581387701, Validation Loss: 0.0077296003610412\n",
      "Epoch [6336/20000], Training Loss: 0.0054455777343329305, Validation Loss: 0.004751288175352134\n",
      "Epoch [6337/20000], Training Loss: 0.0061504645917531365, Validation Loss: 0.0035613876405733436\n",
      "Epoch [6338/20000], Training Loss: 0.004869371679010978, Validation Loss: 0.0056208028357056196\n",
      "Epoch [6339/20000], Training Loss: 0.005695790637608817, Validation Loss: 0.005578047025957338\n",
      "Epoch [6340/20000], Training Loss: 0.007710163633289214, Validation Loss: 0.005831996048576522\n",
      "Epoch [6341/20000], Training Loss: 0.01182712219750621, Validation Loss: 0.00643491405048329\n",
      "Epoch [6342/20000], Training Loss: 0.015858872045230652, Validation Loss: 0.0033122464813938484\n",
      "Epoch [6343/20000], Training Loss: 0.0163646860341292, Validation Loss: 0.0037488875748924295\n",
      "Epoch [6344/20000], Training Loss: 0.007839846753215949, Validation Loss: 0.005332008693399258\n",
      "Epoch [6345/20000], Training Loss: 0.010880012047502012, Validation Loss: 0.006204872881628377\n",
      "Epoch [6346/20000], Training Loss: 0.010181460105481424, Validation Loss: 0.007088682612510436\n",
      "Epoch [6347/20000], Training Loss: 0.004716113251301327, Validation Loss: 0.017084064298619844\n",
      "Epoch [6348/20000], Training Loss: 0.0104398105915087, Validation Loss: 0.009899289379705231\n",
      "Epoch [6349/20000], Training Loss: 0.02118156581439377, Validation Loss: 0.007293926492613678\n",
      "Epoch [6350/20000], Training Loss: 0.015754010337072293, Validation Loss: 0.00898460021193345\n",
      "Epoch [6351/20000], Training Loss: 0.012585416720997143, Validation Loss: 0.004164157066594295\n",
      "Epoch [6352/20000], Training Loss: 0.012034385140395898, Validation Loss: 0.005082260567760447\n",
      "Epoch [6353/20000], Training Loss: 0.012262991398789122, Validation Loss: 0.01830132215374241\n",
      "Epoch [6354/20000], Training Loss: 0.02464951900986177, Validation Loss: 0.015006798232409071\n",
      "Epoch [6355/20000], Training Loss: 0.015484115946622166, Validation Loss: 0.008314763565847118\n",
      "Epoch [6356/20000], Training Loss: 0.006662236975716139, Validation Loss: 0.0033989008740533677\n",
      "Epoch [6357/20000], Training Loss: 0.0046199004599754645, Validation Loss: 0.006167634550091648\n",
      "Epoch [6358/20000], Training Loss: 0.006560379402279588, Validation Loss: 0.0043176378052484565\n",
      "Epoch [6359/20000], Training Loss: 0.004985147783632523, Validation Loss: 0.0035842538047664413\n",
      "Epoch [6360/20000], Training Loss: 0.0037394806162670386, Validation Loss: 0.002800713897350704\n",
      "Epoch [6361/20000], Training Loss: 0.00998428403649346, Validation Loss: 0.0035446062447204796\n",
      "Epoch [6362/20000], Training Loss: 0.005664834283899316, Validation Loss: 0.03379814256965931\n",
      "Epoch [6363/20000], Training Loss: 0.031767602294816503, Validation Loss: 0.00815857607407874\n",
      "Epoch [6364/20000], Training Loss: 0.0612557481092933, Validation Loss: 0.03256609967190473\n",
      "Epoch [6365/20000], Training Loss: 0.05009873048402369, Validation Loss: 0.04845575253914701\n",
      "Epoch [6366/20000], Training Loss: 0.025955794453953525, Validation Loss: 0.013261572027201065\n",
      "Epoch [6367/20000], Training Loss: 0.010724560839922301, Validation Loss: 0.014291041158720676\n",
      "Epoch [6368/20000], Training Loss: 0.008870598311269922, Validation Loss: 0.020363527153416987\n",
      "Epoch [6369/20000], Training Loss: 0.018961818298391466, Validation Loss: 0.010292331256368113\n",
      "Epoch [6370/20000], Training Loss: 0.014999224174451748, Validation Loss: 0.008500527616767466\n",
      "Epoch [6371/20000], Training Loss: 0.014297842917065802, Validation Loss: 0.03751798613346235\n",
      "Epoch [6372/20000], Training Loss: 0.019093389473190264, Validation Loss: 0.02666657811813714\n",
      "Epoch [6373/20000], Training Loss: 0.019167103887801722, Validation Loss: 0.017764796980669868\n",
      "Epoch [6374/20000], Training Loss: 0.011613249113517148, Validation Loss: 0.02729450593939587\n",
      "Epoch [6375/20000], Training Loss: 0.012019133310013379, Validation Loss: 0.011392533637782825\n",
      "Epoch [6376/20000], Training Loss: 0.0147861189782686, Validation Loss: 0.010615854556298083\n",
      "Epoch [6377/20000], Training Loss: 0.02516794581294692, Validation Loss: 0.006388334572778019\n",
      "Epoch [6378/20000], Training Loss: 0.023096250161220917, Validation Loss: 0.00523336722991579\n",
      "Epoch [6379/20000], Training Loss: 0.009080475932153474, Validation Loss: 0.005868098656126364\n",
      "Epoch [6380/20000], Training Loss: 0.0068202336385313955, Validation Loss: 0.005902021991135241\n",
      "Epoch [6381/20000], Training Loss: 0.005997090928888481, Validation Loss: 0.005304435286110528\n",
      "Epoch [6382/20000], Training Loss: 0.0047368623424728995, Validation Loss: 0.005757021052971822\n",
      "Epoch [6383/20000], Training Loss: 0.009070398281827303, Validation Loss: 0.004020274468985008\n",
      "Epoch [6384/20000], Training Loss: 0.0069340681407733685, Validation Loss: 0.00777024538095185\n",
      "Epoch [6385/20000], Training Loss: 0.008744642628828712, Validation Loss: 0.0038345732518515314\n",
      "Epoch [6386/20000], Training Loss: 0.00993372724042274, Validation Loss: 0.0073851209323041915\n",
      "Epoch [6387/20000], Training Loss: 0.005661338461712668, Validation Loss: 0.003548198233467811\n",
      "Epoch [6388/20000], Training Loss: 0.0063652964458599725, Validation Loss: 0.0036823611313232213\n",
      "Epoch [6389/20000], Training Loss: 0.005178547048542116, Validation Loss: 0.004269226144158811\n",
      "Epoch [6390/20000], Training Loss: 0.006315439010255172, Validation Loss: 0.0030466339973987794\n",
      "Epoch [6391/20000], Training Loss: 0.00523817996560995, Validation Loss: 0.004505170536974303\n",
      "Epoch [6392/20000], Training Loss: 0.004045917071381989, Validation Loss: 0.00522611944220118\n",
      "Epoch [6393/20000], Training Loss: 0.008227208062765255, Validation Loss: 0.0074556850439402925\n",
      "Epoch [6394/20000], Training Loss: 0.007227374756309603, Validation Loss: 0.010501808563243944\n",
      "Epoch [6395/20000], Training Loss: 0.006957281701033935, Validation Loss: 0.0026653037086915416\n",
      "Epoch [6396/20000], Training Loss: 0.006152105890838097, Validation Loss: 0.006897818552995919\n",
      "Epoch [6397/20000], Training Loss: 0.004554186604634326, Validation Loss: 0.0061163182518824384\n",
      "Epoch [6398/20000], Training Loss: 0.007106500491772749, Validation Loss: 0.0036501827288101933\n",
      "Epoch [6399/20000], Training Loss: 0.007366253911381396, Validation Loss: 0.002161646596353698\n",
      "Epoch [6400/20000], Training Loss: 0.0030649335154342744, Validation Loss: 0.0029377220517845873\n",
      "Epoch [6401/20000], Training Loss: 0.00510223690467423, Validation Loss: 0.0024495450824925896\n",
      "Epoch [6402/20000], Training Loss: 0.00989903011941351, Validation Loss: 0.003344912467487498\n",
      "Epoch [6403/20000], Training Loss: 0.01554409757591202, Validation Loss: 0.03466515136616571\n",
      "Epoch [6404/20000], Training Loss: 0.028486812782440602, Validation Loss: 0.036513449890272956\n",
      "Epoch [6405/20000], Training Loss: 0.007724538102398323, Validation Loss: 0.04147959436439968\n",
      "Epoch [6406/20000], Training Loss: 0.029465586650675033, Validation Loss: 0.0047019691352535265\n",
      "Epoch [6407/20000], Training Loss: 0.014387161884444919, Validation Loss: 0.004881056192665005\n",
      "Epoch [6408/20000], Training Loss: 0.00843323452442186, Validation Loss: 0.009009688992862867\n",
      "Epoch [6409/20000], Training Loss: 0.012140485368685663, Validation Loss: 0.006356859448747595\n",
      "Epoch [6410/20000], Training Loss: 0.007518170472134703, Validation Loss: 0.005930954531809839\n",
      "Epoch [6411/20000], Training Loss: 0.005671763632562943, Validation Loss: 0.007671405170190491\n",
      "Epoch [6412/20000], Training Loss: 0.005423924537386223, Validation Loss: 0.004938510086695658\n",
      "Epoch [6413/20000], Training Loss: 0.009823079719873411, Validation Loss: 0.005491613098502514\n",
      "Epoch [6414/20000], Training Loss: 0.01611954673301495, Validation Loss: 0.00639466536322126\n",
      "Epoch [6415/20000], Training Loss: 0.008761127527187844, Validation Loss: 0.008780109036848735\n",
      "Epoch [6416/20000], Training Loss: 0.0068216723983433826, Validation Loss: 0.0044875069977305315\n",
      "Epoch [6417/20000], Training Loss: 0.009377299022162333, Validation Loss: 0.004598249680713984\n",
      "Epoch [6418/20000], Training Loss: 0.008184300059968206, Validation Loss: 0.003605947873651015\n",
      "Epoch [6419/20000], Training Loss: 0.006212831269034983, Validation Loss: 0.005956351190430301\n",
      "Epoch [6420/20000], Training Loss: 0.008917438576255725, Validation Loss: 0.009168555910564789\n",
      "Epoch [6421/20000], Training Loss: 0.019267581577878445, Validation Loss: 0.004770070850136128\n",
      "Epoch [6422/20000], Training Loss: 0.004166096988358602, Validation Loss: 0.004316839552531196\n",
      "Epoch [6423/20000], Training Loss: 0.005608331737156342, Validation Loss: 0.005899983525703692\n",
      "Epoch [6424/20000], Training Loss: 0.01075338193602095, Validation Loss: 0.00535589648415939\n",
      "Epoch [6425/20000], Training Loss: 0.008569955856588343, Validation Loss: 0.01727509610662484\n",
      "Epoch [6426/20000], Training Loss: 0.016749989915946832, Validation Loss: 0.004865297951628885\n",
      "Epoch [6427/20000], Training Loss: 0.026692510485060796, Validation Loss: 0.004391878609868545\n",
      "Epoch [6428/20000], Training Loss: 0.014268548319315804, Validation Loss: 0.012358163882579268\n",
      "Epoch [6429/20000], Training Loss: 0.01335609275182443, Validation Loss: 0.015625853889787066\n",
      "Epoch [6430/20000], Training Loss: 0.005114781372997511, Validation Loss: 0.00466493948999818\n",
      "Epoch [6431/20000], Training Loss: 0.006443492870727953, Validation Loss: 0.004521726968597761\n",
      "Epoch [6432/20000], Training Loss: 0.005124358659876245, Validation Loss: 0.015054642089815126\n",
      "Epoch [6433/20000], Training Loss: 0.009086590925497668, Validation Loss: 0.007366507323104286\n",
      "Epoch [6434/20000], Training Loss: 0.007171866215295657, Validation Loss: 0.01754778494799528\n",
      "Epoch [6435/20000], Training Loss: 0.009201718702310504, Validation Loss: 0.006975313580353161\n",
      "Epoch [6436/20000], Training Loss: 0.009165477130742212, Validation Loss: 0.0035282401785412215\n",
      "Epoch [6437/20000], Training Loss: 0.012580278782739438, Validation Loss: 0.004892151822918875\n",
      "Epoch [6438/20000], Training Loss: 0.017677338129422942, Validation Loss: 0.004789105646185986\n",
      "Epoch [6439/20000], Training Loss: 0.010336119869602303, Validation Loss: 0.003801064559496755\n",
      "Epoch [6440/20000], Training Loss: 0.008323243424196594, Validation Loss: 0.016651165272598602\n",
      "Epoch [6441/20000], Training Loss: 0.016526768263637286, Validation Loss: 0.013447738718241453\n",
      "Epoch [6442/20000], Training Loss: 0.01400579139798148, Validation Loss: 0.004227225261413313\n",
      "Epoch [6443/20000], Training Loss: 0.010797693779587851, Validation Loss: 0.0043111291719919774\n",
      "Epoch [6444/20000], Training Loss: 0.014429337946369612, Validation Loss: 0.007314252938439633\n",
      "Epoch [6445/20000], Training Loss: 0.009958824956401355, Validation Loss: 0.007028412417856835\n",
      "Epoch [6446/20000], Training Loss: 0.013721229545938383, Validation Loss: 0.005933351677057536\n",
      "Epoch [6447/20000], Training Loss: 0.006634486429642753, Validation Loss: 0.0029819657294109752\n",
      "Epoch [6448/20000], Training Loss: 0.007501784545622796, Validation Loss: 0.005606477723532348\n",
      "Epoch [6449/20000], Training Loss: 0.005302749255731344, Validation Loss: 0.0051629743562946915\n",
      "Epoch [6450/20000], Training Loss: 0.007059102018372089, Validation Loss: 0.004377920426815888\n",
      "Epoch [6451/20000], Training Loss: 0.005487431264815054, Validation Loss: 0.009100983983718157\n",
      "Epoch [6452/20000], Training Loss: 0.008744179973501584, Validation Loss: 0.019347008525553884\n",
      "Epoch [6453/20000], Training Loss: 0.02049905465329565, Validation Loss: 0.01964014794222229\n",
      "Epoch [6454/20000], Training Loss: 0.01998346314108598, Validation Loss: 0.004016569223105242\n",
      "Epoch [6455/20000], Training Loss: 0.015869899438257562, Validation Loss: 0.01662549307568284\n",
      "Epoch [6456/20000], Training Loss: 0.014828084928012686, Validation Loss: 0.010295220809504306\n",
      "Epoch [6457/20000], Training Loss: 0.008007215034532627, Validation Loss: 0.004916960401585259\n",
      "Epoch [6458/20000], Training Loss: 0.0068707698581939826, Validation Loss: 0.0034263016371239375\n",
      "Epoch [6459/20000], Training Loss: 0.004750092659378424, Validation Loss: 0.003870802643047812\n",
      "Epoch [6460/20000], Training Loss: 0.009375067573273554, Validation Loss: 0.0031347311505669495\n",
      "Epoch [6461/20000], Training Loss: 0.016881003813718314, Validation Loss: 0.007400099434432507\n",
      "Epoch [6462/20000], Training Loss: 0.007577030566088589, Validation Loss: 0.022522375912161458\n",
      "Epoch [6463/20000], Training Loss: 0.009623096722082534, Validation Loss: 0.008854299337274694\n",
      "Epoch [6464/20000], Training Loss: 0.010850900165678468, Validation Loss: 0.008161458355800733\n",
      "Epoch [6465/20000], Training Loss: 0.011478705355589877, Validation Loss: 0.005581928029641858\n",
      "Epoch [6466/20000], Training Loss: 0.013440997575344227, Validation Loss: 0.0035294515930997606\n",
      "Epoch [6467/20000], Training Loss: 0.009402274610433128, Validation Loss: 0.007488609052026212\n",
      "Epoch [6468/20000], Training Loss: 0.020715620930526972, Validation Loss: 0.01642641702353233\n",
      "Epoch [6469/20000], Training Loss: 0.008718385798731885, Validation Loss: 0.023292243480829092\n",
      "Epoch [6470/20000], Training Loss: 0.014285233426042916, Validation Loss: 0.01290450953275776\n",
      "Epoch [6471/20000], Training Loss: 0.012325663608083102, Validation Loss: 0.0038912547365076273\n",
      "Epoch [6472/20000], Training Loss: 0.00602678067765997, Validation Loss: 0.004535650612784333\n",
      "Epoch [6473/20000], Training Loss: 0.007597919366422242, Validation Loss: 0.013233014755314798\n",
      "Epoch [6474/20000], Training Loss: 0.009162836609903025, Validation Loss: 0.005748719666761255\n",
      "Epoch [6475/20000], Training Loss: 0.008577656984146285, Validation Loss: 0.0035325527066691264\n",
      "Epoch [6476/20000], Training Loss: 0.005703986318881756, Validation Loss: 0.0038745201977040517\n",
      "Epoch [6477/20000], Training Loss: 0.011827482557107163, Validation Loss: 0.010151565526159954\n",
      "Epoch [6478/20000], Training Loss: 0.008089957194377868, Validation Loss: 0.004020143020406002\n",
      "Epoch [6479/20000], Training Loss: 0.004288284779704554, Validation Loss: 0.008552135124383702\n",
      "Epoch [6480/20000], Training Loss: 0.00909188924015325, Validation Loss: 0.017955728513178264\n",
      "Epoch [6481/20000], Training Loss: 0.01511787238580707, Validation Loss: 0.00512462428033749\n",
      "Epoch [6482/20000], Training Loss: 0.007019278046105423, Validation Loss: 0.012327732309227965\n",
      "Epoch [6483/20000], Training Loss: 0.017789864841948395, Validation Loss: 0.015682296932417166\n",
      "Epoch [6484/20000], Training Loss: 0.058296660011754806, Validation Loss: 0.01640143376841609\n",
      "Epoch [6485/20000], Training Loss: 0.034658020441254066, Validation Loss: 0.008423826153115701\n",
      "Epoch [6486/20000], Training Loss: 0.019638355139509907, Validation Loss: 0.010776557953591162\n",
      "Epoch [6487/20000], Training Loss: 0.011103285293626999, Validation Loss: 0.02106433606734624\n",
      "Epoch [6488/20000], Training Loss: 0.01281864381495065, Validation Loss: 0.0070592336153692615\n",
      "Epoch [6489/20000], Training Loss: 0.019569751744191826, Validation Loss: 0.022607999589776488\n",
      "Epoch [6490/20000], Training Loss: 0.009324880091298837, Validation Loss: 0.015318422169473576\n",
      "Epoch [6491/20000], Training Loss: 0.022099702926685234, Validation Loss: 0.03950589263858605\n",
      "Epoch [6492/20000], Training Loss: 0.013888818688319102, Validation Loss: 0.004408688320333047\n",
      "Epoch [6493/20000], Training Loss: 0.01192471046150396, Validation Loss: 0.013270956696942449\n",
      "Epoch [6494/20000], Training Loss: 0.010909305859448588, Validation Loss: 0.0076914104362068815\n",
      "Epoch [6495/20000], Training Loss: 0.007917409792883388, Validation Loss: 0.004187084427201171\n",
      "Epoch [6496/20000], Training Loss: 0.005766205653864225, Validation Loss: 0.0035344162634651155\n",
      "Epoch [6497/20000], Training Loss: 0.011044186038946333, Validation Loss: 0.00395971066099524\n",
      "Epoch [6498/20000], Training Loss: 0.010024896565092993, Validation Loss: 0.005796797810010698\n",
      "Epoch [6499/20000], Training Loss: 0.007892548963094928, Validation Loss: 0.00853330642786638\n",
      "Epoch [6500/20000], Training Loss: 0.006798654682435361, Validation Loss: 0.005721827571798193\n",
      "Epoch [6501/20000], Training Loss: 0.006661472674750257, Validation Loss: 0.006952372696950704\n",
      "Epoch [6502/20000], Training Loss: 0.007966144750493445, Validation Loss: 0.011017830878251747\n",
      "Epoch [6503/20000], Training Loss: 0.008580306899343018, Validation Loss: 0.004963148891387651\n",
      "Epoch [6504/20000], Training Loss: 0.004320883703205222, Validation Loss: 0.003331317514989353\n",
      "Epoch [6505/20000], Training Loss: 0.0041138562981879135, Validation Loss: 0.0027637792376091447\n",
      "Epoch [6506/20000], Training Loss: 0.005445169117113632, Validation Loss: 0.008836126523082747\n",
      "Epoch [6507/20000], Training Loss: 0.010849919828874224, Validation Loss: 0.019951316661127854\n",
      "Epoch [6508/20000], Training Loss: 0.03597176755595553, Validation Loss: 0.05745199966269031\n",
      "Epoch [6509/20000], Training Loss: 0.033704857164009026, Validation Loss: 0.011311559756438706\n",
      "Epoch [6510/20000], Training Loss: 0.02417721928213723, Validation Loss: 0.010349103120525446\n",
      "Epoch [6511/20000], Training Loss: 0.01493795688397118, Validation Loss: 0.017750178800717657\n",
      "Epoch [6512/20000], Training Loss: 0.01201754161801156, Validation Loss: 0.005209653092615595\n",
      "Epoch [6513/20000], Training Loss: 0.008490402617358736, Validation Loss: 0.006232857581801555\n",
      "Epoch [6514/20000], Training Loss: 0.0068848066000230445, Validation Loss: 0.005910790393466568\n",
      "Epoch [6515/20000], Training Loss: 0.005805852862457479, Validation Loss: 0.008212969222053612\n",
      "Epoch [6516/20000], Training Loss: 0.009226397226711924, Validation Loss: 0.006201295110707454\n",
      "Epoch [6517/20000], Training Loss: 0.010416562855425582, Validation Loss: 0.010103282594237888\n",
      "Epoch [6518/20000], Training Loss: 0.005654764174193717, Validation Loss: 0.004656464061294433\n",
      "Epoch [6519/20000], Training Loss: 0.007525473678729863, Validation Loss: 0.004012273287530986\n",
      "Epoch [6520/20000], Training Loss: 0.007004355803863811, Validation Loss: 0.005090120290320347\n",
      "Epoch [6521/20000], Training Loss: 0.01035137315193424, Validation Loss: 0.006321684786199354\n",
      "Epoch [6522/20000], Training Loss: 0.006408214479701461, Validation Loss: 0.0031028648544518334\n",
      "Epoch [6523/20000], Training Loss: 0.008629570703695728, Validation Loss: 0.0044337583781839385\n",
      "Epoch [6524/20000], Training Loss: 0.010073486910966625, Validation Loss: 0.0127079530781309\n",
      "Epoch [6525/20000], Training Loss: 0.010274233234148207, Validation Loss: 0.006518015662923712\n",
      "Epoch [6526/20000], Training Loss: 0.0058668261538384415, Validation Loss: 0.008379608783110979\n",
      "Epoch [6527/20000], Training Loss: 0.0076641942806808016, Validation Loss: 0.008312950130629168\n",
      "Epoch [6528/20000], Training Loss: 0.010493037945187618, Validation Loss: 0.07546380162239084\n",
      "Epoch [6529/20000], Training Loss: 0.027550220728569132, Validation Loss: 0.016639293570669733\n",
      "Epoch [6530/20000], Training Loss: 0.012676480600410807, Validation Loss: 0.011894308510124216\n",
      "Epoch [6531/20000], Training Loss: 0.016999828483432924, Validation Loss: 0.07590893122791645\n",
      "Epoch [6532/20000], Training Loss: 0.06575096606242628, Validation Loss: 0.019503826730734936\n",
      "Epoch [6533/20000], Training Loss: 0.026255322967439758, Validation Loss: 0.014877394442491159\n",
      "Epoch [6534/20000], Training Loss: 0.01481210655349839, Validation Loss: 0.021523594332963813\n",
      "Epoch [6535/20000], Training Loss: 0.009408155880269728, Validation Loss: 0.009686124787549488\n",
      "Epoch [6536/20000], Training Loss: 0.015568917132830913, Validation Loss: 0.012199530405009032\n",
      "Epoch [6537/20000], Training Loss: 0.009788367498133863, Validation Loss: 0.005856464838871034\n",
      "Epoch [6538/20000], Training Loss: 0.010963716148190932, Validation Loss: 0.005854755119798938\n",
      "Epoch [6539/20000], Training Loss: 0.00955989904884648, Validation Loss: 0.017841649687117243\n",
      "Epoch [6540/20000], Training Loss: 0.00611325545469299, Validation Loss: 0.005948116062167433\n",
      "Epoch [6541/20000], Training Loss: 0.0075996153905829745, Validation Loss: 0.005371167515347354\n",
      "Epoch [6542/20000], Training Loss: 0.008674771528084031, Validation Loss: 0.003982911829422976\n",
      "Epoch [6543/20000], Training Loss: 0.0045672827691785045, Validation Loss: 0.007960729270347347\n",
      "Epoch [6544/20000], Training Loss: 0.009507703815220989, Validation Loss: 0.0041596787652257005\n",
      "Epoch [6545/20000], Training Loss: 0.014371105203671115, Validation Loss: 0.007786621361300864\n",
      "Epoch [6546/20000], Training Loss: 0.009682616379515301, Validation Loss: 0.018050844711979153\n",
      "Epoch [6547/20000], Training Loss: 0.019911042627687232, Validation Loss: 0.03829688757988541\n",
      "Epoch [6548/20000], Training Loss: 0.01913014837139469, Validation Loss: 0.005568275368945917\n",
      "Epoch [6549/20000], Training Loss: 0.006600412159709127, Validation Loss: 0.007000961889328242\n",
      "Epoch [6550/20000], Training Loss: 0.005735526805178129, Validation Loss: 0.0040987823117900135\n",
      "Epoch [6551/20000], Training Loss: 0.00868681992817853, Validation Loss: 0.0032357179088324172\n",
      "Epoch [6552/20000], Training Loss: 0.007217484784112977, Validation Loss: 0.004628193253665813\n",
      "Epoch [6553/20000], Training Loss: 0.006525907433699883, Validation Loss: 0.003533706574488958\n",
      "Epoch [6554/20000], Training Loss: 0.005857607703157035, Validation Loss: 0.004208170343015841\n",
      "Epoch [6555/20000], Training Loss: 0.004501072756413903, Validation Loss: 0.00905964711377497\n",
      "Epoch [6556/20000], Training Loss: 0.010861403103003145, Validation Loss: 0.00709363110587345\n",
      "Epoch [6557/20000], Training Loss: 0.01638387261274537, Validation Loss: 0.010308354827445654\n",
      "Epoch [6558/20000], Training Loss: 0.022662460837247118, Validation Loss: 0.019909561877804145\n",
      "Epoch [6559/20000], Training Loss: 0.014606823288236878, Validation Loss: 0.03246806287905762\n",
      "Epoch [6560/20000], Training Loss: 0.012054022900494081, Validation Loss: 0.0161827982270295\n",
      "Epoch [6561/20000], Training Loss: 0.020418503180345788, Validation Loss: 0.023310963652577146\n",
      "Epoch [6562/20000], Training Loss: 0.01953898227566242, Validation Loss: 0.010769994785659622\n",
      "Epoch [6563/20000], Training Loss: 0.011660490363803027, Validation Loss: 0.013428917953158976\n",
      "Epoch [6564/20000], Training Loss: 0.006347442519784506, Validation Loss: 0.004418313285421606\n",
      "Epoch [6565/20000], Training Loss: 0.005906327839641433, Validation Loss: 0.005880890609307806\n",
      "Epoch [6566/20000], Training Loss: 0.006222996692356121, Validation Loss: 0.003545876108952533\n",
      "Epoch [6567/20000], Training Loss: 0.011458113611037593, Validation Loss: 0.011407111641596798\n",
      "Epoch [6568/20000], Training Loss: 0.008676146787495651, Validation Loss: 0.00975376642847189\n",
      "Epoch [6569/20000], Training Loss: 0.012700059004211133, Validation Loss: 0.01354000835880623\n",
      "Epoch [6570/20000], Training Loss: 0.007557855380062912, Validation Loss: 0.003783488452593734\n",
      "Epoch [6571/20000], Training Loss: 0.01403173255260169, Validation Loss: 0.013429304365509567\n",
      "Epoch [6572/20000], Training Loss: 0.018908445362675202, Validation Loss: 0.010799735467248819\n",
      "Epoch [6573/20000], Training Loss: 0.01240757288178429, Validation Loss: 0.004832118072840785\n",
      "Epoch [6574/20000], Training Loss: 0.017230670391005072, Validation Loss: 0.03216893970966339\n",
      "Epoch [6575/20000], Training Loss: 0.012080229285337347, Validation Loss: 0.010142941763323548\n",
      "Epoch [6576/20000], Training Loss: 0.008067483789220984, Validation Loss: 0.003962062206074027\n",
      "Epoch [6577/20000], Training Loss: 0.004921563373500248, Validation Loss: 0.003193061737483\n",
      "Epoch [6578/20000], Training Loss: 0.007145582008109029, Validation Loss: 0.0030746984778155756\n",
      "Epoch [6579/20000], Training Loss: 0.007203170739688046, Validation Loss: 0.008866668182281347\n",
      "Epoch [6580/20000], Training Loss: 0.006677962348995996, Validation Loss: 0.005634548355502422\n",
      "Epoch [6581/20000], Training Loss: 0.012475123015909568, Validation Loss: 0.018821494653857462\n",
      "Epoch [6582/20000], Training Loss: 0.010885135266497465, Validation Loss: 0.005725808348239139\n",
      "Epoch [6583/20000], Training Loss: 0.009279038570509459, Validation Loss: 0.0038147218609440514\n",
      "Epoch [6584/20000], Training Loss: 0.01228647726592109, Validation Loss: 0.01569920496186025\n",
      "Epoch [6585/20000], Training Loss: 0.018975164116584762, Validation Loss: 0.055941418110446\n",
      "Epoch [6586/20000], Training Loss: 0.03442970747294437, Validation Loss: 0.020277701885691255\n",
      "Epoch [6587/20000], Training Loss: 0.027375464078171978, Validation Loss: 0.02000275933063936\n",
      "Epoch [6588/20000], Training Loss: 0.023510669998358935, Validation Loss: 0.005561388005539276\n",
      "Epoch [6589/20000], Training Loss: 0.012571024610744124, Validation Loss: 0.01762971670361984\n",
      "Epoch [6590/20000], Training Loss: 0.012983208154245014, Validation Loss: 0.0062692760900253984\n",
      "Epoch [6591/20000], Training Loss: 0.0077564280966596144, Validation Loss: 0.013048051481418412\n",
      "Epoch [6592/20000], Training Loss: 0.01059776165389589, Validation Loss: 0.016102693975039096\n",
      "Epoch [6593/20000], Training Loss: 0.014259790717066068, Validation Loss: 0.00778717179933075\n",
      "Epoch [6594/20000], Training Loss: 0.008937499900119812, Validation Loss: 0.0042850095326296056\n",
      "Epoch [6595/20000], Training Loss: 0.006567782823237524, Validation Loss: 0.019487753510684085\n",
      "Epoch [6596/20000], Training Loss: 0.017014278535498306, Validation Loss: 0.005969832486138297\n",
      "Epoch [6597/20000], Training Loss: 0.01188170091232418, Validation Loss: 0.0064666656381502695\n",
      "Epoch [6598/20000], Training Loss: 0.004334194737436649, Validation Loss: 0.0031538632291552293\n",
      "Epoch [6599/20000], Training Loss: 0.004639436118720498, Validation Loss: 0.017521221722889502\n",
      "Epoch [6600/20000], Training Loss: 0.015593055209442193, Validation Loss: 0.002594459336973395\n",
      "Epoch [6601/20000], Training Loss: 0.02419758288202242, Validation Loss: 0.02013320945106654\n",
      "Epoch [6602/20000], Training Loss: 0.014627660541529102, Validation Loss: 0.009913043828210202\n",
      "Epoch [6603/20000], Training Loss: 0.020100053349908973, Validation Loss: 0.006398673543687648\n",
      "Epoch [6604/20000], Training Loss: 0.00871745789273908, Validation Loss: 0.015663881395446833\n",
      "Epoch [6605/20000], Training Loss: 0.010736459924373776, Validation Loss: 0.012916814026978736\n",
      "Epoch [6606/20000], Training Loss: 0.00946531634794415, Validation Loss: 0.004426818309020015\n",
      "Epoch [6607/20000], Training Loss: 0.00525575497886166, Validation Loss: 0.0036750945028448507\n",
      "Epoch [6608/20000], Training Loss: 0.004396328457265294, Validation Loss: 0.016453306696381072\n",
      "Epoch [6609/20000], Training Loss: 0.011486654973980746, Validation Loss: 0.01820472655461864\n",
      "Epoch [6610/20000], Training Loss: 0.01071930055539789, Validation Loss: 0.007933416268503206\n",
      "Epoch [6611/20000], Training Loss: 0.013628676191144353, Validation Loss: 0.030147640195801375\n",
      "Epoch [6612/20000], Training Loss: 0.011889226653563258, Validation Loss: 0.023599332464592913\n",
      "Epoch [6613/20000], Training Loss: 0.01463452733878512, Validation Loss: 0.009278701979143631\n",
      "Epoch [6614/20000], Training Loss: 0.016951412165196546, Validation Loss: 0.033739805221557624\n",
      "Epoch [6615/20000], Training Loss: 0.031760038899457346, Validation Loss: 0.00621112033978274\n",
      "Epoch [6616/20000], Training Loss: 0.009183155081700534, Validation Loss: 0.006364672247125368\n",
      "Epoch [6617/20000], Training Loss: 0.005307284499784666, Validation Loss: 0.018787324123881826\n",
      "Epoch [6618/20000], Training Loss: 0.009119914973260685, Validation Loss: 0.005373523796411713\n",
      "Epoch [6619/20000], Training Loss: 0.006553301376178362, Validation Loss: 0.0061555845651193\n",
      "Epoch [6620/20000], Training Loss: 0.005404506285101108, Validation Loss: 0.004825233906558424\n",
      "Epoch [6621/20000], Training Loss: 0.004724561919699356, Validation Loss: 0.004126458344542934\n",
      "Epoch [6622/20000], Training Loss: 0.006586066746552076, Validation Loss: 0.003190084866957932\n",
      "Epoch [6623/20000], Training Loss: 0.0038311695388983935, Validation Loss: 0.0028371646029004944\n",
      "Epoch [6624/20000], Training Loss: 0.005490495018810699, Validation Loss: 0.010038101584894751\n",
      "Epoch [6625/20000], Training Loss: 0.008081916195806116, Validation Loss: 0.004086543287907521\n",
      "Epoch [6626/20000], Training Loss: 0.005260729554720456, Validation Loss: 0.009682618835573262\n",
      "Epoch [6627/20000], Training Loss: 0.009834619925917167, Validation Loss: 0.004833211444682612\n",
      "Epoch [6628/20000], Training Loss: 0.006819453247219697, Validation Loss: 0.009024325659970774\n",
      "Epoch [6629/20000], Training Loss: 0.006221565281261324, Validation Loss: 0.0025448008506100245\n",
      "Epoch [6630/20000], Training Loss: 0.00668965795063871, Validation Loss: 0.0023669345394234298\n",
      "Epoch [6631/20000], Training Loss: 0.010637772477057947, Validation Loss: 0.003071872261788019\n",
      "Epoch [6632/20000], Training Loss: 0.00840268811277513, Validation Loss: 0.014507007386003518\n",
      "Epoch [6633/20000], Training Loss: 0.007571807721563216, Validation Loss: 0.005195105798618117\n",
      "Epoch [6634/20000], Training Loss: 0.011908579476117407, Validation Loss: 0.011496608464264162\n",
      "Epoch [6635/20000], Training Loss: 0.004053029252255718, Validation Loss: 0.024414898029395515\n",
      "Epoch [6636/20000], Training Loss: 0.011164803448731877, Validation Loss: 0.027144263897623335\n",
      "Epoch [6637/20000], Training Loss: 0.010788372800431847, Validation Loss: 0.006572908635443908\n",
      "Epoch [6638/20000], Training Loss: 0.009748924029541481, Validation Loss: 0.004915426325996032\n",
      "Epoch [6639/20000], Training Loss: 0.0075144247371167735, Validation Loss: 0.005087558501221025\n",
      "Epoch [6640/20000], Training Loss: 0.01109846044480735, Validation Loss: 0.007600618594038079\n",
      "Epoch [6641/20000], Training Loss: 0.029058370010846244, Validation Loss: 0.02479407244495113\n",
      "Epoch [6642/20000], Training Loss: 0.05888939149320192, Validation Loss: 0.011720682012319619\n",
      "Epoch [6643/20000], Training Loss: 0.017405374433500192, Validation Loss: 0.005761299714704039\n",
      "Epoch [6644/20000], Training Loss: 0.008322375263170605, Validation Loss: 0.009910688471873439\n",
      "Epoch [6645/20000], Training Loss: 0.005639651474508257, Validation Loss: 0.004848517000206292\n",
      "Epoch [6646/20000], Training Loss: 0.0062742761178274774, Validation Loss: 0.004427731427832831\n",
      "Epoch [6647/20000], Training Loss: 0.006265970578949366, Validation Loss: 0.005302699734784255\n",
      "Epoch [6648/20000], Training Loss: 0.007691319927939081, Validation Loss: 0.007551112067499461\n",
      "Epoch [6649/20000], Training Loss: 0.007789415043329687, Validation Loss: 0.0036832863863683735\n",
      "Epoch [6650/20000], Training Loss: 0.006174688893484667, Validation Loss: 0.003921685286497579\n",
      "Epoch [6651/20000], Training Loss: 0.010476176119742128, Validation Loss: 0.004541739537758208\n",
      "Epoch [6652/20000], Training Loss: 0.012649953054018883, Validation Loss: 0.010597012922906808\n",
      "Epoch [6653/20000], Training Loss: 0.009324415336388156, Validation Loss: 0.005820452844107292\n",
      "Epoch [6654/20000], Training Loss: 0.006111158463422076, Validation Loss: 0.009074767388645873\n",
      "Epoch [6655/20000], Training Loss: 0.011954773361919382, Validation Loss: 0.027304290660790036\n",
      "Epoch [6656/20000], Training Loss: 0.011783257792038577, Validation Loss: 0.005608468048344457\n",
      "Epoch [6657/20000], Training Loss: 0.006846692878752947, Validation Loss: 0.022032014493431364\n",
      "Epoch [6658/20000], Training Loss: 0.010641798981461241, Validation Loss: 0.012228927708265308\n",
      "Epoch [6659/20000], Training Loss: 0.0167885444666719, Validation Loss: 0.005734675995804604\n",
      "Epoch [6660/20000], Training Loss: 0.010508084618288243, Validation Loss: 0.008385517368990339\n",
      "Epoch [6661/20000], Training Loss: 0.01880918256626631, Validation Loss: 0.019917508321149007\n",
      "Epoch [6662/20000], Training Loss: 0.009709046896594893, Validation Loss: 0.004501833426411179\n",
      "Epoch [6663/20000], Training Loss: 0.006496136352974905, Validation Loss: 0.005530306248191361\n",
      "Epoch [6664/20000], Training Loss: 0.005742226928954811, Validation Loss: 0.007676295736611542\n",
      "Epoch [6665/20000], Training Loss: 0.006898436065447251, Validation Loss: 0.004764178661146027\n",
      "Epoch [6666/20000], Training Loss: 0.006299366565404593, Validation Loss: 0.005333153848376476\n",
      "Epoch [6667/20000], Training Loss: 0.004936904340521409, Validation Loss: 0.004548813126290138\n",
      "Epoch [6668/20000], Training Loss: 0.006577872842691639, Validation Loss: 0.005138648858243349\n",
      "Epoch [6669/20000], Training Loss: 0.005953327428349959, Validation Loss: 0.0026585628716814724\n",
      "Epoch [6670/20000], Training Loss: 0.008198875309452498, Validation Loss: 0.0064252218208693535\n",
      "Epoch [6671/20000], Training Loss: 0.00946814547343625, Validation Loss: 0.011844666408641045\n",
      "Epoch [6672/20000], Training Loss: 0.00893660116688417, Validation Loss: 0.018598975746759346\n",
      "Epoch [6673/20000], Training Loss: 0.011755489723717412, Validation Loss: 0.007410749746794311\n",
      "Epoch [6674/20000], Training Loss: 0.017512052275768446, Validation Loss: 0.01002578240443868\n",
      "Epoch [6675/20000], Training Loss: 0.010390342035991904, Validation Loss: 0.004222874445402068\n",
      "Epoch [6676/20000], Training Loss: 0.00700758946498224, Validation Loss: 0.007026541583651204\n",
      "Epoch [6677/20000], Training Loss: 0.0053358757964880555, Validation Loss: 0.010795556308169486\n",
      "Epoch [6678/20000], Training Loss: 0.006775006574441379, Validation Loss: 0.0021355119984267645\n",
      "Epoch [6679/20000], Training Loss: 0.004559498688779838, Validation Loss: 0.00446302582639443\n",
      "Epoch [6680/20000], Training Loss: 0.015298051681643951, Validation Loss: 0.023832796433735228\n",
      "Epoch [6681/20000], Training Loss: 0.0289140369672428, Validation Loss: 0.0637928673334045\n",
      "Epoch [6682/20000], Training Loss: 0.03257773915538564, Validation Loss: 0.028494334538273897\n",
      "Epoch [6683/20000], Training Loss: 0.036893011981322034, Validation Loss: 0.06384036055429274\n",
      "Epoch [6684/20000], Training Loss: 0.04201302455995964, Validation Loss: 0.016633919363552456\n",
      "Epoch [6685/20000], Training Loss: 0.016126306939570765, Validation Loss: 0.007469458624265306\n",
      "Epoch [6686/20000], Training Loss: 0.01151661720359698, Validation Loss: 0.007503248362679317\n",
      "Epoch [6687/20000], Training Loss: 0.008962950344929206, Validation Loss: 0.018062120296105735\n",
      "Epoch [6688/20000], Training Loss: 0.009246730865145634, Validation Loss: 0.0056379602901870385\n",
      "Epoch [6689/20000], Training Loss: 0.006325755684104349, Validation Loss: 0.006079038354073418\n",
      "Epoch [6690/20000], Training Loss: 0.005082357200860445, Validation Loss: 0.004739976183632929\n",
      "Epoch [6691/20000], Training Loss: 0.006435970904671454, Validation Loss: 0.004850022162047806\n",
      "Epoch [6692/20000], Training Loss: 0.005534301504667383, Validation Loss: 0.007241720894788679\n",
      "Epoch [6693/20000], Training Loss: 0.005002705255589847, Validation Loss: 0.00660749439092407\n",
      "Epoch [6694/20000], Training Loss: 0.00623782358733089, Validation Loss: 0.004084538429391874\n",
      "Epoch [6695/20000], Training Loss: 0.004670672043305656, Validation Loss: 0.0032250934555122512\n",
      "Epoch [6696/20000], Training Loss: 0.006890516650206077, Validation Loss: 0.004265448868084475\n",
      "Epoch [6697/20000], Training Loss: 0.006263305880046184, Validation Loss: 0.004499715540987381\n",
      "Epoch [6698/20000], Training Loss: 0.008128003218319333, Validation Loss: 0.003154242410021522\n",
      "Epoch [6699/20000], Training Loss: 0.011677345015024392, Validation Loss: 0.0031569059499036356\n",
      "Epoch [6700/20000], Training Loss: 0.032237548576501594, Validation Loss: 0.029370764299178478\n",
      "Epoch [6701/20000], Training Loss: 0.08766476134249908, Validation Loss: 0.07624749607358743\n",
      "Epoch [6702/20000], Training Loss: 0.057492542621080896, Validation Loss: 0.043305774616393525\n",
      "Epoch [6703/20000], Training Loss: 0.014055148798174091, Validation Loss: 0.013592832629374893\n",
      "Epoch [6704/20000], Training Loss: 0.010599664578746473, Validation Loss: 0.008925721386105525\n",
      "Epoch [6705/20000], Training Loss: 0.007838145097983735, Validation Loss: 0.007487939004883347\n",
      "Epoch [6706/20000], Training Loss: 0.007798050349395323, Validation Loss: 0.010329241879666218\n",
      "Epoch [6707/20000], Training Loss: 0.006558377532720832, Validation Loss: 0.006014438018090524\n",
      "Epoch [6708/20000], Training Loss: 0.006189795036334544, Validation Loss: 0.005958704890352757\n",
      "Epoch [6709/20000], Training Loss: 0.005611249590791496, Validation Loss: 0.00621006414855531\n",
      "Epoch [6710/20000], Training Loss: 0.013761897178483196, Validation Loss: 0.009044169066003502\n",
      "Epoch [6711/20000], Training Loss: 0.007481666890920938, Validation Loss: 0.05491091678439781\n",
      "Epoch [6712/20000], Training Loss: 0.061157964627325, Validation Loss: 0.042560611578664975\n",
      "Epoch [6713/20000], Training Loss: 0.0293541956478813, Validation Loss: 0.01613867574633332\n",
      "Epoch [6714/20000], Training Loss: 0.033359023646750884, Validation Loss: 0.008328786832342823\n",
      "Epoch [6715/20000], Training Loss: 0.03170018035286505, Validation Loss: 0.00742841472310829\n",
      "Epoch [6716/20000], Training Loss: 0.02190668536266977, Validation Loss: 0.01573178713761081\n",
      "Epoch [6717/20000], Training Loss: 0.011727948317586976, Validation Loss: 0.006284186914334506\n",
      "Epoch [6718/20000], Training Loss: 0.010435229884543722, Validation Loss: 0.009185444830239118\n",
      "Epoch [6719/20000], Training Loss: 0.007750112211007425, Validation Loss: 0.0069636119984092405\n",
      "Epoch [6720/20000], Training Loss: 0.007006015570368618, Validation Loss: 0.007399080327754096\n",
      "Epoch [6721/20000], Training Loss: 0.012029440549667925, Validation Loss: 0.015079947452619535\n",
      "Epoch [6722/20000], Training Loss: 0.011331843967224944, Validation Loss: 0.012228676264019864\n",
      "Epoch [6723/20000], Training Loss: 0.02290569765526535, Validation Loss: 0.006702959790102016\n",
      "Epoch [6724/20000], Training Loss: 0.01558104069928439, Validation Loss: 0.028179134407504795\n",
      "Epoch [6725/20000], Training Loss: 0.018390950143969218, Validation Loss: 0.005869191366822893\n",
      "Epoch [6726/20000], Training Loss: 0.007129421524171319, Validation Loss: 0.006280941110065085\n",
      "Epoch [6727/20000], Training Loss: 0.007709244380229003, Validation Loss: 0.005143830487603347\n",
      "Epoch [6728/20000], Training Loss: 0.00624126842844167, Validation Loss: 0.006306767876854532\n",
      "Epoch [6729/20000], Training Loss: 0.00771413862821646, Validation Loss: 0.022500530263381995\n",
      "Epoch [6730/20000], Training Loss: 0.018900526860826567, Validation Loss: 0.005124381118954651\n",
      "Epoch [6731/20000], Training Loss: 0.032976092803957205, Validation Loss: 0.014807431707993146\n",
      "Epoch [6732/20000], Training Loss: 0.01599391776185907, Validation Loss: 0.023051406386262636\n",
      "Epoch [6733/20000], Training Loss: 0.01676820970481328, Validation Loss: 0.008210648727396932\n",
      "Epoch [6734/20000], Training Loss: 0.0066638258077935985, Validation Loss: 0.004695499678291526\n",
      "Epoch [6735/20000], Training Loss: 0.00799040200588154, Validation Loss: 0.00799487590453347\n",
      "Epoch [6736/20000], Training Loss: 0.004808910462218462, Validation Loss: 0.007214342718465819\n",
      "Epoch [6737/20000], Training Loss: 0.0076192992533573745, Validation Loss: 0.02913661385258453\n",
      "Epoch [6738/20000], Training Loss: 0.015086236103213326, Validation Loss: 0.01933735292611313\n",
      "Epoch [6739/20000], Training Loss: 0.013438358873827383, Validation Loss: 0.008073209481232928\n",
      "Epoch [6740/20000], Training Loss: 0.007204395708478322, Validation Loss: 0.005790558342263401\n",
      "Epoch [6741/20000], Training Loss: 0.014849875157649097, Validation Loss: 0.010132057829683358\n",
      "Epoch [6742/20000], Training Loss: 0.011525470626241128, Validation Loss: 0.020797705839611984\n",
      "Epoch [6743/20000], Training Loss: 0.007916658157358012, Validation Loss: 0.007312317171389233\n",
      "Epoch [6744/20000], Training Loss: 0.007223171015669193, Validation Loss: 0.0037443793214931376\n",
      "Epoch [6745/20000], Training Loss: 0.007380032564729585, Validation Loss: 0.012211995831631128\n",
      "Epoch [6746/20000], Training Loss: 0.016616192652821025, Validation Loss: 0.011851791857631204\n",
      "Epoch [6747/20000], Training Loss: 0.024177631245038356, Validation Loss: 0.02302259360623257\n",
      "Epoch [6748/20000], Training Loss: 0.022079133085623783, Validation Loss: 0.025757335669847947\n",
      "Epoch [6749/20000], Training Loss: 0.03696136407130065, Validation Loss: 0.06310185577515877\n",
      "Epoch [6750/20000], Training Loss: 0.03564507042184622, Validation Loss: 0.061001464566672645\n",
      "Epoch [6751/20000], Training Loss: 0.05652392667668339, Validation Loss: 0.07240780675783753\n",
      "Epoch [6752/20000], Training Loss: 0.04846475865425808, Validation Loss: 0.05530031924557298\n",
      "Epoch [6753/20000], Training Loss: 0.058266290879276185, Validation Loss: 0.07214611939906249\n",
      "Epoch [6754/20000], Training Loss: 0.024406918773560653, Validation Loss: 0.01825814376551246\n",
      "Epoch [6755/20000], Training Loss: 0.016387631463918036, Validation Loss: 0.00999392232792516\n",
      "Epoch [6756/20000], Training Loss: 0.009233953884436883, Validation Loss: 0.011784944072685124\n",
      "Epoch [6757/20000], Training Loss: 0.008774873684160411, Validation Loss: 0.00537000539445996\n",
      "Epoch [6758/20000], Training Loss: 0.008269767434934952, Validation Loss: 0.003935488079540457\n",
      "Epoch [6759/20000], Training Loss: 0.006634214223595336, Validation Loss: 0.0038157390465260376\n",
      "Epoch [6760/20000], Training Loss: 0.005139070767784558, Validation Loss: 0.010056298978919844\n",
      "Epoch [6761/20000], Training Loss: 0.005003220162637133, Validation Loss: 0.0045467916476858205\n",
      "Epoch [6762/20000], Training Loss: 0.00428440203333074, Validation Loss: 0.012671899215222635\n",
      "Epoch [6763/20000], Training Loss: 0.011284317062355902, Validation Loss: 0.007451866128576512\n",
      "Epoch [6764/20000], Training Loss: 0.01270780953096359, Validation Loss: 0.005618284687592734\n",
      "Epoch [6765/20000], Training Loss: 0.007203429174036761, Validation Loss: 0.002810457449937433\n",
      "Epoch [6766/20000], Training Loss: 0.00493979248182898, Validation Loss: 0.003924220017808011\n",
      "Epoch [6767/20000], Training Loss: 0.007410641519007706, Validation Loss: 0.006388952157872703\n",
      "Epoch [6768/20000], Training Loss: 0.005463505375830989, Validation Loss: 0.004063810915567956\n",
      "Epoch [6769/20000], Training Loss: 0.007958776342482972, Validation Loss: 0.00469712154082507\n",
      "Epoch [6770/20000], Training Loss: 0.011593722308524386, Validation Loss: 0.004004383562862098\n",
      "Epoch [6771/20000], Training Loss: 0.012123784617870115, Validation Loss: 0.013258946346468292\n",
      "Epoch [6772/20000], Training Loss: 0.011723930563935678, Validation Loss: 0.005606850593455258\n",
      "Epoch [6773/20000], Training Loss: 0.009995308452614284, Validation Loss: 0.007899411100325755\n",
      "Epoch [6774/20000], Training Loss: 0.008548778225758724, Validation Loss: 0.01578026185458573\n",
      "Epoch [6775/20000], Training Loss: 0.014167629829898942, Validation Loss: 0.010673691302308188\n",
      "Epoch [6776/20000], Training Loss: 0.01121853086207915, Validation Loss: 0.018651298122416858\n",
      "Epoch [6777/20000], Training Loss: 0.014154968517167228, Validation Loss: 0.019178882990104253\n",
      "Epoch [6778/20000], Training Loss: 0.014316159094700456, Validation Loss: 0.07026017011474193\n",
      "Epoch [6779/20000], Training Loss: 0.0523809978954627, Validation Loss: 0.015233400038726228\n",
      "Epoch [6780/20000], Training Loss: 0.03090118951929201, Validation Loss: 0.02191269132999382\n",
      "Epoch [6781/20000], Training Loss: 0.03038190948843424, Validation Loss: 0.02046083853586213\n",
      "Epoch [6782/20000], Training Loss: 0.01455970196652093, Validation Loss: 0.046012945473393664\n",
      "Epoch [6783/20000], Training Loss: 0.0289914558608351, Validation Loss: 0.00880561950079693\n",
      "Epoch [6784/20000], Training Loss: 0.010921416627752478, Validation Loss: 0.010141971789185586\n",
      "Epoch [6785/20000], Training Loss: 0.007293836562894285, Validation Loss: 0.011418684687529585\n",
      "Epoch [6786/20000], Training Loss: 0.007285331109804767, Validation Loss: 0.0059362978836361435\n",
      "Epoch [6787/20000], Training Loss: 0.005955960086014654, Validation Loss: 0.011145788704158706\n",
      "Epoch [6788/20000], Training Loss: 0.010007783479522914, Validation Loss: 0.0059185384719686596\n",
      "Epoch [6789/20000], Training Loss: 0.009081341519569313, Validation Loss: 0.01844052548741664\n",
      "Epoch [6790/20000], Training Loss: 0.011504994231342738, Validation Loss: 0.007633340841872378\n",
      "Epoch [6791/20000], Training Loss: 0.008924087826439322, Validation Loss: 0.01212409516476219\n",
      "Epoch [6792/20000], Training Loss: 0.00884190337923688, Validation Loss: 0.005055577209109937\n",
      "Epoch [6793/20000], Training Loss: 0.004532681453854691, Validation Loss: 0.00575978499289777\n",
      "Epoch [6794/20000], Training Loss: 0.008919280923042347, Validation Loss: 0.004182289234782794\n",
      "Epoch [6795/20000], Training Loss: 0.00768838162252905, Validation Loss: 0.031575251902865496\n",
      "Epoch [6796/20000], Training Loss: 0.01132330219180793, Validation Loss: 0.0036586484315482997\n",
      "Epoch [6797/20000], Training Loss: 0.012534704072874905, Validation Loss: 0.006615052694223274\n",
      "Epoch [6798/20000], Training Loss: 0.006516072649641761, Validation Loss: 0.035207282341599797\n",
      "Epoch [6799/20000], Training Loss: 0.011130234189165224, Validation Loss: 0.0064048934002076995\n",
      "Epoch [6800/20000], Training Loss: 0.025734692516769946, Validation Loss: 0.005843253807012204\n",
      "Epoch [6801/20000], Training Loss: 0.03445248707430437, Validation Loss: 0.00898318451658093\n",
      "Epoch [6802/20000], Training Loss: 0.016497581897121045, Validation Loss: 0.056498686624503795\n",
      "Epoch [6803/20000], Training Loss: 0.02342803609956588, Validation Loss: 0.005486405022886791\n",
      "Epoch [6804/20000], Training Loss: 0.00854094854730647, Validation Loss: 0.012330950510962167\n",
      "Epoch [6805/20000], Training Loss: 0.008277452931257099, Validation Loss: 0.00536379905393382\n",
      "Epoch [6806/20000], Training Loss: 0.0054617628172439125, Validation Loss: 0.003954265765306252\n",
      "Epoch [6807/20000], Training Loss: 0.004398975055664778, Validation Loss: 0.007920434584006234\n",
      "Epoch [6808/20000], Training Loss: 0.006203894317358001, Validation Loss: 0.0029402140812070853\n",
      "Epoch [6809/20000], Training Loss: 0.007548098144525284, Validation Loss: 0.004027759227006695\n",
      "Epoch [6810/20000], Training Loss: 0.01269760208171127, Validation Loss: 0.005087061789563815\n",
      "Epoch [6811/20000], Training Loss: 0.015146487184860493, Validation Loss: 0.012267340987236761\n",
      "Epoch [6812/20000], Training Loss: 0.00890060599444301, Validation Loss: 0.024455259777693885\n",
      "Epoch [6813/20000], Training Loss: 0.02046606091018686, Validation Loss: 0.01497786625289267\n",
      "Epoch [6814/20000], Training Loss: 0.01408484205395715, Validation Loss: 0.014654467017829142\n",
      "Epoch [6815/20000], Training Loss: 0.006440905671167586, Validation Loss: 0.004885060521183426\n",
      "Epoch [6816/20000], Training Loss: 0.006734004955173337, Validation Loss: 0.004615936788399365\n",
      "Epoch [6817/20000], Training Loss: 0.006774811526950586, Validation Loss: 0.0047961364168051445\n",
      "Epoch [6818/20000], Training Loss: 0.011589834433796195, Validation Loss: 0.00416247263769184\n",
      "Epoch [6819/20000], Training Loss: 0.007370893276986733, Validation Loss: 0.0049947786647879956\n",
      "Epoch [6820/20000], Training Loss: 0.006035631164974932, Validation Loss: 0.006474263340486809\n",
      "Epoch [6821/20000], Training Loss: 0.00854276490079948, Validation Loss: 0.012350952519132817\n",
      "Epoch [6822/20000], Training Loss: 0.00751176100727337, Validation Loss: 0.0046020854130711586\n",
      "Epoch [6823/20000], Training Loss: 0.007374591496747891, Validation Loss: 0.0034605823623032645\n",
      "Epoch [6824/20000], Training Loss: 0.005425203631083215, Validation Loss: 0.002752358093843142\n",
      "Epoch [6825/20000], Training Loss: 0.0036786154253474835, Validation Loss: 0.015543597352150898\n",
      "Epoch [6826/20000], Training Loss: 0.009759122519296528, Validation Loss: 0.011073133376717758\n",
      "Epoch [6827/20000], Training Loss: 0.011673649894094393, Validation Loss: 0.007973980591511998\n",
      "Epoch [6828/20000], Training Loss: 0.0070256725640709715, Validation Loss: 0.05814169874851619\n",
      "Epoch [6829/20000], Training Loss: 0.04396141153730631, Validation Loss: 0.013454376985214752\n",
      "Epoch [6830/20000], Training Loss: 0.01502480517618616, Validation Loss: 0.010315919320103515\n",
      "Epoch [6831/20000], Training Loss: 0.023377115479889885, Validation Loss: 0.020186559792299313\n",
      "Epoch [6832/20000], Training Loss: 0.010293483054348533, Validation Loss: 0.006562994042916793\n",
      "Epoch [6833/20000], Training Loss: 0.007202684656671668, Validation Loss: 0.005648956377000117\n",
      "Epoch [6834/20000], Training Loss: 0.006450338249643599, Validation Loss: 0.004808040273006144\n",
      "Epoch [6835/20000], Training Loss: 0.008069563111413507, Validation Loss: 0.004000578939551679\n",
      "Epoch [6836/20000], Training Loss: 0.015041827170664743, Validation Loss: 0.0065785211388921395\n",
      "Epoch [6837/20000], Training Loss: 0.014827714238338428, Validation Loss: 0.023945284741977664\n",
      "Epoch [6838/20000], Training Loss: 0.019889478987481977, Validation Loss: 0.01325720869606592\n",
      "Epoch [6839/20000], Training Loss: 0.012164724307305213, Validation Loss: 0.0050090585549626655\n",
      "Epoch [6840/20000], Training Loss: 0.007812804742051023, Validation Loss: 0.0235965140164055\n",
      "Epoch [6841/20000], Training Loss: 0.01106390802254152, Validation Loss: 0.006242482187861692\n",
      "Epoch [6842/20000], Training Loss: 0.007590512291114594, Validation Loss: 0.007973782767203286\n",
      "Epoch [6843/20000], Training Loss: 0.0068325980485367055, Validation Loss: 0.007338012343174359\n",
      "Epoch [6844/20000], Training Loss: 0.00726153163746598, Validation Loss: 0.003690673312967082\n",
      "Epoch [6845/20000], Training Loss: 0.005562601804350769, Validation Loss: 0.007852321897592012\n",
      "Epoch [6846/20000], Training Loss: 0.007805506008610662, Validation Loss: 0.0034285900648235446\n",
      "Epoch [6847/20000], Training Loss: 0.012178338109931377, Validation Loss: 0.006649890585793283\n",
      "Epoch [6848/20000], Training Loss: 0.011843131241611056, Validation Loss: 0.0169884046276827\n",
      "Epoch [6849/20000], Training Loss: 0.03310927856780056, Validation Loss: 0.052682176258398196\n",
      "Epoch [6850/20000], Training Loss: 0.015375482146087702, Validation Loss: 0.009419466104884828\n",
      "Epoch [6851/20000], Training Loss: 0.006749928999592417, Validation Loss: 0.005036028983553038\n",
      "Epoch [6852/20000], Training Loss: 0.011609271524402434, Validation Loss: 0.006758876217368093\n",
      "Epoch [6853/20000], Training Loss: 0.023153717280365527, Validation Loss: 0.007718507514425139\n",
      "Epoch [6854/20000], Training Loss: 0.009867831264273264, Validation Loss: 0.006370758786329057\n",
      "Epoch [6855/20000], Training Loss: 0.02337106434944352, Validation Loss: 0.06216502497550991\n",
      "Epoch [6856/20000], Training Loss: 0.051584934582933784, Validation Loss: 0.0347047797819755\n",
      "Epoch [6857/20000], Training Loss: 0.023006352228386668, Validation Loss: 0.009332505407598082\n",
      "Epoch [6858/20000], Training Loss: 0.013307272057448114, Validation Loss: 0.01132602265601003\n",
      "Epoch [6859/20000], Training Loss: 0.01169491747610404, Validation Loss: 0.01176652846685858\n",
      "Epoch [6860/20000], Training Loss: 0.009545080197442855, Validation Loss: 0.005425249536573672\n",
      "Epoch [6861/20000], Training Loss: 0.007130061246113785, Validation Loss: 0.01031325199773495\n",
      "Epoch [6862/20000], Training Loss: 0.008678517115185969, Validation Loss: 0.005521407987250228\n",
      "Epoch [6863/20000], Training Loss: 0.0072946789244139965, Validation Loss: 0.008156958721660503\n",
      "Epoch [6864/20000], Training Loss: 0.007571008449975275, Validation Loss: 0.004077592751941536\n",
      "Epoch [6865/20000], Training Loss: 0.005975875407394986, Validation Loss: 0.009171079047873074\n",
      "Epoch [6866/20000], Training Loss: 0.016111513538687307, Validation Loss: 0.007827471609161933\n",
      "Epoch [6867/20000], Training Loss: 0.021492125103187782, Validation Loss: 0.012280280568883131\n",
      "Epoch [6868/20000], Training Loss: 0.03127340433872762, Validation Loss: 0.015547629278430262\n",
      "Epoch [6869/20000], Training Loss: 0.014587622530086719, Validation Loss: 0.022071122825305496\n",
      "Epoch [6870/20000], Training Loss: 0.0149097074193248, Validation Loss: 0.005299023794901611\n",
      "Epoch [6871/20000], Training Loss: 0.016719915712850968, Validation Loss: 0.005381337681738684\n",
      "Epoch [6872/20000], Training Loss: 0.017918080326385928, Validation Loss: 0.01902883553635106\n",
      "Epoch [6873/20000], Training Loss: 0.010720043902827976, Validation Loss: 0.017819374280784527\n",
      "Epoch [6874/20000], Training Loss: 0.005529294350708369, Validation Loss: 0.0066319344908991286\n",
      "Epoch [6875/20000], Training Loss: 0.005797573946308278, Validation Loss: 0.004330165919805776\n",
      "Epoch [6876/20000], Training Loss: 0.005150976686439078, Validation Loss: 0.006250370061962445\n",
      "Epoch [6877/20000], Training Loss: 0.008341738796194218, Validation Loss: 0.005427881020521657\n",
      "Epoch [6878/20000], Training Loss: 0.02157861109091235, Validation Loss: 0.007460255476585386\n",
      "Epoch [6879/20000], Training Loss: 0.016491573552262447, Validation Loss: 0.009246953546728776\n",
      "Epoch [6880/20000], Training Loss: 0.0070170451368280085, Validation Loss: 0.01047074475466021\n",
      "Epoch [6881/20000], Training Loss: 0.008970511896795383, Validation Loss: 0.0069697568870253955\n",
      "Epoch [6882/20000], Training Loss: 0.006253946699871449, Validation Loss: 0.0032862514560306278\n",
      "Epoch [6883/20000], Training Loss: 0.005034583017862003, Validation Loss: 0.004368727162175805\n",
      "Epoch [6884/20000], Training Loss: 0.0041537394094380686, Validation Loss: 0.004724090247510304\n",
      "Epoch [6885/20000], Training Loss: 0.005373511614117429, Validation Loss: 0.010264319434342908\n",
      "Epoch [6886/20000], Training Loss: 0.007013275753706694, Validation Loss: 0.0049477964376894844\n",
      "Epoch [6887/20000], Training Loss: 0.011072258459275222, Validation Loss: 0.0053480384042816\n",
      "Epoch [6888/20000], Training Loss: 0.03797833199801711, Validation Loss: 0.01847153955273823\n",
      "Epoch [6889/20000], Training Loss: 0.05058845661447516, Validation Loss: 0.1259831831017873\n",
      "Epoch [6890/20000], Training Loss: 0.058536100598368127, Validation Loss: 0.021875176907169176\n",
      "Epoch [6891/20000], Training Loss: 0.02187273443476962, Validation Loss: 0.025862254596778236\n",
      "Epoch [6892/20000], Training Loss: 0.014313275692984462, Validation Loss: 0.009429293511077828\n",
      "Epoch [6893/20000], Training Loss: 0.011457575177441217, Validation Loss: 0.0066860528656564255\n",
      "Epoch [6894/20000], Training Loss: 0.015597401793846595, Validation Loss: 0.011201467856153613\n",
      "Epoch [6895/20000], Training Loss: 0.010083574554300867, Validation Loss: 0.021778089041617932\n",
      "Epoch [6896/20000], Training Loss: 0.01599462322441728, Validation Loss: 0.008484319461526215\n",
      "Epoch [6897/20000], Training Loss: 0.008958619420549698, Validation Loss: 0.005107109716066459\n",
      "Epoch [6898/20000], Training Loss: 0.006853451096610159, Validation Loss: 0.0045167492919842645\n",
      "Epoch [6899/20000], Training Loss: 0.006137437449069694, Validation Loss: 0.008352160791348848\n",
      "Epoch [6900/20000], Training Loss: 0.006684375781333074, Validation Loss: 0.0046470205655038755\n",
      "Epoch [6901/20000], Training Loss: 0.006627510541875381, Validation Loss: 0.0144665606133328\n",
      "Epoch [6902/20000], Training Loss: 0.011214444143531312, Validation Loss: 0.012012631518311017\n",
      "Epoch [6903/20000], Training Loss: 0.009920686186108339, Validation Loss: 0.004370598708451975\n",
      "Epoch [6904/20000], Training Loss: 0.006621281020280938, Validation Loss: 0.005885978940568748\n",
      "Epoch [6905/20000], Training Loss: 0.008087803359168382, Validation Loss: 0.005257666761930783\n",
      "Epoch [6906/20000], Training Loss: 0.00713271158747375, Validation Loss: 0.01049357451990675\n",
      "Epoch [6907/20000], Training Loss: 0.00839125894708559, Validation Loss: 0.006791657126736287\n",
      "Epoch [6908/20000], Training Loss: 0.00557833635580859, Validation Loss: 0.0025154989906306063\n",
      "Epoch [6909/20000], Training Loss: 0.005838294378398652, Validation Loss: 0.006959545784931314\n",
      "Epoch [6910/20000], Training Loss: 0.013400327824326399, Validation Loss: 0.004318632307201499\n",
      "Epoch [6911/20000], Training Loss: 0.006005931728785592, Validation Loss: 0.0030646247460026643\n",
      "Epoch [6912/20000], Training Loss: 0.007880475241108797, Validation Loss: 0.004897087737065574\n",
      "Epoch [6913/20000], Training Loss: 0.011760593351417421, Validation Loss: 0.0026045735842542073\n",
      "Epoch [6914/20000], Training Loss: 0.004913677680763483, Validation Loss: 0.03480678615868909\n",
      "Epoch [6915/20000], Training Loss: 0.017890249828529443, Validation Loss: 0.02691020374472178\n",
      "Epoch [6916/20000], Training Loss: 0.04786247656115198, Validation Loss: 0.04023935512822048\n",
      "Epoch [6917/20000], Training Loss: 0.03483086792402901, Validation Loss: 0.0443947566126874\n",
      "Epoch [6918/20000], Training Loss: 0.03345490343469594, Validation Loss: 0.01966335825501326\n",
      "Epoch [6919/20000], Training Loss: 0.02256528226280352, Validation Loss: 0.010973883941395408\n",
      "Epoch [6920/20000], Training Loss: 0.012328007599405413, Validation Loss: 0.007098617925553948\n",
      "Epoch [6921/20000], Training Loss: 0.0075556001704951215, Validation Loss: 0.0051113639981070135\n",
      "Epoch [6922/20000], Training Loss: 0.006695963085803669, Validation Loss: 0.00471596556417353\n",
      "Epoch [6923/20000], Training Loss: 0.00599686660903639, Validation Loss: 0.0037939703267641433\n",
      "Epoch [6924/20000], Training Loss: 0.005934913052312497, Validation Loss: 0.009467452324965311\n",
      "Epoch [6925/20000], Training Loss: 0.010144175870144474, Validation Loss: 0.006904536437722137\n",
      "Epoch [6926/20000], Training Loss: 0.036103327633879544, Validation Loss: 0.005650240519441597\n",
      "Epoch [6927/20000], Training Loss: 0.056553168737861724, Validation Loss: 0.06312989630428742\n",
      "Epoch [6928/20000], Training Loss: 0.02871863037164855, Validation Loss: 0.016187137148388535\n",
      "Epoch [6929/20000], Training Loss: 0.012263817064064955, Validation Loss: 0.00881598808804743\n",
      "Epoch [6930/20000], Training Loss: 0.009613312811000339, Validation Loss: 0.010340397637524543\n",
      "Epoch [6931/20000], Training Loss: 0.008682300478020417, Validation Loss: 0.007465865491082825\n",
      "Epoch [6932/20000], Training Loss: 0.008252179724097784, Validation Loss: 0.009948511050099893\n",
      "Epoch [6933/20000], Training Loss: 0.008250593608993637, Validation Loss: 0.0057300403212658625\n",
      "Epoch [6934/20000], Training Loss: 0.006306642198394131, Validation Loss: 0.0067304665785294605\n",
      "Epoch [6935/20000], Training Loss: 0.00670460173984923, Validation Loss: 0.005168224503449892\n",
      "Epoch [6936/20000], Training Loss: 0.00796101421084521, Validation Loss: 0.004235982712872653\n",
      "Epoch [6937/20000], Training Loss: 0.00880655830815158, Validation Loss: 0.005245362251687068\n",
      "Epoch [6938/20000], Training Loss: 0.006164490821122724, Validation Loss: 0.005043092789304434\n",
      "Epoch [6939/20000], Training Loss: 0.007998446638696675, Validation Loss: 0.02053567554986046\n",
      "Epoch [6940/20000], Training Loss: 0.01548904089057552, Validation Loss: 0.0062990647648477295\n",
      "Epoch [6941/20000], Training Loss: 0.027299620310908983, Validation Loss: 0.016686474611982964\n",
      "Epoch [6942/20000], Training Loss: 0.04217597699744147, Validation Loss: 0.08076629628054174\n",
      "Epoch [6943/20000], Training Loss: 0.07711413365489404, Validation Loss: 0.0389596620776881\n",
      "Epoch [6944/20000], Training Loss: 0.021388988649310443, Validation Loss: 0.047790750152284164\n",
      "Epoch [6945/20000], Training Loss: 0.027898785974165157, Validation Loss: 0.0107800201590345\n",
      "Epoch [6946/20000], Training Loss: 0.017268527066335082, Validation Loss: 0.014115840811947626\n",
      "Epoch [6947/20000], Training Loss: 0.013818660010916315, Validation Loss: 0.011114731035458634\n",
      "Epoch [6948/20000], Training Loss: 0.00843063463902841, Validation Loss: 0.00953201679979949\n",
      "Epoch [6949/20000], Training Loss: 0.008723758674542685, Validation Loss: 0.012378846857505519\n",
      "Epoch [6950/20000], Training Loss: 0.012033296733794227, Validation Loss: 0.00450409096070611\n",
      "Epoch [6951/20000], Training Loss: 0.012940047782779272, Validation Loss: 0.015352271778895286\n",
      "Epoch [6952/20000], Training Loss: 0.009084261900430388, Validation Loss: 0.003891569521848786\n",
      "Epoch [6953/20000], Training Loss: 0.010507838151949857, Validation Loss: 0.004472106375033036\n",
      "Epoch [6954/20000], Training Loss: 0.005466010793367916, Validation Loss: 0.004624800946917925\n",
      "Epoch [6955/20000], Training Loss: 0.004510158267554029, Validation Loss: 0.007746421457527829\n",
      "Epoch [6956/20000], Training Loss: 0.011647343785235924, Validation Loss: 0.005165080976995894\n",
      "Epoch [6957/20000], Training Loss: 0.009445112269271963, Validation Loss: 0.005969756931465652\n",
      "Epoch [6958/20000], Training Loss: 0.005589010426774621, Validation Loss: 0.003393918127512409\n",
      "Epoch [6959/20000], Training Loss: 0.009367807726708375, Validation Loss: 0.016322752528996683\n",
      "Epoch [6960/20000], Training Loss: 0.018208748764923906, Validation Loss: 0.01628056973948639\n",
      "Epoch [6961/20000], Training Loss: 0.026823029691253657, Validation Loss: 0.017545429450787457\n",
      "Epoch [6962/20000], Training Loss: 0.01081946265289194, Validation Loss: 0.01814005591093561\n",
      "Epoch [6963/20000], Training Loss: 0.018588097419914056, Validation Loss: 0.00816721443438492\n",
      "Epoch [6964/20000], Training Loss: 0.007384499897982876, Validation Loss: 0.008503981564639876\n",
      "Epoch [6965/20000], Training Loss: 0.009791755340951827, Validation Loss: 0.009450409305307923\n",
      "Epoch [6966/20000], Training Loss: 0.016552094107672537, Validation Loss: 0.004561289401863178\n",
      "Epoch [6967/20000], Training Loss: 0.022397531145543326, Validation Loss: 0.005888438162531072\n",
      "Epoch [6968/20000], Training Loss: 0.010083998924528714, Validation Loss: 0.012867924831242914\n",
      "Epoch [6969/20000], Training Loss: 0.01177966593448738, Validation Loss: 0.007436841164089206\n",
      "Epoch [6970/20000], Training Loss: 0.009506489902768018, Validation Loss: 0.010641764261565965\n",
      "Epoch [6971/20000], Training Loss: 0.00811162005369884, Validation Loss: 0.008267306487921035\n",
      "Epoch [6972/20000], Training Loss: 0.008102388241760699, Validation Loss: 0.003942934424579951\n",
      "Epoch [6973/20000], Training Loss: 0.009111271657664994, Validation Loss: 0.004882214588324781\n",
      "Epoch [6974/20000], Training Loss: 0.010794129272393807, Validation Loss: 0.008362885207848844\n",
      "Epoch [6975/20000], Training Loss: 0.017722301395679096, Validation Loss: 0.034097169313099585\n",
      "Epoch [6976/20000], Training Loss: 0.041202901783435664, Validation Loss: 0.014821559894439165\n",
      "Epoch [6977/20000], Training Loss: 0.017353287675567635, Validation Loss: 0.010782731714408087\n",
      "Epoch [6978/20000], Training Loss: 0.007549790736188048, Validation Loss: 0.005100927234155149\n",
      "Epoch [6979/20000], Training Loss: 0.007271861114921714, Validation Loss: 0.008577304151185532\n",
      "Epoch [6980/20000], Training Loss: 0.005143660154552825, Validation Loss: 0.005596256083533392\n",
      "Epoch [6981/20000], Training Loss: 0.00688468934926537, Validation Loss: 0.003938547611135894\n",
      "Epoch [6982/20000], Training Loss: 0.005990942637124265, Validation Loss: 0.0038740792855540868\n",
      "Epoch [6983/20000], Training Loss: 0.006194472489629074, Validation Loss: 0.004322168883390987\n",
      "Epoch [6984/20000], Training Loss: 0.005468748062412487, Validation Loss: 0.0032541638720228355\n",
      "Epoch [6985/20000], Training Loss: 0.007203867648578515, Validation Loss: 0.003917040392326702\n",
      "Epoch [6986/20000], Training Loss: 0.006623734238798663, Validation Loss: 0.004652376070907849\n",
      "Epoch [6987/20000], Training Loss: 0.003973414966562164, Validation Loss: 0.01558134348156456\n",
      "Epoch [6988/20000], Training Loss: 0.00914889172002274, Validation Loss: 0.008571914930626538\n",
      "Epoch [6989/20000], Training Loss: 0.007379016840007223, Validation Loss: 0.008393081208851072\n",
      "Epoch [6990/20000], Training Loss: 0.007840154892099755, Validation Loss: 0.005289886307514686\n",
      "Epoch [6991/20000], Training Loss: 0.005453292327729287, Validation Loss: 0.008602785936371591\n",
      "Epoch [6992/20000], Training Loss: 0.009022729718708433, Validation Loss: 0.005606640086095032\n",
      "Epoch [6993/20000], Training Loss: 0.01083797167978316, Validation Loss: 0.08326371865613122\n",
      "Epoch [6994/20000], Training Loss: 0.03834262034993279, Validation Loss: 0.08766145814032784\n",
      "Epoch [6995/20000], Training Loss: 0.038835212080032634, Validation Loss: 0.010360650074271074\n",
      "Epoch [6996/20000], Training Loss: 0.029412213879238282, Validation Loss: 0.007653581193413369\n",
      "Epoch [6997/20000], Training Loss: 0.008732994594278612, Validation Loss: 0.0065243845570799\n",
      "Epoch [6998/20000], Training Loss: 0.01395375638510034, Validation Loss: 0.0055310573668683515\n",
      "Epoch [6999/20000], Training Loss: 0.01588090363241333, Validation Loss: 0.010503045973991876\n",
      "Epoch [7000/20000], Training Loss: 0.010366966370660131, Validation Loss: 0.007624910262543543\n",
      "Epoch [7001/20000], Training Loss: 0.009380544935993385, Validation Loss: 0.004796267111162576\n",
      "Epoch [7002/20000], Training Loss: 0.005143366271762976, Validation Loss: 0.00418567530560462\n",
      "Epoch [7003/20000], Training Loss: 0.0063239550633755115, Validation Loss: 0.005336954883352364\n",
      "Epoch [7004/20000], Training Loss: 0.004775151918043515, Validation Loss: 0.004128752054185725\n",
      "Epoch [7005/20000], Training Loss: 0.006382797158689105, Validation Loss: 0.005191845525083798\n",
      "Epoch [7006/20000], Training Loss: 0.005071669054034699, Validation Loss: 0.006602940654257848\n",
      "Epoch [7007/20000], Training Loss: 0.006880409349313206, Validation Loss: 0.006320242003754727\n",
      "Epoch [7008/20000], Training Loss: 0.00932892098249535, Validation Loss: 0.006839361011865543\n",
      "Epoch [7009/20000], Training Loss: 0.01900594759717933, Validation Loss: 0.036489804994024704\n",
      "Epoch [7010/20000], Training Loss: 0.01712704656113471, Validation Loss: 0.03370768630078864\n",
      "Epoch [7011/20000], Training Loss: 0.022015192400950973, Validation Loss: 0.03710854716287997\n",
      "Epoch [7012/20000], Training Loss: 0.022659949615315002, Validation Loss: 0.0067676043960081134\n",
      "Epoch [7013/20000], Training Loss: 0.00799059722963388, Validation Loss: 0.007013322414422873\n",
      "Epoch [7014/20000], Training Loss: 0.008063488392508589, Validation Loss: 0.00593680462220839\n",
      "Epoch [7015/20000], Training Loss: 0.005530692019549731, Validation Loss: 0.004070695753487793\n",
      "Epoch [7016/20000], Training Loss: 0.00432515662968009, Validation Loss: 0.0037785422437630295\n",
      "Epoch [7017/20000], Training Loss: 0.004565417708363384, Validation Loss: 0.005121145515431635\n",
      "Epoch [7018/20000], Training Loss: 0.00790681251757113, Validation Loss: 0.003399075295196801\n",
      "Epoch [7019/20000], Training Loss: 0.007053167241143196, Validation Loss: 0.014218307717007652\n",
      "Epoch [7020/20000], Training Loss: 0.016062739604032168, Validation Loss: 0.011704563163235085\n",
      "Epoch [7021/20000], Training Loss: 0.013528074143284512, Validation Loss: 0.007579841707930817\n",
      "Epoch [7022/20000], Training Loss: 0.009660008737617838, Validation Loss: 0.010110215689564939\n",
      "Epoch [7023/20000], Training Loss: 0.013426056273601716, Validation Loss: 0.007144801314813114\n",
      "Epoch [7024/20000], Training Loss: 0.006642009699135087, Validation Loss: 0.030916381095775596\n",
      "Epoch [7025/20000], Training Loss: 0.02042663291545718, Validation Loss: 0.012748896011425858\n",
      "Epoch [7026/20000], Training Loss: 0.02096334259009122, Validation Loss: 0.005182360482454637\n",
      "Epoch [7027/20000], Training Loss: 0.008253050421314714, Validation Loss: 0.006886620906160846\n",
      "Epoch [7028/20000], Training Loss: 0.006463215841254818, Validation Loss: 0.015503950949415196\n",
      "Epoch [7029/20000], Training Loss: 0.016284224802023215, Validation Loss: 0.00670754778785262\n",
      "Epoch [7030/20000], Training Loss: 0.025317888810864782, Validation Loss: 0.006837770027419461\n",
      "Epoch [7031/20000], Training Loss: 0.018829380997755964, Validation Loss: 0.011138234789801313\n",
      "Epoch [7032/20000], Training Loss: 0.009645227325173826, Validation Loss: 0.005393314713415975\n",
      "Epoch [7033/20000], Training Loss: 0.005124387124461853, Validation Loss: 0.012587027132211579\n",
      "Epoch [7034/20000], Training Loss: 0.00840933413116675, Validation Loss: 0.005591255673725085\n",
      "Epoch [7035/20000], Training Loss: 0.008228677166958473, Validation Loss: 0.01829632690989021\n",
      "Epoch [7036/20000], Training Loss: 0.013581966415845923, Validation Loss: 0.013678229946129753\n",
      "Epoch [7037/20000], Training Loss: 0.014828584279582304, Validation Loss: 0.009480693818411197\n",
      "Epoch [7038/20000], Training Loss: 0.012014143143980098, Validation Loss: 0.004833729689758893\n",
      "Epoch [7039/20000], Training Loss: 0.0069177371832276026, Validation Loss: 0.006824020974371852\n",
      "Epoch [7040/20000], Training Loss: 0.006100089093316845, Validation Loss: 0.0038354844634567492\n",
      "Epoch [7041/20000], Training Loss: 0.00671716412463346, Validation Loss: 0.007823229140529162\n",
      "Epoch [7042/20000], Training Loss: 0.010474944867123017, Validation Loss: 0.014642910046924433\n",
      "Epoch [7043/20000], Training Loss: 0.022077191867408277, Validation Loss: 0.012757108413748382\n",
      "Epoch [7044/20000], Training Loss: 0.05493924338328985, Validation Loss: 0.016652896473123593\n",
      "Epoch [7045/20000], Training Loss: 0.02200510340792659, Validation Loss: 0.02755021490156743\n",
      "Epoch [7046/20000], Training Loss: 0.023293540164429163, Validation Loss: 0.024528198495242872\n",
      "Epoch [7047/20000], Training Loss: 0.01188493686640868, Validation Loss: 0.008941503508532218\n",
      "Epoch [7048/20000], Training Loss: 0.009357748195595508, Validation Loss: 0.00791542140190314\n",
      "Epoch [7049/20000], Training Loss: 0.013872027010490586, Validation Loss: 0.004612058737728931\n",
      "Epoch [7050/20000], Training Loss: 0.007219836065944817, Validation Loss: 0.005734082537164108\n",
      "Epoch [7051/20000], Training Loss: 0.00687745983512806, Validation Loss: 0.004512719321190113\n",
      "Epoch [7052/20000], Training Loss: 0.008675302497327462, Validation Loss: 0.006568571526720238\n",
      "Epoch [7053/20000], Training Loss: 0.008145412411457593, Validation Loss: 0.0037337679128078044\n",
      "Epoch [7054/20000], Training Loss: 0.006659115519141778, Validation Loss: 0.012117848997672593\n",
      "Epoch [7055/20000], Training Loss: 0.006835055941694789, Validation Loss: 0.02328976403423938\n",
      "Epoch [7056/20000], Training Loss: 0.022195180381199213, Validation Loss: 0.03209063463977405\n",
      "Epoch [7057/20000], Training Loss: 0.022048631369736022, Validation Loss: 0.025585104578307698\n",
      "Epoch [7058/20000], Training Loss: 0.00767028571031655, Validation Loss: 0.005264764145777637\n",
      "Epoch [7059/20000], Training Loss: 0.007969006987390717, Validation Loss: 0.00419337366386942\n",
      "Epoch [7060/20000], Training Loss: 0.0047801609471207485, Validation Loss: 0.0037485278725274407\n",
      "Epoch [7061/20000], Training Loss: 0.00600489521457348, Validation Loss: 0.0034946108673745624\n",
      "Epoch [7062/20000], Training Loss: 0.0066710877997268525, Validation Loss: 0.007479917657414312\n",
      "Epoch [7063/20000], Training Loss: 0.0065370092784827905, Validation Loss: 0.009880995172742197\n",
      "Epoch [7064/20000], Training Loss: 0.010387552572605532, Validation Loss: 0.03203581692365073\n",
      "Epoch [7065/20000], Training Loss: 0.023529549877691482, Validation Loss: 0.004496808333119812\n",
      "Epoch [7066/20000], Training Loss: 0.006231834105814674, Validation Loss: 0.0035265565839096443\n",
      "Epoch [7067/20000], Training Loss: 0.009017201969462414, Validation Loss: 0.04745460408083358\n",
      "Epoch [7068/20000], Training Loss: 0.013090388017839618, Validation Loss: 0.008741945959627658\n",
      "Epoch [7069/20000], Training Loss: 0.006702003272526521, Validation Loss: 0.00943868691762008\n",
      "Epoch [7070/20000], Training Loss: 0.00454529988512929, Validation Loss: 0.005317880772064432\n",
      "Epoch [7071/20000], Training Loss: 0.013296619248909078, Validation Loss: 0.004106281857780472\n",
      "Epoch [7072/20000], Training Loss: 0.01610963993165829, Validation Loss: 0.0170480843101229\n",
      "Epoch [7073/20000], Training Loss: 0.0049880835686053615, Validation Loss: 0.008485214535280061\n",
      "Epoch [7074/20000], Training Loss: 0.00754208110291594, Validation Loss: 0.0062846359809238805\n",
      "Epoch [7075/20000], Training Loss: 0.009442191475444856, Validation Loss: 0.010487737517918459\n",
      "Epoch [7076/20000], Training Loss: 0.011507941714496286, Validation Loss: 0.012327082290588416\n",
      "Epoch [7077/20000], Training Loss: 0.019835316921151907, Validation Loss: 0.003962506008659913\n",
      "Epoch [7078/20000], Training Loss: 0.009249821850971784, Validation Loss: 0.014001463673484054\n",
      "Epoch [7079/20000], Training Loss: 0.007809535017128967, Validation Loss: 0.0032719647625213855\n",
      "Epoch [7080/20000], Training Loss: 0.005241355235341686, Validation Loss: 0.0030878745624252865\n",
      "Epoch [7081/20000], Training Loss: 0.004587667524382206, Validation Loss: 0.0033305728750837866\n",
      "Epoch [7082/20000], Training Loss: 0.004353745067159512, Validation Loss: 0.006525731480470365\n",
      "Epoch [7083/20000], Training Loss: 0.00857203693937793, Validation Loss: 0.0030764423353973223\n",
      "Epoch [7084/20000], Training Loss: 0.005878385362198709, Validation Loss: 0.003303546858027437\n",
      "Epoch [7085/20000], Training Loss: 0.007632384168183697, Validation Loss: 0.00814241245827412\n",
      "Epoch [7086/20000], Training Loss: 0.01027546539549462, Validation Loss: 0.0049425030260977915\n",
      "Epoch [7087/20000], Training Loss: 0.017317118750984913, Validation Loss: 0.005926782132259781\n",
      "Epoch [7088/20000], Training Loss: 0.015776907125891654, Validation Loss: 0.003809359881090092\n",
      "Epoch [7089/20000], Training Loss: 0.015781605802974278, Validation Loss: 0.004186462660865679\n",
      "Epoch [7090/20000], Training Loss: 0.0074651986760727596, Validation Loss: 0.0048146884946729624\n",
      "Epoch [7091/20000], Training Loss: 0.0065529886550003925, Validation Loss: 0.005253993115993743\n",
      "Epoch [7092/20000], Training Loss: 0.004670275081090429, Validation Loss: 0.006235095938406618\n",
      "Epoch [7093/20000], Training Loss: 0.005867579769983422, Validation Loss: 0.009933278496776577\n",
      "Epoch [7094/20000], Training Loss: 0.0075341032227567795, Validation Loss: 0.003531821135107223\n",
      "Epoch [7095/20000], Training Loss: 0.008864186697922247, Validation Loss: 0.005553968231914627\n",
      "Epoch [7096/20000], Training Loss: 0.0055527388641556695, Validation Loss: 0.008173816438232206\n",
      "Epoch [7097/20000], Training Loss: 0.009632153865823057, Validation Loss: 0.005106800579754616\n",
      "Epoch [7098/20000], Training Loss: 0.00745202720281668, Validation Loss: 0.0036725585981377887\n",
      "Epoch [7099/20000], Training Loss: 0.005590510775878751, Validation Loss: 0.0024620172870523905\n",
      "Epoch [7100/20000], Training Loss: 0.013215868898571768, Validation Loss: 0.0037373157351170325\n",
      "Epoch [7101/20000], Training Loss: 0.015695037193446688, Validation Loss: 0.0029008937837909188\n",
      "Epoch [7102/20000], Training Loss: 0.023492120203237783, Validation Loss: 0.011542969782447047\n",
      "Epoch [7103/20000], Training Loss: 0.026340272678290994, Validation Loss: 0.022595799395016266\n",
      "Epoch [7104/20000], Training Loss: 0.07251127181682802, Validation Loss: 0.01708175561257774\n",
      "Epoch [7105/20000], Training Loss: 0.02396026286961777, Validation Loss: 0.012949006871262927\n",
      "Epoch [7106/20000], Training Loss: 0.014393692246812861, Validation Loss: 0.02005447256538151\n",
      "Epoch [7107/20000], Training Loss: 0.01824824813541324, Validation Loss: 0.007855397497865013\n",
      "Epoch [7108/20000], Training Loss: 0.00840846324406032, Validation Loss: 0.009568475452938471\n",
      "Epoch [7109/20000], Training Loss: 0.008974956947245769, Validation Loss: 0.004783383581939237\n",
      "Epoch [7110/20000], Training Loss: 0.009080398769583553, Validation Loss: 0.0047623310546120855\n",
      "Epoch [7111/20000], Training Loss: 0.005998123495373875, Validation Loss: 0.005310402155249186\n",
      "Epoch [7112/20000], Training Loss: 0.004931216392183809, Validation Loss: 0.009619066491726993\n",
      "Epoch [7113/20000], Training Loss: 0.006301136097005967, Validation Loss: 0.009625406668618552\n",
      "Epoch [7114/20000], Training Loss: 0.006943742815305346, Validation Loss: 0.010229766169296871\n",
      "Epoch [7115/20000], Training Loss: 0.016383899523394314, Validation Loss: 0.005159974331052648\n",
      "Epoch [7116/20000], Training Loss: 0.010239124957089578, Validation Loss: 0.015219897031784113\n",
      "Epoch [7117/20000], Training Loss: 0.01762382992559911, Validation Loss: 0.020376963807013086\n",
      "Epoch [7118/20000], Training Loss: 0.013856113148254476, Validation Loss: 0.010954312048853217\n",
      "Epoch [7119/20000], Training Loss: 0.006749894054207418, Validation Loss: 0.005044758452902843\n",
      "Epoch [7120/20000], Training Loss: 0.007398757825025574, Validation Loss: 0.003968339446674332\n",
      "Epoch [7121/20000], Training Loss: 0.005873374191911093, Validation Loss: 0.0032924171537170336\n",
      "Epoch [7122/20000], Training Loss: 0.005851848708386699, Validation Loss: 0.00593596909727369\n",
      "Epoch [7123/20000], Training Loss: 0.0170080665390872, Validation Loss: 0.014282305325780596\n",
      "Epoch [7124/20000], Training Loss: 0.0077963573816565, Validation Loss: 0.006335339475689808\n",
      "Epoch [7125/20000], Training Loss: 0.0070539358795420936, Validation Loss: 0.0034838604042333487\n",
      "Epoch [7126/20000], Training Loss: 0.005770661303553685, Validation Loss: 0.004129565836495547\n",
      "Epoch [7127/20000], Training Loss: 0.0031961507847881876, Validation Loss: 0.007485636576477971\n",
      "Epoch [7128/20000], Training Loss: 0.014323757395426842, Validation Loss: 0.015600384917982118\n",
      "Epoch [7129/20000], Training Loss: 0.03384044811627973, Validation Loss: 0.038805191181073236\n",
      "Epoch [7130/20000], Training Loss: 0.02163279668976819, Validation Loss: 0.020735968871122168\n",
      "Epoch [7131/20000], Training Loss: 0.01625149456127214, Validation Loss: 0.00743347001353862\n",
      "Epoch [7132/20000], Training Loss: 0.007923307870182075, Validation Loss: 0.005195452879044231\n",
      "Epoch [7133/20000], Training Loss: 0.006462869594023297, Validation Loss: 0.0048173847487435395\n",
      "Epoch [7134/20000], Training Loss: 0.007097082498928232, Validation Loss: 0.0036438078399377055\n",
      "Epoch [7135/20000], Training Loss: 0.005516946997106841, Validation Loss: 0.014105835534365571\n",
      "Epoch [7136/20000], Training Loss: 0.010823025283960825, Validation Loss: 0.0059658313867548985\n",
      "Epoch [7137/20000], Training Loss: 0.00952667712200699, Validation Loss: 0.028864610102963133\n",
      "Epoch [7138/20000], Training Loss: 0.01750290830594687, Validation Loss: 0.05909721231468471\n",
      "Epoch [7139/20000], Training Loss: 0.02568103858045236, Validation Loss: 0.016929770721097675\n",
      "Epoch [7140/20000], Training Loss: 0.014596928589136431, Validation Loss: 0.0226760958934327\n",
      "Epoch [7141/20000], Training Loss: 0.013973808398337237, Validation Loss: 0.011777815548030198\n",
      "Epoch [7142/20000], Training Loss: 0.009477116425322103, Validation Loss: 0.01405388731817594\n",
      "Epoch [7143/20000], Training Loss: 0.012927796052260021, Validation Loss: 0.008606858132744132\n",
      "Epoch [7144/20000], Training Loss: 0.016930296696955338, Validation Loss: 0.01010630295602485\n",
      "Epoch [7145/20000], Training Loss: 0.00925985028568123, Validation Loss: 0.004774768950224788\n",
      "Epoch [7146/20000], Training Loss: 0.008445587321018268, Validation Loss: 0.004857952734155521\n",
      "Epoch [7147/20000], Training Loss: 0.005780614140218988, Validation Loss: 0.005550986165779799\n",
      "Epoch [7148/20000], Training Loss: 0.006499673730494189, Validation Loss: 0.0060586720347828305\n",
      "Epoch [7149/20000], Training Loss: 0.021865727063933655, Validation Loss: 0.005094434268552764\n",
      "Epoch [7150/20000], Training Loss: 0.04400105228705797, Validation Loss: 0.021436173166658034\n",
      "Epoch [7151/20000], Training Loss: 0.03884129214072475, Validation Loss: 0.04398398365135238\n",
      "Epoch [7152/20000], Training Loss: 0.04389106349221298, Validation Loss: 0.00734520174684656\n",
      "Epoch [7153/20000], Training Loss: 0.00950764049983783, Validation Loss: 0.00610221293298528\n",
      "Epoch [7154/20000], Training Loss: 0.006427367375830987, Validation Loss: 0.008120754441496783\n",
      "Epoch [7155/20000], Training Loss: 0.006240234444183963, Validation Loss: 0.005431682354453317\n",
      "Epoch [7156/20000], Training Loss: 0.006001280587822423, Validation Loss: 0.0054780114423904945\n",
      "Epoch [7157/20000], Training Loss: 0.0058252421076758765, Validation Loss: 0.0069385771500285465\n",
      "Epoch [7158/20000], Training Loss: 0.007513848840192493, Validation Loss: 0.004032046077237997\n",
      "Epoch [7159/20000], Training Loss: 0.013510012333426857, Validation Loss: 0.004171225399172077\n",
      "Epoch [7160/20000], Training Loss: 0.011077403681286211, Validation Loss: 0.034345056248639594\n",
      "Epoch [7161/20000], Training Loss: 0.02538001337104236, Validation Loss: 0.00763544342869084\n",
      "Epoch [7162/20000], Training Loss: 0.007125295442944791, Validation Loss: 0.006687370306397082\n",
      "Epoch [7163/20000], Training Loss: 0.007468533831083083, Validation Loss: 0.010895808633046478\n",
      "Epoch [7164/20000], Training Loss: 0.009058707582880743, Validation Loss: 0.007666246746504362\n",
      "Epoch [7165/20000], Training Loss: 0.010044491399813893, Validation Loss: 0.009119323452774117\n",
      "Epoch [7166/20000], Training Loss: 0.014992707693246692, Validation Loss: 0.010097181997212244\n",
      "Epoch [7167/20000], Training Loss: 0.056740600732155144, Validation Loss: 0.013343677504183558\n",
      "Epoch [7168/20000], Training Loss: 0.0502605036433254, Validation Loss: 0.055941551594254894\n",
      "Epoch [7169/20000], Training Loss: 0.028682022137218155, Validation Loss: 0.010393388910754555\n",
      "Epoch [7170/20000], Training Loss: 0.017253122769034235, Validation Loss: 0.017674265516689047\n",
      "Epoch [7171/20000], Training Loss: 0.020875859772786498, Validation Loss: 0.012222906526176303\n",
      "Epoch [7172/20000], Training Loss: 0.0151591206584791, Validation Loss: 0.008451501785284152\n",
      "Epoch [7173/20000], Training Loss: 0.012544540459722546, Validation Loss: 0.008297645432273118\n",
      "Epoch [7174/20000], Training Loss: 0.0072514789228859755, Validation Loss: 0.005881772485021689\n",
      "Epoch [7175/20000], Training Loss: 0.0065768624647587004, Validation Loss: 0.005622848458122982\n",
      "Epoch [7176/20000], Training Loss: 0.00806629545227874, Validation Loss: 0.004762945944739678\n",
      "Epoch [7177/20000], Training Loss: 0.006884456957258018, Validation Loss: 0.00791388560238764\n",
      "Epoch [7178/20000], Training Loss: 0.006683880323959913, Validation Loss: 0.016625396908061394\n",
      "Epoch [7179/20000], Training Loss: 0.010949202892204215, Validation Loss: 0.005639749485410983\n",
      "Epoch [7180/20000], Training Loss: 0.006932758964986923, Validation Loss: 0.004620032697023887\n",
      "Epoch [7181/20000], Training Loss: 0.008472720701076599, Validation Loss: 0.003587467403564304\n",
      "Epoch [7182/20000], Training Loss: 0.006730585608628254, Validation Loss: 0.0050780207233210636\n",
      "Epoch [7183/20000], Training Loss: 0.00901208184645189, Validation Loss: 0.006881979163264148\n",
      "Epoch [7184/20000], Training Loss: 0.008403594601466986, Validation Loss: 0.026131261623699435\n",
      "Epoch [7185/20000], Training Loss: 0.016845853849580244, Validation Loss: 0.039267928942665876\n",
      "Epoch [7186/20000], Training Loss: 0.05382350550814863, Validation Loss: 0.029312087634783626\n",
      "Epoch [7187/20000], Training Loss: 0.02847372689783307, Validation Loss: 0.011584766112155443\n",
      "Epoch [7188/20000], Training Loss: 0.01705193457538761, Validation Loss: 0.02398566954902961\n",
      "Epoch [7189/20000], Training Loss: 0.013169819992201934, Validation Loss: 0.005293207189522166\n",
      "Epoch [7190/20000], Training Loss: 0.006673269415581932, Validation Loss: 0.0056401734605953735\n",
      "Epoch [7191/20000], Training Loss: 0.005937376069985996, Validation Loss: 0.0048682632930701075\n",
      "Epoch [7192/20000], Training Loss: 0.007243976858912252, Validation Loss: 0.006768945811586491\n",
      "Epoch [7193/20000], Training Loss: 0.005314451351296157, Validation Loss: 0.006729769138396117\n",
      "Epoch [7194/20000], Training Loss: 0.007403892564720341, Validation Loss: 0.008614823089611066\n",
      "Epoch [7195/20000], Training Loss: 0.007313991730121728, Validation Loss: 0.016496970904326145\n",
      "Epoch [7196/20000], Training Loss: 0.014658157090057753, Validation Loss: 0.006863264387423637\n",
      "Epoch [7197/20000], Training Loss: 0.008676014809420199, Validation Loss: 0.0077227760505585285\n",
      "Epoch [7198/20000], Training Loss: 0.009009573762471388, Validation Loss: 0.014947167730763928\n",
      "Epoch [7199/20000], Training Loss: 0.02060856174375658, Validation Loss: 0.025989298309598664\n",
      "Epoch [7200/20000], Training Loss: 0.019870743013695704, Validation Loss: 0.019556487245220015\n",
      "Epoch [7201/20000], Training Loss: 0.029815667856122934, Validation Loss: 0.02108988112636977\n",
      "Epoch [7202/20000], Training Loss: 0.020262091187760234, Validation Loss: 0.005744944693294071\n",
      "Epoch [7203/20000], Training Loss: 0.010594290858924589, Validation Loss: 0.013222183691433591\n",
      "Epoch [7204/20000], Training Loss: 0.015598943869658146, Validation Loss: 0.004565511832051021\n",
      "Epoch [7205/20000], Training Loss: 0.006623818426825372, Validation Loss: 0.005342075836555264\n",
      "Epoch [7206/20000], Training Loss: 0.00986104620720393, Validation Loss: 0.0037593002113536123\n",
      "Epoch [7207/20000], Training Loss: 0.02653193804352278, Validation Loss: 0.00662428509845755\n",
      "Epoch [7208/20000], Training Loss: 0.04355491736870525, Validation Loss: 0.10260457331226215\n",
      "Epoch [7209/20000], Training Loss: 0.02672115528132833, Validation Loss: 0.014695052376087656\n",
      "Epoch [7210/20000], Training Loss: 0.01475885434774682, Validation Loss: 0.006785972925727991\n",
      "Epoch [7211/20000], Training Loss: 0.011967810365604237, Validation Loss: 0.010781057312026143\n",
      "Epoch [7212/20000], Training Loss: 0.006885416656067329, Validation Loss: 0.00535281162457952\n",
      "Epoch [7213/20000], Training Loss: 0.00774563113061179, Validation Loss: 0.007205451440034995\n",
      "Epoch [7214/20000], Training Loss: 0.00847617965025295, Validation Loss: 0.004737422808574817\n",
      "Epoch [7215/20000], Training Loss: 0.006882580751802639, Validation Loss: 0.00670385773174785\n",
      "Epoch [7216/20000], Training Loss: 0.006119854665095252, Validation Loss: 0.007441388378370399\n",
      "Epoch [7217/20000], Training Loss: 0.0089658391246173, Validation Loss: 0.019547863198181886\n",
      "Epoch [7218/20000], Training Loss: 0.01374569763616559, Validation Loss: 0.012034404490675775\n",
      "Epoch [7219/20000], Training Loss: 0.009594625735189766, Validation Loss: 0.0076753873644170075\n",
      "Epoch [7220/20000], Training Loss: 0.009201446254987136, Validation Loss: 0.0035279902307340277\n",
      "Epoch [7221/20000], Training Loss: 0.009384914320045417, Validation Loss: 0.00803662823963183\n",
      "Epoch [7222/20000], Training Loss: 0.015503426352682124, Validation Loss: 0.011519374325872493\n",
      "Epoch [7223/20000], Training Loss: 0.04350562893835429, Validation Loss: 0.01159758939008633\n",
      "Epoch [7224/20000], Training Loss: 0.012837908372083413, Validation Loss: 0.014986249751278406\n",
      "Epoch [7225/20000], Training Loss: 0.011764225581178575, Validation Loss: 0.013878492106284456\n",
      "Epoch [7226/20000], Training Loss: 0.013937305933463253, Validation Loss: 0.013926060205059442\n",
      "Epoch [7227/20000], Training Loss: 0.019981785508986962, Validation Loss: 0.007043022948775933\n",
      "Epoch [7228/20000], Training Loss: 0.010426597070526051, Validation Loss: 0.018955979081738406\n",
      "Epoch [7229/20000], Training Loss: 0.014757246560683208, Validation Loss: 0.006667567499298333\n",
      "Epoch [7230/20000], Training Loss: 0.012027019037369817, Validation Loss: 0.00984881844903831\n",
      "Epoch [7231/20000], Training Loss: 0.013257825858447956, Validation Loss: 0.004050238500592488\n",
      "Epoch [7232/20000], Training Loss: 0.01035331302778429, Validation Loss: 0.0238709726838322\n",
      "Epoch [7233/20000], Training Loss: 0.03882062521422215, Validation Loss: 0.01743468791927669\n",
      "Epoch [7234/20000], Training Loss: 0.057808238416636674, Validation Loss: 0.050567918739148676\n",
      "Epoch [7235/20000], Training Loss: 0.04557904805239689, Validation Loss: 0.024549215924967678\n",
      "Epoch [7236/20000], Training Loss: 0.024694795770171498, Validation Loss: 0.0304917923149863\n",
      "Epoch [7237/20000], Training Loss: 0.019639783547193344, Validation Loss: 0.017741694657664602\n",
      "Epoch [7238/20000], Training Loss: 0.011418246713999127, Validation Loss: 0.009703018347493955\n",
      "Epoch [7239/20000], Training Loss: 0.010878898095273013, Validation Loss: 0.008581495143208464\n",
      "Epoch [7240/20000], Training Loss: 0.008011291942758752, Validation Loss: 0.006937119319527071\n",
      "Epoch [7241/20000], Training Loss: 0.007089342855449233, Validation Loss: 0.007111251246328958\n",
      "Epoch [7242/20000], Training Loss: 0.0069553223771176165, Validation Loss: 0.017611230800166616\n",
      "Epoch [7243/20000], Training Loss: 0.007896444585639983, Validation Loss: 0.006872158894777515\n",
      "Epoch [7244/20000], Training Loss: 0.00973724271437147, Validation Loss: 0.009337773410282222\n",
      "Epoch [7245/20000], Training Loss: 0.012601082845191871, Validation Loss: 0.01965505150812111\n",
      "Epoch [7246/20000], Training Loss: 0.011877316623992686, Validation Loss: 0.019681019403777682\n",
      "Epoch [7247/20000], Training Loss: 0.010285971541046364, Validation Loss: 0.002436841537415635\n",
      "Epoch [7248/20000], Training Loss: 0.013426468871849855, Validation Loss: 0.006032025132262879\n",
      "Epoch [7249/20000], Training Loss: 0.009843366670663403, Validation Loss: 0.00891409092296312\n",
      "Epoch [7250/20000], Training Loss: 0.006170616835463859, Validation Loss: 0.004577234163227624\n",
      "Epoch [7251/20000], Training Loss: 0.005368018331604877, Validation Loss: 0.004527914943989028\n",
      "Epoch [7252/20000], Training Loss: 0.005807311617120702, Validation Loss: 0.006255146308863654\n",
      "Epoch [7253/20000], Training Loss: 0.006254248110833162, Validation Loss: 0.0035079143462074364\n",
      "Epoch [7254/20000], Training Loss: 0.008392574045241677, Validation Loss: 0.04230590058224542\n",
      "Epoch [7255/20000], Training Loss: 0.023371245075915276, Validation Loss: 0.025445370801863324\n",
      "Epoch [7256/20000], Training Loss: 0.024509001770508933, Validation Loss: 0.031077623333999327\n",
      "Epoch [7257/20000], Training Loss: 0.028791993559674926, Validation Loss: 0.007010974435961056\n",
      "Epoch [7258/20000], Training Loss: 0.025860320733045228, Validation Loss: 0.005154551120158756\n",
      "Epoch [7259/20000], Training Loss: 0.011485564450725048, Validation Loss: 0.01084022900492879\n",
      "Epoch [7260/20000], Training Loss: 0.019756500600903695, Validation Loss: 0.019815218464094324\n",
      "Epoch [7261/20000], Training Loss: 0.010819284885136378, Validation Loss: 0.013316061480769612\n",
      "Epoch [7262/20000], Training Loss: 0.010259159053800562, Validation Loss: 0.004177217384211522\n",
      "Epoch [7263/20000], Training Loss: 0.006617475165382659, Validation Loss: 0.0038620831540531037\n",
      "Epoch [7264/20000], Training Loss: 0.0066459224773487745, Validation Loss: 0.004722156444888778\n",
      "Epoch [7265/20000], Training Loss: 0.006527214554515532, Validation Loss: 0.0054253343397814\n",
      "Epoch [7266/20000], Training Loss: 0.005869353781106058, Validation Loss: 0.003742933432975667\n",
      "Epoch [7267/20000], Training Loss: 0.008630413577910596, Validation Loss: 0.0055840732900416246\n",
      "Epoch [7268/20000], Training Loss: 0.0360571908881866, Validation Loss: 0.015028544301588096\n",
      "Epoch [7269/20000], Training Loss: 0.024674921529367566, Validation Loss: 0.01956208289827253\n",
      "Epoch [7270/20000], Training Loss: 0.01730924802125498, Validation Loss: 0.01571577067261387\n",
      "Epoch [7271/20000], Training Loss: 0.01194507148050304, Validation Loss: 0.006567067054285352\n",
      "Epoch [7272/20000], Training Loss: 0.007112809332154159, Validation Loss: 0.008616897098493479\n",
      "Epoch [7273/20000], Training Loss: 0.007428238076889622, Validation Loss: 0.006183858299274496\n",
      "Epoch [7274/20000], Training Loss: 0.007307089378757935, Validation Loss: 0.007867478209716085\n",
      "Epoch [7275/20000], Training Loss: 0.0076201600708632865, Validation Loss: 0.006725719072814432\n",
      "Epoch [7276/20000], Training Loss: 0.007822669849182213, Validation Loss: 0.004776865470245996\n",
      "Epoch [7277/20000], Training Loss: 0.005754094575032858, Validation Loss: 0.004768942732880532\n",
      "Epoch [7278/20000], Training Loss: 0.004217402423299583, Validation Loss: 0.005531032760862737\n",
      "Epoch [7279/20000], Training Loss: 0.006533397871992618, Validation Loss: 0.004840116039271639\n",
      "Epoch [7280/20000], Training Loss: 0.0047898204887003104, Validation Loss: 0.005470652065560395\n",
      "Epoch [7281/20000], Training Loss: 0.008371600610969056, Validation Loss: 0.0059169213231768936\n",
      "Epoch [7282/20000], Training Loss: 0.017925308296720947, Validation Loss: 0.028760970760473015\n",
      "Epoch [7283/20000], Training Loss: 0.017529155511251053, Validation Loss: 0.021701220955692054\n",
      "Epoch [7284/20000], Training Loss: 0.036005552577989874, Validation Loss: 0.0159737145794533\n",
      "Epoch [7285/20000], Training Loss: 0.03383440290365668, Validation Loss: 0.052688642116159645\n",
      "Epoch [7286/20000], Training Loss: 0.03459790936644173, Validation Loss: 0.014082459140532981\n",
      "Epoch [7287/20000], Training Loss: 0.012379659468673967, Validation Loss: 0.009272344468237341\n",
      "Epoch [7288/20000], Training Loss: 0.015501636219726476, Validation Loss: 0.02306400026593905\n",
      "Epoch [7289/20000], Training Loss: 0.006604945149774721, Validation Loss: 0.0052091421654349036\n",
      "Epoch [7290/20000], Training Loss: 0.008084795219474472, Validation Loss: 0.004093907506545845\n",
      "Epoch [7291/20000], Training Loss: 0.0071138740916337284, Validation Loss: 0.0037097566057216875\n",
      "Epoch [7292/20000], Training Loss: 0.007219986360691369, Validation Loss: 0.004301433540892218\n",
      "Epoch [7293/20000], Training Loss: 0.01169456192292273, Validation Loss: 0.0038746015833129705\n",
      "Epoch [7294/20000], Training Loss: 0.0052473073717140195, Validation Loss: 0.004974777883618323\n",
      "Epoch [7295/20000], Training Loss: 0.006868540562988658, Validation Loss: 0.011207386727148463\n",
      "Epoch [7296/20000], Training Loss: 0.019048118225652746, Validation Loss: 0.009426365758970727\n",
      "Epoch [7297/20000], Training Loss: 0.014877166408171303, Validation Loss: 0.003635900446583827\n",
      "Epoch [7298/20000], Training Loss: 0.02489137893280713, Validation Loss: 0.04385580677745952\n",
      "Epoch [7299/20000], Training Loss: 0.024118943076505923, Validation Loss: 0.03963288824473109\n",
      "Epoch [7300/20000], Training Loss: 0.019493985507974036, Validation Loss: 0.0055509608005638944\n",
      "Epoch [7301/20000], Training Loss: 0.004996798116865518, Validation Loss: 0.005166977788267532\n",
      "Epoch [7302/20000], Training Loss: 0.009530235222233127, Validation Loss: 0.008265199610343676\n",
      "Epoch [7303/20000], Training Loss: 0.007496602362932338, Validation Loss: 0.009806128887673024\n",
      "Epoch [7304/20000], Training Loss: 0.01430526609588664, Validation Loss: 0.003784634248924616\n",
      "Epoch [7305/20000], Training Loss: 0.01429562786012996, Validation Loss: 0.013287550071073564\n",
      "Epoch [7306/20000], Training Loss: 0.016856358868868222, Validation Loss: 0.004495662782249792\n",
      "Epoch [7307/20000], Training Loss: 0.018203945271514903, Validation Loss: 0.004758596525430417\n",
      "Epoch [7308/20000], Training Loss: 0.008180570150476083, Validation Loss: 0.03589054356728348\n",
      "Epoch [7309/20000], Training Loss: 0.011552147269737491, Validation Loss: 0.015408176263657692\n",
      "Epoch [7310/20000], Training Loss: 0.009462618248237829, Validation Loss: 0.005481375286878456\n",
      "Epoch [7311/20000], Training Loss: 0.007754436716952894, Validation Loss: 0.00807713044491365\n",
      "Epoch [7312/20000], Training Loss: 0.005779130413429812, Validation Loss: 0.008629402544910464\n",
      "Epoch [7313/20000], Training Loss: 0.0055097934288953964, Validation Loss: 0.002925119279856211\n",
      "Epoch [7314/20000], Training Loss: 0.005989661532761862, Validation Loss: 0.008979623467899198\n",
      "Epoch [7315/20000], Training Loss: 0.033733625276519366, Validation Loss: 0.006696259195846567\n",
      "Epoch [7316/20000], Training Loss: 0.0075180222472681635, Validation Loss: 0.017540291489462585\n",
      "Epoch [7317/20000], Training Loss: 0.006845965902487349, Validation Loss: 0.009669866424909928\n",
      "Epoch [7318/20000], Training Loss: 0.018310949351351673, Validation Loss: 0.015398624335440607\n",
      "Epoch [7319/20000], Training Loss: 0.012960684774952824, Validation Loss: 0.013325861743847358\n",
      "Epoch [7320/20000], Training Loss: 0.026598258170581954, Validation Loss: 0.07308464280400326\n",
      "Epoch [7321/20000], Training Loss: 0.026903011785699555, Validation Loss: 0.02495892452546424\n",
      "Epoch [7322/20000], Training Loss: 0.02632164493187312, Validation Loss: 0.02124240969926307\n",
      "Epoch [7323/20000], Training Loss: 0.019361466050863134, Validation Loss: 0.018040552008013116\n",
      "Epoch [7324/20000], Training Loss: 0.015959250384184998, Validation Loss: 0.00889219005744454\n",
      "Epoch [7325/20000], Training Loss: 0.015760637387367233, Validation Loss: 0.015269759089478871\n",
      "Epoch [7326/20000], Training Loss: 0.009692795890649515, Validation Loss: 0.006422696891209715\n",
      "Epoch [7327/20000], Training Loss: 0.007827688253850542, Validation Loss: 0.0039018612209460457\n",
      "Epoch [7328/20000], Training Loss: 0.005723714373224149, Validation Loss: 0.003978099287112664\n",
      "Epoch [7329/20000], Training Loss: 0.0070593637813414845, Validation Loss: 0.005326039391083852\n",
      "Epoch [7330/20000], Training Loss: 0.0070920935083579805, Validation Loss: 0.0034847596873782694\n",
      "Epoch [7331/20000], Training Loss: 0.00992178764343927, Validation Loss: 0.004474777936878227\n",
      "Epoch [7332/20000], Training Loss: 0.01342908841125531, Validation Loss: 0.005715638666425248\n",
      "Epoch [7333/20000], Training Loss: 0.01037247466384932, Validation Loss: 0.004419248864280507\n",
      "Epoch [7334/20000], Training Loss: 0.01223507843990644, Validation Loss: 0.007218660464315354\n",
      "Epoch [7335/20000], Training Loss: 0.017293625527859798, Validation Loss: 0.03024643659750209\n",
      "Epoch [7336/20000], Training Loss: 0.020569733942725828, Validation Loss: 0.05924636240757635\n",
      "Epoch [7337/20000], Training Loss: 0.029221382335370954, Validation Loss: 0.012497011862295382\n",
      "Epoch [7338/20000], Training Loss: 0.013041850270903004, Validation Loss: 0.01735522123860156\n",
      "Epoch [7339/20000], Training Loss: 0.016349760467684455, Validation Loss: 0.020177062089337245\n",
      "Epoch [7340/20000], Training Loss: 0.009544831267703557, Validation Loss: 0.050807662119560906\n",
      "Epoch [7341/20000], Training Loss: 0.030131106059083583, Validation Loss: 0.00890893272750556\n",
      "Epoch [7342/20000], Training Loss: 0.017846220256095485, Validation Loss: 0.007561259555989773\n",
      "Epoch [7343/20000], Training Loss: 0.013616584727839966, Validation Loss: 0.005345793370462156\n",
      "Epoch [7344/20000], Training Loss: 0.009425425309538176, Validation Loss: 0.004342548495505939\n",
      "Epoch [7345/20000], Training Loss: 0.013978990193988596, Validation Loss: 0.0041659437043944135\n",
      "Epoch [7346/20000], Training Loss: 0.029598156861279028, Validation Loss: 0.005883840792007179\n",
      "Epoch [7347/20000], Training Loss: 0.021276313092260222, Validation Loss: 0.0153636764464509\n",
      "Epoch [7348/20000], Training Loss: 0.021724862562093352, Validation Loss: 0.007110898090211164\n",
      "Epoch [7349/20000], Training Loss: 0.007882212570750977, Validation Loss: 0.01402573870504328\n",
      "Epoch [7350/20000], Training Loss: 0.010526688569890601, Validation Loss: 0.004836991914498664\n",
      "Epoch [7351/20000], Training Loss: 0.010273388275111626, Validation Loss: 0.005073181425698538\n",
      "Epoch [7352/20000], Training Loss: 0.005396139849576035, Validation Loss: 0.004862410964717988\n",
      "Epoch [7353/20000], Training Loss: 0.007073611242438866, Validation Loss: 0.005154764062169193\n",
      "Epoch [7354/20000], Training Loss: 0.009934287402886963, Validation Loss: 0.004938634656408806\n",
      "Epoch [7355/20000], Training Loss: 0.014751186188992247, Validation Loss: 0.005378706850638753\n",
      "Epoch [7356/20000], Training Loss: 0.006078987423929253, Validation Loss: 0.011388955378876355\n",
      "Epoch [7357/20000], Training Loss: 0.014601778804457613, Validation Loss: 0.008285233689280323\n",
      "Epoch [7358/20000], Training Loss: 0.028914060302278295, Validation Loss: 0.010625795684178499\n",
      "Epoch [7359/20000], Training Loss: 0.02422577681656029, Validation Loss: 0.006054736348945219\n",
      "Epoch [7360/20000], Training Loss: 0.012329380518557238, Validation Loss: 0.007048048801123573\n",
      "Epoch [7361/20000], Training Loss: 0.014165927776567904, Validation Loss: 0.006964820246461979\n",
      "Epoch [7362/20000], Training Loss: 0.009946842834844054, Validation Loss: 0.005526491864336894\n",
      "Epoch [7363/20000], Training Loss: 0.012379178041425933, Validation Loss: 0.0057077774106042345\n",
      "Epoch [7364/20000], Training Loss: 0.007415545376716182, Validation Loss: 0.007607518911860163\n",
      "Epoch [7365/20000], Training Loss: 0.008872743616458527, Validation Loss: 0.004409867772569116\n",
      "Epoch [7366/20000], Training Loss: 0.007176240544400311, Validation Loss: 0.014083604727623013\n",
      "Epoch [7367/20000], Training Loss: 0.005798406875588366, Validation Loss: 0.00385540786956506\n",
      "Epoch [7368/20000], Training Loss: 0.0056495091983898805, Validation Loss: 0.0040936466192811\n",
      "Epoch [7369/20000], Training Loss: 0.00459100492779336, Validation Loss: 0.020391014005456714\n",
      "Epoch [7370/20000], Training Loss: 0.008701160869220206, Validation Loss: 0.005127528221382181\n",
      "Epoch [7371/20000], Training Loss: 0.008063203788749109, Validation Loss: 0.026477177842675052\n",
      "Epoch [7372/20000], Training Loss: 0.00979326561874976, Validation Loss: 0.006016594642800387\n",
      "Epoch [7373/20000], Training Loss: 0.01118291670149201, Validation Loss: 0.00442819405874223\n",
      "Epoch [7374/20000], Training Loss: 0.006084092578281083, Validation Loss: 0.0045251541167522485\n",
      "Epoch [7375/20000], Training Loss: 0.007448254370891456, Validation Loss: 0.003535423850707988\n",
      "Epoch [7376/20000], Training Loss: 0.007203501689218683, Validation Loss: 0.0077531764403321185\n",
      "Epoch [7377/20000], Training Loss: 0.011515791082464779, Validation Loss: 0.019924649713497722\n",
      "Epoch [7378/20000], Training Loss: 0.027289417250878096, Validation Loss: 0.03117860719897538\n",
      "Epoch [7379/20000], Training Loss: 0.05145617803749961, Validation Loss: 0.0846385576909857\n",
      "Epoch [7380/20000], Training Loss: 0.061447938334562684, Validation Loss: 0.01833511965508348\n",
      "Epoch [7381/20000], Training Loss: 0.0376772701108296, Validation Loss: 0.015736291620653882\n",
      "Epoch [7382/20000], Training Loss: 0.0208526270580478, Validation Loss: 0.03604786404199034\n",
      "Epoch [7383/20000], Training Loss: 0.01941255860687566, Validation Loss: 0.008084062746694123\n",
      "Epoch [7384/20000], Training Loss: 0.01011862458627937, Validation Loss: 0.007117252179471377\n",
      "Epoch [7385/20000], Training Loss: 0.007673392375831359, Validation Loss: 0.007169646917938691\n",
      "Epoch [7386/20000], Training Loss: 0.008219969471870823, Validation Loss: 0.005741151412962846\n",
      "Epoch [7387/20000], Training Loss: 0.008263178563668459, Validation Loss: 0.005818650244041886\n",
      "Epoch [7388/20000], Training Loss: 0.008743636368308216, Validation Loss: 0.004995711930990053\n",
      "Epoch [7389/20000], Training Loss: 0.007463836497793507, Validation Loss: 0.006288067926886599\n",
      "Epoch [7390/20000], Training Loss: 0.007233443295782698, Validation Loss: 0.007091940786137586\n",
      "Epoch [7391/20000], Training Loss: 0.008181104925045344, Validation Loss: 0.009373624847025244\n",
      "Epoch [7392/20000], Training Loss: 0.005357894755044461, Validation Loss: 0.009152719441447386\n",
      "Epoch [7393/20000], Training Loss: 0.007790104236586818, Validation Loss: 0.005545568623282244\n",
      "Epoch [7394/20000], Training Loss: 0.007222192274639383, Validation Loss: 0.006579757223010217\n",
      "Epoch [7395/20000], Training Loss: 0.004510553107268477, Validation Loss: 0.004839321800123538\n",
      "Epoch [7396/20000], Training Loss: 0.005844987008458702, Validation Loss: 0.0042716237719235195\n",
      "Epoch [7397/20000], Training Loss: 0.0076222409302967465, Validation Loss: 0.0037983385906435224\n",
      "Epoch [7398/20000], Training Loss: 0.00570058634288476, Validation Loss: 0.0034914751461526195\n",
      "Epoch [7399/20000], Training Loss: 0.004932337917515335, Validation Loss: 0.007742177299470703\n",
      "Epoch [7400/20000], Training Loss: 0.008108128368413807, Validation Loss: 0.0038490011536071378\n",
      "Epoch [7401/20000], Training Loss: 0.008928033986433418, Validation Loss: 0.011986741186262861\n",
      "Epoch [7402/20000], Training Loss: 0.0111795991953321, Validation Loss: 0.011386770744811172\n",
      "Epoch [7403/20000], Training Loss: 0.014485836132994987, Validation Loss: 0.0037672443241133025\n",
      "Epoch [7404/20000], Training Loss: 0.007980082080134057, Validation Loss: 0.0037133385574179556\n",
      "Epoch [7405/20000], Training Loss: 0.004352315098263456, Validation Loss: 0.006085172593689744\n",
      "Epoch [7406/20000], Training Loss: 0.006691861169071801, Validation Loss: 0.004306177931052194\n",
      "Epoch [7407/20000], Training Loss: 0.009651871142393378, Validation Loss: 0.030880018802625798\n",
      "Epoch [7408/20000], Training Loss: 0.016918609759029226, Validation Loss: 0.004444200507675434\n",
      "Epoch [7409/20000], Training Loss: 0.011932408512490968, Validation Loss: 0.013430379588614653\n",
      "Epoch [7410/20000], Training Loss: 0.011854637677190891, Validation Loss: 0.004017689052976233\n",
      "Epoch [7411/20000], Training Loss: 0.00965863955207169, Validation Loss: 0.004060377682110682\n",
      "Epoch [7412/20000], Training Loss: 0.00982407939071501, Validation Loss: 0.006282103704279507\n",
      "Epoch [7413/20000], Training Loss: 0.007909167757653839, Validation Loss: 0.006195497880266235\n",
      "Epoch [7414/20000], Training Loss: 0.010750008366552979, Validation Loss: 0.028206168380815404\n",
      "Epoch [7415/20000], Training Loss: 0.007714278296459399, Validation Loss: 0.004619614161356367\n",
      "Epoch [7416/20000], Training Loss: 0.00439562600721339, Validation Loss: 0.011735858024520342\n",
      "Epoch [7417/20000], Training Loss: 0.016290661678112883, Validation Loss: 0.020467269141062974\n",
      "Epoch [7418/20000], Training Loss: 0.0154761808132337, Validation Loss: 0.00910971427804595\n",
      "Epoch [7419/20000], Training Loss: 0.012656318749187838, Validation Loss: 0.004953248914944197\n",
      "Epoch [7420/20000], Training Loss: 0.038006113114533946, Validation Loss: 0.07656957517013195\n",
      "Epoch [7421/20000], Training Loss: 0.04799513777522536, Validation Loss: 0.07045995944021602\n",
      "Epoch [7422/20000], Training Loss: 0.049129809672844464, Validation Loss: 0.026333028512453893\n",
      "Epoch [7423/20000], Training Loss: 0.023761832088764225, Validation Loss: 0.0068589711330892455\n",
      "Epoch [7424/20000], Training Loss: 0.011590430895531816, Validation Loss: 0.006428552861507342\n",
      "Epoch [7425/20000], Training Loss: 0.01231404091231525, Validation Loss: 0.01724598826472398\n",
      "Epoch [7426/20000], Training Loss: 0.014455829077633098, Validation Loss: 0.015168158663885498\n",
      "Epoch [7427/20000], Training Loss: 0.007612494569392376, Validation Loss: 0.007182488769055172\n",
      "Epoch [7428/20000], Training Loss: 0.006512322047326181, Validation Loss: 0.005243073586175991\n",
      "Epoch [7429/20000], Training Loss: 0.007043200769528214, Validation Loss: 0.005048657621890535\n",
      "Epoch [7430/20000], Training Loss: 0.008714152233941215, Validation Loss: 0.005499354589574068\n",
      "Epoch [7431/20000], Training Loss: 0.007903416050664549, Validation Loss: 0.004434058838406989\n",
      "Epoch [7432/20000], Training Loss: 0.006817735526835479, Validation Loss: 0.007569521627634198\n",
      "Epoch [7433/20000], Training Loss: 0.005753729043395391, Validation Loss: 0.006692223561783196\n",
      "Epoch [7434/20000], Training Loss: 0.005045989391094606, Validation Loss: 0.004996396465750357\n",
      "Epoch [7435/20000], Training Loss: 0.011964480953923027, Validation Loss: 0.004539735654881432\n",
      "Epoch [7436/20000], Training Loss: 0.005500853205765972, Validation Loss: 0.01236808664021056\n",
      "Epoch [7437/20000], Training Loss: 0.008383433750298406, Validation Loss: 0.003357147264458555\n",
      "Epoch [7438/20000], Training Loss: 0.004718941528283592, Validation Loss: 0.008933640584213467\n",
      "Epoch [7439/20000], Training Loss: 0.012997716615375663, Validation Loss: 0.007329525195570698\n",
      "Epoch [7440/20000], Training Loss: 0.018814674963193414, Validation Loss: 0.005081093031419316\n",
      "Epoch [7441/20000], Training Loss: 0.05192133928358089, Validation Loss: 0.038393953609735666\n",
      "Epoch [7442/20000], Training Loss: 0.06737238802920079, Validation Loss: 0.04815675363654187\n",
      "Epoch [7443/20000], Training Loss: 0.02130791260528245, Validation Loss: 0.007734541892156531\n",
      "Epoch [7444/20000], Training Loss: 0.009062687999435834, Validation Loss: 0.006130910077381265\n",
      "Epoch [7445/20000], Training Loss: 0.007955495900075351, Validation Loss: 0.007825947740846979\n",
      "Epoch [7446/20000], Training Loss: 0.010860058695210941, Validation Loss: 0.00898922058192039\n",
      "Epoch [7447/20000], Training Loss: 0.007334349641626302, Validation Loss: 0.005289853822514076\n",
      "Epoch [7448/20000], Training Loss: 0.009456639203043389, Validation Loss: 0.008448841528011794\n",
      "Epoch [7449/20000], Training Loss: 0.008484241536020167, Validation Loss: 0.006591473609351829\n",
      "Epoch [7450/20000], Training Loss: 0.007759532945263865, Validation Loss: 0.005185540655345123\n",
      "Epoch [7451/20000], Training Loss: 0.00984515923274947, Validation Loss: 0.015476565429707989\n",
      "Epoch [7452/20000], Training Loss: 0.016630228432144838, Validation Loss: 0.005649301187336927\n",
      "Epoch [7453/20000], Training Loss: 0.00846904520793552, Validation Loss: 0.0050464070832276775\n",
      "Epoch [7454/20000], Training Loss: 0.006623459031938442, Validation Loss: 0.004568436360646696\n",
      "Epoch [7455/20000], Training Loss: 0.004577532699581103, Validation Loss: 0.00611628363438805\n",
      "Epoch [7456/20000], Training Loss: 0.006842182952920796, Validation Loss: 0.009285339951617289\n",
      "Epoch [7457/20000], Training Loss: 0.005954283962117708, Validation Loss: 0.008893328391068796\n",
      "Epoch [7458/20000], Training Loss: 0.018486290589602765, Validation Loss: 0.04947065465863228\n",
      "Epoch [7459/20000], Training Loss: 0.02142482457566075, Validation Loss: 0.018256080405895072\n",
      "Epoch [7460/20000], Training Loss: 0.02170241108355445, Validation Loss: 0.016405348859344877\n",
      "Epoch [7461/20000], Training Loss: 0.007607336900296754, Validation Loss: 0.004567065325610591\n",
      "Epoch [7462/20000], Training Loss: 0.008938858522534636, Validation Loss: 0.008888968231985928\n",
      "Epoch [7463/20000], Training Loss: 0.010237954631780408, Validation Loss: 0.004426799982411467\n",
      "Epoch [7464/20000], Training Loss: 0.005953708660984246, Validation Loss: 0.00509825896246444\n",
      "Epoch [7465/20000], Training Loss: 0.006280807747056575, Validation Loss: 0.005040569128206178\n",
      "Epoch [7466/20000], Training Loss: 0.007204796114820056, Validation Loss: 0.004921985530779287\n",
      "Epoch [7467/20000], Training Loss: 0.01041154112525484, Validation Loss: 0.006825464022637918\n",
      "Epoch [7468/20000], Training Loss: 0.009210163879678086, Validation Loss: 0.010559690511464786\n",
      "Epoch [7469/20000], Training Loss: 0.006447070089052431, Validation Loss: 0.0036002298262311733\n",
      "Epoch [7470/20000], Training Loss: 0.013361620240695109, Validation Loss: 0.014649200918424293\n",
      "Epoch [7471/20000], Training Loss: 0.022370363215616505, Validation Loss: 0.04511154230151858\n",
      "Epoch [7472/20000], Training Loss: 0.025254576545973708, Validation Loss: 0.009523611470935325\n",
      "Epoch [7473/20000], Training Loss: 0.046806687094431254, Validation Loss: 0.06834954215654372\n",
      "Epoch [7474/20000], Training Loss: 0.02488230742788541, Validation Loss: 0.01479216809665169\n",
      "Epoch [7475/20000], Training Loss: 0.017016842020633964, Validation Loss: 0.005936930376200118\n",
      "Epoch [7476/20000], Training Loss: 0.009758901374880224, Validation Loss: 0.006039728987161652\n",
      "Epoch [7477/20000], Training Loss: 0.00883883563697704, Validation Loss: 0.00762635268098677\n",
      "Epoch [7478/20000], Training Loss: 0.006736901488953403, Validation Loss: 0.005196485377317523\n",
      "Epoch [7479/20000], Training Loss: 0.008305948115386335, Validation Loss: 0.009943730250682847\n",
      "Epoch [7480/20000], Training Loss: 0.010573370837456813, Validation Loss: 0.004977515473131332\n",
      "Epoch [7481/20000], Training Loss: 0.010254515168656195, Validation Loss: 0.005654141570922937\n",
      "Epoch [7482/20000], Training Loss: 0.008361112862725608, Validation Loss: 0.004969076764479853\n",
      "Epoch [7483/20000], Training Loss: 0.004464493636208188, Validation Loss: 0.0036607154583439944\n",
      "Epoch [7484/20000], Training Loss: 0.005288865948078377, Validation Loss: 0.004127660043195647\n",
      "Epoch [7485/20000], Training Loss: 0.008449138296003054, Validation Loss: 0.005805447180170477\n",
      "Epoch [7486/20000], Training Loss: 0.014401729077300323, Validation Loss: 0.017475132697394927\n",
      "Epoch [7487/20000], Training Loss: 0.010840686443511263, Validation Loss: 0.007000140007603152\n",
      "Epoch [7488/20000], Training Loss: 0.010508345383901283, Validation Loss: 0.0061466414044356755\n",
      "Epoch [7489/20000], Training Loss: 0.0051089928529108874, Validation Loss: 0.008087402903843863\n",
      "Epoch [7490/20000], Training Loss: 0.009807545144602565, Validation Loss: 0.003089872133062142\n",
      "Epoch [7491/20000], Training Loss: 0.008838897175987117, Validation Loss: 0.0050166798278501136\n",
      "Epoch [7492/20000], Training Loss: 0.006734031693797858, Validation Loss: 0.00543716603543284\n",
      "Epoch [7493/20000], Training Loss: 0.004405588017626931, Validation Loss: 0.003554391930244094\n",
      "Epoch [7494/20000], Training Loss: 0.007194356054242235, Validation Loss: 0.006852771322095657\n",
      "Epoch [7495/20000], Training Loss: 0.010178807725813905, Validation Loss: 0.025133488965890925\n",
      "Epoch [7496/20000], Training Loss: 0.021708055600746384, Validation Loss: 0.00859020972106642\n",
      "Epoch [7497/20000], Training Loss: 0.008751732939604804, Validation Loss: 0.005224077429143306\n",
      "Epoch [7498/20000], Training Loss: 0.013052213159556101, Validation Loss: 0.004486847886515274\n",
      "Epoch [7499/20000], Training Loss: 0.01124926612309147, Validation Loss: 0.006883360079728647\n",
      "Epoch [7500/20000], Training Loss: 0.005201503017198255, Validation Loss: 0.003751273252549124\n",
      "Epoch [7501/20000], Training Loss: 0.006682250186681163, Validation Loss: 0.00777483488595213\n",
      "Epoch [7502/20000], Training Loss: 0.015301682370201368, Validation Loss: 0.1013090930396824\n",
      "Epoch [7503/20000], Training Loss: 0.045921904750035276, Validation Loss: 0.08665953462150355\n",
      "Epoch [7504/20000], Training Loss: 0.044444841519829685, Validation Loss: 0.02316193701547929\n",
      "Epoch [7505/20000], Training Loss: 0.03989399903678402, Validation Loss: 0.035716926412923\n",
      "Epoch [7506/20000], Training Loss: 0.02427758537565491, Validation Loss: 0.006074693167306815\n",
      "Epoch [7507/20000], Training Loss: 0.007249162041781736, Validation Loss: 0.00750184011837185\n",
      "Epoch [7508/20000], Training Loss: 0.00858654860556791, Validation Loss: 0.005923961343147052\n",
      "Epoch [7509/20000], Training Loss: 0.007463088464906572, Validation Loss: 0.009386485114171773\n",
      "Epoch [7510/20000], Training Loss: 0.005431337903636242, Validation Loss: 0.005097258704673939\n",
      "Epoch [7511/20000], Training Loss: 0.007056860908051021, Validation Loss: 0.004863272409498417\n",
      "Epoch [7512/20000], Training Loss: 0.008366183216561953, Validation Loss: 0.009712190238480786\n",
      "Epoch [7513/20000], Training Loss: 0.007794779754476622, Validation Loss: 0.013755728490665921\n",
      "Epoch [7514/20000], Training Loss: 0.0053629945389275235, Validation Loss: 0.010525346461268523\n",
      "Epoch [7515/20000], Training Loss: 0.014924903599811452, Validation Loss: 0.04699750691309289\n",
      "Epoch [7516/20000], Training Loss: 0.03463440337717267, Validation Loss: 0.0076628442566207666\n",
      "Epoch [7517/20000], Training Loss: 0.023943531014343274, Validation Loss: 0.020827257305167483\n",
      "Epoch [7518/20000], Training Loss: 0.04046999350092457, Validation Loss: 0.044190975697268264\n",
      "Epoch [7519/20000], Training Loss: 0.02232166860319142, Validation Loss: 0.012382674391371208\n",
      "Epoch [7520/20000], Training Loss: 0.013695168238492832, Validation Loss: 0.012007489562815212\n",
      "Epoch [7521/20000], Training Loss: 0.009173405111401476, Validation Loss: 0.006799405428158981\n",
      "Epoch [7522/20000], Training Loss: 0.007282631729529905, Validation Loss: 0.01093930453966425\n",
      "Epoch [7523/20000], Training Loss: 0.008261040385280336, Validation Loss: 0.005847514163863453\n",
      "Epoch [7524/20000], Training Loss: 0.007081822787378249, Validation Loss: 0.006641690235158292\n",
      "Epoch [7525/20000], Training Loss: 0.006925907794668872, Validation Loss: 0.012366856051130911\n",
      "Epoch [7526/20000], Training Loss: 0.011045945230372516, Validation Loss: 0.02235612645745278\n",
      "Epoch [7527/20000], Training Loss: 0.014200012083165348, Validation Loss: 0.018210165069571564\n",
      "Epoch [7528/20000], Training Loss: 0.01906103383849508, Validation Loss: 0.035450266408068795\n",
      "Epoch [7529/20000], Training Loss: 0.03230305291435798, Validation Loss: 0.021620409012809563\n",
      "Epoch [7530/20000], Training Loss: 0.014458114006889186, Validation Loss: 0.010325540795617729\n",
      "Epoch [7531/20000], Training Loss: 0.010130079841474071, Validation Loss: 0.00539454149096993\n",
      "Epoch [7532/20000], Training Loss: 0.006543871264771691, Validation Loss: 0.005598674183998112\n",
      "Epoch [7533/20000], Training Loss: 0.006376020789113162, Validation Loss: 0.004348293804263952\n",
      "Epoch [7534/20000], Training Loss: 0.005705388797131101, Validation Loss: 0.006928597064703481\n",
      "Epoch [7535/20000], Training Loss: 0.009473105703623983, Validation Loss: 0.00559109279096605\n",
      "Epoch [7536/20000], Training Loss: 0.008751607100878443, Validation Loss: 0.005069206895864146\n",
      "Epoch [7537/20000], Training Loss: 0.005292243064364551, Validation Loss: 0.004009737260806985\n",
      "Epoch [7538/20000], Training Loss: 0.007457131963747088, Validation Loss: 0.004874605427838409\n",
      "Epoch [7539/20000], Training Loss: 0.008711058232750344, Validation Loss: 0.006103807461784233\n",
      "Epoch [7540/20000], Training Loss: 0.006472184634601165, Validation Loss: 0.004080895649955047\n",
      "Epoch [7541/20000], Training Loss: 0.004367713305068069, Validation Loss: 0.0033953398381937816\n",
      "Epoch [7542/20000], Training Loss: 0.005105105615387272, Validation Loss: 0.00706131723094122\n",
      "Epoch [7543/20000], Training Loss: 0.008945630063992991, Validation Loss: 0.008890222226072655\n",
      "Epoch [7544/20000], Training Loss: 0.015644314861024862, Validation Loss: 0.008194330281517354\n",
      "Epoch [7545/20000], Training Loss: 0.011852619216889642, Validation Loss: 0.005128812502396708\n",
      "Epoch [7546/20000], Training Loss: 0.009371091283225854, Validation Loss: 0.015297778110712963\n",
      "Epoch [7547/20000], Training Loss: 0.018347451047572707, Validation Loss: 0.017210639855095775\n",
      "Epoch [7548/20000], Training Loss: 0.038950942543773896, Validation Loss: 0.034027993164349776\n",
      "Epoch [7549/20000], Training Loss: 0.018486065444449196, Validation Loss: 0.005449420639481316\n",
      "Epoch [7550/20000], Training Loss: 0.005954173981860679, Validation Loss: 0.004558462449744434\n",
      "Epoch [7551/20000], Training Loss: 0.005711946707119101, Validation Loss: 0.005634963242434716\n",
      "Epoch [7552/20000], Training Loss: 0.01087897486523153, Validation Loss: 0.01637713677648987\n",
      "Epoch [7553/20000], Training Loss: 0.02818925625407636, Validation Loss: 0.04848829497184072\n",
      "Epoch [7554/20000], Training Loss: 0.031045532517185035, Validation Loss: 0.019602193497121334\n",
      "Epoch [7555/20000], Training Loss: 0.011817364450377812, Validation Loss: 0.005633869669501175\n",
      "Epoch [7556/20000], Training Loss: 0.007711225216293575, Validation Loss: 0.007264551043797275\n",
      "Epoch [7557/20000], Training Loss: 0.0109942140178256, Validation Loss: 0.01643661196742757\n",
      "Epoch [7558/20000], Training Loss: 0.024145057994506454, Validation Loss: 0.007428408409240166\n",
      "Epoch [7559/20000], Training Loss: 0.03825200754051496, Validation Loss: 0.009650877700291207\n",
      "Epoch [7560/20000], Training Loss: 0.012728445015714638, Validation Loss: 0.006183272284082315\n",
      "Epoch [7561/20000], Training Loss: 0.012482100677776284, Validation Loss: 0.00942846623782457\n",
      "Epoch [7562/20000], Training Loss: 0.006497745302892456, Validation Loss: 0.012563481448783673\n",
      "Epoch [7563/20000], Training Loss: 0.010878157872606866, Validation Loss: 0.00476933732262097\n",
      "Epoch [7564/20000], Training Loss: 0.011604176451718169, Validation Loss: 0.0071565590450423655\n",
      "Epoch [7565/20000], Training Loss: 0.005883630959682965, Validation Loss: 0.0052467962062652885\n",
      "Epoch [7566/20000], Training Loss: 0.00837935202123065, Validation Loss: 0.0034330087635951706\n",
      "Epoch [7567/20000], Training Loss: 0.004767838183657399, Validation Loss: 0.005853025120467699\n",
      "Epoch [7568/20000], Training Loss: 0.010558893711277051, Validation Loss: 0.004242736969835332\n",
      "Epoch [7569/20000], Training Loss: 0.005587254424296718, Validation Loss: 0.0032000351608190308\n",
      "Epoch [7570/20000], Training Loss: 0.005129053093177001, Validation Loss: 0.00646996770854723\n",
      "Epoch [7571/20000], Training Loss: 0.006103626737188149, Validation Loss: 0.0070260741168023545\n",
      "Epoch [7572/20000], Training Loss: 0.008782109424438593, Validation Loss: 0.0032896476423855273\n",
      "Epoch [7573/20000], Training Loss: 0.0077244485983101185, Validation Loss: 0.0033478723438145866\n",
      "Epoch [7574/20000], Training Loss: 0.02183022314273218, Validation Loss: 0.02223925984331745\n",
      "Epoch [7575/20000], Training Loss: 0.017219339234608406, Validation Loss: 0.038247263325112205\n",
      "Epoch [7576/20000], Training Loss: 0.023274707915594002, Validation Loss: 0.04846532429967608\n",
      "Epoch [7577/20000], Training Loss: 0.027517501499443955, Validation Loss: 0.018679512930767878\n",
      "Epoch [7578/20000], Training Loss: 0.028016376392250613, Validation Loss: 0.019532677049159588\n",
      "Epoch [7579/20000], Training Loss: 0.02956967541207892, Validation Loss: 0.020734635314853596\n",
      "Epoch [7580/20000], Training Loss: 0.012429969628491173, Validation Loss: 0.02385473753015215\n",
      "Epoch [7581/20000], Training Loss: 0.01116991278831847, Validation Loss: 0.005953131541002933\n",
      "Epoch [7582/20000], Training Loss: 0.00753387398747561, Validation Loss: 0.005126316265676286\n",
      "Epoch [7583/20000], Training Loss: 0.005906367716046849, Validation Loss: 0.008389139015782096\n",
      "Epoch [7584/20000], Training Loss: 0.007932483700902335, Validation Loss: 0.006522579820406398\n",
      "Epoch [7585/20000], Training Loss: 0.0075329573550594175, Validation Loss: 0.0046719155482129225\n",
      "Epoch [7586/20000], Training Loss: 0.006159090619933393, Validation Loss: 0.0034851701505793998\n",
      "Epoch [7587/20000], Training Loss: 0.0058283671760688905, Validation Loss: 0.012356687603252373\n",
      "Epoch [7588/20000], Training Loss: 0.011201701943978801, Validation Loss: 0.00823925801419799\n",
      "Epoch [7589/20000], Training Loss: 0.009092345663313088, Validation Loss: 0.0032173799871958184\n",
      "Epoch [7590/20000], Training Loss: 0.008907043152638445, Validation Loss: 0.0076439169685821725\n",
      "Epoch [7591/20000], Training Loss: 0.011339788661254195, Validation Loss: 0.005668929592859776\n",
      "Epoch [7592/20000], Training Loss: 0.0061683873924006805, Validation Loss: 0.006463140396634688\n",
      "Epoch [7593/20000], Training Loss: 0.00921784296942642, Validation Loss: 0.00994957090595656\n",
      "Epoch [7594/20000], Training Loss: 0.012949688989985069, Validation Loss: 0.006053046084654983\n",
      "Epoch [7595/20000], Training Loss: 0.027350680351706354, Validation Loss: 0.018727009350663643\n",
      "Epoch [7596/20000], Training Loss: 0.01577445534556838, Validation Loss: 0.007159813023589153\n",
      "Epoch [7597/20000], Training Loss: 0.00990234496670642, Validation Loss: 0.006519340603994347\n",
      "Epoch [7598/20000], Training Loss: 0.017908995184240797, Validation Loss: 0.02073915426877144\n",
      "Epoch [7599/20000], Training Loss: 0.02579273651229284, Validation Loss: 0.01754547848250557\n",
      "Epoch [7600/20000], Training Loss: 0.015214945031662605, Validation Loss: 0.013246056756802968\n",
      "Epoch [7601/20000], Training Loss: 0.011163231426118208, Validation Loss: 0.032090618674244197\n",
      "Epoch [7602/20000], Training Loss: 0.014474545482654224, Validation Loss: 0.0189204181405161\n",
      "Epoch [7603/20000], Training Loss: 0.02031387108114099, Validation Loss: 0.022917094747224975\n",
      "Epoch [7604/20000], Training Loss: 0.029330078736945455, Validation Loss: 0.014060494090829576\n",
      "Epoch [7605/20000], Training Loss: 0.020595605867648765, Validation Loss: 0.027277149260044098\n",
      "Epoch [7606/20000], Training Loss: 0.010993487465644389, Validation Loss: 0.009011627308079365\n",
      "Epoch [7607/20000], Training Loss: 0.007592935701333252, Validation Loss: 0.0054240083222432895\n",
      "Epoch [7608/20000], Training Loss: 0.006393134742601043, Validation Loss: 0.0045224302961284196\n",
      "Epoch [7609/20000], Training Loss: 0.006825484705456931, Validation Loss: 0.004871581248727554\n",
      "Epoch [7610/20000], Training Loss: 0.010235458098967294, Validation Loss: 0.0036303283256678143\n",
      "Epoch [7611/20000], Training Loss: 0.022510671818703747, Validation Loss: 0.005422376457451028\n",
      "Epoch [7612/20000], Training Loss: 0.07105664649160255, Validation Loss: 0.03297437416663992\n",
      "Epoch [7613/20000], Training Loss: 0.07343483424899334, Validation Loss: 0.07112036223491357\n",
      "Epoch [7614/20000], Training Loss: 0.03466473666152784, Validation Loss: 0.014799127171688298\n",
      "Epoch [7615/20000], Training Loss: 0.01868740888312459, Validation Loss: 0.01632402083514974\n",
      "Epoch [7616/20000], Training Loss: 0.015942836288429265, Validation Loss: 0.010549275201951909\n",
      "Epoch [7617/20000], Training Loss: 0.010010978040684546, Validation Loss: 0.006999285944614171\n",
      "Epoch [7618/20000], Training Loss: 0.008140810615649181, Validation Loss: 0.006003166822535734\n",
      "Epoch [7619/20000], Training Loss: 0.006998070935618931, Validation Loss: 0.004825141984448627\n",
      "Epoch [7620/20000], Training Loss: 0.005136283370250437, Validation Loss: 0.0033457325205940185\n",
      "Epoch [7621/20000], Training Loss: 0.005248782444531181, Validation Loss: 0.0034642454613203555\n",
      "Epoch [7622/20000], Training Loss: 0.006244575153297254, Validation Loss: 0.007029240469218426\n",
      "Epoch [7623/20000], Training Loss: 0.013415626063111372, Validation Loss: 0.005673569673478239\n",
      "Epoch [7624/20000], Training Loss: 0.007425624623075626, Validation Loss: 0.014450442445080952\n",
      "Epoch [7625/20000], Training Loss: 0.011354522903015354, Validation Loss: 0.006332804485191039\n",
      "Epoch [7626/20000], Training Loss: 0.012407277639015644, Validation Loss: 0.007105850803258461\n",
      "Epoch [7627/20000], Training Loss: 0.0214054327810475, Validation Loss: 0.029150172535862245\n",
      "Epoch [7628/20000], Training Loss: 0.0659753729534194, Validation Loss: 0.01947227622050767\n",
      "Epoch [7629/20000], Training Loss: 0.04853655426943858, Validation Loss: 0.04693950591692036\n",
      "Epoch [7630/20000], Training Loss: 0.030246196609888493, Validation Loss: 0.009062231610536335\n",
      "Epoch [7631/20000], Training Loss: 0.012961606087628752, Validation Loss: 0.01648353480387827\n",
      "Epoch [7632/20000], Training Loss: 0.01246824352918858, Validation Loss: 0.007945283450177547\n",
      "Epoch [7633/20000], Training Loss: 0.010588628630752541, Validation Loss: 0.00955166573155505\n",
      "Epoch [7634/20000], Training Loss: 0.006796942549824182, Validation Loss: 0.007021751272661548\n",
      "Epoch [7635/20000], Training Loss: 0.006857941559116755, Validation Loss: 0.005725671198864087\n",
      "Epoch [7636/20000], Training Loss: 0.006077601582676705, Validation Loss: 0.005102331636792253\n",
      "Epoch [7637/20000], Training Loss: 0.0064466127469456425, Validation Loss: 0.005553902288213841\n",
      "Epoch [7638/20000], Training Loss: 0.011246435868088156, Validation Loss: 0.004568430447571714\n",
      "Epoch [7639/20000], Training Loss: 0.012031485883718622, Validation Loss: 0.006524699173205385\n",
      "Epoch [7640/20000], Training Loss: 0.005954025000391994, Validation Loss: 0.0032355782044041774\n",
      "Epoch [7641/20000], Training Loss: 0.013967969004531662, Validation Loss: 0.005276687576581796\n",
      "Epoch [7642/20000], Training Loss: 0.009883654209553632, Validation Loss: 0.025568809892538482\n",
      "Epoch [7643/20000], Training Loss: 0.026270509515338096, Validation Loss: 0.003794385716348702\n",
      "Epoch [7644/20000], Training Loss: 0.006007921994060845, Validation Loss: 0.002886378747182079\n",
      "Epoch [7645/20000], Training Loss: 0.0035482938910718076, Validation Loss: 0.002071643174459052\n",
      "Epoch [7646/20000], Training Loss: 0.00962746615290858, Validation Loss: 0.010076610837171401\n",
      "Epoch [7647/20000], Training Loss: 0.020966301257366076, Validation Loss: 0.051698515418268344\n",
      "Epoch [7648/20000], Training Loss: 0.03647297007617973, Validation Loss: 0.0072329424735114245\n",
      "Epoch [7649/20000], Training Loss: 0.012917237123474479, Validation Loss: 0.007660377398078059\n",
      "Epoch [7650/20000], Training Loss: 0.008666990395535581, Validation Loss: 0.006919899257809448\n",
      "Epoch [7651/20000], Training Loss: 0.007693983830644616, Validation Loss: 0.005652554913857395\n",
      "Epoch [7652/20000], Training Loss: 0.007487518112092013, Validation Loss: 0.006842038045371217\n",
      "Epoch [7653/20000], Training Loss: 0.006480906125424164, Validation Loss: 0.004932350435280698\n",
      "Epoch [7654/20000], Training Loss: 0.005279075138137809, Validation Loss: 0.004708357303960449\n",
      "Epoch [7655/20000], Training Loss: 0.006602119746390229, Validation Loss: 0.005936936427669285\n",
      "Epoch [7656/20000], Training Loss: 0.008809631490813834, Validation Loss: 0.009580921429763427\n",
      "Epoch [7657/20000], Training Loss: 0.008494670710725976, Validation Loss: 0.011235890100236037\n",
      "Epoch [7658/20000], Training Loss: 0.016201979051402304, Validation Loss: 0.005240651463891156\n",
      "Epoch [7659/20000], Training Loss: 0.0099064020780913, Validation Loss: 0.01927388743635155\n",
      "Epoch [7660/20000], Training Loss: 0.01522293458297749, Validation Loss: 0.03903362580768329\n",
      "Epoch [7661/20000], Training Loss: 0.011403254017100803, Validation Loss: 0.011938825900542278\n",
      "Epoch [7662/20000], Training Loss: 0.009289782972440921, Validation Loss: 0.009156317048130549\n",
      "Epoch [7663/20000], Training Loss: 0.007706411422467292, Validation Loss: 0.00621653467095865\n",
      "Epoch [7664/20000], Training Loss: 0.007636520604137331, Validation Loss: 0.00462708054635641\n",
      "Epoch [7665/20000], Training Loss: 0.005649304302226353, Validation Loss: 0.0047410036127431\n",
      "Epoch [7666/20000], Training Loss: 0.008055231883190572, Validation Loss: 0.006347120095695554\n",
      "Epoch [7667/20000], Training Loss: 0.006227519961872271, Validation Loss: 0.003229639660647519\n",
      "Epoch [7668/20000], Training Loss: 0.005189134496114483, Validation Loss: 0.004193601943016085\n",
      "Epoch [7669/20000], Training Loss: 0.0038607649855423786, Validation Loss: 0.003705170517096639\n",
      "Epoch [7670/20000], Training Loss: 0.004608044552566882, Validation Loss: 0.0037199624182449043\n",
      "Epoch [7671/20000], Training Loss: 0.0038182576806450796, Validation Loss: 0.0029675581318096634\n",
      "Epoch [7672/20000], Training Loss: 0.005701119124881059, Validation Loss: 0.0031963042920380197\n",
      "Epoch [7673/20000], Training Loss: 0.009805468209378887, Validation Loss: 0.004123251045572525\n",
      "Epoch [7674/20000], Training Loss: 0.011532469264368825, Validation Loss: 0.02198805163923063\n",
      "Epoch [7675/20000], Training Loss: 0.00935831543806996, Validation Loss: 0.003963573232071622\n",
      "Epoch [7676/20000], Training Loss: 0.004482356727423412, Validation Loss: 0.0038250365672219794\n",
      "Epoch [7677/20000], Training Loss: 0.0063493123452644795, Validation Loss: 0.003637440859156373\n",
      "Epoch [7678/20000], Training Loss: 0.010940507823241725, Validation Loss: 0.006553022519704703\n",
      "Epoch [7679/20000], Training Loss: 0.0046604358680529655, Validation Loss: 0.002150166326800778\n",
      "Epoch [7680/20000], Training Loss: 0.003684447784118155, Validation Loss: 0.004268072511891573\n",
      "Epoch [7681/20000], Training Loss: 0.01174383673696866, Validation Loss: 0.007612057317208816\n",
      "Epoch [7682/20000], Training Loss: 0.014101215183213103, Validation Loss: 0.04967147218329566\n",
      "Epoch [7683/20000], Training Loss: 0.011996899849951401, Validation Loss: 0.0034926611183366658\n",
      "Epoch [7684/20000], Training Loss: 0.005505803530468256, Validation Loss: 0.003920344674955326\n",
      "Epoch [7685/20000], Training Loss: 0.013537533518696623, Validation Loss: 0.004858666077761293\n",
      "Epoch [7686/20000], Training Loss: 0.005407791343064413, Validation Loss: 0.0057537246638403716\n",
      "Epoch [7687/20000], Training Loss: 0.006843024410045473, Validation Loss: 0.006760568816448297\n",
      "Epoch [7688/20000], Training Loss: 0.013528239024578528, Validation Loss: 0.008946941016147443\n",
      "Epoch [7689/20000], Training Loss: 0.01920127629533194, Validation Loss: 0.016833062987613192\n",
      "Epoch [7690/20000], Training Loss: 0.08032478589354598, Validation Loss: 0.027757435577638847\n",
      "Epoch [7691/20000], Training Loss: 0.049041644664872104, Validation Loss: 0.013892688429872837\n",
      "Epoch [7692/20000], Training Loss: 0.025297149496119737, Validation Loss: 0.01657768283621408\n",
      "Epoch [7693/20000], Training Loss: 0.010237883818003215, Validation Loss: 0.005446032135556119\n",
      "Epoch [7694/20000], Training Loss: 0.006318615980230139, Validation Loss: 0.0058661776243492825\n",
      "Epoch [7695/20000], Training Loss: 0.0060522133052082995, Validation Loss: 0.006333552001576338\n",
      "Epoch [7696/20000], Training Loss: 0.008507462567649782, Validation Loss: 0.004825358598152109\n",
      "Epoch [7697/20000], Training Loss: 0.011030782122231488, Validation Loss: 0.008777123167549559\n",
      "Epoch [7698/20000], Training Loss: 0.008011787595543345, Validation Loss: 0.009292547853484783\n",
      "Epoch [7699/20000], Training Loss: 0.007541401416201442, Validation Loss: 0.004752255762079065\n",
      "Epoch [7700/20000], Training Loss: 0.00643421386900757, Validation Loss: 0.0044692104955694435\n",
      "Epoch [7701/20000], Training Loss: 0.004710427073275371, Validation Loss: 0.006129769338648268\n",
      "Epoch [7702/20000], Training Loss: 0.005308494682789647, Validation Loss: 0.005368320288003555\n",
      "Epoch [7703/20000], Training Loss: 0.0055568550264329785, Validation Loss: 0.003732847757419222\n",
      "Epoch [7704/20000], Training Loss: 0.004486112626701859, Validation Loss: 0.010042441415082846\n",
      "Epoch [7705/20000], Training Loss: 0.006500535280346347, Validation Loss: 0.004195081607836723\n",
      "Epoch [7706/20000], Training Loss: 0.007191557845674522, Validation Loss: 0.012587439971347132\n",
      "Epoch [7707/20000], Training Loss: 0.005356579501364779, Validation Loss: 0.008514687612789982\n",
      "Epoch [7708/20000], Training Loss: 0.01648529355445914, Validation Loss: 0.025063399505597772\n",
      "Epoch [7709/20000], Training Loss: 0.007082324391896171, Validation Loss: 0.013563730284139248\n",
      "Epoch [7710/20000], Training Loss: 0.007817188816261478, Validation Loss: 0.007506420136061414\n",
      "Epoch [7711/20000], Training Loss: 0.009734562722899551, Validation Loss: 0.0034874985592198626\n",
      "Epoch [7712/20000], Training Loss: 0.0059270533134362525, Validation Loss: 0.004467535994730757\n",
      "Epoch [7713/20000], Training Loss: 0.008292126040032599, Validation Loss: 0.0048809099276435775\n",
      "Epoch [7714/20000], Training Loss: 0.00659748112359791, Validation Loss: 0.004819388676099022\n",
      "Epoch [7715/20000], Training Loss: 0.005933506726120997, Validation Loss: 0.0031885810707795565\n",
      "Epoch [7716/20000], Training Loss: 0.007378955319706749, Validation Loss: 0.0045405903941043236\n",
      "Epoch [7717/20000], Training Loss: 0.006952675465332244, Validation Loss: 0.005083352309546691\n",
      "Epoch [7718/20000], Training Loss: 0.004109544623393698, Validation Loss: 0.002599299774764014\n",
      "Epoch [7719/20000], Training Loss: 0.004469580153194589, Validation Loss: 0.0055454907149266675\n",
      "Epoch [7720/20000], Training Loss: 0.00743038150384174, Validation Loss: 0.008906844232829567\n",
      "Epoch [7721/20000], Training Loss: 0.009957056301313319, Validation Loss: 0.008055209291926855\n",
      "Epoch [7722/20000], Training Loss: 0.012959920512262866, Validation Loss: 0.004410270952900776\n",
      "Epoch [7723/20000], Training Loss: 0.037403241194884425, Validation Loss: 0.033596588129642314\n",
      "Epoch [7724/20000], Training Loss: 0.07390933080270148, Validation Loss: 0.03510151289603008\n",
      "Epoch [7725/20000], Training Loss: 0.07999540062987112, Validation Loss: 0.008576816729536014\n",
      "Epoch [7726/20000], Training Loss: 0.046930730276341946, Validation Loss: 0.0781343651758012\n",
      "Epoch [7727/20000], Training Loss: 0.040774588173787506, Validation Loss: 0.016632166425032274\n",
      "Epoch [7728/20000], Training Loss: 0.02614068439496415, Validation Loss: 0.021218085536799793\n",
      "Epoch [7729/20000], Training Loss: 0.017158538701811006, Validation Loss: 0.011274246552992346\n",
      "Epoch [7730/20000], Training Loss: 0.013349697126873903, Validation Loss: 0.01171470016434736\n",
      "Epoch [7731/20000], Training Loss: 0.00853664827133928, Validation Loss: 0.005816195696317549\n",
      "Epoch [7732/20000], Training Loss: 0.01683126192905807, Validation Loss: 0.02318428412333523\n",
      "Epoch [7733/20000], Training Loss: 0.014539314708339848, Validation Loss: 0.00835926809148597\n",
      "Epoch [7734/20000], Training Loss: 0.009705530138619776, Validation Loss: 0.018562081453897657\n",
      "Epoch [7735/20000], Training Loss: 0.014588711725082248, Validation Loss: 0.005964722537360753\n",
      "Epoch [7736/20000], Training Loss: 0.012857134757463686, Validation Loss: 0.005646758720623828\n",
      "Epoch [7737/20000], Training Loss: 0.009056492234646742, Validation Loss: 0.009819549210525176\n",
      "Epoch [7738/20000], Training Loss: 0.00908188507595956, Validation Loss: 0.021408227376273965\n",
      "Epoch [7739/20000], Training Loss: 0.012574266004126653, Validation Loss: 0.007807053089651715\n",
      "Epoch [7740/20000], Training Loss: 0.01181902109861507, Validation Loss: 0.00533206695519896\n",
      "Epoch [7741/20000], Training Loss: 0.009591603593435138, Validation Loss: 0.010785120582590022\n",
      "Epoch [7742/20000], Training Loss: 0.008263131460158288, Validation Loss: 0.007378617714633583\n",
      "Epoch [7743/20000], Training Loss: 0.011368407352295305, Validation Loss: 0.004368947938408902\n",
      "Epoch [7744/20000], Training Loss: 0.012795706243196037, Validation Loss: 0.006232017744602152\n",
      "Epoch [7745/20000], Training Loss: 0.00499839879297984, Validation Loss: 0.010899429310468374\n",
      "Epoch [7746/20000], Training Loss: 0.006955940592368799, Validation Loss: 0.0038529392819798758\n",
      "Epoch [7747/20000], Training Loss: 0.006350257740580544, Validation Loss: 0.004833466699502813\n",
      "Epoch [7748/20000], Training Loss: 0.007156851460292403, Validation Loss: 0.00934799332234044\n",
      "Epoch [7749/20000], Training Loss: 0.01134086585703439, Validation Loss: 0.004112644058494652\n",
      "Epoch [7750/20000], Training Loss: 0.0052508162272196, Validation Loss: 0.005434003240067804\n",
      "Epoch [7751/20000], Training Loss: 0.009379578145204246, Validation Loss: 0.007014918126272204\n",
      "Epoch [7752/20000], Training Loss: 0.008449987470937361, Validation Loss: 0.0066920351128782075\n",
      "Epoch [7753/20000], Training Loss: 0.015617685758375697, Validation Loss: 0.03052570617631434\n",
      "Epoch [7754/20000], Training Loss: 0.05492627114316357, Validation Loss: 0.06848084713731493\n",
      "Epoch [7755/20000], Training Loss: 0.0398300302154634, Validation Loss: 0.10166660785675471\n",
      "Epoch [7756/20000], Training Loss: 0.03379326488357037, Validation Loss: 0.007672152250437746\n",
      "Epoch [7757/20000], Training Loss: 0.014774086554617887, Validation Loss: 0.00707069741025406\n",
      "Epoch [7758/20000], Training Loss: 0.010671187772199378, Validation Loss: 0.005157824149526797\n",
      "Epoch [7759/20000], Training Loss: 0.00684224813968675, Validation Loss: 0.004936265026896892\n",
      "Epoch [7760/20000], Training Loss: 0.017272514116484672, Validation Loss: 0.012776069560767045\n",
      "Epoch [7761/20000], Training Loss: 0.006832821993157268, Validation Loss: 0.0047394503493965855\n",
      "Epoch [7762/20000], Training Loss: 0.0065561272804188775, Validation Loss: 0.006942339710024369\n",
      "Epoch [7763/20000], Training Loss: 0.009803547074885241, Validation Loss: 0.007324778012560833\n",
      "Epoch [7764/20000], Training Loss: 0.009674219693157024, Validation Loss: 0.024987725301098475\n",
      "Epoch [7765/20000], Training Loss: 0.007619523955716951, Validation Loss: 0.013932365136806115\n",
      "Epoch [7766/20000], Training Loss: 0.01463538297049638, Validation Loss: 0.05369310946636168\n",
      "Epoch [7767/20000], Training Loss: 0.04442090292494478, Validation Loss: 0.008017750078319555\n",
      "Epoch [7768/20000], Training Loss: 0.015809845362257744, Validation Loss: 0.009633722599415282\n",
      "Epoch [7769/20000], Training Loss: 0.010514428219591667, Validation Loss: 0.02667520089147866\n",
      "Epoch [7770/20000], Training Loss: 0.008752382535021752, Validation Loss: 0.004995490055892203\n",
      "Epoch [7771/20000], Training Loss: 0.008173492296399283, Validation Loss: 0.005630458687262164\n",
      "Epoch [7772/20000], Training Loss: 0.01014344284443983, Validation Loss: 0.010755073657534642\n",
      "Epoch [7773/20000], Training Loss: 0.014986014758116133, Validation Loss: 0.027356620345802017\n",
      "Epoch [7774/20000], Training Loss: 0.01679465915003675, Validation Loss: 0.010416971592478928\n",
      "Epoch [7775/20000], Training Loss: 0.011287665342803978, Validation Loss: 0.007791926357083737\n",
      "Epoch [7776/20000], Training Loss: 0.009053877534045438, Validation Loss: 0.011931677748132852\n",
      "Epoch [7777/20000], Training Loss: 0.00988546058319376, Validation Loss: 0.007370810849038936\n",
      "Epoch [7778/20000], Training Loss: 0.010059796868258022, Validation Loss: 0.003976259845919523\n",
      "Epoch [7779/20000], Training Loss: 0.009656559364105175, Validation Loss: 0.004852425087449279\n",
      "Epoch [7780/20000], Training Loss: 0.006569901958885437, Validation Loss: 0.016606348325727334\n",
      "Epoch [7781/20000], Training Loss: 0.014520879994961433, Validation Loss: 0.008490028698842775\n",
      "Epoch [7782/20000], Training Loss: 0.005436040483930681, Validation Loss: 0.010648450415051535\n",
      "Epoch [7783/20000], Training Loss: 0.005762156861497455, Validation Loss: 0.008472072522454679\n",
      "Epoch [7784/20000], Training Loss: 0.009980085771530867, Validation Loss: 0.004979017727454301\n",
      "Epoch [7785/20000], Training Loss: 0.01363771868013178, Validation Loss: 0.011214511069868835\n",
      "Epoch [7786/20000], Training Loss: 0.01007155913742151, Validation Loss: 0.005000312017608277\n",
      "Epoch [7787/20000], Training Loss: 0.004590815831241863, Validation Loss: 0.004225946720832573\n",
      "Epoch [7788/20000], Training Loss: 0.0074362183030254, Validation Loss: 0.004646237344375049\n",
      "Epoch [7789/20000], Training Loss: 0.012689022598216582, Validation Loss: 0.007720592085348699\n",
      "Epoch [7790/20000], Training Loss: 0.008757811330724508, Validation Loss: 0.0068019210252562145\n",
      "Epoch [7791/20000], Training Loss: 0.008281908212146456, Validation Loss: 0.00373785508828869\n",
      "Epoch [7792/20000], Training Loss: 0.004658101637947506, Validation Loss: 0.003621906492612364\n",
      "Epoch [7793/20000], Training Loss: 0.011875956138413, Validation Loss: 0.00889468785109332\n",
      "Epoch [7794/20000], Training Loss: 0.009856604503251478, Validation Loss: 0.005008369885328369\n",
      "Epoch [7795/20000], Training Loss: 0.005396477564707831, Validation Loss: 0.007401157968814174\n",
      "Epoch [7796/20000], Training Loss: 0.0074427031691552, Validation Loss: 0.0529268582972041\n",
      "Epoch [7797/20000], Training Loss: 0.03025638288841037, Validation Loss: 0.011940635446697863\n",
      "Epoch [7798/20000], Training Loss: 0.01569116763961834, Validation Loss: 0.00916350821149438\n",
      "Epoch [7799/20000], Training Loss: 0.012333273480180651, Validation Loss: 0.008892444494596238\n",
      "Epoch [7800/20000], Training Loss: 0.012747267608314619, Validation Loss: 0.011924270473952833\n",
      "Epoch [7801/20000], Training Loss: 0.0187535799466007, Validation Loss: 0.024163721410936374\n",
      "Epoch [7802/20000], Training Loss: 0.03737662018725781, Validation Loss: 0.06010461302030656\n",
      "Epoch [7803/20000], Training Loss: 0.0750692765333432, Validation Loss: 0.015520313105038699\n",
      "Epoch [7804/20000], Training Loss: 0.01789685386964785, Validation Loss: 0.014264410291600273\n",
      "Epoch [7805/20000], Training Loss: 0.018321188216629838, Validation Loss: 0.008423956120850562\n",
      "Epoch [7806/20000], Training Loss: 0.014039789455377363, Validation Loss: 0.007074024212415367\n",
      "Epoch [7807/20000], Training Loss: 0.007401467583674405, Validation Loss: 0.008273889717705585\n",
      "Epoch [7808/20000], Training Loss: 0.009244381603951166, Validation Loss: 0.007642092983990113\n",
      "Epoch [7809/20000], Training Loss: 0.008152617723681033, Validation Loss: 0.005369751524851117\n",
      "Epoch [7810/20000], Training Loss: 0.00782695057361187, Validation Loss: 0.006917248813695675\n",
      "Epoch [7811/20000], Training Loss: 0.007212187662454588, Validation Loss: 0.004848299700002556\n",
      "Epoch [7812/20000], Training Loss: 0.00781731768609192, Validation Loss: 0.007426938443385568\n",
      "Epoch [7813/20000], Training Loss: 0.006174993701279163, Validation Loss: 0.005247048236796638\n",
      "Epoch [7814/20000], Training Loss: 0.005514183823834173, Validation Loss: 0.00804633341204167\n",
      "Epoch [7815/20000], Training Loss: 0.005627450837762028, Validation Loss: 0.009053952834370145\n",
      "Epoch [7816/20000], Training Loss: 0.0061666052268784756, Validation Loss: 0.0040812927833367655\n",
      "Epoch [7817/20000], Training Loss: 0.007050938622374555, Validation Loss: 0.012691031077079072\n",
      "Epoch [7818/20000], Training Loss: 0.006152048869158274, Validation Loss: 0.004016909815363252\n",
      "Epoch [7819/20000], Training Loss: 0.006845023014518249, Validation Loss: 0.00430184029361202\n",
      "Epoch [7820/20000], Training Loss: 0.00706561873708519, Validation Loss: 0.0069200361280581215\n",
      "Epoch [7821/20000], Training Loss: 0.009485959216752755, Validation Loss: 0.015888492709824627\n",
      "Epoch [7822/20000], Training Loss: 0.0074884674900853754, Validation Loss: 0.004515351045745337\n",
      "Epoch [7823/20000], Training Loss: 0.005600337598350181, Validation Loss: 0.0035583937009319192\n",
      "Epoch [7824/20000], Training Loss: 0.007502517775704051, Validation Loss: 0.018563348267759596\n",
      "Epoch [7825/20000], Training Loss: 0.01257530891078724, Validation Loss: 0.012298233079491844\n",
      "Epoch [7826/20000], Training Loss: 0.012062945559072042, Validation Loss: 0.02433099422470183\n",
      "Epoch [7827/20000], Training Loss: 0.014020375458780368, Validation Loss: 0.009081908169301387\n",
      "Epoch [7828/20000], Training Loss: 0.009169516817532244, Validation Loss: 0.012308780636106218\n",
      "Epoch [7829/20000], Training Loss: 0.008803300661903839, Validation Loss: 0.0030174710887885597\n",
      "Epoch [7830/20000], Training Loss: 0.007691910055850063, Validation Loss: 0.005602502530177097\n",
      "Epoch [7831/20000], Training Loss: 0.005266782003932998, Validation Loss: 0.0033068642661533147\n",
      "Epoch [7832/20000], Training Loss: 0.00859891069571209, Validation Loss: 0.007853298494419545\n",
      "Epoch [7833/20000], Training Loss: 0.006468407231295714, Validation Loss: 0.004784790266837478\n",
      "Epoch [7834/20000], Training Loss: 0.005810562950500753, Validation Loss: 0.00377621885854756\n",
      "Epoch [7835/20000], Training Loss: 0.00800217533729405, Validation Loss: 0.005025025789759019\n",
      "Epoch [7836/20000], Training Loss: 0.012599663165539303, Validation Loss: 0.0034965669723800675\n",
      "Epoch [7837/20000], Training Loss: 0.007760997067375034, Validation Loss: 0.003924660526016611\n",
      "Epoch [7838/20000], Training Loss: 0.018020385328522286, Validation Loss: 0.006936311183421291\n",
      "Epoch [7839/20000], Training Loss: 0.0171494871414747, Validation Loss: 0.008287018667293264\n",
      "Epoch [7840/20000], Training Loss: 0.007382707351748, Validation Loss: 0.007166405683822319\n",
      "Epoch [7841/20000], Training Loss: 0.0059740579766055036, Validation Loss: 0.004826609569136053\n",
      "Epoch [7842/20000], Training Loss: 0.005783705544932413, Validation Loss: 0.002847145448738177\n",
      "Epoch [7843/20000], Training Loss: 0.008413615966718777, Validation Loss: 0.01843688650322937\n",
      "Epoch [7844/20000], Training Loss: 0.010335429850134201, Validation Loss: 0.016536836899811242\n",
      "Epoch [7845/20000], Training Loss: 0.02622247775278603, Validation Loss: 0.008502733605033492\n",
      "Epoch [7846/20000], Training Loss: 0.03527656872756779, Validation Loss: 0.028127144117497505\n",
      "Epoch [7847/20000], Training Loss: 0.02427097871467205, Validation Loss: 0.023783471884858694\n",
      "Epoch [7848/20000], Training Loss: 0.016900785159253116, Validation Loss: 0.03706203104593442\n",
      "Epoch [7849/20000], Training Loss: 0.023195396210732206, Validation Loss: 0.011061013478995716\n",
      "Epoch [7850/20000], Training Loss: 0.017127989250314255, Validation Loss: 0.0069793802002615325\n",
      "Epoch [7851/20000], Training Loss: 0.010027243186154269, Validation Loss: 0.006178817056414638\n",
      "Epoch [7852/20000], Training Loss: 0.0067023369795476484, Validation Loss: 0.005572381343556897\n",
      "Epoch [7853/20000], Training Loss: 0.009102372487957058, Validation Loss: 0.004086368978927827\n",
      "Epoch [7854/20000], Training Loss: 0.0142974388145376, Validation Loss: 0.020400351711693417\n",
      "Epoch [7855/20000], Training Loss: 0.008592125473244647, Validation Loss: 0.0522978241954531\n",
      "Epoch [7856/20000], Training Loss: 0.030650858143677136, Validation Loss: 0.02881147606032331\n",
      "Epoch [7857/20000], Training Loss: 0.028522910703031812, Validation Loss: 0.009597758067989388\n",
      "Epoch [7858/20000], Training Loss: 0.009719094955861303, Validation Loss: 0.006414279926441883\n",
      "Epoch [7859/20000], Training Loss: 0.006047905312568348, Validation Loss: 0.0044776431650461745\n",
      "Epoch [7860/20000], Training Loss: 0.01119267093184005, Validation Loss: 0.004177349878123745\n",
      "Epoch [7861/20000], Training Loss: 0.010583261377178133, Validation Loss: 0.008435410809722401\n",
      "Epoch [7862/20000], Training Loss: 0.010765198855161933, Validation Loss: 0.004635946960986205\n",
      "Epoch [7863/20000], Training Loss: 0.0036896701868889587, Validation Loss: 0.005931240575100765\n",
      "Epoch [7864/20000], Training Loss: 0.005786761275625655, Validation Loss: 0.004346924405776268\n",
      "Epoch [7865/20000], Training Loss: 0.005752353822962115, Validation Loss: 0.003613051271534979\n",
      "Epoch [7866/20000], Training Loss: 0.005934587191664572, Validation Loss: 0.006236076380497073\n",
      "Epoch [7867/20000], Training Loss: 0.01074719158766259, Validation Loss: 0.0036093264805166264\n",
      "Epoch [7868/20000], Training Loss: 0.008293471498680966, Validation Loss: 0.003884857508247868\n",
      "Epoch [7869/20000], Training Loss: 0.008697832877390153, Validation Loss: 0.023746927401988343\n",
      "Epoch [7870/20000], Training Loss: 0.01139006551550535, Validation Loss: 0.006239638130836119\n",
      "Epoch [7871/20000], Training Loss: 0.0077837747226437515, Validation Loss: 0.004648986771111855\n",
      "Epoch [7872/20000], Training Loss: 0.00731490441519002, Validation Loss: 0.008866439910592928\n",
      "Epoch [7873/20000], Training Loss: 0.006641199918833861, Validation Loss: 0.003628025090603972\n",
      "Epoch [7874/20000], Training Loss: 0.008673260892724752, Validation Loss: 0.007883325818075125\n",
      "Epoch [7875/20000], Training Loss: 0.04383069334486273, Validation Loss: 0.10558279071535383\n",
      "Epoch [7876/20000], Training Loss: 0.035873816558575654, Validation Loss: 0.010469358225658911\n",
      "Epoch [7877/20000], Training Loss: 0.027878685496814017, Validation Loss: 0.023375113105879336\n",
      "Epoch [7878/20000], Training Loss: 0.033002097641916146, Validation Loss: 0.0150513638375518\n",
      "Epoch [7879/20000], Training Loss: 0.01753656501283071, Validation Loss: 0.011248915884834787\n",
      "Epoch [7880/20000], Training Loss: 0.018756656333737607, Validation Loss: 0.019121671628026755\n",
      "Epoch [7881/20000], Training Loss: 0.013170235964935273, Validation Loss: 0.00721986372820993\n",
      "Epoch [7882/20000], Training Loss: 0.008292534575697832, Validation Loss: 0.008518720484036837\n",
      "Epoch [7883/20000], Training Loss: 0.00581946298812649, Validation Loss: 0.0046912092183382425\n",
      "Epoch [7884/20000], Training Loss: 0.0056335033067235986, Validation Loss: 0.005496365608775834\n",
      "Epoch [7885/20000], Training Loss: 0.0056836931305172455, Validation Loss: 0.005636607133778527\n",
      "Epoch [7886/20000], Training Loss: 0.005586257462450054, Validation Loss: 0.005316122466800672\n",
      "Epoch [7887/20000], Training Loss: 0.009652431186363433, Validation Loss: 0.006816699070766065\n",
      "Epoch [7888/20000], Training Loss: 0.009950521457150379, Validation Loss: 0.00594785273434744\n",
      "Epoch [7889/20000], Training Loss: 0.010281644499627873, Validation Loss: 0.012931177925267422\n",
      "Epoch [7890/20000], Training Loss: 0.009161415464144998, Validation Loss: 0.006206383632039498\n",
      "Epoch [7891/20000], Training Loss: 0.0076023630504745855, Validation Loss: 0.010838531585250166\n",
      "Epoch [7892/20000], Training Loss: 0.008489469947692539, Validation Loss: 0.004253266043706715\n",
      "Epoch [7893/20000], Training Loss: 0.014727944288876773, Validation Loss: 0.004888509158397066\n",
      "Epoch [7894/20000], Training Loss: 0.006114187367659595, Validation Loss: 0.005445485182036437\n",
      "Epoch [7895/20000], Training Loss: 0.005514644806680735, Validation Loss: 0.003612019386037803\n",
      "Epoch [7896/20000], Training Loss: 0.00515795433394877, Validation Loss: 0.004386055296994381\n",
      "Epoch [7897/20000], Training Loss: 0.005356661191399326, Validation Loss: 0.005020748823349257\n",
      "Epoch [7898/20000], Training Loss: 0.008124300715540553, Validation Loss: 0.016751984094298678\n",
      "Epoch [7899/20000], Training Loss: 0.018099957877503976, Validation Loss: 0.019293540573313443\n",
      "Epoch [7900/20000], Training Loss: 0.01616876759648481, Validation Loss: 0.009355047158058889\n",
      "Epoch [7901/20000], Training Loss: 0.014395080198612829, Validation Loss: 0.011153022593718325\n",
      "Epoch [7902/20000], Training Loss: 0.01365307401465543, Validation Loss: 0.0050728767584854294\n",
      "Epoch [7903/20000], Training Loss: 0.005911700489897547, Validation Loss: 0.004435619505672125\n",
      "Epoch [7904/20000], Training Loss: 0.006217445049385008, Validation Loss: 0.0033033565867859644\n",
      "Epoch [7905/20000], Training Loss: 0.0064082810921328405, Validation Loss: 0.006342578456933447\n",
      "Epoch [7906/20000], Training Loss: 0.009920866018242793, Validation Loss: 0.004987224547221148\n",
      "Epoch [7907/20000], Training Loss: 0.006387261890397765, Validation Loss: 0.008874547734442175\n",
      "Epoch [7908/20000], Training Loss: 0.005093536358409827, Validation Loss: 0.0029845720214208998\n",
      "Epoch [7909/20000], Training Loss: 0.00765338414307085, Validation Loss: 0.0049829286496219215\n",
      "Epoch [7910/20000], Training Loss: 0.012873525812080646, Validation Loss: 0.010353570791527123\n",
      "Epoch [7911/20000], Training Loss: 0.008861186706261443, Validation Loss: 0.007441421878768649\n",
      "Epoch [7912/20000], Training Loss: 0.008432090933118681, Validation Loss: 0.015278383889900786\n",
      "Epoch [7913/20000], Training Loss: 0.01001710475663588, Validation Loss: 0.004056381576444993\n",
      "Epoch [7914/20000], Training Loss: 0.004316959166252283, Validation Loss: 0.0039985854995490755\n",
      "Epoch [7915/20000], Training Loss: 0.006744331869413145, Validation Loss: 0.003876787504566437\n",
      "Epoch [7916/20000], Training Loss: 0.017738704796101956, Validation Loss: 0.056351853688511255\n",
      "Epoch [7917/20000], Training Loss: 0.014811206484279995, Validation Loss: 0.0031117152329451464\n",
      "Epoch [7918/20000], Training Loss: 0.010387716428730138, Validation Loss: 0.005290377756304879\n",
      "Epoch [7919/20000], Training Loss: 0.0059377128678274205, Validation Loss: 0.004294510959192913\n",
      "Epoch [7920/20000], Training Loss: 0.006003047373919149, Validation Loss: 0.005822080635889971\n",
      "Epoch [7921/20000], Training Loss: 0.01412636213769604, Validation Loss: 0.017408011747311207\n",
      "Epoch [7922/20000], Training Loss: 0.03353108870032884, Validation Loss: 0.05880632357937949\n",
      "Epoch [7923/20000], Training Loss: 0.09389832896912205, Validation Loss: 0.0543116009440772\n",
      "Epoch [7924/20000], Training Loss: 0.04736250495937254, Validation Loss: 0.039437423002839624\n",
      "Epoch [7925/20000], Training Loss: 0.031358966163160015, Validation Loss: 0.013890423789056361\n",
      "Epoch [7926/20000], Training Loss: 0.012267070706002414, Validation Loss: 0.00934079032122448\n",
      "Epoch [7927/20000], Training Loss: 0.008207465555252773, Validation Loss: 0.006418107041099574\n",
      "Epoch [7928/20000], Training Loss: 0.005669938608272267, Validation Loss: 0.020812747380169045\n",
      "Epoch [7929/20000], Training Loss: 0.013979320139956794, Validation Loss: 0.02402167859914428\n",
      "Epoch [7930/20000], Training Loss: 0.006526834637043066, Validation Loss: 0.003139967592947609\n",
      "Epoch [7931/20000], Training Loss: 0.01197721770235408, Validation Loss: 0.010453459468976689\n",
      "Epoch [7932/20000], Training Loss: 0.011629358502432297, Validation Loss: 0.005907204591884303\n",
      "Epoch [7933/20000], Training Loss: 0.013863158919515886, Validation Loss: 0.005241744874313424\n",
      "Epoch [7934/20000], Training Loss: 0.009097145299684988, Validation Loss: 0.004785748545015979\n",
      "Epoch [7935/20000], Training Loss: 0.007275854266481474, Validation Loss: 0.002728902288903815\n",
      "Epoch [7936/20000], Training Loss: 0.005217008866436247, Validation Loss: 0.005256868341608164\n",
      "Epoch [7937/20000], Training Loss: 0.007264422042810891, Validation Loss: 0.0049852001466206275\n",
      "Epoch [7938/20000], Training Loss: 0.01163303272187477, Validation Loss: 0.006613800661528201\n",
      "Epoch [7939/20000], Training Loss: 0.010518413013154973, Validation Loss: 0.022213307634913564\n",
      "Epoch [7940/20000], Training Loss: 0.008438233758040172, Validation Loss: 0.0029790961246232784\n",
      "Epoch [7941/20000], Training Loss: 0.003471488702676392, Validation Loss: 0.002583533412624612\n",
      "Epoch [7942/20000], Training Loss: 0.006254084472727429, Validation Loss: 0.0027102700905743404\n",
      "Epoch [7943/20000], Training Loss: 0.007452763914313566, Validation Loss: 0.0020887670169551192\n",
      "Epoch [7944/20000], Training Loss: 0.005480595108175683, Validation Loss: 0.0051454058957623216\n",
      "Epoch [7945/20000], Training Loss: 0.004913997393617008, Validation Loss: 0.00436510823172804\n",
      "Epoch [7946/20000], Training Loss: 0.005213796563371683, Validation Loss: 0.0025854765268117524\n",
      "Epoch [7947/20000], Training Loss: 0.005146718868488546, Validation Loss: 0.004663813658745701\n",
      "Epoch [7948/20000], Training Loss: 0.010223459165510056, Validation Loss: 0.006463894205548318\n",
      "Epoch [7949/20000], Training Loss: 0.007363049391512634, Validation Loss: 0.007383975322113656\n",
      "Epoch [7950/20000], Training Loss: 0.03971580346634645, Validation Loss: 0.03254718401849033\n",
      "Epoch [7951/20000], Training Loss: 0.05501693921801883, Validation Loss: 0.14996593566168034\n",
      "Epoch [7952/20000], Training Loss: 0.03924737158896668, Validation Loss: 0.04568374136704329\n",
      "Epoch [7953/20000], Training Loss: 0.030572978563993405, Validation Loss: 0.030771889357392453\n",
      "Epoch [7954/20000], Training Loss: 0.013534476747736335, Validation Loss: 0.012712178542660142\n",
      "Epoch [7955/20000], Training Loss: 0.019446815235174393, Validation Loss: 0.01542555200197577\n",
      "Epoch [7956/20000], Training Loss: 0.01537956576378617, Validation Loss: 0.03699206538606502\n",
      "Epoch [7957/20000], Training Loss: 0.011674578870221402, Validation Loss: 0.008252359482997105\n",
      "Epoch [7958/20000], Training Loss: 0.007162664177095783, Validation Loss: 0.010678701509949953\n",
      "Epoch [7959/20000], Training Loss: 0.01181698004698514, Validation Loss: 0.00539637797862748\n",
      "Epoch [7960/20000], Training Loss: 0.008140662520807902, Validation Loss: 0.00807432220400577\n",
      "Epoch [7961/20000], Training Loss: 0.008475169148628734, Validation Loss: 0.004298476532962273\n",
      "Epoch [7962/20000], Training Loss: 0.007669727783455268, Validation Loss: 0.0036441233855189887\n",
      "Epoch [7963/20000], Training Loss: 0.012761581452131006, Validation Loss: 0.00456343967513411\n",
      "Epoch [7964/20000], Training Loss: 0.0076145753056542686, Validation Loss: 0.004495696185326129\n",
      "Epoch [7965/20000], Training Loss: 0.006031192456638175, Validation Loss: 0.006201130976478453\n",
      "Epoch [7966/20000], Training Loss: 0.004834795653420899, Validation Loss: 0.003415263946538159\n",
      "Epoch [7967/20000], Training Loss: 0.0055194532136998275, Validation Loss: 0.0026284821487934096\n",
      "Epoch [7968/20000], Training Loss: 0.004875777965025918, Validation Loss: 0.009384689849996772\n",
      "Epoch [7969/20000], Training Loss: 0.014893708361861562, Validation Loss: 0.00739997510464363\n",
      "Epoch [7970/20000], Training Loss: 0.012346069561317563, Validation Loss: 0.030530983848616366\n",
      "Epoch [7971/20000], Training Loss: 0.013775941519270418, Validation Loss: 0.003936924663418852\n",
      "Epoch [7972/20000], Training Loss: 0.006022493834149957, Validation Loss: 0.00980936639097558\n",
      "Epoch [7973/20000], Training Loss: 0.011768353078098568, Validation Loss: 0.0034336756437239402\n",
      "Epoch [7974/20000], Training Loss: 0.015041510722117632, Validation Loss: 0.010139862216810356\n",
      "Epoch [7975/20000], Training Loss: 0.007903021791467577, Validation Loss: 0.0049479763268079425\n",
      "Epoch [7976/20000], Training Loss: 0.006953404430532828, Validation Loss: 0.007244188911608417\n",
      "Epoch [7977/20000], Training Loss: 0.005560770990476678, Validation Loss: 0.0027960539649386973\n",
      "Epoch [7978/20000], Training Loss: 0.006930754621862434, Validation Loss: 0.003278839463551516\n",
      "Epoch [7979/20000], Training Loss: 0.00789906362894856, Validation Loss: 0.006115718401360937\n",
      "Epoch [7980/20000], Training Loss: 0.021596571908698285, Validation Loss: 0.027979174532602662\n",
      "Epoch [7981/20000], Training Loss: 0.018488752017479522, Validation Loss: 0.012280102521669636\n",
      "Epoch [7982/20000], Training Loss: 0.010504306437464297, Validation Loss: 0.0067508442456930395\n",
      "Epoch [7983/20000], Training Loss: 0.009697144652558823, Validation Loss: 0.00741116721526071\n",
      "Epoch [7984/20000], Training Loss: 0.021436205341680243, Validation Loss: 0.011327192320506356\n",
      "Epoch [7985/20000], Training Loss: 0.009498377261999329, Validation Loss: 0.01504721872367806\n",
      "Epoch [7986/20000], Training Loss: 0.015044444307152714, Validation Loss: 0.015426559307213386\n",
      "Epoch [7987/20000], Training Loss: 0.01197572693906425, Validation Loss: 0.011811411401140504\n",
      "Epoch [7988/20000], Training Loss: 0.010924041121532875, Validation Loss: 0.0038881428811602553\n",
      "Epoch [7989/20000], Training Loss: 0.00810140041693419, Validation Loss: 0.0344312222940601\n",
      "Epoch [7990/20000], Training Loss: 0.041966766002587974, Validation Loss: 0.09039429536887589\n",
      "Epoch [7991/20000], Training Loss: 0.049123043269251605, Validation Loss: 0.038114470938841585\n",
      "Epoch [7992/20000], Training Loss: 0.033862849371092, Validation Loss: 0.01625631439725522\n",
      "Epoch [7993/20000], Training Loss: 0.02413119008925995, Validation Loss: 0.017255089214553924\n",
      "Epoch [7994/20000], Training Loss: 0.013610826015272843, Validation Loss: 0.012593134494433746\n",
      "Epoch [7995/20000], Training Loss: 0.010415268995399986, Validation Loss: 0.008484891053048758\n",
      "Epoch [7996/20000], Training Loss: 0.008515290267366384, Validation Loss: 0.005394865918841926\n",
      "Epoch [7997/20000], Training Loss: 0.006550085749560302, Validation Loss: 0.004764746272949237\n",
      "Epoch [7998/20000], Training Loss: 0.004344129632118309, Validation Loss: 0.004242399625322183\n",
      "Epoch [7999/20000], Training Loss: 0.006120972047418556, Validation Loss: 0.005581641529856833\n",
      "Epoch [8000/20000], Training Loss: 0.004734013628746782, Validation Loss: 0.004450469398761925\n",
      "Epoch [8001/20000], Training Loss: 0.006045647638927422, Validation Loss: 0.005554266816066474\n",
      "Epoch [8002/20000], Training Loss: 0.011298291618004441, Validation Loss: 0.00462590636107052\n",
      "Epoch [8003/20000], Training Loss: 0.008052893411201825, Validation Loss: 0.005410102740685055\n",
      "Epoch [8004/20000], Training Loss: 0.006359703016122304, Validation Loss: 0.004907632490098227\n",
      "Epoch [8005/20000], Training Loss: 0.0065978138133816954, Validation Loss: 0.021235968119334713\n",
      "Epoch [8006/20000], Training Loss: 0.012474883319945158, Validation Loss: 0.014919653867504433\n",
      "Epoch [8007/20000], Training Loss: 0.015576288990684719, Validation Loss: 0.004424795574975945\n",
      "Epoch [8008/20000], Training Loss: 0.014715130916946302, Validation Loss: 0.01490649873656886\n",
      "Epoch [8009/20000], Training Loss: 0.013057520738844428, Validation Loss: 0.010724422701904197\n",
      "Epoch [8010/20000], Training Loss: 0.014314347962424685, Validation Loss: 0.0081773748623736\n",
      "Epoch [8011/20000], Training Loss: 0.018231761470295687, Validation Loss: 0.04850528516752612\n",
      "Epoch [8012/20000], Training Loss: 0.03487327802317201, Validation Loss: 0.05881324082396792\n",
      "Epoch [8013/20000], Training Loss: 0.026920213418114663, Validation Loss: 0.02040662457979384\n",
      "Epoch [8014/20000], Training Loss: 0.017293392661875778, Validation Loss: 0.01254572755926033\n",
      "Epoch [8015/20000], Training Loss: 0.00866002612331483, Validation Loss: 0.007807331904457117\n",
      "Epoch [8016/20000], Training Loss: 0.009365997948131755, Validation Loss: 0.00928394098497977\n",
      "Epoch [8017/20000], Training Loss: 0.0121187006485083, Validation Loss: 0.0067878987229050836\n",
      "Epoch [8018/20000], Training Loss: 0.006839676780925531, Validation Loss: 0.004127911732038874\n",
      "Epoch [8019/20000], Training Loss: 0.005976107412217451, Validation Loss: 0.003775274890175468\n",
      "Epoch [8020/20000], Training Loss: 0.0046393112919902125, Validation Loss: 0.008081780704612487\n",
      "Epoch [8021/20000], Training Loss: 0.008611634914164565, Validation Loss: 0.004198724013967211\n",
      "Epoch [8022/20000], Training Loss: 0.006601088085777259, Validation Loss: 0.019021480743374144\n",
      "Epoch [8023/20000], Training Loss: 0.008022025067475624, Validation Loss: 0.028436075363840376\n",
      "Epoch [8024/20000], Training Loss: 0.02086228306247254, Validation Loss: 0.005991758059723067\n",
      "Epoch [8025/20000], Training Loss: 0.007351497612294874, Validation Loss: 0.005089658495662215\n",
      "Epoch [8026/20000], Training Loss: 0.0065114470401437885, Validation Loss: 0.023303627435650145\n",
      "Epoch [8027/20000], Training Loss: 0.027692198686833893, Validation Loss: 0.03691011560814721\n",
      "Epoch [8028/20000], Training Loss: 0.013723315814526618, Validation Loss: 0.0126312471527075\n",
      "Epoch [8029/20000], Training Loss: 0.016933561834905828, Validation Loss: 0.008288765392673028\n",
      "Epoch [8030/20000], Training Loss: 0.013589888697944323, Validation Loss: 0.030436948110166537\n",
      "Epoch [8031/20000], Training Loss: 0.014306538185337558, Validation Loss: 0.0061826842364942946\n",
      "Epoch [8032/20000], Training Loss: 0.010436622783475156, Validation Loss: 0.004751157720087958\n",
      "Epoch [8033/20000], Training Loss: 0.006449118381299611, Validation Loss: 0.004802772378483536\n",
      "Epoch [8034/20000], Training Loss: 0.005138307789041262, Validation Loss: 0.0034233986747736777\n",
      "Epoch [8035/20000], Training Loss: 0.005876278379998569, Validation Loss: 0.01211919302919148\n",
      "Epoch [8036/20000], Training Loss: 0.015558300273759025, Validation Loss: 0.019290396544550146\n",
      "Epoch [8037/20000], Training Loss: 0.015421667631017044, Validation Loss: 0.006643321144841658\n",
      "Epoch [8038/20000], Training Loss: 0.0362761054711882, Validation Loss: 0.07000467741402515\n",
      "Epoch [8039/20000], Training Loss: 0.07378722183057107, Validation Loss: 0.22782412142883654\n",
      "Epoch [8040/20000], Training Loss: 0.13230734810765302, Validation Loss: 0.08449823229187002\n",
      "Epoch [8041/20000], Training Loss: 0.036790668864601424, Validation Loss: 0.022286440331884348\n",
      "Epoch [8042/20000], Training Loss: 0.023178248899057508, Validation Loss: 0.015285842365141125\n",
      "Epoch [8043/20000], Training Loss: 0.012488529152636017, Validation Loss: 0.012244082068714286\n",
      "Epoch [8044/20000], Training Loss: 0.011893773185355323, Validation Loss: 0.008868908650973546\n",
      "Epoch [8045/20000], Training Loss: 0.009422975251384611, Validation Loss: 0.005275653991960283\n",
      "Epoch [8046/20000], Training Loss: 0.006629515585830502, Validation Loss: 0.005370956211542932\n",
      "Epoch [8047/20000], Training Loss: 0.0054348365374607965, Validation Loss: 0.010011110457004533\n",
      "Epoch [8048/20000], Training Loss: 0.012918790506124165, Validation Loss: 0.02922861190868412\n",
      "Epoch [8049/20000], Training Loss: 0.0509239232040792, Validation Loss: 0.07761237094976536\n",
      "Epoch [8050/20000], Training Loss: 0.038487190476319356, Validation Loss: 0.02360439908790113\n",
      "Epoch [8051/20000], Training Loss: 0.029226155693842366, Validation Loss: 0.018349640497494896\n",
      "Epoch [8052/20000], Training Loss: 0.00993265285589067, Validation Loss: 0.006808862493098374\n",
      "Epoch [8053/20000], Training Loss: 0.006788293416146222, Validation Loss: 0.007247403798178961\n",
      "Epoch [8054/20000], Training Loss: 0.006718478922266513, Validation Loss: 0.005419944049208425\n",
      "Epoch [8055/20000], Training Loss: 0.00792383520248612, Validation Loss: 0.004614766353240286\n",
      "Epoch [8056/20000], Training Loss: 0.01457856855060007, Validation Loss: 0.00442118644553245\n",
      "Epoch [8057/20000], Training Loss: 0.013351328163740359, Validation Loss: 0.004387929096472882\n",
      "Epoch [8058/20000], Training Loss: 0.008884825169973607, Validation Loss: 0.005050061687061965\n",
      "Epoch [8059/20000], Training Loss: 0.005445879472452881, Validation Loss: 0.004266797319855543\n",
      "Epoch [8060/20000], Training Loss: 0.004764654807526573, Validation Loss: 0.002481489456548453\n",
      "Epoch [8061/20000], Training Loss: 0.004850299105653123, Validation Loss: 0.005755628942518084\n",
      "Epoch [8062/20000], Training Loss: 0.014414384909287037, Validation Loss: 0.004916555827354127\n",
      "Epoch [8063/20000], Training Loss: 0.005015581920555893, Validation Loss: 0.007224442905421061\n",
      "Epoch [8064/20000], Training Loss: 0.006420936932824718, Validation Loss: 0.005000934912564148\n",
      "Epoch [8065/20000], Training Loss: 0.004069048218817832, Validation Loss: 0.0023004600442878393\n",
      "Epoch [8066/20000], Training Loss: 0.006300567450255455, Validation Loss: 0.002332657306530856\n",
      "Epoch [8067/20000], Training Loss: 0.009872328442205409, Validation Loss: 0.001993000716441072\n",
      "Epoch [8068/20000], Training Loss: 0.02402992189932515, Validation Loss: 0.003494610579244701\n",
      "Epoch [8069/20000], Training Loss: 0.007816965875203355, Validation Loss: 0.0027753773300836265\n",
      "Epoch [8070/20000], Training Loss: 0.006801554783773359, Validation Loss: 0.006283967935709757\n",
      "Epoch [8071/20000], Training Loss: 0.0077781128722043446, Validation Loss: 0.0021733141237681203\n",
      "Epoch [8072/20000], Training Loss: 0.01593450391062236, Validation Loss: 0.02520315398792913\n",
      "Epoch [8073/20000], Training Loss: 0.042651839541836774, Validation Loss: 0.019365672398894924\n",
      "Epoch [8074/20000], Training Loss: 0.03231485004848115, Validation Loss: 0.0420535343706823\n",
      "Epoch [8075/20000], Training Loss: 0.03587232308096385, Validation Loss: 0.03444747323503959\n",
      "Epoch [8076/20000], Training Loss: 0.01261268247373794, Validation Loss: 0.006244514978464265\n",
      "Epoch [8077/20000], Training Loss: 0.00896397851257851, Validation Loss: 0.007318140655201548\n",
      "Epoch [8078/20000], Training Loss: 0.008477376624276596, Validation Loss: 0.010454090290972757\n",
      "Epoch [8079/20000], Training Loss: 0.008435544375970494, Validation Loss: 0.008374777402398654\n",
      "Epoch [8080/20000], Training Loss: 0.0066337121035238466, Validation Loss: 0.013330787255004648\n",
      "Epoch [8081/20000], Training Loss: 0.013395622385100328, Validation Loss: 0.01599705372297779\n",
      "Epoch [8082/20000], Training Loss: 0.015324603171133535, Validation Loss: 0.006666068865992462\n",
      "Epoch [8083/20000], Training Loss: 0.011636117272636122, Validation Loss: 0.009039672928728188\n",
      "Epoch [8084/20000], Training Loss: 0.01427636067741592, Validation Loss: 0.00663314758821798\n",
      "Epoch [8085/20000], Training Loss: 0.03402567912624883, Validation Loss: 0.05041601939407655\n",
      "Epoch [8086/20000], Training Loss: 0.020227238346706145, Validation Loss: 0.019803653034225266\n",
      "Epoch [8087/20000], Training Loss: 0.013662404991919175, Validation Loss: 0.007667442326316268\n",
      "Epoch [8088/20000], Training Loss: 0.010056742361484794, Validation Loss: 0.004564479992991112\n",
      "Epoch [8089/20000], Training Loss: 0.015848331837982448, Validation Loss: 0.008727039701885328\n",
      "Epoch [8090/20000], Training Loss: 0.009208991204754316, Validation Loss: 0.012833110244953008\n",
      "Epoch [8091/20000], Training Loss: 0.016761307049559297, Validation Loss: 0.007201209435854926\n",
      "Epoch [8092/20000], Training Loss: 0.01858961876444352, Validation Loss: 0.005952599775734336\n",
      "Epoch [8093/20000], Training Loss: 0.032416173911769874, Validation Loss: 0.04309501389693271\n",
      "Epoch [8094/20000], Training Loss: 0.013484354413646673, Validation Loss: 0.008838927443710938\n",
      "Epoch [8095/20000], Training Loss: 0.009539770723287282, Validation Loss: 0.00577183914755811\n",
      "Epoch [8096/20000], Training Loss: 0.010568111885472067, Validation Loss: 0.004018475824701844\n",
      "Epoch [8097/20000], Training Loss: 0.015366164684694792, Validation Loss: 0.01177044932008913\n",
      "Epoch [8098/20000], Training Loss: 0.043756806502642576, Validation Loss: 0.02365414608098851\n",
      "Epoch [8099/20000], Training Loss: 0.021626688049374416, Validation Loss: 0.009507489464834722\n",
      "Epoch [8100/20000], Training Loss: 0.011478668282506987, Validation Loss: 0.007383467695984335\n",
      "Epoch [8101/20000], Training Loss: 0.008045729476309913, Validation Loss: 0.006771480499871085\n",
      "Epoch [8102/20000], Training Loss: 0.005600028904154897, Validation Loss: 0.006791378221156005\n",
      "Epoch [8103/20000], Training Loss: 0.007512053291845534, Validation Loss: 0.008083409603449918\n",
      "Epoch [8104/20000], Training Loss: 0.009182464812641098, Validation Loss: 0.0035705610085688478\n",
      "Epoch [8105/20000], Training Loss: 0.013387160529548834, Validation Loss: 0.004804395900879983\n",
      "Epoch [8106/20000], Training Loss: 0.015280423685908318, Validation Loss: 0.0043471960312052105\n",
      "Epoch [8107/20000], Training Loss: 0.012072522001939692, Validation Loss: 0.013324319613767297\n",
      "Epoch [8108/20000], Training Loss: 0.012828882049819055, Validation Loss: 0.027668769795014998\n",
      "Epoch [8109/20000], Training Loss: 0.022987094973879203, Validation Loss: 0.01928119813766636\n",
      "Epoch [8110/20000], Training Loss: 0.027634177450506416, Validation Loss: 0.014071811545237876\n",
      "Epoch [8111/20000], Training Loss: 0.013735350333653125, Validation Loss: 0.006591479714717201\n",
      "Epoch [8112/20000], Training Loss: 0.008392835061905706, Validation Loss: 0.004941363795352467\n",
      "Epoch [8113/20000], Training Loss: 0.005474271639416527, Validation Loss: 0.006687418584727592\n",
      "Epoch [8114/20000], Training Loss: 0.0071817280518839, Validation Loss: 0.007589182940198441\n",
      "Epoch [8115/20000], Training Loss: 0.008052305451461248, Validation Loss: 0.0043578208147425\n",
      "Epoch [8116/20000], Training Loss: 0.004941857718222309, Validation Loss: 0.011714848424613553\n",
      "Epoch [8117/20000], Training Loss: 0.007229016079041425, Validation Loss: 0.012186278899857175\n",
      "Epoch [8118/20000], Training Loss: 0.009459268243517727, Validation Loss: 0.009041788078329813\n",
      "Epoch [8119/20000], Training Loss: 0.012859742258504931, Validation Loss: 0.003688120064635671\n",
      "Epoch [8120/20000], Training Loss: 0.00820896303672011, Validation Loss: 0.0054839709236953246\n",
      "Epoch [8121/20000], Training Loss: 0.004810239019986641, Validation Loss: 0.003710597291979058\n",
      "Epoch [8122/20000], Training Loss: 0.007739466822905732, Validation Loss: 0.003170625130983825\n",
      "Epoch [8123/20000], Training Loss: 0.005160675492204193, Validation Loss: 0.004311692476353082\n",
      "Epoch [8124/20000], Training Loss: 0.009857179937950735, Validation Loss: 0.0035055333384570206\n",
      "Epoch [8125/20000], Training Loss: 0.0068150669295781495, Validation Loss: 0.01544618631893562\n",
      "Epoch [8126/20000], Training Loss: 0.011187418683091113, Validation Loss: 0.0044358090778796695\n",
      "Epoch [8127/20000], Training Loss: 0.00519577727910863, Validation Loss: 0.0038626622284347765\n",
      "Epoch [8128/20000], Training Loss: 0.00573031230851484, Validation Loss: 0.0026184773640207976\n",
      "Epoch [8129/20000], Training Loss: 0.006274035487357261, Validation Loss: 0.011315287986750347\n",
      "Epoch [8130/20000], Training Loss: 0.010793197619802544, Validation Loss: 0.0030976488630492754\n",
      "Epoch [8131/20000], Training Loss: 0.0030679054075985084, Validation Loss: 0.0031807244182832256\n",
      "Epoch [8132/20000], Training Loss: 0.008517735189213584, Validation Loss: 0.002336789083074561\n",
      "Epoch [8133/20000], Training Loss: 0.010297473559140664, Validation Loss: 0.004239633218531935\n",
      "Epoch [8134/20000], Training Loss: 0.012661625443018108, Validation Loss: 0.01467473964625632\n",
      "Epoch [8135/20000], Training Loss: 0.039429140362439545, Validation Loss: 0.01807816256769173\n",
      "Epoch [8136/20000], Training Loss: 0.03272283712950801, Validation Loss: 0.015217033148344074\n",
      "Epoch [8137/20000], Training Loss: 0.02450880066524925, Validation Loss: 0.006197735333939493\n",
      "Epoch [8138/20000], Training Loss: 0.006503952186903916, Validation Loss: 0.007240439782134735\n",
      "Epoch [8139/20000], Training Loss: 0.005851364756901083, Validation Loss: 0.004725259904021823\n",
      "Epoch [8140/20000], Training Loss: 0.00992907549438574, Validation Loss: 0.012117799770873445\n",
      "Epoch [8141/20000], Training Loss: 0.010696797145231227, Validation Loss: 0.0038216823265719213\n",
      "Epoch [8142/20000], Training Loss: 0.005920988008646029, Validation Loss: 0.005364832010444687\n",
      "Epoch [8143/20000], Training Loss: 0.012092229992309253, Validation Loss: 0.006960843108385818\n",
      "Epoch [8144/20000], Training Loss: 0.007529271487978154, Validation Loss: 0.0035587240667732272\n",
      "Epoch [8145/20000], Training Loss: 0.006030331704615881, Validation Loss: 0.0031822766138739833\n",
      "Epoch [8146/20000], Training Loss: 0.006162637017301417, Validation Loss: 0.0058793399630311174\n",
      "Epoch [8147/20000], Training Loss: 0.007989420173024493, Validation Loss: 0.0037531395374649384\n",
      "Epoch [8148/20000], Training Loss: 0.009064006921107648, Validation Loss: 0.0064582424577804085\n",
      "Epoch [8149/20000], Training Loss: 0.007964554704293343, Validation Loss: 0.0062143633063478625\n",
      "Epoch [8150/20000], Training Loss: 0.008007080145554417, Validation Loss: 0.022448018401255303\n",
      "Epoch [8151/20000], Training Loss: 0.015009918825983602, Validation Loss: 0.004107102671576561\n",
      "Epoch [8152/20000], Training Loss: 0.007600933215144323, Validation Loss: 0.009901114960306196\n",
      "Epoch [8153/20000], Training Loss: 0.010619926420206736, Validation Loss: 0.010066995131225113\n",
      "Epoch [8154/20000], Training Loss: 0.004676238939444894, Validation Loss: 0.003711565627841251\n",
      "Epoch [8155/20000], Training Loss: 0.009384466210966431, Validation Loss: 0.023430860719859732\n",
      "Epoch [8156/20000], Training Loss: 0.03163743196117658, Validation Loss: 0.01457449465413685\n",
      "Epoch [8157/20000], Training Loss: 0.013520271804606767, Validation Loss: 0.006927257795496189\n",
      "Epoch [8158/20000], Training Loss: 0.015187361169539924, Validation Loss: 0.01663388726001358\n",
      "Epoch [8159/20000], Training Loss: 0.012647631936228467, Validation Loss: 0.003228445045602895\n",
      "Epoch [8160/20000], Training Loss: 0.013857253344862588, Validation Loss: 0.005860898247418194\n",
      "Epoch [8161/20000], Training Loss: 0.019717231039456756, Validation Loss: 0.010571926832612741\n",
      "Epoch [8162/20000], Training Loss: 0.007363211205561778, Validation Loss: 0.004620305498782639\n",
      "Epoch [8163/20000], Training Loss: 0.004803868664956619, Validation Loss: 0.004476523496938548\n",
      "Epoch [8164/20000], Training Loss: 0.00764149452048254, Validation Loss: 0.0029890166619825054\n",
      "Epoch [8165/20000], Training Loss: 0.006331638319742134, Validation Loss: 0.005348296570142479\n",
      "Epoch [8166/20000], Training Loss: 0.007434401930108184, Validation Loss: 0.004283771185699851\n",
      "Epoch [8167/20000], Training Loss: 0.008963502748624055, Validation Loss: 0.005000882812691065\n",
      "Epoch [8168/20000], Training Loss: 0.007344326390011702, Validation Loss: 0.00889344519929937\n",
      "Epoch [8169/20000], Training Loss: 0.016181667973017153, Validation Loss: 0.025571327098207672\n",
      "Epoch [8170/20000], Training Loss: 0.020113404384963878, Validation Loss: 0.020215087131600775\n",
      "Epoch [8171/20000], Training Loss: 0.022431045243657927, Validation Loss: 0.014092319641753346\n",
      "Epoch [8172/20000], Training Loss: 0.01977515558974119, Validation Loss: 0.009302831330598695\n",
      "Epoch [8173/20000], Training Loss: 0.009033091056543137, Validation Loss: 0.003927907725823069\n",
      "Epoch [8174/20000], Training Loss: 0.009789337304287724, Validation Loss: 0.00635920785974729\n",
      "Epoch [8175/20000], Training Loss: 0.011118094152022553, Validation Loss: 0.00535580201781067\n",
      "Epoch [8176/20000], Training Loss: 0.00852758254040964, Validation Loss: 0.007103419597681077\n",
      "Epoch [8177/20000], Training Loss: 0.01308096268855609, Validation Loss: 0.005514962200295895\n",
      "Epoch [8178/20000], Training Loss: 0.01588877772779337, Validation Loss: 0.00461806197279316\n",
      "Epoch [8179/20000], Training Loss: 0.01338807784918572, Validation Loss: 0.030724567811440857\n",
      "Epoch [8180/20000], Training Loss: 0.013447600607053443, Validation Loss: 0.006167872942330323\n",
      "Epoch [8181/20000], Training Loss: 0.010760624847275071, Validation Loss: 0.006641696473406456\n",
      "Epoch [8182/20000], Training Loss: 0.008477184272903417, Validation Loss: 0.013799629972450975\n",
      "Epoch [8183/20000], Training Loss: 0.008773675205312819, Validation Loss: 0.0035170193965067477\n",
      "Epoch [8184/20000], Training Loss: 0.006212766810287056, Validation Loss: 0.004026240918869846\n",
      "Epoch [8185/20000], Training Loss: 0.010242261952433702, Validation Loss: 0.003382685720177437\n",
      "Epoch [8186/20000], Training Loss: 0.013936400983116723, Validation Loss: 0.040790231430422964\n",
      "Epoch [8187/20000], Training Loss: 0.043812683783471584, Validation Loss: 0.021297731403734\n",
      "Epoch [8188/20000], Training Loss: 0.01081324856815107, Validation Loss: 0.0077753966987251216\n",
      "Epoch [8189/20000], Training Loss: 0.007429669820989407, Validation Loss: 0.017239544673689794\n",
      "Epoch [8190/20000], Training Loss: 0.012839201755663712, Validation Loss: 0.005988002156049749\n",
      "Epoch [8191/20000], Training Loss: 0.010959311032430767, Validation Loss: 0.004312555472035636\n",
      "Epoch [8192/20000], Training Loss: 0.004241657254169695, Validation Loss: 0.005416560238818949\n",
      "Epoch [8193/20000], Training Loss: 0.007868914509474832, Validation Loss: 0.009193803250257652\n",
      "Epoch [8194/20000], Training Loss: 0.008345695506639978, Validation Loss: 0.01772757539791729\n",
      "Epoch [8195/20000], Training Loss: 0.020109706204883487, Validation Loss: 0.01151447809700461\n",
      "Epoch [8196/20000], Training Loss: 0.01241080477952242, Validation Loss: 0.0038126191190996977\n",
      "Epoch [8197/20000], Training Loss: 0.010996110721602495, Validation Loss: 0.004857828641047011\n",
      "Epoch [8198/20000], Training Loss: 0.004988341847950194, Validation Loss: 0.010145716419789421\n",
      "Epoch [8199/20000], Training Loss: 0.008316073598897284, Validation Loss: 0.0034064987807530026\n",
      "Epoch [8200/20000], Training Loss: 0.00916654291255067, Validation Loss: 0.009025975429852114\n",
      "Epoch [8201/20000], Training Loss: 0.006308051847424524, Validation Loss: 0.0048272571419837665\n",
      "Epoch [8202/20000], Training Loss: 0.005161167279766232, Validation Loss: 0.005162037243800653\n",
      "Epoch [8203/20000], Training Loss: 0.004192425448146813, Validation Loss: 0.0035399390617387315\n",
      "Epoch [8204/20000], Training Loss: 0.006136389226802359, Validation Loss: 0.004547030997595654\n",
      "Epoch [8205/20000], Training Loss: 0.018671174138684625, Validation Loss: 0.01143600938043424\n",
      "Epoch [8206/20000], Training Loss: 0.02977067754779585, Validation Loss: 0.017475346045102924\n",
      "Epoch [8207/20000], Training Loss: 0.019727694334099915, Validation Loss: 0.04222578370744096\n",
      "Epoch [8208/20000], Training Loss: 0.028362462687904814, Validation Loss: 0.008450177412894872\n",
      "Epoch [8209/20000], Training Loss: 0.011070402879600547, Validation Loss: 0.011191547257551433\n",
      "Epoch [8210/20000], Training Loss: 0.00797625231695877, Validation Loss: 0.0039109297408036325\n",
      "Epoch [8211/20000], Training Loss: 0.007594697751171354, Validation Loss: 0.004263576400132608\n",
      "Epoch [8212/20000], Training Loss: 0.008481759671537605, Validation Loss: 0.006374670975073319\n",
      "Epoch [8213/20000], Training Loss: 0.006538708257721737, Validation Loss: 0.004680647412344895\n",
      "Epoch [8214/20000], Training Loss: 0.0061600344480601575, Validation Loss: 0.005809382361044462\n",
      "Epoch [8215/20000], Training Loss: 0.01982935931976369, Validation Loss: 0.014748333930262853\n",
      "Epoch [8216/20000], Training Loss: 0.014010301082497345, Validation Loss: 0.0034349485147596886\n",
      "Epoch [8217/20000], Training Loss: 0.0045479679978208355, Validation Loss: 0.015109688642830734\n",
      "Epoch [8218/20000], Training Loss: 0.005352140753010255, Validation Loss: 0.005831436749141565\n",
      "Epoch [8219/20000], Training Loss: 0.02050417004491984, Validation Loss: 0.030102120350940304\n",
      "Epoch [8220/20000], Training Loss: 0.046488371844004305, Validation Loss: 0.05482002975726833\n",
      "Epoch [8221/20000], Training Loss: 0.027963289452600293, Validation Loss: 0.011038197458536831\n",
      "Epoch [8222/20000], Training Loss: 0.025863437579898476, Validation Loss: 0.029427012009931915\n",
      "Epoch [8223/20000], Training Loss: 0.016600994218606502, Validation Loss: 0.010982061040036914\n",
      "Epoch [8224/20000], Training Loss: 0.009674907977958875, Validation Loss: 0.008470502442670684\n",
      "Epoch [8225/20000], Training Loss: 0.011640549953361707, Validation Loss: 0.005092123110704029\n",
      "Epoch [8226/20000], Training Loss: 0.016478658445911214, Validation Loss: 0.007063197064310229\n",
      "Epoch [8227/20000], Training Loss: 0.01617936982698406, Validation Loss: 0.005656080189729502\n",
      "Epoch [8228/20000], Training Loss: 0.01578529961573492, Validation Loss: 0.01727983316683484\n",
      "Epoch [8229/20000], Training Loss: 0.011625293983213072, Validation Loss: 0.012746657172063216\n",
      "Epoch [8230/20000], Training Loss: 0.006422351153949941, Validation Loss: 0.0055549898947901966\n",
      "Epoch [8231/20000], Training Loss: 0.009224938644495393, Validation Loss: 0.005053663443650943\n",
      "Epoch [8232/20000], Training Loss: 0.004473603555067841, Validation Loss: 0.005989492156613453\n",
      "Epoch [8233/20000], Training Loss: 0.004224809965567796, Validation Loss: 0.0038712158242576932\n",
      "Epoch [8234/20000], Training Loss: 0.004388657809093794, Validation Loss: 0.0040465973907787755\n",
      "Epoch [8235/20000], Training Loss: 0.008258419326531501, Validation Loss: 0.004410473306125239\n",
      "Epoch [8236/20000], Training Loss: 0.008544422243306014, Validation Loss: 0.0026524762348084812\n",
      "Epoch [8237/20000], Training Loss: 0.007839553584095224, Validation Loss: 0.004843137334625476\n",
      "Epoch [8238/20000], Training Loss: 0.010913631938040323, Validation Loss: 0.013035441467393534\n",
      "Epoch [8239/20000], Training Loss: 0.01518352929321866, Validation Loss: 0.0029434885531551508\n",
      "Epoch [8240/20000], Training Loss: 0.020149732905598024, Validation Loss: 0.0035064842911002154\n",
      "Epoch [8241/20000], Training Loss: 0.008593049088501305, Validation Loss: 0.0032767204441646364\n",
      "Epoch [8242/20000], Training Loss: 0.0109218169111825, Validation Loss: 0.0034008482492059295\n",
      "Epoch [8243/20000], Training Loss: 0.004886114352432612, Validation Loss: 0.015253037759241908\n",
      "Epoch [8244/20000], Training Loss: 0.014642905665563635, Validation Loss: 0.014825454270280107\n",
      "Epoch [8245/20000], Training Loss: 0.024203883948627793, Validation Loss: 0.023458823561682145\n",
      "Epoch [8246/20000], Training Loss: 0.027473270696126356, Validation Loss: 0.008187266889089122\n",
      "Epoch [8247/20000], Training Loss: 0.01983127871036621, Validation Loss: 0.005989890668615772\n",
      "Epoch [8248/20000], Training Loss: 0.007673914980370812, Validation Loss: 0.005949137867527757\n",
      "Epoch [8249/20000], Training Loss: 0.006876458806800656, Validation Loss: 0.007026364042206167\n",
      "Epoch [8250/20000], Training Loss: 0.007349743478698656, Validation Loss: 0.00807239949080715\n",
      "Epoch [8251/20000], Training Loss: 0.007015980858081353, Validation Loss: 0.00458771780607492\n",
      "Epoch [8252/20000], Training Loss: 0.00654323521303013, Validation Loss: 0.004152398992524857\n",
      "Epoch [8253/20000], Training Loss: 0.0059892072396061325, Validation Loss: 0.004221391645947798\n",
      "Epoch [8254/20000], Training Loss: 0.0076717340437296245, Validation Loss: 0.0030738218739624506\n",
      "Epoch [8255/20000], Training Loss: 0.007313735636021842, Validation Loss: 0.007589752838913658\n",
      "Epoch [8256/20000], Training Loss: 0.007351612598410741, Validation Loss: 0.005740694128312336\n",
      "Epoch [8257/20000], Training Loss: 0.006333686155390491, Validation Loss: 0.008893698807306854\n",
      "Epoch [8258/20000], Training Loss: 0.01763435388277555, Validation Loss: 0.006311763968239704\n",
      "Epoch [8259/20000], Training Loss: 0.007367303663548748, Validation Loss: 0.0037161022412500904\n",
      "Epoch [8260/20000], Training Loss: 0.005664273243382922, Validation Loss: 0.005105971508925476\n",
      "Epoch [8261/20000], Training Loss: 0.0068488031828120655, Validation Loss: 0.004102266399399923\n",
      "Epoch [8262/20000], Training Loss: 0.004645587901385235, Validation Loss: 0.0030922089578585394\n",
      "Epoch [8263/20000], Training Loss: 0.004536257651385053, Validation Loss: 0.014269478914655404\n",
      "Epoch [8264/20000], Training Loss: 0.021149952481989334, Validation Loss: 0.004201244224313021\n",
      "Epoch [8265/20000], Training Loss: 0.010727023236410917, Validation Loss: 0.0042817800954415884\n",
      "Epoch [8266/20000], Training Loss: 0.009260233456578655, Validation Loss: 0.005229689707992488\n",
      "Epoch [8267/20000], Training Loss: 0.006912843933325316, Validation Loss: 0.005833242467374536\n",
      "Epoch [8268/20000], Training Loss: 0.010118184803039705, Validation Loss: 0.009495843617767123\n",
      "Epoch [8269/20000], Training Loss: 0.05081457887510104, Validation Loss: 0.041446651937641424\n",
      "Epoch [8270/20000], Training Loss: 0.020886363068582017, Validation Loss: 0.0075465237239313865\n",
      "Epoch [8271/20000], Training Loss: 0.005794789471110562, Validation Loss: 0.005377790165409481\n",
      "Epoch [8272/20000], Training Loss: 0.0073009873490913636, Validation Loss: 0.004855431464615582\n",
      "Epoch [8273/20000], Training Loss: 0.007319693002760427, Validation Loss: 0.0069130360162229666\n",
      "Epoch [8274/20000], Training Loss: 0.005395550060451829, Validation Loss: 0.008988272053159616\n",
      "Epoch [8275/20000], Training Loss: 0.004729146087291676, Validation Loss: 0.005112152156828286\n",
      "Epoch [8276/20000], Training Loss: 0.00829902735547096, Validation Loss: 0.009001717001345761\n",
      "Epoch [8277/20000], Training Loss: 0.010431334207428986, Validation Loss: 0.007013673693287219\n",
      "Epoch [8278/20000], Training Loss: 0.0069714359035596574, Validation Loss: 0.00893426788680231\n",
      "Epoch [8279/20000], Training Loss: 0.008102440885912594, Validation Loss: 0.006063586096642227\n",
      "Epoch [8280/20000], Training Loss: 0.007163977697408492, Validation Loss: 0.004791347333496431\n",
      "Epoch [8281/20000], Training Loss: 0.010869520167554063, Validation Loss: 0.0036911406660595147\n",
      "Epoch [8282/20000], Training Loss: 0.013239163232356077, Validation Loss: 0.008981699016903346\n",
      "Epoch [8283/20000], Training Loss: 0.009446541053746062, Validation Loss: 0.00390494591631198\n",
      "Epoch [8284/20000], Training Loss: 0.008544524443485508, Validation Loss: 0.004761339431037673\n",
      "Epoch [8285/20000], Training Loss: 0.010660468103846401, Validation Loss: 0.003302854264887602\n",
      "Epoch [8286/20000], Training Loss: 0.004993370393094665, Validation Loss: 0.0072340581946014676\n",
      "Epoch [8287/20000], Training Loss: 0.013263418324640952, Validation Loss: 0.023219210762975933\n",
      "Epoch [8288/20000], Training Loss: 0.009815086312300991, Validation Loss: 0.0034739155972788793\n",
      "Epoch [8289/20000], Training Loss: 0.009344768808398487, Validation Loss: 0.003813886032636406\n",
      "Epoch [8290/20000], Training Loss: 0.0067586857623689, Validation Loss: 0.002995558687933299\n",
      "Epoch [8291/20000], Training Loss: 0.006891531911865708, Validation Loss: 0.007968117660894931\n",
      "Epoch [8292/20000], Training Loss: 0.016383298678972227, Validation Loss: 0.027948936979706067\n",
      "Epoch [8293/20000], Training Loss: 0.02133544010926666, Validation Loss: 0.0070092816419473946\n",
      "Epoch [8294/20000], Training Loss: 0.01837907866664474, Validation Loss: 0.013560370144485247\n",
      "Epoch [8295/20000], Training Loss: 0.01292107384818207, Validation Loss: 0.007106562374393174\n",
      "Epoch [8296/20000], Training Loss: 0.008275910414730398, Validation Loss: 0.0033268188026340795\n",
      "Epoch [8297/20000], Training Loss: 0.005266968120330213, Validation Loss: 0.003556147834452921\n",
      "Epoch [8298/20000], Training Loss: 0.006275554836195495, Validation Loss: 0.00443750451811883\n",
      "Epoch [8299/20000], Training Loss: 0.005792587754383151, Validation Loss: 0.0063808804424711025\n",
      "Epoch [8300/20000], Training Loss: 0.008719788386120595, Validation Loss: 0.004568744785600812\n",
      "Epoch [8301/20000], Training Loss: 0.007655654747005818, Validation Loss: 0.006930770805915252\n",
      "Epoch [8302/20000], Training Loss: 0.01501689652546442, Validation Loss: 0.002917091621647724\n",
      "Epoch [8303/20000], Training Loss: 0.008663295838882082, Validation Loss: 0.009494613151718465\n",
      "Epoch [8304/20000], Training Loss: 0.006261602276156607, Validation Loss: 0.003883074059325103\n",
      "Epoch [8305/20000], Training Loss: 0.007336056606228729, Validation Loss: 0.0038043955004093888\n",
      "Epoch [8306/20000], Training Loss: 0.006720862852359589, Validation Loss: 0.003607342697130938\n",
      "Epoch [8307/20000], Training Loss: 0.004881485150098642, Validation Loss: 0.003155446441683069\n",
      "Epoch [8308/20000], Training Loss: 0.0051846741771441884, Validation Loss: 0.002682666958382437\n",
      "Epoch [8309/20000], Training Loss: 0.005036404416646941, Validation Loss: 0.006552899833975191\n",
      "Epoch [8310/20000], Training Loss: 0.011450066533922967, Validation Loss: 0.0036630678595225125\n",
      "Epoch [8311/20000], Training Loss: 0.006583661222360394, Validation Loss: 0.006385624910628713\n",
      "Epoch [8312/20000], Training Loss: 0.004138857895408624, Validation Loss: 0.006923219718568622\n",
      "Epoch [8313/20000], Training Loss: 0.008592981154963906, Validation Loss: 0.0024503924424347395\n",
      "Epoch [8314/20000], Training Loss: 0.014801429832004942, Validation Loss: 0.004261397452182791\n",
      "Epoch [8315/20000], Training Loss: 0.014204861652355507, Validation Loss: 0.009314057262088905\n",
      "Epoch [8316/20000], Training Loss: 0.009209442260076426, Validation Loss: 0.007901037852867507\n",
      "Epoch [8317/20000], Training Loss: 0.009220142829040665, Validation Loss: 0.003422131313698681\n",
      "Epoch [8318/20000], Training Loss: 0.009252955040990076, Validation Loss: 0.014525988059631345\n",
      "Epoch [8319/20000], Training Loss: 0.03095370362851619, Validation Loss: 0.09829296942091657\n",
      "Epoch [8320/20000], Training Loss: 0.04150128975327659, Validation Loss: 0.022630918116192333\n",
      "Epoch [8321/20000], Training Loss: 0.02764841299670349, Validation Loss: 0.00846014189106167\n",
      "Epoch [8322/20000], Training Loss: 0.012637006295595452, Validation Loss: 0.009774809896924768\n",
      "Epoch [8323/20000], Training Loss: 0.007444144583132584, Validation Loss: 0.005505113411995158\n",
      "Epoch [8324/20000], Training Loss: 0.00656726659092653, Validation Loss: 0.0055692236880012\n",
      "Epoch [8325/20000], Training Loss: 0.006303016762623364, Validation Loss: 0.008531397391055049\n",
      "Epoch [8326/20000], Training Loss: 0.005707940450520255, Validation Loss: 0.00595538919697144\n",
      "Epoch [8327/20000], Training Loss: 0.008056838368897193, Validation Loss: 0.004029731193181111\n",
      "Epoch [8328/20000], Training Loss: 0.02594294016931339, Validation Loss: 0.004577833059881543\n",
      "Epoch [8329/20000], Training Loss: 0.019413096429031742, Validation Loss: 0.015384648793851399\n",
      "Epoch [8330/20000], Training Loss: 0.009425690119054966, Validation Loss: 0.014000005185182423\n",
      "Epoch [8331/20000], Training Loss: 0.011371283030842148, Validation Loss: 0.007405649259005109\n",
      "Epoch [8332/20000], Training Loss: 0.011135256183998925, Validation Loss: 0.0043093758461328745\n",
      "Epoch [8333/20000], Training Loss: 0.006603602785617113, Validation Loss: 0.006683330198809147\n",
      "Epoch [8334/20000], Training Loss: 0.013762372197041779, Validation Loss: 0.0532599467426605\n",
      "Epoch [8335/20000], Training Loss: 0.049796950740918486, Validation Loss: 0.02608433057126344\n",
      "Epoch [8336/20000], Training Loss: 0.1333324411285243, Validation Loss: 0.10902929848125506\n",
      "Epoch [8337/20000], Training Loss: 0.0941769667468699, Validation Loss: 0.029731683107651667\n",
      "Epoch [8338/20000], Training Loss: 0.033170760941824744, Validation Loss: 0.027851302796128557\n",
      "Epoch [8339/20000], Training Loss: 0.018310424622281322, Validation Loss: 0.013311802641380512\n",
      "Epoch [8340/20000], Training Loss: 0.014568495444629142, Validation Loss: 0.015278941239335962\n",
      "Epoch [8341/20000], Training Loss: 0.017953883881481097, Validation Loss: 0.008661530655141953\n",
      "Epoch [8342/20000], Training Loss: 0.01331224208531369, Validation Loss: 0.010847519724110148\n",
      "Epoch [8343/20000], Training Loss: 0.01717748071366389, Validation Loss: 0.0074178452572977805\n",
      "Epoch [8344/20000], Training Loss: 0.00946914405877968, Validation Loss: 0.03809097323768843\n",
      "Epoch [8345/20000], Training Loss: 0.014484456291289203, Validation Loss: 0.02512163068812307\n",
      "Epoch [8346/20000], Training Loss: 0.03186123473902366, Validation Loss: 0.014598831625854862\n",
      "Epoch [8347/20000], Training Loss: 0.019788772341728742, Validation Loss: 0.017366223869982867\n",
      "Epoch [8348/20000], Training Loss: 0.014955138442538944, Validation Loss: 0.009106878929599509\n",
      "Epoch [8349/20000], Training Loss: 0.007092578802257776, Validation Loss: 0.006448804286568434\n",
      "Epoch [8350/20000], Training Loss: 0.006940174506502116, Validation Loss: 0.00517387811034301\n",
      "Epoch [8351/20000], Training Loss: 0.00469783936361117, Validation Loss: 0.012051218322914196\n",
      "Epoch [8352/20000], Training Loss: 0.009878717061448177, Validation Loss: 0.013370640842742336\n",
      "Epoch [8353/20000], Training Loss: 0.02134471241151914, Validation Loss: 0.011076223912170767\n",
      "Epoch [8354/20000], Training Loss: 0.021236991528163735, Validation Loss: 0.007021507495838567\n",
      "Epoch [8355/20000], Training Loss: 0.012042856359455203, Validation Loss: 0.008149421082737005\n",
      "Epoch [8356/20000], Training Loss: 0.009377074908798022, Validation Loss: 0.006802090359574972\n",
      "Epoch [8357/20000], Training Loss: 0.010364317883337597, Validation Loss: 0.006537718140537179\n",
      "Epoch [8358/20000], Training Loss: 0.00643051274424319, Validation Loss: 0.004344723906462011\n",
      "Epoch [8359/20000], Training Loss: 0.0065542404315367874, Validation Loss: 0.006719898772972381\n",
      "Epoch [8360/20000], Training Loss: 0.006909851843374781, Validation Loss: 0.0038708879409686653\n",
      "Epoch [8361/20000], Training Loss: 0.004624257791744769, Validation Loss: 0.0038846733775879105\n",
      "Epoch [8362/20000], Training Loss: 0.0070545432229534655, Validation Loss: 0.00412038166459575\n",
      "Epoch [8363/20000], Training Loss: 0.004300438768301059, Validation Loss: 0.0047317383611975355\n",
      "Epoch [8364/20000], Training Loss: 0.010835682313362278, Validation Loss: 0.005877464811516694\n",
      "Epoch [8365/20000], Training Loss: 0.010201002816237243, Validation Loss: 0.011184966453616394\n",
      "Epoch [8366/20000], Training Loss: 0.01301207032104555, Validation Loss: 0.004449899348121492\n",
      "Epoch [8367/20000], Training Loss: 0.01898534893397092, Validation Loss: 0.0049210212440484805\n",
      "Epoch [8368/20000], Training Loss: 0.009208388406217896, Validation Loss: 0.008224078729611907\n",
      "Epoch [8369/20000], Training Loss: 0.007017252833195796, Validation Loss: 0.007232636780112067\n",
      "Epoch [8370/20000], Training Loss: 0.010923433688731166, Validation Loss: 0.01497529336508904\n",
      "Epoch [8371/20000], Training Loss: 0.00943725301242791, Validation Loss: 0.005119964593963086\n",
      "Epoch [8372/20000], Training Loss: 0.02115011335782973, Validation Loss: 0.012580420322988761\n",
      "Epoch [8373/20000], Training Loss: 0.020246545563817823, Validation Loss: 0.004896305903197832\n",
      "Epoch [8374/20000], Training Loss: 0.01491574783363652, Validation Loss: 0.005754270435457801\n",
      "Epoch [8375/20000], Training Loss: 0.008555251015682839, Validation Loss: 0.003937083902382296\n",
      "Epoch [8376/20000], Training Loss: 0.007319517589797478, Validation Loss: 0.00591802677437397\n",
      "Epoch [8377/20000], Training Loss: 0.007011351405578482, Validation Loss: 0.008289573509046022\n",
      "Epoch [8378/20000], Training Loss: 0.0083857462668675, Validation Loss: 0.0055864258084979005\n",
      "Epoch [8379/20000], Training Loss: 0.0053630946037758675, Validation Loss: 0.0036391786491310235\n",
      "Epoch [8380/20000], Training Loss: 0.006110961161279452, Validation Loss: 0.0064325610047844765\n",
      "Epoch [8381/20000], Training Loss: 0.00694477667483235, Validation Loss: 0.006465130021054472\n",
      "Epoch [8382/20000], Training Loss: 0.011179032693429949, Validation Loss: 0.013197713198942685\n",
      "Epoch [8383/20000], Training Loss: 0.017557223976187482, Validation Loss: 0.005404207519529032\n",
      "Epoch [8384/20000], Training Loss: 0.015104365043781496, Validation Loss: 0.004420589820536154\n",
      "Epoch [8385/20000], Training Loss: 0.010197481442966299, Validation Loss: 0.012031012335752132\n",
      "Epoch [8386/20000], Training Loss: 0.01698145659819212, Validation Loss: 0.009871811248883335\n",
      "Epoch [8387/20000], Training Loss: 0.02385126142429986, Validation Loss: 0.01370620335053147\n",
      "Epoch [8388/20000], Training Loss: 0.015632786239231273, Validation Loss: 0.01731480977376382\n",
      "Epoch [8389/20000], Training Loss: 0.013391109559701622, Validation Loss: 0.005647914893862209\n",
      "Epoch [8390/20000], Training Loss: 0.007095027000104892, Validation Loss: 0.004232065584148562\n",
      "Epoch [8391/20000], Training Loss: 0.005110722272157935, Validation Loss: 0.0037083240673538623\n",
      "Epoch [8392/20000], Training Loss: 0.005162220060786942, Validation Loss: 0.006166945383671114\n",
      "Epoch [8393/20000], Training Loss: 0.012307589123208475, Validation Loss: 0.01685649592796474\n",
      "Epoch [8394/20000], Training Loss: 0.02097542667999213, Validation Loss: 0.0070754549498923906\n",
      "Epoch [8395/20000], Training Loss: 0.014214424410965876, Validation Loss: 0.006447066686063862\n",
      "Epoch [8396/20000], Training Loss: 0.007657783879299781, Validation Loss: 0.0043819394214447875\n",
      "Epoch [8397/20000], Training Loss: 0.0047462819181548965, Validation Loss: 0.004047829562487269\n",
      "Epoch [8398/20000], Training Loss: 0.006471446358773392, Validation Loss: 0.01047074156153988\n",
      "Epoch [8399/20000], Training Loss: 0.0081864197700615, Validation Loss: 0.0632905568641269\n",
      "Epoch [8400/20000], Training Loss: 0.015749176500581337, Validation Loss: 0.005669108161907898\n",
      "Epoch [8401/20000], Training Loss: 0.006231310622422167, Validation Loss: 0.006169949958130086\n",
      "Epoch [8402/20000], Training Loss: 0.007932611188152805, Validation Loss: 0.006395498334072158\n",
      "Epoch [8403/20000], Training Loss: 0.0059248470678979915, Validation Loss: 0.004177052062161876\n",
      "Epoch [8404/20000], Training Loss: 0.005913021920215604, Validation Loss: 0.0030488958888830304\n",
      "Epoch [8405/20000], Training Loss: 0.008996970025012476, Validation Loss: 0.034034471692783494\n",
      "Epoch [8406/20000], Training Loss: 0.009313611893438585, Validation Loss: 0.0032390629479670174\n",
      "Epoch [8407/20000], Training Loss: 0.009060910002777487, Validation Loss: 0.009231530595571177\n",
      "Epoch [8408/20000], Training Loss: 0.010256655913898223, Validation Loss: 0.0040733140693813\n",
      "Epoch [8409/20000], Training Loss: 0.012436993995444416, Validation Loss: 0.013906199617057299\n",
      "Epoch [8410/20000], Training Loss: 0.017974848314435934, Validation Loss: 0.010203695670771135\n",
      "Epoch [8411/20000], Training Loss: 0.02044685487739376, Validation Loss: 0.00990057772717661\n",
      "Epoch [8412/20000], Training Loss: 0.019357971752853587, Validation Loss: 0.0077583388034368735\n",
      "Epoch [8413/20000], Training Loss: 0.009810913815661999, Validation Loss: 0.007340486593816539\n",
      "Epoch [8414/20000], Training Loss: 0.021533959260263406, Validation Loss: 0.012382581830860925\n",
      "Epoch [8415/20000], Training Loss: 0.007903851857658342, Validation Loss: 0.010025377651412004\n",
      "Epoch [8416/20000], Training Loss: 0.008994917776102998, Validation Loss: 0.005238104702433085\n",
      "Epoch [8417/20000], Training Loss: 0.006738431372338839, Validation Loss: 0.0035442680326693743\n",
      "Epoch [8418/20000], Training Loss: 0.011773372669787412, Validation Loss: 0.003717290770717308\n",
      "Epoch [8419/20000], Training Loss: 0.017228650581143614, Validation Loss: 0.010322758063142848\n",
      "Epoch [8420/20000], Training Loss: 0.010826101715240708, Validation Loss: 0.004719766385673071\n",
      "Epoch [8421/20000], Training Loss: 0.007440770131194897, Validation Loss: 0.0037046103529958163\n",
      "Epoch [8422/20000], Training Loss: 0.004783338896152551, Validation Loss: 0.003960900507673778\n",
      "Epoch [8423/20000], Training Loss: 0.00362630481166499, Validation Loss: 0.0031464963356050775\n",
      "Epoch [8424/20000], Training Loss: 0.0049905743362614885, Validation Loss: 0.0025779659999208733\n",
      "Epoch [8425/20000], Training Loss: 0.006000259783685838, Validation Loss: 0.0031083576281762193\n",
      "Epoch [8426/20000], Training Loss: 0.00618152805756316, Validation Loss: 0.0032009101748535006\n",
      "Epoch [8427/20000], Training Loss: 0.003894622941775846, Validation Loss: 0.00969787141574288\n",
      "Epoch [8428/20000], Training Loss: 0.0064869419675233075, Validation Loss: 0.0030859863619730377\n",
      "Epoch [8429/20000], Training Loss: 0.012064604559522454, Validation Loss: 0.003437146013269512\n",
      "Epoch [8430/20000], Training Loss: 0.01232401054906924, Validation Loss: 0.013492348736950334\n",
      "Epoch [8431/20000], Training Loss: 0.02063372901778036, Validation Loss: 0.006871791290385612\n",
      "Epoch [8432/20000], Training Loss: 0.009682625374158047, Validation Loss: 0.007982193822957031\n",
      "Epoch [8433/20000], Training Loss: 0.010442235359716636, Validation Loss: 0.0061401912410368655\n",
      "Epoch [8434/20000], Training Loss: 0.006329729957672369, Validation Loss: 0.006232624075242588\n",
      "Epoch [8435/20000], Training Loss: 0.008890269927121608, Validation Loss: 0.0035858422925243365\n",
      "Epoch [8436/20000], Training Loss: 0.005272399621649778, Validation Loss: 0.003289152801571253\n",
      "Epoch [8437/20000], Training Loss: 0.0038523177754541393, Validation Loss: 0.0030106265497020757\n",
      "Epoch [8438/20000], Training Loss: 0.003341717923051744, Validation Loss: 0.0031161779936963894\n",
      "Epoch [8439/20000], Training Loss: 0.0048238116911047, Validation Loss: 0.0032746279862449178\n",
      "Epoch [8440/20000], Training Loss: 0.006838751634144761, Validation Loss: 0.0023457153002330593\n",
      "Epoch [8441/20000], Training Loss: 0.006514744292612055, Validation Loss: 0.015305874603135245\n",
      "Epoch [8442/20000], Training Loss: 0.01460760402171982, Validation Loss: 0.07293405596699033\n",
      "Epoch [8443/20000], Training Loss: 0.029066429337815083, Validation Loss: 0.04358856273548944\n",
      "Epoch [8444/20000], Training Loss: 0.026408076407182093, Validation Loss: 0.012873265418299121\n",
      "Epoch [8445/20000], Training Loss: 0.023863452559902094, Validation Loss: 0.027100385033658574\n",
      "Epoch [8446/20000], Training Loss: 0.01734852882301701, Validation Loss: 0.005959879411730396\n",
      "Epoch [8447/20000], Training Loss: 0.009031136399633963, Validation Loss: 0.007266826858931612\n",
      "Epoch [8448/20000], Training Loss: 0.008582210470738769, Validation Loss: 0.004172413643748574\n",
      "Epoch [8449/20000], Training Loss: 0.014443563635203256, Validation Loss: 0.00712060519230777\n",
      "Epoch [8450/20000], Training Loss: 0.009251640066395339, Validation Loss: 0.0073033225323475915\n",
      "Epoch [8451/20000], Training Loss: 0.006693115053557059, Validation Loss: 0.01212775547589712\n",
      "Epoch [8452/20000], Training Loss: 0.014646426907607488, Validation Loss: 0.009872938838920422\n",
      "Epoch [8453/20000], Training Loss: 0.00890931334093738, Validation Loss: 0.010599208594886314\n",
      "Epoch [8454/20000], Training Loss: 0.009138302679665295, Validation Loss: 0.012436508201664998\n",
      "Epoch [8455/20000], Training Loss: 0.008775362875623114, Validation Loss: 0.017454645374008187\n",
      "Epoch [8456/20000], Training Loss: 0.020019462032775794, Validation Loss: 0.014755445143066778\n",
      "Epoch [8457/20000], Training Loss: 0.007486409950932479, Validation Loss: 0.0038165043151455586\n",
      "Epoch [8458/20000], Training Loss: 0.0073520203722442345, Validation Loss: 0.004835496670369009\n",
      "Epoch [8459/20000], Training Loss: 0.007136388350391242, Validation Loss: 0.002980491920067964\n",
      "Epoch [8460/20000], Training Loss: 0.005566492678813769, Validation Loss: 0.0039704218749519226\n",
      "Epoch [8461/20000], Training Loss: 0.005404189602164219, Validation Loss: 0.004982596939218225\n",
      "Epoch [8462/20000], Training Loss: 0.0050170320990738605, Validation Loss: 0.006824362771892213\n",
      "Epoch [8463/20000], Training Loss: 0.013257689898767109, Validation Loss: 0.007302430546587857\n",
      "Epoch [8464/20000], Training Loss: 0.011433230955195281, Validation Loss: 0.00421220024493366\n",
      "Epoch [8465/20000], Training Loss: 0.006568131790897626, Validation Loss: 0.0030704630771555435\n",
      "Epoch [8466/20000], Training Loss: 0.0041238431583354085, Validation Loss: 0.004426318200590108\n",
      "Epoch [8467/20000], Training Loss: 0.0056048073699993695, Validation Loss: 0.007955489503339257\n",
      "Epoch [8468/20000], Training Loss: 0.009963220870499103, Validation Loss: 0.013223999434688136\n",
      "Epoch [8469/20000], Training Loss: 0.02993835046982101, Validation Loss: 0.04820012095727374\n",
      "Epoch [8470/20000], Training Loss: 0.0509682644208494, Validation Loss: 0.02654424808081655\n",
      "Epoch [8471/20000], Training Loss: 0.03297524276422337, Validation Loss: 0.020181696506499845\n",
      "Epoch [8472/20000], Training Loss: 0.04328415509456006, Validation Loss: 0.013375453151947372\n",
      "Epoch [8473/20000], Training Loss: 0.023296814894170632, Validation Loss: 0.03752734192812294\n",
      "Epoch [8474/20000], Training Loss: 0.019079450800615762, Validation Loss: 0.017517170475354272\n",
      "Epoch [8475/20000], Training Loss: 0.019616211294695467, Validation Loss: 0.008476996421645058\n",
      "Epoch [8476/20000], Training Loss: 0.014487817634030111, Validation Loss: 0.007215158609012308\n",
      "Epoch [8477/20000], Training Loss: 0.007400378726223218, Validation Loss: 0.0071466693252010115\n",
      "Epoch [8478/20000], Training Loss: 0.00707642557764692, Validation Loss: 0.005546282181010197\n",
      "Epoch [8479/20000], Training Loss: 0.008352888573426753, Validation Loss: 0.00463320973874813\n",
      "Epoch [8480/20000], Training Loss: 0.007515571595701788, Validation Loss: 0.00949151176203398\n",
      "Epoch [8481/20000], Training Loss: 0.00597528914970878, Validation Loss: 0.00477486070311501\n",
      "Epoch [8482/20000], Training Loss: 0.00634583690771251, Validation Loss: 0.00303017292997814\n",
      "Epoch [8483/20000], Training Loss: 0.004936689485149058, Validation Loss: 0.0029846243783749416\n",
      "Epoch [8484/20000], Training Loss: 0.009998702415941807, Validation Loss: 0.0029786851347237686\n",
      "Epoch [8485/20000], Training Loss: 0.0247238233157207, Validation Loss: 0.009324624290073556\n",
      "Epoch [8486/20000], Training Loss: 0.010848269510980961, Validation Loss: 0.01608716525639418\n",
      "Epoch [8487/20000], Training Loss: 0.022994377957989594, Validation Loss: 0.020674519278015526\n",
      "Epoch [8488/20000], Training Loss: 0.0241878913698851, Validation Loss: 0.012478259574488147\n",
      "Epoch [8489/20000], Training Loss: 0.01539195047631178, Validation Loss: 0.014492270848825043\n",
      "Epoch [8490/20000], Training Loss: 0.009844070396086733, Validation Loss: 0.0051819453806666626\n",
      "Epoch [8491/20000], Training Loss: 0.00722801050869748, Validation Loss: 0.007175370014573733\n",
      "Epoch [8492/20000], Training Loss: 0.006561119658207255, Validation Loss: 0.0040927890133493705\n",
      "Epoch [8493/20000], Training Loss: 0.009059558080999912, Validation Loss: 0.007239981156286929\n",
      "Epoch [8494/20000], Training Loss: 0.00669970349450263, Validation Loss: 0.010526554792061073\n",
      "Epoch [8495/20000], Training Loss: 0.020336179599001167, Validation Loss: 0.011996557403995935\n",
      "Epoch [8496/20000], Training Loss: 0.030111785083136056, Validation Loss: 0.009579558471306843\n",
      "Epoch [8497/20000], Training Loss: 0.020542999795517453, Validation Loss: 0.04515419652593015\n",
      "Epoch [8498/20000], Training Loss: 0.027792147057230716, Validation Loss: 0.025274173328982994\n",
      "Epoch [8499/20000], Training Loss: 0.01926987059830156, Validation Loss: 0.006037730265656397\n",
      "Epoch [8500/20000], Training Loss: 0.010719554206713968, Validation Loss: 0.014205054779234973\n",
      "Epoch [8501/20000], Training Loss: 0.006440322237072645, Validation Loss: 0.007325448207370451\n",
      "Epoch [8502/20000], Training Loss: 0.005575400552645858, Validation Loss: 0.005896228632954438\n",
      "Epoch [8503/20000], Training Loss: 0.005665048866053108, Validation Loss: 0.007237506378059168\n",
      "Epoch [8504/20000], Training Loss: 0.011856014424535846, Validation Loss: 0.007276781952413103\n",
      "Epoch [8505/20000], Training Loss: 0.011192307527900474, Validation Loss: 0.004055433032373189\n",
      "Epoch [8506/20000], Training Loss: 0.006621427753998432, Validation Loss: 0.018746287961091495\n",
      "Epoch [8507/20000], Training Loss: 0.011646772312047344, Validation Loss: 0.004029652923076411\n",
      "Epoch [8508/20000], Training Loss: 0.010037757123687438, Validation Loss: 0.019970327382388024\n",
      "Epoch [8509/20000], Training Loss: 0.005218613083377883, Validation Loss: 0.005125773638324449\n",
      "Epoch [8510/20000], Training Loss: 0.0058059535575531685, Validation Loss: 0.004599694494453236\n",
      "Epoch [8511/20000], Training Loss: 0.012282865895290993, Validation Loss: 0.0033405078653738138\n",
      "Epoch [8512/20000], Training Loss: 0.008087997642308307, Validation Loss: 0.003510502667569554\n",
      "Epoch [8513/20000], Training Loss: 0.009578551710416962, Validation Loss: 0.007646974641836689\n",
      "Epoch [8514/20000], Training Loss: 0.005887068564594041, Validation Loss: 0.00759632835563906\n",
      "Epoch [8515/20000], Training Loss: 0.005522441873576359, Validation Loss: 0.003122235094832244\n",
      "Epoch [8516/20000], Training Loss: 0.007028191730829089, Validation Loss: 0.003279525472045307\n",
      "Epoch [8517/20000], Training Loss: 0.011208019102923572, Validation Loss: 0.003939669168258635\n",
      "Epoch [8518/20000], Training Loss: 0.0061293505709159035, Validation Loss: 0.004069629461129958\n",
      "Epoch [8519/20000], Training Loss: 0.005893493208402235, Validation Loss: 0.004853470592981703\n",
      "Epoch [8520/20000], Training Loss: 0.010836434453917068, Validation Loss: 0.010566642112499787\n",
      "Epoch [8521/20000], Training Loss: 0.008587348671946009, Validation Loss: 0.007685300727774749\n",
      "Epoch [8522/20000], Training Loss: 0.007269052036048963, Validation Loss: 0.011007517709265476\n",
      "Epoch [8523/20000], Training Loss: 0.009431220010290937, Validation Loss: 0.009731830496873673\n",
      "Epoch [8524/20000], Training Loss: 0.005563868915617475, Validation Loss: 0.010837633050816262\n",
      "Epoch [8525/20000], Training Loss: 0.006152519006962802, Validation Loss: 0.005571836805307022\n",
      "Epoch [8526/20000], Training Loss: 0.010671471438009965, Validation Loss: 0.00991054504577871\n",
      "Epoch [8527/20000], Training Loss: 0.00941993322235898, Validation Loss: 0.028297756399610446\n",
      "Epoch [8528/20000], Training Loss: 0.01345407176631852, Validation Loss: 0.025299131870269796\n",
      "Epoch [8529/20000], Training Loss: 0.0124814842628049, Validation Loss: 0.00858772093699835\n",
      "Epoch [8530/20000], Training Loss: 0.00715832447999024, Validation Loss: 0.009714090198810742\n",
      "Epoch [8531/20000], Training Loss: 0.013506622012105904, Validation Loss: 0.003133228680692429\n",
      "Epoch [8532/20000], Training Loss: 0.02559066562519417, Validation Loss: 0.015500696392596833\n",
      "Epoch [8533/20000], Training Loss: 0.07561749179440572, Validation Loss: 0.01249151607834631\n",
      "Epoch [8534/20000], Training Loss: 0.035302700002960465, Validation Loss: 0.03250529831452046\n",
      "Epoch [8535/20000], Training Loss: 0.021832193609692956, Validation Loss: 0.01294485839073088\n",
      "Epoch [8536/20000], Training Loss: 0.018498793809807727, Validation Loss: 0.018442198634165845\n",
      "Epoch [8537/20000], Training Loss: 0.021136154337520047, Validation Loss: 0.02262851649097175\n",
      "Epoch [8538/20000], Training Loss: 0.014619182423172918, Validation Loss: 0.008134800721749361\n",
      "Epoch [8539/20000], Training Loss: 0.007729605379738912, Validation Loss: 0.005328317176114081\n",
      "Epoch [8540/20000], Training Loss: 0.0059937537049076384, Validation Loss: 0.00637470978342528\n",
      "Epoch [8541/20000], Training Loss: 0.006189247767906636, Validation Loss: 0.005354760264591909\n",
      "Epoch [8542/20000], Training Loss: 0.0064033331527753035, Validation Loss: 0.004700325604173822\n",
      "Epoch [8543/20000], Training Loss: 0.007108585112811332, Validation Loss: 0.0045622482929113485\n",
      "Epoch [8544/20000], Training Loss: 0.004541039785440911, Validation Loss: 0.004442591872898979\n",
      "Epoch [8545/20000], Training Loss: 0.0065437442307094374, Validation Loss: 0.013183457244719778\n",
      "Epoch [8546/20000], Training Loss: 0.004672410377107131, Validation Loss: 0.17506338017327444\n",
      "Epoch [8547/20000], Training Loss: 0.09346738382291164, Validation Loss: 0.014805625322420468\n",
      "Epoch [8548/20000], Training Loss: 0.03924682508555374, Validation Loss: 0.010683010078667263\n",
      "Epoch [8549/20000], Training Loss: 0.020920785992140218, Validation Loss: 0.01611164069328197\n",
      "Epoch [8550/20000], Training Loss: 0.012374954788746046, Validation Loss: 0.01922838074108389\n",
      "Epoch [8551/20000], Training Loss: 0.012311648377882583, Validation Loss: 0.008122560600375956\n",
      "Epoch [8552/20000], Training Loss: 0.007850943025654747, Validation Loss: 0.007286527859313148\n",
      "Epoch [8553/20000], Training Loss: 0.006560301626450382, Validation Loss: 0.0055380757169457085\n",
      "Epoch [8554/20000], Training Loss: 0.007365886171880577, Validation Loss: 0.004983155251725943\n",
      "Epoch [8555/20000], Training Loss: 0.005189819457038019, Validation Loss: 0.006590991684788605\n",
      "Epoch [8556/20000], Training Loss: 0.007641865081885564, Validation Loss: 0.009758806945877703\n",
      "Epoch [8557/20000], Training Loss: 0.009467262741444367, Validation Loss: 0.009498963678977037\n",
      "Epoch [8558/20000], Training Loss: 0.006424374945449277, Validation Loss: 0.015861653591736555\n",
      "Epoch [8559/20000], Training Loss: 0.010003764685409675, Validation Loss: 0.0058591883893289575\n",
      "Epoch [8560/20000], Training Loss: 0.006852164390823289, Validation Loss: 0.012260136236883616\n",
      "Epoch [8561/20000], Training Loss: 0.011891454362609823, Validation Loss: 0.004335239052888288\n",
      "Epoch [8562/20000], Training Loss: 0.008291581458511896, Validation Loss: 0.013444949966126123\n",
      "Epoch [8563/20000], Training Loss: 0.007920735833717376, Validation Loss: 0.007259321327930708\n",
      "Epoch [8564/20000], Training Loss: 0.01339196229569747, Validation Loss: 0.004021384073252777\n",
      "Epoch [8565/20000], Training Loss: 0.005274442262426808, Validation Loss: 0.004394727837539644\n",
      "Epoch [8566/20000], Training Loss: 0.005435589120939507, Validation Loss: 0.007789035836144371\n",
      "Epoch [8567/20000], Training Loss: 0.0068715569213964045, Validation Loss: 0.004315978369959339\n",
      "Epoch [8568/20000], Training Loss: 0.0043252793589739925, Validation Loss: 0.006707540779264086\n",
      "Epoch [8569/20000], Training Loss: 0.005546132452894069, Validation Loss: 0.0038446465494708093\n",
      "Epoch [8570/20000], Training Loss: 0.007379179924360609, Validation Loss: 0.005328658438364593\n",
      "Epoch [8571/20000], Training Loss: 0.006651646362096285, Validation Loss: 0.005962049520575629\n",
      "Epoch [8572/20000], Training Loss: 0.00910254668685541, Validation Loss: 0.0032652358128653397\n",
      "Epoch [8573/20000], Training Loss: 0.033870160796653, Validation Loss: 0.025385603062334536\n",
      "Epoch [8574/20000], Training Loss: 0.015544994285197131, Validation Loss: 0.032862478441529674\n",
      "Epoch [8575/20000], Training Loss: 0.01528274621055711, Validation Loss: 0.008037451762405112\n",
      "Epoch [8576/20000], Training Loss: 0.01504734671470942, Validation Loss: 0.008931223319564958\n",
      "Epoch [8577/20000], Training Loss: 0.012118462266308987, Validation Loss: 0.0039311533858251224\n",
      "Epoch [8578/20000], Training Loss: 0.00850926457046443, Validation Loss: 0.0070633805293514994\n",
      "Epoch [8579/20000], Training Loss: 0.008740427527560055, Validation Loss: 0.00886840240287809\n",
      "Epoch [8580/20000], Training Loss: 0.006763487335677253, Validation Loss: 0.004104896049385941\n",
      "Epoch [8581/20000], Training Loss: 0.007752084191971724, Validation Loss: 0.003482027026434155\n",
      "Epoch [8582/20000], Training Loss: 0.012457729909499449, Validation Loss: 0.014468396219658603\n",
      "Epoch [8583/20000], Training Loss: 0.010608333434252668, Validation Loss: 0.007767983481819333\n",
      "Epoch [8584/20000], Training Loss: 0.027608352246586167, Validation Loss: 0.006088042383231579\n",
      "Epoch [8585/20000], Training Loss: 0.013346343724931233, Validation Loss: 0.0074227416804920465\n",
      "Epoch [8586/20000], Training Loss: 0.01115084112201917, Validation Loss: 0.011337678882792256\n",
      "Epoch [8587/20000], Training Loss: 0.004614231213054154, Validation Loss: 0.017389574017995204\n",
      "Epoch [8588/20000], Training Loss: 0.018441883813855902, Validation Loss: 0.027088051699008377\n",
      "Epoch [8589/20000], Training Loss: 0.02657863877123288, Validation Loss: 0.015496123240280808\n",
      "Epoch [8590/20000], Training Loss: 0.008956834233166384, Validation Loss: 0.008902331313788927\n",
      "Epoch [8591/20000], Training Loss: 0.006710032976116054, Validation Loss: 0.0036812552425869527\n",
      "Epoch [8592/20000], Training Loss: 0.006599257862295157, Validation Loss: 0.004198067028735153\n",
      "Epoch [8593/20000], Training Loss: 0.0050961944486646515, Validation Loss: 0.0036492187853076935\n",
      "Epoch [8594/20000], Training Loss: 0.004598129629625548, Validation Loss: 0.0033828481439123997\n",
      "Epoch [8595/20000], Training Loss: 0.0074845632288327935, Validation Loss: 0.00461311515754551\n",
      "Epoch [8596/20000], Training Loss: 0.005786499609322553, Validation Loss: 0.010162156577804029\n",
      "Epoch [8597/20000], Training Loss: 0.01683723799245789, Validation Loss: 0.023877215704747774\n",
      "Epoch [8598/20000], Training Loss: 0.011261240195640962, Validation Loss: 0.004857290897406433\n",
      "Epoch [8599/20000], Training Loss: 0.022186901755243786, Validation Loss: 0.004576854492837258\n",
      "Epoch [8600/20000], Training Loss: 0.01669806117258434, Validation Loss: 0.022421487075916957\n",
      "Epoch [8601/20000], Training Loss: 0.022253986589411006, Validation Loss: 0.01321257353348772\n",
      "Epoch [8602/20000], Training Loss: 0.012469679069909034, Validation Loss: 0.023582708762530404\n",
      "Epoch [8603/20000], Training Loss: 0.026882344783059255, Validation Loss: 0.008457474011265731\n",
      "Epoch [8604/20000], Training Loss: 0.009967279454161013, Validation Loss: 0.007330081513709589\n",
      "Epoch [8605/20000], Training Loss: 0.006102440570040406, Validation Loss: 0.005653457212616738\n",
      "Epoch [8606/20000], Training Loss: 0.007446773566710719, Validation Loss: 0.007022221097587025\n",
      "Epoch [8607/20000], Training Loss: 0.010829121423219996, Validation Loss: 0.007403780826337985\n",
      "Epoch [8608/20000], Training Loss: 0.006961778276101021, Validation Loss: 0.00684675853504554\n",
      "Epoch [8609/20000], Training Loss: 0.01473830829698792, Validation Loss: 0.010563726803028705\n",
      "Epoch [8610/20000], Training Loss: 0.03346845683076286, Validation Loss: 0.009713131910806234\n",
      "Epoch [8611/20000], Training Loss: 0.023232605225140496, Validation Loss: 0.028926591255835125\n",
      "Epoch [8612/20000], Training Loss: 0.01859163206089371, Validation Loss: 0.013707693000990463\n",
      "Epoch [8613/20000], Training Loss: 0.017512964600298022, Validation Loss: 0.018245478800703756\n",
      "Epoch [8614/20000], Training Loss: 0.016280874102709016, Validation Loss: 0.019918687240923254\n",
      "Epoch [8615/20000], Training Loss: 0.010363309677424175, Validation Loss: 0.005245440314559894\n",
      "Epoch [8616/20000], Training Loss: 0.006893653936620987, Validation Loss: 0.006337189449331987\n",
      "Epoch [8617/20000], Training Loss: 0.0055674626708648945, Validation Loss: 0.004415007951225866\n",
      "Epoch [8618/20000], Training Loss: 0.0050719371026519055, Validation Loss: 0.004425201606309358\n",
      "Epoch [8619/20000], Training Loss: 0.009217497905150853, Validation Loss: 0.007038158990967348\n",
      "Epoch [8620/20000], Training Loss: 0.015381054833653707, Validation Loss: 0.006823625465465985\n",
      "Epoch [8621/20000], Training Loss: 0.017402766442890943, Validation Loss: 0.006706707412388898\n",
      "Epoch [8622/20000], Training Loss: 0.007967982283698756, Validation Loss: 0.007834680943883923\n",
      "Epoch [8623/20000], Training Loss: 0.008387225933672329, Validation Loss: 0.005337054293148318\n",
      "Epoch [8624/20000], Training Loss: 0.0048559268117449915, Validation Loss: 0.0035713750625793496\n",
      "Epoch [8625/20000], Training Loss: 0.005326864727456788, Validation Loss: 0.010423493571580162\n",
      "Epoch [8626/20000], Training Loss: 0.008732247101696495, Validation Loss: 0.011101695642407466\n",
      "Epoch [8627/20000], Training Loss: 0.008422107690372929, Validation Loss: 0.005023712862038421\n",
      "Epoch [8628/20000], Training Loss: 0.006063609180273488, Validation Loss: 0.0037202657826590846\n",
      "Epoch [8629/20000], Training Loss: 0.005687643678226907, Validation Loss: 0.003269182902515995\n",
      "Epoch [8630/20000], Training Loss: 0.0073668053082656115, Validation Loss: 0.005827859709307545\n",
      "Epoch [8631/20000], Training Loss: 0.010970732637880636, Validation Loss: 0.02030589697616492\n",
      "Epoch [8632/20000], Training Loss: 0.02081233988328027, Validation Loss: 0.013830799675945747\n",
      "Epoch [8633/20000], Training Loss: 0.033544068625555416, Validation Loss: 0.08007672216211047\n",
      "Epoch [8634/20000], Training Loss: 0.07000263391195663, Validation Loss: 0.017792930774053924\n",
      "Epoch [8635/20000], Training Loss: 0.022239449372656445, Validation Loss: 0.0077265168469499\n",
      "Epoch [8636/20000], Training Loss: 0.013519665089136521, Validation Loss: 0.006766151736623475\n",
      "Epoch [8637/20000], Training Loss: 0.00895963138983851, Validation Loss: 0.010593723969837552\n",
      "Epoch [8638/20000], Training Loss: 0.007766412188565093, Validation Loss: 0.005833221136916629\n",
      "Epoch [8639/20000], Training Loss: 0.006059972827123212, Validation Loss: 0.005958946213857936\n",
      "Epoch [8640/20000], Training Loss: 0.0055870628524904275, Validation Loss: 0.0051562897289321495\n",
      "Epoch [8641/20000], Training Loss: 0.005914910239100989, Validation Loss: 0.00609186384800913\n",
      "Epoch [8642/20000], Training Loss: 0.006027668939038579, Validation Loss: 0.005210161229375184\n",
      "Epoch [8643/20000], Training Loss: 0.0056645319522691095, Validation Loss: 0.00667907514876528\n",
      "Epoch [8644/20000], Training Loss: 0.006335690517776779, Validation Loss: 0.010891876742919391\n",
      "Epoch [8645/20000], Training Loss: 0.006810550648619288, Validation Loss: 0.003769695825342621\n",
      "Epoch [8646/20000], Training Loss: 0.004827150663913926, Validation Loss: 0.006598257252335316\n",
      "Epoch [8647/20000], Training Loss: 0.010602134741930058, Validation Loss: 0.003248227515801513\n",
      "Epoch [8648/20000], Training Loss: 0.009761655131504605, Validation Loss: 0.004554266820767745\n",
      "Epoch [8649/20000], Training Loss: 0.006346165786193784, Validation Loss: 0.005941036445749758\n",
      "Epoch [8650/20000], Training Loss: 0.007829219485366983, Validation Loss: 0.026944007748949974\n",
      "Epoch [8651/20000], Training Loss: 0.012090199524079383, Validation Loss: 0.00411230896159915\n",
      "Epoch [8652/20000], Training Loss: 0.014602293765295014, Validation Loss: 0.004036329421718879\n",
      "Epoch [8653/20000], Training Loss: 0.005542991084179708, Validation Loss: 0.009076607089939768\n",
      "Epoch [8654/20000], Training Loss: 0.005895765203604242, Validation Loss: 0.005727408276600597\n",
      "Epoch [8655/20000], Training Loss: 0.00490665838255414, Validation Loss: 0.0027770480067244663\n",
      "Epoch [8656/20000], Training Loss: 0.0056666351748780085, Validation Loss: 0.006226922490246319\n",
      "Epoch [8657/20000], Training Loss: 0.01169448186036399, Validation Loss: 0.013012365517300038\n",
      "Epoch [8658/20000], Training Loss: 0.010783283106450523, Validation Loss: 0.06018277809990228\n",
      "Epoch [8659/20000], Training Loss: 0.03095389114293669, Validation Loss: 0.05294770714382529\n",
      "Epoch [8660/20000], Training Loss: 0.061635910370439105, Validation Loss: 0.06893804669380188\n",
      "Epoch [8661/20000], Training Loss: 0.053509681177924255, Validation Loss: 0.026529713470539427\n",
      "Epoch [8662/20000], Training Loss: 0.026929645720104287, Validation Loss: 0.007874161182761392\n",
      "Epoch [8663/20000], Training Loss: 0.009336818799576057, Validation Loss: 0.007775462530324668\n",
      "Epoch [8664/20000], Training Loss: 0.007925441392996748, Validation Loss: 0.01601756348048054\n",
      "Epoch [8665/20000], Training Loss: 0.012049206253972702, Validation Loss: 0.005763244323004203\n",
      "Epoch [8666/20000], Training Loss: 0.00864265754717053, Validation Loss: 0.005715765551160855\n",
      "Epoch [8667/20000], Training Loss: 0.012329593761283572, Validation Loss: 0.007518851250227142\n",
      "Epoch [8668/20000], Training Loss: 0.00680778909424719, Validation Loss: 0.007700201782946741\n",
      "Epoch [8669/20000], Training Loss: 0.008015763677706542, Validation Loss: 0.008332780251943664\n",
      "Epoch [8670/20000], Training Loss: 0.0080507832629207, Validation Loss: 0.005845424687712304\n",
      "Epoch [8671/20000], Training Loss: 0.005126233445610394, Validation Loss: 0.0040159150069354966\n",
      "Epoch [8672/20000], Training Loss: 0.006322307981983093, Validation Loss: 0.005432779353213846\n",
      "Epoch [8673/20000], Training Loss: 0.00971321186287761, Validation Loss: 0.009949424421522832\n",
      "Epoch [8674/20000], Training Loss: 0.008778105602167281, Validation Loss: 0.003399885668971235\n",
      "Epoch [8675/20000], Training Loss: 0.0071313763349150705, Validation Loss: 0.0039300306005835796\n",
      "Epoch [8676/20000], Training Loss: 0.006660217981594282, Validation Loss: 0.007064391252502641\n",
      "Epoch [8677/20000], Training Loss: 0.00923904951248135, Validation Loss: 0.009179815268518203\n",
      "Epoch [8678/20000], Training Loss: 0.010835379805420027, Validation Loss: 0.0042441316414364594\n",
      "Epoch [8679/20000], Training Loss: 0.00527248910241594, Validation Loss: 0.0035079831360625186\n",
      "Epoch [8680/20000], Training Loss: 0.005379994869664577, Validation Loss: 0.00753724951372143\n",
      "Epoch [8681/20000], Training Loss: 0.007514886650564482, Validation Loss: 0.0031048223630542843\n",
      "Epoch [8682/20000], Training Loss: 0.0073883920904336264, Validation Loss: 0.0040911985291001785\n",
      "Epoch [8683/20000], Training Loss: 0.006218494931610101, Validation Loss: 0.0027465119620891138\n",
      "Epoch [8684/20000], Training Loss: 0.00623722598538734, Validation Loss: 0.004407385295702885\n",
      "Epoch [8685/20000], Training Loss: 0.008065030265632751, Validation Loss: 0.002657249358045582\n",
      "Epoch [8686/20000], Training Loss: 0.017760107609027336, Validation Loss: 0.006782621640143459\n",
      "Epoch [8687/20000], Training Loss: 0.0070405227628985555, Validation Loss: 0.014130796601885922\n",
      "Epoch [8688/20000], Training Loss: 0.009038919785650381, Validation Loss: 0.005159061186012488\n",
      "Epoch [8689/20000], Training Loss: 0.013774270927699814, Validation Loss: 0.003348098567844967\n",
      "Epoch [8690/20000], Training Loss: 0.010755147981399205, Validation Loss: 0.03739864796716574\n",
      "Epoch [8691/20000], Training Loss: 0.030600714127233784, Validation Loss: 0.049475149707567265\n",
      "Epoch [8692/20000], Training Loss: 0.022359186426651183, Validation Loss: 0.01426020030521613\n",
      "Epoch [8693/20000], Training Loss: 0.01225748493015999, Validation Loss: 0.0050381921856309676\n",
      "Epoch [8694/20000], Training Loss: 0.0066833442742202055, Validation Loss: 0.009886607280220363\n",
      "Epoch [8695/20000], Training Loss: 0.01084067512783804, Validation Loss: 0.006503443860635473\n",
      "Epoch [8696/20000], Training Loss: 0.004845236937269094, Validation Loss: 0.006399338144936857\n",
      "Epoch [8697/20000], Training Loss: 0.00883637498820836, Validation Loss: 0.00475077835751238\n",
      "Epoch [8698/20000], Training Loss: 0.0055949744481560105, Validation Loss: 0.0038560471508130184\n",
      "Epoch [8699/20000], Training Loss: 0.003607693057607061, Validation Loss: 0.0034602456003735176\n",
      "Epoch [8700/20000], Training Loss: 0.0037301017372166306, Validation Loss: 0.00765916395468074\n",
      "Epoch [8701/20000], Training Loss: 0.020408472334565886, Validation Loss: 0.004113238194414797\n",
      "Epoch [8702/20000], Training Loss: 0.0150776838127058, Validation Loss: 0.00608075207771565\n",
      "Epoch [8703/20000], Training Loss: 0.013522294137146673, Validation Loss: 0.009837397347057828\n",
      "Epoch [8704/20000], Training Loss: 0.010678210532725123, Validation Loss: 0.019137038908931054\n",
      "Epoch [8705/20000], Training Loss: 0.011348817928852182, Validation Loss: 0.004944513899447754\n",
      "Epoch [8706/20000], Training Loss: 0.01512656720686729, Validation Loss: 0.005418433205639823\n",
      "Epoch [8707/20000], Training Loss: 0.01416086427135659, Validation Loss: 0.006456004977800538\n",
      "Epoch [8708/20000], Training Loss: 0.012967334133073953, Validation Loss: 0.024698864685758184\n",
      "Epoch [8709/20000], Training Loss: 0.02741549898096959, Validation Loss: 0.03608213957133038\n",
      "Epoch [8710/20000], Training Loss: 0.03173828974787796, Validation Loss: 0.005967662412266301\n",
      "Epoch [8711/20000], Training Loss: 0.007747493617768798, Validation Loss: 0.01756490720254078\n",
      "Epoch [8712/20000], Training Loss: 0.007806149668925043, Validation Loss: 0.005543356692084801\n",
      "Epoch [8713/20000], Training Loss: 0.007292979355302772, Validation Loss: 0.0075521832346307095\n",
      "Epoch [8714/20000], Training Loss: 0.011173046188819822, Validation Loss: 0.004015100606661041\n",
      "Epoch [8715/20000], Training Loss: 0.009235412883364396, Validation Loss: 0.013917375459123558\n",
      "Epoch [8716/20000], Training Loss: 0.013430971361231059, Validation Loss: 0.038728668301752735\n",
      "Epoch [8717/20000], Training Loss: 0.026237239855098387, Validation Loss: 0.014678116363842975\n",
      "Epoch [8718/20000], Training Loss: 0.010226892901950382, Validation Loss: 0.004659536905949478\n",
      "Epoch [8719/20000], Training Loss: 0.006343734887195751, Validation Loss: 0.0048279449844942845\n",
      "Epoch [8720/20000], Training Loss: 0.01075941990555813, Validation Loss: 0.0148202823853215\n",
      "Epoch [8721/20000], Training Loss: 0.014273313377543673, Validation Loss: 0.03703233120696887\n",
      "Epoch [8722/20000], Training Loss: 0.034464605672318224, Validation Loss: 0.04183269145294487\n",
      "Epoch [8723/20000], Training Loss: 0.018092934749282513, Validation Loss: 0.011070609430340994\n",
      "Epoch [8724/20000], Training Loss: 0.008971058771879013, Validation Loss: 0.006879471967295103\n",
      "Epoch [8725/20000], Training Loss: 0.012596780535074816, Validation Loss: 0.007484424279677374\n",
      "Epoch [8726/20000], Training Loss: 0.006168992449862084, Validation Loss: 0.004324492940148568\n",
      "Epoch [8727/20000], Training Loss: 0.0071549709604628985, Validation Loss: 0.007935112093231769\n",
      "Epoch [8728/20000], Training Loss: 0.007216182380424081, Validation Loss: 0.005316595330376523\n",
      "Epoch [8729/20000], Training Loss: 0.007501357625837305, Validation Loss: 0.007924495478099354\n",
      "Epoch [8730/20000], Training Loss: 0.012997086462356882, Validation Loss: 0.006520634569337095\n",
      "Epoch [8731/20000], Training Loss: 0.00704834816289284, Validation Loss: 0.006909564853620915\n",
      "Epoch [8732/20000], Training Loss: 0.008906033247188003, Validation Loss: 0.018313217184224375\n",
      "Epoch [8733/20000], Training Loss: 0.012032020398432255, Validation Loss: 0.004775581309136864\n",
      "Epoch [8734/20000], Training Loss: 0.0054986272317754424, Validation Loss: 0.003962126566355957\n",
      "Epoch [8735/20000], Training Loss: 0.006520515729139983, Validation Loss: 0.006657597838008249\n",
      "Epoch [8736/20000], Training Loss: 0.008205188159731083, Validation Loss: 0.010349438549272552\n",
      "Epoch [8737/20000], Training Loss: 0.010192316193881976, Validation Loss: 0.005299113777961598\n",
      "Epoch [8738/20000], Training Loss: 0.041064919325955476, Validation Loss: 0.01738128656483833\n",
      "Epoch [8739/20000], Training Loss: 0.04923311985320262, Validation Loss: 0.01556058523406786\n",
      "Epoch [8740/20000], Training Loss: 0.023262222314120402, Validation Loss: 0.07492421598505773\n",
      "Epoch [8741/20000], Training Loss: 0.050289894545650374, Validation Loss: 0.020986359839558384\n",
      "Epoch [8742/20000], Training Loss: 0.028939098585397005, Validation Loss: 0.014477935468627665\n",
      "Epoch [8743/20000], Training Loss: 0.014141596250868003, Validation Loss: 0.008285332878585905\n",
      "Epoch [8744/20000], Training Loss: 0.01034544880634972, Validation Loss: 0.007407911045967401\n",
      "Epoch [8745/20000], Training Loss: 0.009373254957608879, Validation Loss: 0.00905852746083318\n",
      "Epoch [8746/20000], Training Loss: 0.007708673614875546, Validation Loss: 0.005821754471721111\n",
      "Epoch [8747/20000], Training Loss: 0.006933833505692226, Validation Loss: 0.0051103881991626365\n",
      "Epoch [8748/20000], Training Loss: 0.005540545081852802, Validation Loss: 0.007566382902950863\n",
      "Epoch [8749/20000], Training Loss: 0.006958149207223739, Validation Loss: 0.005148171563372931\n",
      "Epoch [8750/20000], Training Loss: 0.005348359983015273, Validation Loss: 0.004006597529431539\n",
      "Epoch [8751/20000], Training Loss: 0.006369948441195967, Validation Loss: 0.0047452908102059155\n",
      "Epoch [8752/20000], Training Loss: 0.006151652300364471, Validation Loss: 0.004600162769877768\n",
      "Epoch [8753/20000], Training Loss: 0.005970244258475889, Validation Loss: 0.007056209602325225\n",
      "Epoch [8754/20000], Training Loss: 0.005303464397521955, Validation Loss: 0.003974633524064432\n",
      "Epoch [8755/20000], Training Loss: 0.006024498916563711, Validation Loss: 0.005039518978524679\n",
      "Epoch [8756/20000], Training Loss: 0.007805706160638692, Validation Loss: 0.003928651395264231\n",
      "Epoch [8757/20000], Training Loss: 0.006634918742617758, Validation Loss: 0.009551673919104926\n",
      "Epoch [8758/20000], Training Loss: 0.010237937962561514, Validation Loss: 0.004229820648106397\n",
      "Epoch [8759/20000], Training Loss: 0.010225050541664147, Validation Loss: 0.004453010749135241\n",
      "Epoch [8760/20000], Training Loss: 0.0075428298836673745, Validation Loss: 0.004067032297679621\n",
      "Epoch [8761/20000], Training Loss: 0.012973298362599703, Validation Loss: 0.006472915200387825\n",
      "Epoch [8762/20000], Training Loss: 0.01134213733166689, Validation Loss: 0.010513805848598734\n",
      "Epoch [8763/20000], Training Loss: 0.005144463563088461, Validation Loss: 0.0031695559446883897\n",
      "Epoch [8764/20000], Training Loss: 0.008426237230586204, Validation Loss: 0.003258238484119959\n",
      "Epoch [8765/20000], Training Loss: 0.012138991788690743, Validation Loss: 0.008232324522720904\n",
      "Epoch [8766/20000], Training Loss: 0.008687206935844318, Validation Loss: 0.027118332468879607\n",
      "Epoch [8767/20000], Training Loss: 0.01431580399262852, Validation Loss: 0.013963146676815086\n",
      "Epoch [8768/20000], Training Loss: 0.02859141098971512, Validation Loss: 0.05562701345011841\n",
      "Epoch [8769/20000], Training Loss: 0.056848854438190495, Validation Loss: 0.06575049931000519\n",
      "Epoch [8770/20000], Training Loss: 0.022131654266559053, Validation Loss: 0.031781071648635716\n",
      "Epoch [8771/20000], Training Loss: 0.013189517772324117, Validation Loss: 0.006523369753397544\n",
      "Epoch [8772/20000], Training Loss: 0.00827925319442459, Validation Loss: 0.005072019746096755\n",
      "Epoch [8773/20000], Training Loss: 0.007053309857188391, Validation Loss: 0.007563065281439256\n",
      "Epoch [8774/20000], Training Loss: 0.00826213211153767, Validation Loss: 0.0045328793003136525\n",
      "Epoch [8775/20000], Training Loss: 0.009235569022296528, Validation Loss: 0.00926175399505423\n",
      "Epoch [8776/20000], Training Loss: 0.010558399649328618, Validation Loss: 0.009498080860404505\n",
      "Epoch [8777/20000], Training Loss: 0.011223450926731207, Validation Loss: 0.007194480037355788\n",
      "Epoch [8778/20000], Training Loss: 0.01864737695203595, Validation Loss: 0.004308999336314757\n",
      "Epoch [8779/20000], Training Loss: 0.006452462933598976, Validation Loss: 0.004565100356103358\n",
      "Epoch [8780/20000], Training Loss: 0.006150968771960054, Validation Loss: 0.0063622653526229965\n",
      "Epoch [8781/20000], Training Loss: 0.008379510279545295, Validation Loss: 0.004074143002201579\n",
      "Epoch [8782/20000], Training Loss: 0.004683738728220176, Validation Loss: 0.008548968069960543\n",
      "Epoch [8783/20000], Training Loss: 0.0081189005203279, Validation Loss: 0.003953244989052759\n",
      "Epoch [8784/20000], Training Loss: 0.009743038531658905, Validation Loss: 0.003973360974863941\n",
      "Epoch [8785/20000], Training Loss: 0.007940118816415114, Validation Loss: 0.0041258162446605896\n",
      "Epoch [8786/20000], Training Loss: 0.009990908072463103, Validation Loss: 0.008244195888827173\n",
      "Epoch [8787/20000], Training Loss: 0.01301299306630556, Validation Loss: 0.01745762990568827\n",
      "Epoch [8788/20000], Training Loss: 0.018296029152614728, Validation Loss: 0.006505245218639695\n",
      "Epoch [8789/20000], Training Loss: 0.03534866286483813, Validation Loss: 0.1377050391802186\n",
      "Epoch [8790/20000], Training Loss: 0.08184015576262027, Validation Loss: 0.021460359323620132\n",
      "Epoch [8791/20000], Training Loss: 0.04250116998861943, Validation Loss: 0.06411817028449766\n",
      "Epoch [8792/20000], Training Loss: 0.016174903721548617, Validation Loss: 0.023300078341500403\n",
      "Epoch [8793/20000], Training Loss: 0.015093064634129405, Validation Loss: 0.009267526576617715\n",
      "Epoch [8794/20000], Training Loss: 0.008742406980932824, Validation Loss: 0.00850231553379542\n",
      "Epoch [8795/20000], Training Loss: 0.00883439792752532, Validation Loss: 0.006567622473344922\n",
      "Epoch [8796/20000], Training Loss: 0.008034932300574203, Validation Loss: 0.005214156677019284\n",
      "Epoch [8797/20000], Training Loss: 0.007566594003167536, Validation Loss: 0.005538305039668298\n",
      "Epoch [8798/20000], Training Loss: 0.008771609013950053, Validation Loss: 0.00693566283851916\n",
      "Epoch [8799/20000], Training Loss: 0.007125278124086825, Validation Loss: 0.0054272726558467965\n",
      "Epoch [8800/20000], Training Loss: 0.007539664070333986, Validation Loss: 0.0067688330280881405\n",
      "Epoch [8801/20000], Training Loss: 0.011242807242004866, Validation Loss: 0.005057162279626937\n",
      "Epoch [8802/20000], Training Loss: 0.010547808092919045, Validation Loss: 0.006054373757121668\n",
      "Epoch [8803/20000], Training Loss: 0.011404735032036635, Validation Loss: 0.014063706659808597\n",
      "Epoch [8804/20000], Training Loss: 0.0056997272352288875, Validation Loss: 0.004388840360011922\n",
      "Epoch [8805/20000], Training Loss: 0.006629006101450484, Validation Loss: 0.005039616004718742\n",
      "Epoch [8806/20000], Training Loss: 0.0061385065076007906, Validation Loss: 0.006085919377515633\n",
      "Epoch [8807/20000], Training Loss: 0.00509882063904245, Validation Loss: 0.006338904097526006\n",
      "Epoch [8808/20000], Training Loss: 0.0067653771124501615, Validation Loss: 0.004844452748036687\n",
      "Epoch [8809/20000], Training Loss: 0.008703801315277815, Validation Loss: 0.003786198538512459\n",
      "Epoch [8810/20000], Training Loss: 0.006707186298236982, Validation Loss: 0.007769142579555981\n",
      "Epoch [8811/20000], Training Loss: 0.019033020760356782, Validation Loss: 0.017643047497797073\n",
      "Epoch [8812/20000], Training Loss: 0.01733238887339082, Validation Loss: 0.010990092232532334\n",
      "Epoch [8813/20000], Training Loss: 0.026186783499724697, Validation Loss: 0.00888798859633815\n",
      "Epoch [8814/20000], Training Loss: 0.04848034682007502, Validation Loss: 0.015549225569081155\n",
      "Epoch [8815/20000], Training Loss: 0.01944796655873558, Validation Loss: 0.014297527200689981\n",
      "Epoch [8816/20000], Training Loss: 0.013230824132084049, Validation Loss: 0.014038634926772962\n",
      "Epoch [8817/20000], Training Loss: 0.008049999154796492, Validation Loss: 0.009106266678177885\n",
      "Epoch [8818/20000], Training Loss: 0.010543082487337025, Validation Loss: 0.005755058862921\n",
      "Epoch [8819/20000], Training Loss: 0.007809503239680485, Validation Loss: 0.0073830758450250345\n",
      "Epoch [8820/20000], Training Loss: 0.011957607722641634, Validation Loss: 0.004446153735508622\n",
      "Epoch [8821/20000], Training Loss: 0.013551924780975761, Validation Loss: 0.005970123558548883\n",
      "Epoch [8822/20000], Training Loss: 0.008145294148042532, Validation Loss: 0.013343537976132731\n",
      "Epoch [8823/20000], Training Loss: 0.0194100905752878, Validation Loss: 0.015771340831049332\n",
      "Epoch [8824/20000], Training Loss: 0.014763049049569028, Validation Loss: 0.008853191205238852\n",
      "Epoch [8825/20000], Training Loss: 0.015597788026492967, Validation Loss: 0.026052838457639647\n",
      "Epoch [8826/20000], Training Loss: 0.010233798139129899, Validation Loss: 0.004585890053086976\n",
      "Epoch [8827/20000], Training Loss: 0.009272737632272765, Validation Loss: 0.0049059736680387035\n",
      "Epoch [8828/20000], Training Loss: 0.00724382179656199, Validation Loss: 0.00464384899755491\n",
      "Epoch [8829/20000], Training Loss: 0.0067190821448873195, Validation Loss: 0.004020225102067343\n",
      "Epoch [8830/20000], Training Loss: 0.006097123092851169, Validation Loss: 0.008185781301789753\n",
      "Epoch [8831/20000], Training Loss: 0.006482055402427379, Validation Loss: 0.006802184351922084\n",
      "Epoch [8832/20000], Training Loss: 0.007335265147828197, Validation Loss: 0.007353251484021582\n",
      "Epoch [8833/20000], Training Loss: 0.00884718558102447, Validation Loss: 0.006405520221860772\n",
      "Epoch [8834/20000], Training Loss: 0.006221386503187075, Validation Loss: 0.003360081250109559\n",
      "Epoch [8835/20000], Training Loss: 0.009331865055303621, Validation Loss: 0.003818448324661589\n",
      "Epoch [8836/20000], Training Loss: 0.005707879999785551, Validation Loss: 0.0036843601720678416\n",
      "Epoch [8837/20000], Training Loss: 0.007187958382668772, Validation Loss: 0.003046887386574707\n",
      "Epoch [8838/20000], Training Loss: 0.007978148484003864, Validation Loss: 0.0034697257036297685\n",
      "Epoch [8839/20000], Training Loss: 0.005580580992240617, Validation Loss: 0.010613188546738947\n",
      "Epoch [8840/20000], Training Loss: 0.00792192532831645, Validation Loss: 0.005933039313872247\n",
      "Epoch [8841/20000], Training Loss: 0.01532370755843918, Validation Loss: 0.007256356693035757\n",
      "Epoch [8842/20000], Training Loss: 0.017392309695424046, Validation Loss: 0.005251976901346441\n",
      "Epoch [8843/20000], Training Loss: 0.007795481546054361, Validation Loss: 0.006490566844182207\n",
      "Epoch [8844/20000], Training Loss: 0.010309183005509632, Validation Loss: 0.006469497298641816\n",
      "Epoch [8845/20000], Training Loss: 0.016611538046812972, Validation Loss: 0.008891797354416095\n",
      "Epoch [8846/20000], Training Loss: 0.019621125702022773, Validation Loss: 0.0035526814230685743\n",
      "Epoch [8847/20000], Training Loss: 0.010691899930991764, Validation Loss: 0.012632891539410804\n",
      "Epoch [8848/20000], Training Loss: 0.009473589066016887, Validation Loss: 0.006099501262086003\n",
      "Epoch [8849/20000], Training Loss: 0.010791501816129312, Validation Loss: 0.0033370703623469516\n",
      "Epoch [8850/20000], Training Loss: 0.008197222002698774, Validation Loss: 0.005642569643215316\n",
      "Epoch [8851/20000], Training Loss: 0.007545896777807164, Validation Loss: 0.0034556173124484146\n",
      "Epoch [8852/20000], Training Loss: 0.004142334370823976, Validation Loss: 0.0029873044188309145\n",
      "Epoch [8853/20000], Training Loss: 0.0063112062406227255, Validation Loss: 0.02226736100549585\n",
      "Epoch [8854/20000], Training Loss: 0.010042831593636947, Validation Loss: 0.0057333395184147404\n",
      "Epoch [8855/20000], Training Loss: 0.009704093450896576, Validation Loss: 0.017426645383243098\n",
      "Epoch [8856/20000], Training Loss: 0.008641126774656576, Validation Loss: 0.00438300663416223\n",
      "Epoch [8857/20000], Training Loss: 0.02353503994527273, Validation Loss: 0.02279307320713997\n",
      "Epoch [8858/20000], Training Loss: 0.010751630736714495, Validation Loss: 0.004825700544520104\n",
      "Epoch [8859/20000], Training Loss: 0.006259584722491647, Validation Loss: 0.005167349239390059\n",
      "Epoch [8860/20000], Training Loss: 0.005431861242478979, Validation Loss: 0.012592883647601938\n",
      "Epoch [8861/20000], Training Loss: 0.01478931108483396, Validation Loss: 0.011586878714831272\n",
      "Epoch [8862/20000], Training Loss: 0.01823076751679764, Validation Loss: 0.0043550184898896015\n",
      "Epoch [8863/20000], Training Loss: 0.00711407244203396, Validation Loss: 0.006087167652862847\n",
      "Epoch [8864/20000], Training Loss: 0.00641832816992454, Validation Loss: 0.006058557890896615\n",
      "Epoch [8865/20000], Training Loss: 0.005261468701064587, Validation Loss: 0.005590394281472329\n",
      "Epoch [8866/20000], Training Loss: 0.008064846479880674, Validation Loss: 0.005713980190104654\n",
      "Epoch [8867/20000], Training Loss: 0.010141751027601589, Validation Loss: 0.004925599305587574\n",
      "Epoch [8868/20000], Training Loss: 0.007173913894023697, Validation Loss: 0.004809186462876342\n",
      "Epoch [8869/20000], Training Loss: 0.007324704608306222, Validation Loss: 0.013436536705715624\n",
      "Epoch [8870/20000], Training Loss: 0.012049552570325821, Validation Loss: 0.022215607443026458\n",
      "Epoch [8871/20000], Training Loss: 0.012532280271573524, Validation Loss: 0.006648437137014814\n",
      "Epoch [8872/20000], Training Loss: 0.007043719368604278, Validation Loss: 0.004111781228235348\n",
      "Epoch [8873/20000], Training Loss: 0.024873946081372975, Validation Loss: 0.010609780867371552\n",
      "Epoch [8874/20000], Training Loss: 0.011353421108131962, Validation Loss: 0.007013138392028598\n",
      "Epoch [8875/20000], Training Loss: 0.009227243300171852, Validation Loss: 0.003962441118625902\n",
      "Epoch [8876/20000], Training Loss: 0.005936971503875351, Validation Loss: 0.008813059755833672\n",
      "Epoch [8877/20000], Training Loss: 0.006843956330807747, Validation Loss: 0.004565325898564966\n",
      "Epoch [8878/20000], Training Loss: 0.0043824769776879945, Validation Loss: 0.015056950025512248\n",
      "Epoch [8879/20000], Training Loss: 0.01632869767284514, Validation Loss: 0.0058646349812518294\n",
      "Epoch [8880/20000], Training Loss: 0.006524193229519629, Validation Loss: 0.004548750620416657\n",
      "Epoch [8881/20000], Training Loss: 0.007057870823375977, Validation Loss: 0.0036618532985968314\n",
      "Epoch [8882/20000], Training Loss: 0.007988596229617022, Validation Loss: 0.007344835211207672\n",
      "Epoch [8883/20000], Training Loss: 0.008518680389637925, Validation Loss: 0.009891165606561003\n",
      "Epoch [8884/20000], Training Loss: 0.016249596559126594, Validation Loss: 0.01078994576609771\n",
      "Epoch [8885/20000], Training Loss: 0.019028361545029578, Validation Loss: 0.007604797109959737\n",
      "Epoch [8886/20000], Training Loss: 0.006383020990272469, Validation Loss: 0.011101423363602746\n",
      "Epoch [8887/20000], Training Loss: 0.006984859157975214, Validation Loss: 0.0050336048675907455\n",
      "Epoch [8888/20000], Training Loss: 0.007666498231368938, Validation Loss: 0.007710408016078375\n",
      "Epoch [8889/20000], Training Loss: 0.007861720817995124, Validation Loss: 0.008682731007170782\n",
      "Epoch [8890/20000], Training Loss: 0.013914000931021877, Validation Loss: 0.009229541357076666\n",
      "Epoch [8891/20000], Training Loss: 0.02657659569292394, Validation Loss: 0.07821203129632134\n",
      "Epoch [8892/20000], Training Loss: 0.02575481325246593, Validation Loss: 0.005536940753283043\n",
      "Epoch [8893/20000], Training Loss: 0.015896204387640216, Validation Loss: 0.023779131472110752\n",
      "Epoch [8894/20000], Training Loss: 0.016272515752851695, Validation Loss: 0.013482800461978124\n",
      "Epoch [8895/20000], Training Loss: 0.012160223184504762, Validation Loss: 0.004100078276060721\n",
      "Epoch [8896/20000], Training Loss: 0.01897772038423032, Validation Loss: 0.005175615396366345\n",
      "Epoch [8897/20000], Training Loss: 0.01941601853260571, Validation Loss: 0.00871389702297165\n",
      "Epoch [8898/20000], Training Loss: 0.008739883658043774, Validation Loss: 0.010861506569218485\n",
      "Epoch [8899/20000], Training Loss: 0.009434909780143894, Validation Loss: 0.015280061599046999\n",
      "Epoch [8900/20000], Training Loss: 0.00832180615335736, Validation Loss: 0.00611925303151613\n",
      "Epoch [8901/20000], Training Loss: 0.005365540955348739, Validation Loss: 0.0037799996967160204\n",
      "Epoch [8902/20000], Training Loss: 0.004944750880125882, Validation Loss: 0.00586203898221011\n",
      "Epoch [8903/20000], Training Loss: 0.004696884099498025, Validation Loss: 0.003557235641954993\n",
      "Epoch [8904/20000], Training Loss: 0.006924899265868589, Validation Loss: 0.0032721674217942747\n",
      "Epoch [8905/20000], Training Loss: 0.005185783220245607, Validation Loss: 0.02212493121623993\n",
      "Epoch [8906/20000], Training Loss: 0.024934526731937825, Validation Loss: 0.03067462891366343\n",
      "Epoch [8907/20000], Training Loss: 0.009764927674301102, Validation Loss: 0.0074052290459013185\n",
      "Epoch [8908/20000], Training Loss: 0.007659459323414402, Validation Loss: 0.004296818576740564\n",
      "Epoch [8909/20000], Training Loss: 0.009143383387968893, Validation Loss: 0.004299249616038519\n",
      "Epoch [8910/20000], Training Loss: 0.008219581971258907, Validation Loss: 0.005977728404462307\n",
      "Epoch [8911/20000], Training Loss: 0.004856970827469402, Validation Loss: 0.0036267793419852007\n",
      "Epoch [8912/20000], Training Loss: 0.007948368902101979, Validation Loss: 0.0029434335575402315\n",
      "Epoch [8913/20000], Training Loss: 0.0071946186627818475, Validation Loss: 0.007159336750553231\n",
      "Epoch [8914/20000], Training Loss: 0.008402782768826, Validation Loss: 0.005719307261349442\n",
      "Epoch [8915/20000], Training Loss: 0.011035419478243316, Validation Loss: 0.009933596543437912\n",
      "Epoch [8916/20000], Training Loss: 0.008297095402797692, Validation Loss: 0.0033358847708013493\n",
      "Epoch [8917/20000], Training Loss: 0.00806503461093858, Validation Loss: 0.007195411383745279\n",
      "Epoch [8918/20000], Training Loss: 0.015500397406867705, Validation Loss: 0.012675869777532585\n",
      "Epoch [8919/20000], Training Loss: 0.011768161253712606, Validation Loss: 0.005863891161886848\n",
      "Epoch [8920/20000], Training Loss: 0.008568767204167216, Validation Loss: 0.004590465663506548\n",
      "Epoch [8921/20000], Training Loss: 0.012757767740757637, Validation Loss: 0.0054409177222213756\n",
      "Epoch [8922/20000], Training Loss: 0.006223881775180676, Validation Loss: 0.005487825040592761\n",
      "Epoch [8923/20000], Training Loss: 0.010022312301901235, Validation Loss: 0.003581558633075425\n",
      "Epoch [8924/20000], Training Loss: 0.005508033720616368, Validation Loss: 0.00575460388795377\n",
      "Epoch [8925/20000], Training Loss: 0.0062582853882174405, Validation Loss: 0.002521499444102166\n",
      "Epoch [8926/20000], Training Loss: 0.012441052336985845, Validation Loss: 0.028136481152874433\n",
      "Epoch [8927/20000], Training Loss: 0.01890496205079931, Validation Loss: 0.04319311238123191\n",
      "Epoch [8928/20000], Training Loss: 0.03211762886660706, Validation Loss: 0.12515201838585557\n",
      "Epoch [8929/20000], Training Loss: 0.0552922172604927, Validation Loss: 0.013889864394773863\n",
      "Epoch [8930/20000], Training Loss: 0.028206568129982252, Validation Loss: 0.027240114349542526\n",
      "Epoch [8931/20000], Training Loss: 0.03308491124854689, Validation Loss: 0.02214300526509707\n",
      "Epoch [8932/20000], Training Loss: 0.017337410469605987, Validation Loss: 0.006780061532396139\n",
      "Epoch [8933/20000], Training Loss: 0.006644837882569326, Validation Loss: 0.005345134093652908\n",
      "Epoch [8934/20000], Training Loss: 0.005837406618021694, Validation Loss: 0.005795619995061381\n",
      "Epoch [8935/20000], Training Loss: 0.009244340968897242, Validation Loss: 0.0042229153461467005\n",
      "Epoch [8936/20000], Training Loss: 0.00499429372783717, Validation Loss: 0.004998672086093376\n",
      "Epoch [8937/20000], Training Loss: 0.009117623886725466, Validation Loss: 0.004435709681209711\n",
      "Epoch [8938/20000], Training Loss: 0.005781474131310915, Validation Loss: 0.0036867688486136884\n",
      "Epoch [8939/20000], Training Loss: 0.008236548783315811, Validation Loss: 0.0036089713999145545\n",
      "Epoch [8940/20000], Training Loss: 0.01328387689344319, Validation Loss: 0.005148897113888054\n",
      "Epoch [8941/20000], Training Loss: 0.007474987752045438, Validation Loss: 0.00651393591294363\n",
      "Epoch [8942/20000], Training Loss: 0.006066250278049016, Validation Loss: 0.010855455640986682\n",
      "Epoch [8943/20000], Training Loss: 0.009384033333065287, Validation Loss: 0.0034870440144347367\n",
      "Epoch [8944/20000], Training Loss: 0.005593814496283552, Validation Loss: 0.011702644505671307\n",
      "Epoch [8945/20000], Training Loss: 0.010594211286453563, Validation Loss: 0.012692116600068979\n",
      "Epoch [8946/20000], Training Loss: 0.022960759755083666, Validation Loss: 0.007359264534898159\n",
      "Epoch [8947/20000], Training Loss: 0.030590344018492033, Validation Loss: 0.01084493473178734\n",
      "Epoch [8948/20000], Training Loss: 0.040120565176558945, Validation Loss: 0.011819134274737004\n",
      "Epoch [8949/20000], Training Loss: 0.05226640291532801, Validation Loss: 0.0190612195941802\n",
      "Epoch [8950/20000], Training Loss: 0.022656693291220824, Validation Loss: 0.013178026148774993\n",
      "Epoch [8951/20000], Training Loss: 0.02268483249853099, Validation Loss: 0.010111869649108485\n",
      "Epoch [8952/20000], Training Loss: 0.014355782635553689, Validation Loss: 0.025502513029758735\n",
      "Epoch [8953/20000], Training Loss: 0.014604282365845782, Validation Loss: 0.008323377644008482\n",
      "Epoch [8954/20000], Training Loss: 0.011368902824480236, Validation Loss: 0.006211900185936615\n",
      "Epoch [8955/20000], Training Loss: 0.005834821755083145, Validation Loss: 0.006066708679339973\n",
      "Epoch [8956/20000], Training Loss: 0.006697204704063812, Validation Loss: 0.005893585699579156\n",
      "Epoch [8957/20000], Training Loss: 0.005154708092699626, Validation Loss: 0.004705316824894469\n",
      "Epoch [8958/20000], Training Loss: 0.0052605138189392164, Validation Loss: 0.006799000583961418\n",
      "Epoch [8959/20000], Training Loss: 0.005696036821453683, Validation Loss: 0.00510904201767579\n",
      "Epoch [8960/20000], Training Loss: 0.00621746271333125, Validation Loss: 0.005603426068810892\n",
      "Epoch [8961/20000], Training Loss: 0.006901807997240082, Validation Loss: 0.00444425922675473\n",
      "Epoch [8962/20000], Training Loss: 0.007874076473070198, Validation Loss: 0.004312646866539792\n",
      "Epoch [8963/20000], Training Loss: 0.02176976842539651, Validation Loss: 0.009598072341334378\n",
      "Epoch [8964/20000], Training Loss: 0.012443203562871272, Validation Loss: 0.011073833264951316\n",
      "Epoch [8965/20000], Training Loss: 0.007038342883010021, Validation Loss: 0.005144573239797687\n",
      "Epoch [8966/20000], Training Loss: 0.007463330531858706, Validation Loss: 0.00675589391825659\n",
      "Epoch [8967/20000], Training Loss: 0.008242524819382067, Validation Loss: 0.003938974566718764\n",
      "Epoch [8968/20000], Training Loss: 0.008188784411946213, Validation Loss: 0.004956772019124975\n",
      "Epoch [8969/20000], Training Loss: 0.005985027204068112, Validation Loss: 0.004924541273207693\n",
      "Epoch [8970/20000], Training Loss: 0.004558359140251663, Validation Loss: 0.013667434531267222\n",
      "Epoch [8971/20000], Training Loss: 0.00980581131339672, Validation Loss: 0.0039070232808233836\n",
      "Epoch [8972/20000], Training Loss: 0.005084158866541851, Validation Loss: 0.01600483006664775\n",
      "Epoch [8973/20000], Training Loss: 0.00997516719611927, Validation Loss: 0.005405024592105885\n",
      "Epoch [8974/20000], Training Loss: 0.0055132606357801706, Validation Loss: 0.006869780966766179\n",
      "Epoch [8975/20000], Training Loss: 0.007951320638312609, Validation Loss: 0.004791076561209399\n",
      "Epoch [8976/20000], Training Loss: 0.011637588378761098, Validation Loss: 0.008131694429587404\n",
      "Epoch [8977/20000], Training Loss: 0.014129380886775575, Validation Loss: 0.0036311090043454897\n",
      "Epoch [8978/20000], Training Loss: 0.019977019499232744, Validation Loss: 0.004310552226213206\n",
      "Epoch [8979/20000], Training Loss: 0.015915145956179395, Validation Loss: 0.02311958699297598\n",
      "Epoch [8980/20000], Training Loss: 0.025994931256198988, Validation Loss: 0.021481906197940197\n",
      "Epoch [8981/20000], Training Loss: 0.022262395928610852, Validation Loss: 0.008159751549620913\n",
      "Epoch [8982/20000], Training Loss: 0.008640712867158331, Validation Loss: 0.005393040376865039\n",
      "Epoch [8983/20000], Training Loss: 0.012880698076125035, Validation Loss: 0.0061960156552751345\n",
      "Epoch [8984/20000], Training Loss: 0.018886457226471975, Validation Loss: 0.013435434329039708\n",
      "Epoch [8985/20000], Training Loss: 0.009285276942786627, Validation Loss: 0.015675343843603123\n",
      "Epoch [8986/20000], Training Loss: 0.01223871938418597, Validation Loss: 0.005936878598211999\n",
      "Epoch [8987/20000], Training Loss: 0.005667002795754732, Validation Loss: 0.004583593229004431\n",
      "Epoch [8988/20000], Training Loss: 0.005121197950627122, Validation Loss: 0.0038369534704543184\n",
      "Epoch [8989/20000], Training Loss: 0.006947430028438768, Validation Loss: 0.005507587354389086\n",
      "Epoch [8990/20000], Training Loss: 0.006260973318213863, Validation Loss: 0.004032034902463367\n",
      "Epoch [8991/20000], Training Loss: 0.012259829174061971, Validation Loss: 0.004067541787922632\n",
      "Epoch [8992/20000], Training Loss: 0.007289385568583384, Validation Loss: 0.008509174418394747\n",
      "Epoch [8993/20000], Training Loss: 0.009476169798290357, Validation Loss: 0.015660864109067165\n",
      "Epoch [8994/20000], Training Loss: 0.010620188837687497, Validation Loss: 0.004664520309233922\n",
      "Epoch [8995/20000], Training Loss: 0.020204472891886587, Validation Loss: 0.008333287954883417\n",
      "Epoch [8996/20000], Training Loss: 0.02891763226697159, Validation Loss: 0.018127482830095687\n",
      "Epoch [8997/20000], Training Loss: 0.01386799141667965, Validation Loss: 0.014096846224169226\n",
      "Epoch [8998/20000], Training Loss: 0.009563120215066842, Validation Loss: 0.005767011590380987\n",
      "Epoch [8999/20000], Training Loss: 0.007035615568124091, Validation Loss: 0.004064398061278293\n",
      "Epoch [9000/20000], Training Loss: 0.0053357713331934065, Validation Loss: 0.005571133166345784\n",
      "Epoch [9001/20000], Training Loss: 0.004378340756894821, Validation Loss: 0.0058027706180853655\n",
      "Epoch [9002/20000], Training Loss: 0.005371044469939079, Validation Loss: 0.012416182328576855\n",
      "Epoch [9003/20000], Training Loss: 0.013773182313473496, Validation Loss: 0.004010378078549236\n",
      "Epoch [9004/20000], Training Loss: 0.011318657254118339, Validation Loss: 0.005167383102161693\n",
      "Epoch [9005/20000], Training Loss: 0.04359159774321597, Validation Loss: 0.02323420624899037\n",
      "Epoch [9006/20000], Training Loss: 0.022696211744914763, Validation Loss: 0.05166111691530111\n",
      "Epoch [9007/20000], Training Loss: 0.017560807726502908, Validation Loss: 0.02603736905915033\n",
      "Epoch [9008/20000], Training Loss: 0.02109031479631085, Validation Loss: 0.011168151675574833\n",
      "Epoch [9009/20000], Training Loss: 0.010799950389939892, Validation Loss: 0.004747287833587117\n",
      "Epoch [9010/20000], Training Loss: 0.005138127478338512, Validation Loss: 0.0068610784301768035\n",
      "Epoch [9011/20000], Training Loss: 0.0048680883789659546, Validation Loss: 0.003922457231185815\n",
      "Epoch [9012/20000], Training Loss: 0.0069632850770306376, Validation Loss: 0.003933469030536279\n",
      "Epoch [9013/20000], Training Loss: 0.006636194817214606, Validation Loss: 0.004046889885363011\n",
      "Epoch [9014/20000], Training Loss: 0.008522528055306924, Validation Loss: 0.0037994004787183905\n",
      "Epoch [9015/20000], Training Loss: 0.014530109000458782, Validation Loss: 0.0050360623828932406\n",
      "Epoch [9016/20000], Training Loss: 0.0264687682762477, Validation Loss: 0.09906588155666084\n",
      "Epoch [9017/20000], Training Loss: 0.036886415510837524, Validation Loss: 0.008647870917554559\n",
      "Epoch [9018/20000], Training Loss: 0.008641573912297775, Validation Loss: 0.011408643963302116\n",
      "Epoch [9019/20000], Training Loss: 0.00984577066536271, Validation Loss: 0.004913203877994273\n",
      "Epoch [9020/20000], Training Loss: 0.005941069981905197, Validation Loss: 0.005142629484063426\n",
      "Epoch [9021/20000], Training Loss: 0.006422123048521046, Validation Loss: 0.0049641808827597545\n",
      "Epoch [9022/20000], Training Loss: 0.011173356528161094, Validation Loss: 0.013822018035821353\n",
      "Epoch [9023/20000], Training Loss: 0.008361650034203194, Validation Loss: 0.007194363450346202\n",
      "Epoch [9024/20000], Training Loss: 0.008105664481783086, Validation Loss: 0.003463422353105372\n",
      "Epoch [9025/20000], Training Loss: 0.004741627534843117, Validation Loss: 0.0028466200453668273\n",
      "Epoch [9026/20000], Training Loss: 0.005229001017661565, Validation Loss: 0.006498904236671915\n",
      "Epoch [9027/20000], Training Loss: 0.00686187147896687, Validation Loss: 0.007116801504579302\n",
      "Epoch [9028/20000], Training Loss: 0.010217871603734758, Validation Loss: 0.02527475228559961\n",
      "Epoch [9029/20000], Training Loss: 0.021581909361494973, Validation Loss: 0.029211177765017027\n",
      "Epoch [9030/20000], Training Loss: 0.03515768202383437, Validation Loss: 0.040546469232780415\n",
      "Epoch [9031/20000], Training Loss: 0.0244914786735535, Validation Loss: 0.03320221513926062\n",
      "Epoch [9032/20000], Training Loss: 0.018174300614711165, Validation Loss: 0.004079290115086184\n",
      "Epoch [9033/20000], Training Loss: 0.012809725937456409, Validation Loss: 0.005386142803732952\n",
      "Epoch [9034/20000], Training Loss: 0.007981236980672943, Validation Loss: 0.01595484731573171\n",
      "Epoch [9035/20000], Training Loss: 0.015085352890309878, Validation Loss: 0.006309412423532842\n",
      "Epoch [9036/20000], Training Loss: 0.005691391947922041, Validation Loss: 0.004271163131398811\n",
      "Epoch [9037/20000], Training Loss: 0.006139078261574988, Validation Loss: 0.004798030323006027\n",
      "Epoch [9038/20000], Training Loss: 0.008179615336952597, Validation Loss: 0.004490352984587632\n",
      "Epoch [9039/20000], Training Loss: 0.006644354618770697, Validation Loss: 0.00521152993029228\n",
      "Epoch [9040/20000], Training Loss: 0.006595689302060543, Validation Loss: 0.003928246402250249\n",
      "Epoch [9041/20000], Training Loss: 0.009953117134045897, Validation Loss: 0.011979011552674541\n",
      "Epoch [9042/20000], Training Loss: 0.006784319074150906, Validation Loss: 0.007000824238740483\n",
      "Epoch [9043/20000], Training Loss: 0.008895705273191976, Validation Loss: 0.008957010294713139\n",
      "Epoch [9044/20000], Training Loss: 0.012642353868354153, Validation Loss: 0.005229796583925998\n",
      "Epoch [9045/20000], Training Loss: 0.005813482020260251, Validation Loss: 0.006184555129534657\n",
      "Epoch [9046/20000], Training Loss: 0.009178029091864508, Validation Loss: 0.008355482082690447\n",
      "Epoch [9047/20000], Training Loss: 0.018731591770379703, Validation Loss: 0.005552329959918161\n",
      "Epoch [9048/20000], Training Loss: 0.02514805793433033, Validation Loss: 0.004842162823918924\n",
      "Epoch [9049/20000], Training Loss: 0.009110202153221638, Validation Loss: 0.004775149008458181\n",
      "Epoch [9050/20000], Training Loss: 0.008489087637696815, Validation Loss: 0.006595621169679784\n",
      "Epoch [9051/20000], Training Loss: 0.01126301788605945, Validation Loss: 0.004782582783070406\n",
      "Epoch [9052/20000], Training Loss: 0.010322413610992953, Validation Loss: 0.007241810597186002\n",
      "Epoch [9053/20000], Training Loss: 0.0067486580748144275, Validation Loss: 0.004788342459617313\n",
      "Epoch [9054/20000], Training Loss: 0.00522060103755706, Validation Loss: 0.004214988375653229\n",
      "Epoch [9055/20000], Training Loss: 0.0063801704142991056, Validation Loss: 0.008872055714719603\n",
      "Epoch [9056/20000], Training Loss: 0.0073841669273926, Validation Loss: 0.004125780410871095\n",
      "Epoch [9057/20000], Training Loss: 0.013511200827573908, Validation Loss: 0.00405114387216976\n",
      "Epoch [9058/20000], Training Loss: 0.006946158852770168, Validation Loss: 0.003375093372849136\n",
      "Epoch [9059/20000], Training Loss: 0.00816012016028773, Validation Loss: 0.004516269027594991\n",
      "Epoch [9060/20000], Training Loss: 0.008453760533666152, Validation Loss: 0.005603659973294357\n",
      "Epoch [9061/20000], Training Loss: 0.010042066251896489, Validation Loss: 0.005698770297049404\n",
      "Epoch [9062/20000], Training Loss: 0.005992694938417539, Validation Loss: 0.008644226506087893\n",
      "Epoch [9063/20000], Training Loss: 0.009786607036257269, Validation Loss: 0.005230873968527316\n",
      "Epoch [9064/20000], Training Loss: 0.01731129444747889, Validation Loss: 0.007483848032627709\n",
      "Epoch [9065/20000], Training Loss: 0.01809579513870371, Validation Loss: 0.03370352674807821\n",
      "Epoch [9066/20000], Training Loss: 0.01905760233057663, Validation Loss: 0.01206636540327389\n",
      "Epoch [9067/20000], Training Loss: 0.008996670547016297, Validation Loss: 0.004438339919714508\n",
      "Epoch [9068/20000], Training Loss: 0.005316881233219257, Validation Loss: 0.003867940946588964\n",
      "Epoch [9069/20000], Training Loss: 0.009539726760392244, Validation Loss: 0.014631132728287378\n",
      "Epoch [9070/20000], Training Loss: 0.012543756303947313, Validation Loss: 0.011415522479053817\n",
      "Epoch [9071/20000], Training Loss: 0.004613034341153772, Validation Loss: 0.003608294796561765\n",
      "Epoch [9072/20000], Training Loss: 0.005372862003595864, Validation Loss: 0.004462160592471699\n",
      "Epoch [9073/20000], Training Loss: 0.0053014521067130515, Validation Loss: 0.00325943141973793\n",
      "Epoch [9074/20000], Training Loss: 0.006528104631278049, Validation Loss: 0.0036013918874446193\n",
      "Epoch [9075/20000], Training Loss: 0.00674806830777795, Validation Loss: 0.0029942200566817783\n",
      "Epoch [9076/20000], Training Loss: 0.007309065826543052, Validation Loss: 0.004680501339514491\n",
      "Epoch [9077/20000], Training Loss: 0.011964312596975089, Validation Loss: 0.03815126878925871\n",
      "Epoch [9078/20000], Training Loss: 0.023478798216858974, Validation Loss: 0.014930706325500356\n",
      "Epoch [9079/20000], Training Loss: 0.011781182174502192, Validation Loss: 0.0040561236452522865\n",
      "Epoch [9080/20000], Training Loss: 0.0052628831491737015, Validation Loss: 0.006114141827757588\n",
      "Epoch [9081/20000], Training Loss: 0.008109930704189796, Validation Loss: 0.008663280135369633\n",
      "Epoch [9082/20000], Training Loss: 0.008592000101957962, Validation Loss: 0.007615910337831521\n",
      "Epoch [9083/20000], Training Loss: 0.004297753233004187, Validation Loss: 0.005583037223310307\n",
      "Epoch [9084/20000], Training Loss: 0.006658154969565138, Validation Loss: 0.004728686538721466\n",
      "Epoch [9085/20000], Training Loss: 0.007336998177493654, Validation Loss: 0.004548996313117337\n",
      "Epoch [9086/20000], Training Loss: 0.005456274731067927, Validation Loss: 0.01025671418756582\n",
      "Epoch [9087/20000], Training Loss: 0.0048724482777678145, Validation Loss: 0.00341323544817975\n",
      "Epoch [9088/20000], Training Loss: 0.005629612268031841, Validation Loss: 0.006166601635460266\n",
      "Epoch [9089/20000], Training Loss: 0.01118557876075751, Validation Loss: 0.00953612794019827\n",
      "Epoch [9090/20000], Training Loss: 0.009772393103828238, Validation Loss: 0.0030690686827221014\n",
      "Epoch [9091/20000], Training Loss: 0.015648873803521774, Validation Loss: 0.006336250213544622\n",
      "Epoch [9092/20000], Training Loss: 0.010443998063757525, Validation Loss: 0.01040100160885425\n",
      "Epoch [9093/20000], Training Loss: 0.021755930334620643, Validation Loss: 0.006720518161523199\n",
      "Epoch [9094/20000], Training Loss: 0.03929285412531109, Validation Loss: 0.0050440238192359255\n",
      "Epoch [9095/20000], Training Loss: 0.05946417085319159, Validation Loss: 0.0062619273052903724\n",
      "Epoch [9096/20000], Training Loss: 0.08180695723311666, Validation Loss: 0.09571274072407394\n",
      "Epoch [9097/20000], Training Loss: 0.04481303472337978, Validation Loss: 0.029202909830277894\n",
      "Epoch [9098/20000], Training Loss: 0.030412128195166588, Validation Loss: 0.016529294589235275\n",
      "Epoch [9099/20000], Training Loss: 0.01766258085678731, Validation Loss: 0.013896701032175583\n",
      "Epoch [9100/20000], Training Loss: 0.010611003331307853, Validation Loss: 0.007734981479685321\n",
      "Epoch [9101/20000], Training Loss: 0.010734262568543531, Validation Loss: 0.0047521962281850605\n",
      "Epoch [9102/20000], Training Loss: 0.01363238840869079, Validation Loss: 0.011252709092394144\n",
      "Epoch [9103/20000], Training Loss: 0.005065263733350938, Validation Loss: 0.009597113210264185\n",
      "Epoch [9104/20000], Training Loss: 0.0047544403918392064, Validation Loss: 0.008685761340552338\n",
      "Epoch [9105/20000], Training Loss: 0.005884038113955674, Validation Loss: 0.004067795251500504\n",
      "Epoch [9106/20000], Training Loss: 0.007803313440360528, Validation Loss: 0.003942452343849504\n",
      "Epoch [9107/20000], Training Loss: 0.009249270568294119, Validation Loss: 0.0031354763785559825\n",
      "Epoch [9108/20000], Training Loss: 0.005215966670740662, Validation Loss: 0.008637066947167192\n",
      "Epoch [9109/20000], Training Loss: 0.007787883853779931, Validation Loss: 0.007262831045632063\n",
      "Epoch [9110/20000], Training Loss: 0.0062584759616746465, Validation Loss: 0.00239862296633353\n",
      "Epoch [9111/20000], Training Loss: 0.006346854703809056, Validation Loss: 0.0023987837325178635\n",
      "Epoch [9112/20000], Training Loss: 0.005587665025473143, Validation Loss: 0.003085158634934882\n",
      "Epoch [9113/20000], Training Loss: 0.007329584699229079, Validation Loss: 0.012520459879763024\n",
      "Epoch [9114/20000], Training Loss: 0.011166056313543646, Validation Loss: 0.0027116988530210434\n",
      "Epoch [9115/20000], Training Loss: 0.015510094636251128, Validation Loss: 0.016009145779614795\n",
      "Epoch [9116/20000], Training Loss: 0.010121014433938822, Validation Loss: 0.007373621963071945\n",
      "Epoch [9117/20000], Training Loss: 0.008507898641255451, Validation Loss: 0.005960094635565238\n",
      "Epoch [9118/20000], Training Loss: 0.006167356029542199, Validation Loss: 0.007477774551118797\n",
      "Epoch [9119/20000], Training Loss: 0.0045892029665992595, Validation Loss: 0.006854116503753005\n",
      "Epoch [9120/20000], Training Loss: 0.005451756912017507, Validation Loss: 0.0036093516756153026\n",
      "Epoch [9121/20000], Training Loss: 0.004519072518243255, Validation Loss: 0.0026588044654980436\n",
      "Epoch [9122/20000], Training Loss: 0.004747175881522102, Validation Loss: 0.004020968089134645\n",
      "Epoch [9123/20000], Training Loss: 0.004185375319388689, Validation Loss: 0.0036011971233474833\n",
      "Epoch [9124/20000], Training Loss: 0.0068203467713569155, Validation Loss: 0.002446259694083297\n",
      "Epoch [9125/20000], Training Loss: 0.00743178922749418, Validation Loss: 0.0024379851820416037\n",
      "Epoch [9126/20000], Training Loss: 0.00964570137174243, Validation Loss: 0.005130036093784401\n",
      "Epoch [9127/20000], Training Loss: 0.011012915192363184, Validation Loss: 0.010079280340387992\n",
      "Epoch [9128/20000], Training Loss: 0.005099364748600367, Validation Loss: 0.0032757202875738924\n",
      "Epoch [9129/20000], Training Loss: 0.006068927303910446, Validation Loss: 0.004466173820031116\n",
      "Epoch [9130/20000], Training Loss: 0.009679318376153592, Validation Loss: 0.004894330083154663\n",
      "Epoch [9131/20000], Training Loss: 0.005471505024397555, Validation Loss: 0.0322027642812089\n",
      "Epoch [9132/20000], Training Loss: 0.06583735373499573, Validation Loss: 0.01040279679010192\n",
      "Epoch [9133/20000], Training Loss: 0.058039728190384006, Validation Loss: 0.16325928585872365\n",
      "Epoch [9134/20000], Training Loss: 0.053130415981075076, Validation Loss: 0.012016491240956157\n",
      "Epoch [9135/20000], Training Loss: 0.0241206185115256, Validation Loss: 0.033855324869883976\n",
      "Epoch [9136/20000], Training Loss: 0.019348334138547734, Validation Loss: 0.00965765881967887\n",
      "Epoch [9137/20000], Training Loss: 0.011783688661775418, Validation Loss: 0.008889562158921243\n",
      "Epoch [9138/20000], Training Loss: 0.008124237688856997, Validation Loss: 0.007485059480611004\n",
      "Epoch [9139/20000], Training Loss: 0.007043053370060599, Validation Loss: 0.0055929173346506945\n",
      "Epoch [9140/20000], Training Loss: 0.0067838469653257304, Validation Loss: 0.005313568714358651\n",
      "Epoch [9141/20000], Training Loss: 0.007940994376050574, Validation Loss: 0.004926894375675569\n",
      "Epoch [9142/20000], Training Loss: 0.008271118380694784, Validation Loss: 0.011825704103610437\n",
      "Epoch [9143/20000], Training Loss: 0.007507006671013576, Validation Loss: 0.007156461534175865\n",
      "Epoch [9144/20000], Training Loss: 0.010503357591427889, Validation Loss: 0.005365121495563991\n",
      "Epoch [9145/20000], Training Loss: 0.010264575539622456, Validation Loss: 0.01710633111204572\n",
      "Epoch [9146/20000], Training Loss: 0.007423065127243588, Validation Loss: 0.005928697594879954\n",
      "Epoch [9147/20000], Training Loss: 0.00676233198360673, Validation Loss: 0.005936034518218288\n",
      "Epoch [9148/20000], Training Loss: 0.006126453008229353, Validation Loss: 0.006527188586102938\n",
      "Epoch [9149/20000], Training Loss: 0.0046892680583953605, Validation Loss: 0.004043052343843405\n",
      "Epoch [9150/20000], Training Loss: 0.0051930098670547, Validation Loss: 0.004751675867499979\n",
      "Epoch [9151/20000], Training Loss: 0.004833991198860791, Validation Loss: 0.004304464878730263\n",
      "Epoch [9152/20000], Training Loss: 0.005734778988491078, Validation Loss: 0.009320418352085668\n",
      "Epoch [9153/20000], Training Loss: 0.009506991070728483, Validation Loss: 0.007989625281349428\n",
      "Epoch [9154/20000], Training Loss: 0.005270934443355405, Validation Loss: 0.009193060278659817\n",
      "Epoch [9155/20000], Training Loss: 0.007206138724411305, Validation Loss: 0.003426538315337981\n",
      "Epoch [9156/20000], Training Loss: 0.004711979550068334, Validation Loss: 0.005672737238345672\n",
      "Epoch [9157/20000], Training Loss: 0.00427581462775249, Validation Loss: 0.0037428994217315126\n",
      "Epoch [9158/20000], Training Loss: 0.006342630566515644, Validation Loss: 0.003408396786806148\n",
      "Epoch [9159/20000], Training Loss: 0.008579923804583294, Validation Loss: 0.005116471905504121\n",
      "Epoch [9160/20000], Training Loss: 0.03894683120753533, Validation Loss: 0.011147350921035975\n",
      "Epoch [9161/20000], Training Loss: 0.023091216322167644, Validation Loss: 0.006403337835957257\n",
      "Epoch [9162/20000], Training Loss: 0.018381530455176938, Validation Loss: 0.00812166234555026\n",
      "Epoch [9163/20000], Training Loss: 0.012186971348585627, Validation Loss: 0.04108631661779327\n",
      "Epoch [9164/20000], Training Loss: 0.02034621669664765, Validation Loss: 0.020217042601416324\n",
      "Epoch [9165/20000], Training Loss: 0.019960940682462285, Validation Loss: 0.004883237865454605\n",
      "Epoch [9166/20000], Training Loss: 0.00911466360724132, Validation Loss: 0.006231630583320832\n",
      "Epoch [9167/20000], Training Loss: 0.005303087773167395, Validation Loss: 0.004950712239896404\n",
      "Epoch [9168/20000], Training Loss: 0.005008755896207211, Validation Loss: 0.006971135371272062\n",
      "Epoch [9169/20000], Training Loss: 0.008160693471381819, Validation Loss: 0.013327808653981865\n",
      "Epoch [9170/20000], Training Loss: 0.012324088634548909, Validation Loss: 0.02197544569415713\n",
      "Epoch [9171/20000], Training Loss: 0.011973442717654896, Validation Loss: 0.004664011663229368\n",
      "Epoch [9172/20000], Training Loss: 0.00900751883247202, Validation Loss: 0.007654809143925182\n",
      "Epoch [9173/20000], Training Loss: 0.02052371523885605, Validation Loss: 0.008468096836622863\n",
      "Epoch [9174/20000], Training Loss: 0.00762076947596922, Validation Loss: 0.019712550831694438\n",
      "Epoch [9175/20000], Training Loss: 0.018666572614462012, Validation Loss: 0.02319713149754402\n",
      "Epoch [9176/20000], Training Loss: 0.021306294253268528, Validation Loss: 0.02303086115635918\n",
      "Epoch [9177/20000], Training Loss: 0.03689844343586758, Validation Loss: 0.07744751667996752\n",
      "Epoch [9178/20000], Training Loss: 0.02898271703959056, Validation Loss: 0.017801244945855745\n",
      "Epoch [9179/20000], Training Loss: 0.024026312832055346, Validation Loss: 0.011831385350096304\n",
      "Epoch [9180/20000], Training Loss: 0.016112086196829165, Validation Loss: 0.028358296624285104\n",
      "Epoch [9181/20000], Training Loss: 0.009054119392918178, Validation Loss: 0.00874564084152293\n",
      "Epoch [9182/20000], Training Loss: 0.009096796561997118, Validation Loss: 0.007066519207374508\n",
      "Epoch [9183/20000], Training Loss: 0.009940155964743878, Validation Loss: 0.009502063607767555\n",
      "Epoch [9184/20000], Training Loss: 0.007260448350280058, Validation Loss: 0.004865998474383356\n",
      "Epoch [9185/20000], Training Loss: 0.007430638319679669, Validation Loss: 0.004630703151737033\n",
      "Epoch [9186/20000], Training Loss: 0.006008241003395857, Validation Loss: 0.0058965978255563255\n",
      "Epoch [9187/20000], Training Loss: 0.008123630489405644, Validation Loss: 0.007081848344528042\n",
      "Epoch [9188/20000], Training Loss: 0.004826084723650378, Validation Loss: 0.004627370510255397\n",
      "Epoch [9189/20000], Training Loss: 0.0059702214417691425, Validation Loss: 0.005703894160900193\n",
      "Epoch [9190/20000], Training Loss: 0.008311122055082316, Validation Loss: 0.008134752439770767\n",
      "Epoch [9191/20000], Training Loss: 0.007589778314598204, Validation Loss: 0.005415542892889531\n",
      "Epoch [9192/20000], Training Loss: 0.006701274593069684, Validation Loss: 0.0063565934039030115\n",
      "Epoch [9193/20000], Training Loss: 0.009765998895870456, Validation Loss: 0.00577800849095362\n",
      "Epoch [9194/20000], Training Loss: 0.006945754429450192, Validation Loss: 0.013095635181989402\n",
      "Epoch [9195/20000], Training Loss: 0.014349799235787941, Validation Loss: 0.016881093382888488\n",
      "Epoch [9196/20000], Training Loss: 0.01680537121969142, Validation Loss: 0.030653083964897875\n",
      "Epoch [9197/20000], Training Loss: 0.012781028690889278, Validation Loss: 0.008303915052961875\n",
      "Epoch [9198/20000], Training Loss: 0.007913693253483092, Validation Loss: 0.004368635790869657\n",
      "Epoch [9199/20000], Training Loss: 0.006355864800363114, Validation Loss: 0.004481426293700709\n",
      "Epoch [9200/20000], Training Loss: 0.004915883913172182, Validation Loss: 0.00864995191276812\n",
      "Epoch [9201/20000], Training Loss: 0.004675303656508082, Validation Loss: 0.0037056044544177625\n",
      "Epoch [9202/20000], Training Loss: 0.005980416991775428, Validation Loss: 0.005026841494360009\n",
      "Epoch [9203/20000], Training Loss: 0.0038307916630791233, Validation Loss: 0.010037860569301633\n",
      "Epoch [9204/20000], Training Loss: 0.016172539517875912, Validation Loss: 0.09169236251286098\n",
      "Epoch [9205/20000], Training Loss: 0.0246099523792509, Validation Loss: 0.013071930501620003\n",
      "Epoch [9206/20000], Training Loss: 0.020552080120199077, Validation Loss: 0.0046327433050701704\n",
      "Epoch [9207/20000], Training Loss: 0.006396383982584146, Validation Loss: 0.0060660959364245825\n",
      "Epoch [9208/20000], Training Loss: 0.00643517347219001, Validation Loss: 0.005422475499929065\n",
      "Epoch [9209/20000], Training Loss: 0.005246775309220876, Validation Loss: 0.007643448410622763\n",
      "Epoch [9210/20000], Training Loss: 0.007572250488530179, Validation Loss: 0.0038225510771902854\n",
      "Epoch [9211/20000], Training Loss: 0.0059492143648510265, Validation Loss: 0.0035071038464536486\n",
      "Epoch [9212/20000], Training Loss: 0.00830656848432097, Validation Loss: 0.006942176285544434\n",
      "Epoch [9213/20000], Training Loss: 0.010563871856512768, Validation Loss: 0.010157903061815984\n",
      "Epoch [9214/20000], Training Loss: 0.007377911139898734, Validation Loss: 0.002799196334214787\n",
      "Epoch [9215/20000], Training Loss: 0.006194696010425105, Validation Loss: 0.013428138569011703\n",
      "Epoch [9216/20000], Training Loss: 0.007298376409120725, Validation Loss: 0.008867909129629505\n",
      "Epoch [9217/20000], Training Loss: 0.010045290011995738, Validation Loss: 0.010709870353301605\n",
      "Epoch [9218/20000], Training Loss: 0.02581504813861102, Validation Loss: 0.022005434269983328\n",
      "Epoch [9219/20000], Training Loss: 0.026330561922285205, Validation Loss: 0.07785774397383907\n",
      "Epoch [9220/20000], Training Loss: 0.03115994328722341, Validation Loss: 0.030896502946107653\n",
      "Epoch [9221/20000], Training Loss: 0.02237552496886305, Validation Loss: 0.07219194514410836\n",
      "Epoch [9222/20000], Training Loss: 0.034311428433284163, Validation Loss: 0.005687151213208301\n",
      "Epoch [9223/20000], Training Loss: 0.0065051888356850085, Validation Loss: 0.00721742389604125\n",
      "Epoch [9224/20000], Training Loss: 0.007551269114108956, Validation Loss: 0.012155908833705634\n",
      "Epoch [9225/20000], Training Loss: 0.009987138586130382, Validation Loss: 0.016708924834373775\n",
      "Epoch [9226/20000], Training Loss: 0.015881100409647582, Validation Loss: 0.006870395174834819\n",
      "Epoch [9227/20000], Training Loss: 0.01878808905270749, Validation Loss: 0.004713346919490594\n",
      "Epoch [9228/20000], Training Loss: 0.02028056560006267, Validation Loss: 0.00862297926591157\n",
      "Epoch [9229/20000], Training Loss: 0.009424426181275132, Validation Loss: 0.007903905331640917\n",
      "Epoch [9230/20000], Training Loss: 0.009196364857156627, Validation Loss: 0.007288512499785936\n",
      "Epoch [9231/20000], Training Loss: 0.012772019956693319, Validation Loss: 0.012123506782312088\n",
      "Epoch [9232/20000], Training Loss: 0.02333886957694631, Validation Loss: 0.02133873690451989\n",
      "Epoch [9233/20000], Training Loss: 0.010461879706001907, Validation Loss: 0.012228190903015645\n",
      "Epoch [9234/20000], Training Loss: 0.013162531353633053, Validation Loss: 0.026508772479641066\n",
      "Epoch [9235/20000], Training Loss: 0.02980202215909295, Validation Loss: 0.020687757326029083\n",
      "Epoch [9236/20000], Training Loss: 0.026573938203780147, Validation Loss: 0.008059453821282684\n",
      "Epoch [9237/20000], Training Loss: 0.008237884259350332, Validation Loss: 0.005556465408910733\n",
      "Epoch [9238/20000], Training Loss: 0.006026076148763033, Validation Loss: 0.004380291922020417\n",
      "Epoch [9239/20000], Training Loss: 0.009475447526451066, Validation Loss: 0.004197861073108758\n",
      "Epoch [9240/20000], Training Loss: 0.014416906086418229, Validation Loss: 0.015666053419084034\n",
      "Epoch [9241/20000], Training Loss: 0.011664964857378177, Validation Loss: 0.005149685331600397\n",
      "Epoch [9242/20000], Training Loss: 0.008120516626929333, Validation Loss: 0.005345607128918723\n",
      "Epoch [9243/20000], Training Loss: 0.006157949100660127, Validation Loss: 0.004731078393583031\n",
      "Epoch [9244/20000], Training Loss: 0.007733631633787549, Validation Loss: 0.009059313434475533\n",
      "Epoch [9245/20000], Training Loss: 0.005479574783447357, Validation Loss: 0.0035054408967789235\n",
      "Epoch [9246/20000], Training Loss: 0.006798378952745614, Validation Loss: 0.004461430440048478\n",
      "Epoch [9247/20000], Training Loss: 0.00837012036312704, Validation Loss: 0.012907550537308096\n",
      "Epoch [9248/20000], Training Loss: 0.00995720257092866, Validation Loss: 0.003591226845764644\n",
      "Epoch [9249/20000], Training Loss: 0.004037085213764969, Validation Loss: 0.007112665236248703\n",
      "Epoch [9250/20000], Training Loss: 0.008506443688280083, Validation Loss: 0.0031200974111545827\n",
      "Epoch [9251/20000], Training Loss: 0.013687831607447671, Validation Loss: 0.005661525999027016\n",
      "Epoch [9252/20000], Training Loss: 0.013694200298881956, Validation Loss: 0.01575424575386782\n",
      "Epoch [9253/20000], Training Loss: 0.008166044751753128, Validation Loss: 0.0044968169259766355\n",
      "Epoch [9254/20000], Training Loss: 0.006356833096883666, Validation Loss: 0.0045335393394991185\n",
      "Epoch [9255/20000], Training Loss: 0.003959738079304641, Validation Loss: 0.0029526488367191484\n",
      "Epoch [9256/20000], Training Loss: 0.005012284072498525, Validation Loss: 0.008420006438567055\n",
      "Epoch [9257/20000], Training Loss: 0.008371591244732761, Validation Loss: 0.007287658358724239\n",
      "Epoch [9258/20000], Training Loss: 0.00803823405242708, Validation Loss: 0.004444524438574079\n",
      "Epoch [9259/20000], Training Loss: 0.006115552478669477, Validation Loss: 0.005368552827593108\n",
      "Epoch [9260/20000], Training Loss: 0.005612431049675999, Validation Loss: 0.005761392015813937\n",
      "Epoch [9261/20000], Training Loss: 0.004583030009858443, Validation Loss: 0.002709216848083606\n",
      "Epoch [9262/20000], Training Loss: 0.006900711557266602, Validation Loss: 0.014650232468273064\n",
      "Epoch [9263/20000], Training Loss: 0.008255692495367839, Validation Loss: 0.015274280326601846\n",
      "Epoch [9264/20000], Training Loss: 0.02267842771574838, Validation Loss: 0.011214430277634954\n",
      "Epoch [9265/20000], Training Loss: 0.009128086474707484, Validation Loss: 0.004032635462909625\n",
      "Epoch [9266/20000], Training Loss: 0.013382194033640969, Validation Loss: 0.01928440933781011\n",
      "Epoch [9267/20000], Training Loss: 0.007592886972166265, Validation Loss: 0.004013727303522856\n",
      "Epoch [9268/20000], Training Loss: 0.006113326637465174, Validation Loss: 0.002757927309555097\n",
      "Epoch [9269/20000], Training Loss: 0.006141099665131021, Validation Loss: 0.0055684588489234715\n",
      "Epoch [9270/20000], Training Loss: 0.008230451939328174, Validation Loss: 0.005169699783445732\n",
      "Epoch [9271/20000], Training Loss: 0.005689414128677787, Validation Loss: 0.003909460551768862\n",
      "Epoch [9272/20000], Training Loss: 0.007774025513884096, Validation Loss: 0.002280267289579831\n",
      "Epoch [9273/20000], Training Loss: 0.008569966251213503, Validation Loss: 0.00265884068398273\n",
      "Epoch [9274/20000], Training Loss: 0.005480370801316374, Validation Loss: 0.005248664645507069\n",
      "Epoch [9275/20000], Training Loss: 0.007442843640871745, Validation Loss: 0.005834729015600082\n",
      "Epoch [9276/20000], Training Loss: 0.01131323107983917, Validation Loss: 0.003944190695190618\n",
      "Epoch [9277/20000], Training Loss: 0.02534801628522538, Validation Loss: 0.016203462999994502\n",
      "Epoch [9278/20000], Training Loss: 0.04377470862618793, Validation Loss: 0.07006313758237044\n",
      "Epoch [9279/20000], Training Loss: 0.03218634319741146, Validation Loss: 0.016161992879931593\n",
      "Epoch [9280/20000], Training Loss: 0.011110642991427864, Validation Loss: 0.005932117322385823\n",
      "Epoch [9281/20000], Training Loss: 0.009182444158276277, Validation Loss: 0.004081544188525196\n",
      "Epoch [9282/20000], Training Loss: 0.009525468555823733, Validation Loss: 0.004666610324772403\n",
      "Epoch [9283/20000], Training Loss: 0.012107238996707435, Validation Loss: 0.00468212049977877\n",
      "Epoch [9284/20000], Training Loss: 0.019072853319812566, Validation Loss: 0.004793534966420436\n",
      "Epoch [9285/20000], Training Loss: 0.03039439394238538, Validation Loss: 0.031775821838044505\n",
      "Epoch [9286/20000], Training Loss: 0.01832362349107695, Validation Loss: 0.0067513800200167074\n",
      "Epoch [9287/20000], Training Loss: 0.006829575331981427, Validation Loss: 0.006338023859702391\n",
      "Epoch [9288/20000], Training Loss: 0.0066235838166903704, Validation Loss: 0.004917649867887722\n",
      "Epoch [9289/20000], Training Loss: 0.006085500798820119, Validation Loss: 0.007956979931458037\n",
      "Epoch [9290/20000], Training Loss: 0.008876666301927929, Validation Loss: 0.005115708287976061\n",
      "Epoch [9291/20000], Training Loss: 0.008813526569740913, Validation Loss: 0.004703443743959923\n",
      "Epoch [9292/20000], Training Loss: 0.006380903399466271, Validation Loss: 0.026427141257697215\n",
      "Epoch [9293/20000], Training Loss: 0.021602051419904456, Validation Loss: 0.012996824458264214\n",
      "Epoch [9294/20000], Training Loss: 0.015689411042590758, Validation Loss: 0.0043961233164208835\n",
      "Epoch [9295/20000], Training Loss: 0.008406026417755388, Validation Loss: 0.005650824153779662\n",
      "Epoch [9296/20000], Training Loss: 0.007902390582395518, Validation Loss: 0.005887714448402212\n",
      "Epoch [9297/20000], Training Loss: 0.007959925898051421, Validation Loss: 0.006252614854052548\n",
      "Epoch [9298/20000], Training Loss: 0.009922947919108083, Validation Loss: 0.003201709111079389\n",
      "Epoch [9299/20000], Training Loss: 0.004924043341556431, Validation Loss: 0.008662949981509344\n",
      "Epoch [9300/20000], Training Loss: 0.008355390413850602, Validation Loss: 0.0033146073525634747\n",
      "Epoch [9301/20000], Training Loss: 0.00529272190026729, Validation Loss: 0.004003001609332456\n",
      "Epoch [9302/20000], Training Loss: 0.00596455320088093, Validation Loss: 0.003903476887960705\n",
      "Epoch [9303/20000], Training Loss: 0.005658905441772991, Validation Loss: 0.004669168150450297\n",
      "Epoch [9304/20000], Training Loss: 0.01029713892161713, Validation Loss: 0.058801497437538534\n",
      "Epoch [9305/20000], Training Loss: 0.03450833482689676, Validation Loss: 0.007080635940000032\n",
      "Epoch [9306/20000], Training Loss: 0.006994162672331835, Validation Loss: 0.0038638781349107504\n",
      "Epoch [9307/20000], Training Loss: 0.004371045938959079, Validation Loss: 0.005517625915388613\n",
      "Epoch [9308/20000], Training Loss: 0.0047953419624328876, Validation Loss: 0.002697558971221593\n",
      "Epoch [9309/20000], Training Loss: 0.008171959690766275, Validation Loss: 0.0028127078430238567\n",
      "Epoch [9310/20000], Training Loss: 0.005532552563179495, Validation Loss: 0.00802813277447283\n",
      "Epoch [9311/20000], Training Loss: 0.005420658625163404, Validation Loss: 0.0049561553125764745\n",
      "Epoch [9312/20000], Training Loss: 0.014566394961937996, Validation Loss: 0.020355565371443975\n",
      "Epoch [9313/20000], Training Loss: 0.02522591958162344, Validation Loss: 0.02885451216217965\n",
      "Epoch [9314/20000], Training Loss: 0.0303423234742825, Validation Loss: 0.006357679914027438\n",
      "Epoch [9315/20000], Training Loss: 0.01278259011451155, Validation Loss: 0.011982272113008162\n",
      "Epoch [9316/20000], Training Loss: 0.005548505173335018, Validation Loss: 0.0040037132661559505\n",
      "Epoch [9317/20000], Training Loss: 0.009736029491510376, Validation Loss: 0.004059938526823149\n",
      "Epoch [9318/20000], Training Loss: 0.008374505643067616, Validation Loss: 0.01365848870149875\n",
      "Epoch [9319/20000], Training Loss: 0.013413303469860693, Validation Loss: 0.03427431147013392\n",
      "Epoch [9320/20000], Training Loss: 0.01468442426162905, Validation Loss: 0.014914347128122113\n",
      "Epoch [9321/20000], Training Loss: 0.012776159689695175, Validation Loss: 0.012906438271915815\n",
      "Epoch [9322/20000], Training Loss: 0.008716376916189412, Validation Loss: 0.0041073926687069506\n",
      "Epoch [9323/20000], Training Loss: 0.008684606536657416, Validation Loss: 0.0035947657788674117\n",
      "Epoch [9324/20000], Training Loss: 0.013434953746452396, Validation Loss: 0.003868488404212869\n",
      "Epoch [9325/20000], Training Loss: 0.011369482746106638, Validation Loss: 0.008250422443131263\n",
      "Epoch [9326/20000], Training Loss: 0.009606635162137016, Validation Loss: 0.004274743842081242\n",
      "Epoch [9327/20000], Training Loss: 0.007820213579439692, Validation Loss: 0.004855088673421685\n",
      "Epoch [9328/20000], Training Loss: 0.0055076753633329645, Validation Loss: 0.005412752758306651\n",
      "Epoch [9329/20000], Training Loss: 0.006105160881166479, Validation Loss: 0.011319466068276707\n",
      "Epoch [9330/20000], Training Loss: 0.006874359716935682, Validation Loss: 0.0029412182940817756\n",
      "Epoch [9331/20000], Training Loss: 0.006904462132557195, Validation Loss: 0.002685582724124863\n",
      "Epoch [9332/20000], Training Loss: 0.0034422923125824972, Validation Loss: 0.005892837212222363\n",
      "Epoch [9333/20000], Training Loss: 0.006238356211789713, Validation Loss: 0.0069334418929582215\n",
      "Epoch [9334/20000], Training Loss: 0.011216737141824393, Validation Loss: 0.004965501758451347\n",
      "Epoch [9335/20000], Training Loss: 0.006470300715591293, Validation Loss: 0.014937581228363781\n",
      "Epoch [9336/20000], Training Loss: 0.013956904111182666, Validation Loss: 0.004036265733065375\n",
      "Epoch [9337/20000], Training Loss: 0.007365757425232695, Validation Loss: 0.003321806318180856\n",
      "Epoch [9338/20000], Training Loss: 0.00932620062355584, Validation Loss: 0.0054157595124162084\n",
      "Epoch [9339/20000], Training Loss: 0.02042903644048368, Validation Loss: 0.010314367011642853\n",
      "Epoch [9340/20000], Training Loss: 0.04109651466225322, Validation Loss: 0.017913893929612228\n",
      "Epoch [9341/20000], Training Loss: 0.02294839159003459, Validation Loss: 0.015312832114951982\n",
      "Epoch [9342/20000], Training Loss: 0.007480223313905299, Validation Loss: 0.007762899623212728\n",
      "Epoch [9343/20000], Training Loss: 0.00776644582012003, Validation Loss: 0.008749468494963499\n",
      "Epoch [9344/20000], Training Loss: 0.00946514356681811, Validation Loss: 0.008503540818751586\n",
      "Epoch [9345/20000], Training Loss: 0.011919876336053545, Validation Loss: 0.012240705041403819\n",
      "Epoch [9346/20000], Training Loss: 0.010026476108150106, Validation Loss: 0.009270785174337564\n",
      "Epoch [9347/20000], Training Loss: 0.009097001599002397, Validation Loss: 0.005096736963002112\n",
      "Epoch [9348/20000], Training Loss: 0.004173411934711372, Validation Loss: 0.005073594595349641\n",
      "Epoch [9349/20000], Training Loss: 0.004922766691894002, Validation Loss: 0.004051433477136318\n",
      "Epoch [9350/20000], Training Loss: 0.006758634541516325, Validation Loss: 0.0038280768120971666\n",
      "Epoch [9351/20000], Training Loss: 0.012794936581380105, Validation Loss: 0.0036251295740989853\n",
      "Epoch [9352/20000], Training Loss: 0.009067946618448073, Validation Loss: 0.024033388389008387\n",
      "Epoch [9353/20000], Training Loss: 0.012352189982526138, Validation Loss: 0.013737626772906103\n",
      "Epoch [9354/20000], Training Loss: 0.01557435090736752, Validation Loss: 0.010222836497372824\n",
      "Epoch [9355/20000], Training Loss: 0.009361006041477335, Validation Loss: 0.006222377721459234\n",
      "Epoch [9356/20000], Training Loss: 0.006246273013987645, Validation Loss: 0.003989042198703751\n",
      "Epoch [9357/20000], Training Loss: 0.010554430874305711, Validation Loss: 0.004904528997177144\n",
      "Epoch [9358/20000], Training Loss: 0.020112467370511564, Validation Loss: 0.006918460463306951\n",
      "Epoch [9359/20000], Training Loss: 0.023569329218714432, Validation Loss: 0.018395652196238278\n",
      "Epoch [9360/20000], Training Loss: 0.02181672805454582, Validation Loss: 0.007715092722459628\n",
      "Epoch [9361/20000], Training Loss: 0.01481492908247414, Validation Loss: 0.010804127901808885\n",
      "Epoch [9362/20000], Training Loss: 0.0074162195004256705, Validation Loss: 0.0062003533655248105\n",
      "Epoch [9363/20000], Training Loss: 0.005132728476642764, Validation Loss: 0.00403419822204693\n",
      "Epoch [9364/20000], Training Loss: 0.005693388159020937, Validation Loss: 0.006343116784202317\n",
      "Epoch [9365/20000], Training Loss: 0.009343819440240557, Validation Loss: 0.007232709676038158\n",
      "Epoch [9366/20000], Training Loss: 0.006025745215343444, Validation Loss: 0.007629450610173605\n",
      "Epoch [9367/20000], Training Loss: 0.006100902734033298, Validation Loss: 0.006842489287789392\n",
      "Epoch [9368/20000], Training Loss: 0.008504248797700191, Validation Loss: 0.00796419685826944\n",
      "Epoch [9369/20000], Training Loss: 0.010544296594129523, Validation Loss: 0.010987563478795795\n",
      "Epoch [9370/20000], Training Loss: 0.00710096701590146, Validation Loss: 0.005459246491747247\n",
      "Epoch [9371/20000], Training Loss: 0.004500518470324876, Validation Loss: 0.005895223634849705\n",
      "Epoch [9372/20000], Training Loss: 0.008246976137992792, Validation Loss: 0.006809190614148974\n",
      "Epoch [9373/20000], Training Loss: 0.006332855635264423, Validation Loss: 0.006178642818537994\n",
      "Epoch [9374/20000], Training Loss: 0.009494326210187864, Validation Loss: 0.005111728358315304\n",
      "Epoch [9375/20000], Training Loss: 0.012448767357495259, Validation Loss: 0.016896652991168333\n",
      "Epoch [9376/20000], Training Loss: 0.01584386188993189, Validation Loss: 0.02219528918765182\n",
      "Epoch [9377/20000], Training Loss: 0.024554122154118625, Validation Loss: 0.08073099373788344\n",
      "Epoch [9378/20000], Training Loss: 0.04791984273470007, Validation Loss: 0.008909159192076932\n",
      "Epoch [9379/20000], Training Loss: 0.049919014877689606, Validation Loss: 0.025451332699341665\n",
      "Epoch [9380/20000], Training Loss: 0.014459240375020142, Validation Loss: 0.008089622142300388\n",
      "Epoch [9381/20000], Training Loss: 0.0068716829160361415, Validation Loss: 0.0112062090125359\n",
      "Epoch [9382/20000], Training Loss: 0.011673713778756272, Validation Loss: 0.007077555966686625\n",
      "Epoch [9383/20000], Training Loss: 0.015379356110185784, Validation Loss: 0.020206804520413124\n",
      "Epoch [9384/20000], Training Loss: 0.009749879009697386, Validation Loss: 0.008576406340867723\n",
      "Epoch [9385/20000], Training Loss: 0.008311542416257518, Validation Loss: 0.007017706806436763\n",
      "Epoch [9386/20000], Training Loss: 0.011139031890447118, Validation Loss: 0.017751204382161125\n",
      "Epoch [9387/20000], Training Loss: 0.0098710142109277, Validation Loss: 0.00991834815312639\n",
      "Epoch [9388/20000], Training Loss: 0.007977140067461213, Validation Loss: 0.005239815207623114\n",
      "Epoch [9389/20000], Training Loss: 0.006191914313311437, Validation Loss: 0.012219559051760294\n",
      "Epoch [9390/20000], Training Loss: 0.007956775075789275, Validation Loss: 0.010909739899287053\n",
      "Epoch [9391/20000], Training Loss: 0.006762406637010697, Validation Loss: 0.004398760787717266\n",
      "Epoch [9392/20000], Training Loss: 0.008312733126721079, Validation Loss: 0.00461424331627569\n",
      "Epoch [9393/20000], Training Loss: 0.006338436667907185, Validation Loss: 0.00394997646886956\n",
      "Epoch [9394/20000], Training Loss: 0.0061181863389460234, Validation Loss: 0.006027801031159634\n",
      "Epoch [9395/20000], Training Loss: 0.0049508943540482265, Validation Loss: 0.003401632235618255\n",
      "Epoch [9396/20000], Training Loss: 0.004717138598997346, Validation Loss: 0.005919230171765905\n",
      "Epoch [9397/20000], Training Loss: 0.006698305316344236, Validation Loss: 0.013764490243513453\n",
      "Epoch [9398/20000], Training Loss: 0.010881630462660854, Validation Loss: 0.007288842681984431\n",
      "Epoch [9399/20000], Training Loss: 0.016831254333965262, Validation Loss: 0.007354590888943286\n",
      "Epoch [9400/20000], Training Loss: 0.012692580393331035, Validation Loss: 0.005204197518898509\n",
      "Epoch [9401/20000], Training Loss: 0.013731816452231474, Validation Loss: 0.008850633534085313\n",
      "Epoch [9402/20000], Training Loss: 0.012095803991542198, Validation Loss: 0.006330069555031801\n",
      "Epoch [9403/20000], Training Loss: 0.008099795705805133, Validation Loss: 0.004163159463462262\n",
      "Epoch [9404/20000], Training Loss: 0.004282611214356231, Validation Loss: 0.003929204238440057\n",
      "Epoch [9405/20000], Training Loss: 0.00535853851225251, Validation Loss: 0.003186737069568899\n",
      "Epoch [9406/20000], Training Loss: 0.00533568666703234, Validation Loss: 0.005717218588310694\n",
      "Epoch [9407/20000], Training Loss: 0.004544325078713262, Validation Loss: 0.003844289488920715\n",
      "Epoch [9408/20000], Training Loss: 0.0077790868282526, Validation Loss: 0.0035095080172942938\n",
      "Epoch [9409/20000], Training Loss: 0.006822965442020339, Validation Loss: 0.0032066290759740035\n",
      "Epoch [9410/20000], Training Loss: 0.0073984785028317545, Validation Loss: 0.002607467506891622\n",
      "Epoch [9411/20000], Training Loss: 0.005425236275706473, Validation Loss: 0.009828089337274738\n",
      "Epoch [9412/20000], Training Loss: 0.007109988546289969, Validation Loss: 0.00306661534597514\n",
      "Epoch [9413/20000], Training Loss: 0.011550907441103066, Validation Loss: 0.051283401676987035\n",
      "Epoch [9414/20000], Training Loss: 0.056616235960128894, Validation Loss: 0.012289254766907056\n",
      "Epoch [9415/20000], Training Loss: 0.09267400041322357, Validation Loss: 0.050593513463564575\n",
      "Epoch [9416/20000], Training Loss: 0.09743815707042813, Validation Loss: 0.037448604282749036\n",
      "Epoch [9417/20000], Training Loss: 0.06775244009414953, Validation Loss: 0.022457343187830197\n",
      "Epoch [9418/20000], Training Loss: 0.02930329811559724, Validation Loss: 0.023920087049799883\n",
      "Epoch [9419/20000], Training Loss: 0.019604795313041126, Validation Loss: 0.012504444982058573\n",
      "Epoch [9420/20000], Training Loss: 0.010365864469869328, Validation Loss: 0.005896926806402911\n",
      "Epoch [9421/20000], Training Loss: 0.007331735543890058, Validation Loss: 0.01218268244752004\n",
      "Epoch [9422/20000], Training Loss: 0.006629050942657548, Validation Loss: 0.00384260411847328\n",
      "Epoch [9423/20000], Training Loss: 0.006183550469618889, Validation Loss: 0.005337209574193131\n",
      "Epoch [9424/20000], Training Loss: 0.00503173448045605, Validation Loss: 0.004202523002883612\n",
      "Epoch [9425/20000], Training Loss: 0.012637692769625833, Validation Loss: 0.009862851979844687\n",
      "Epoch [9426/20000], Training Loss: 0.010462558096540826, Validation Loss: 0.012840724567003733\n",
      "Epoch [9427/20000], Training Loss: 0.008359425050262612, Validation Loss: 0.003817236956368244\n",
      "Epoch [9428/20000], Training Loss: 0.012649932928200412, Validation Loss: 0.01607255436374271\n",
      "Epoch [9429/20000], Training Loss: 0.03157108328274002, Validation Loss: 0.04719742128093328\n",
      "Epoch [9430/20000], Training Loss: 0.04678942635655403, Validation Loss: 0.016495499333602837\n",
      "Epoch [9431/20000], Training Loss: 0.02288535619819803, Validation Loss: 0.02144609005694034\n",
      "Epoch [9432/20000], Training Loss: 0.011901562085508235, Validation Loss: 0.004781955887208393\n",
      "Epoch [9433/20000], Training Loss: 0.004919854880946721, Validation Loss: 0.002778109556421523\n",
      "Epoch [9434/20000], Training Loss: 0.008959305695108404, Validation Loss: 0.007076168033927376\n",
      "Epoch [9435/20000], Training Loss: 0.02228381998637425, Validation Loss: 0.021055619186990164\n",
      "Epoch [9436/20000], Training Loss: 0.02365410065976903, Validation Loss: 0.005536448756469139\n",
      "Epoch [9437/20000], Training Loss: 0.006187768598985193, Validation Loss: 0.003979106300988221\n",
      "Epoch [9438/20000], Training Loss: 0.00543859636153294, Validation Loss: 0.003282748528112072\n",
      "Epoch [9439/20000], Training Loss: 0.007874362460695141, Validation Loss: 0.004912808205906548\n",
      "Epoch [9440/20000], Training Loss: 0.0091013634940152, Validation Loss: 0.004121915106757058\n",
      "Epoch [9441/20000], Training Loss: 0.0077361177306037076, Validation Loss: 0.013181290722430053\n",
      "Epoch [9442/20000], Training Loss: 0.006535674780025147, Validation Loss: 0.0025452921519814836\n",
      "Epoch [9443/20000], Training Loss: 0.004743313513764795, Validation Loss: 0.0029692838170974948\n",
      "Epoch [9444/20000], Training Loss: 0.005121292775584152, Validation Loss: 0.004908581265358365\n",
      "Epoch [9445/20000], Training Loss: 0.006787869940418594, Validation Loss: 0.02423778947975026\n",
      "Epoch [9446/20000], Training Loss: 0.009486734505376912, Validation Loss: 0.008440067525987253\n",
      "Epoch [9447/20000], Training Loss: 0.027484151197963262, Validation Loss: 0.012381174737531395\n",
      "Epoch [9448/20000], Training Loss: 0.02078386976005017, Validation Loss: 0.012664834697957554\n",
      "Epoch [9449/20000], Training Loss: 0.012892157897209497, Validation Loss: 0.01427226037409506\n",
      "Epoch [9450/20000], Training Loss: 0.010871767748280295, Validation Loss: 0.0048237142467073574\n",
      "Epoch [9451/20000], Training Loss: 0.007591989781108818, Validation Loss: 0.007713783798698593\n",
      "Epoch [9452/20000], Training Loss: 0.004718895183162074, Validation Loss: 0.0033186444134516396\n",
      "Epoch [9453/20000], Training Loss: 0.0038642747884491007, Validation Loss: 0.006419731503700485\n",
      "Epoch [9454/20000], Training Loss: 0.007851589829264023, Validation Loss: 0.006173463804525244\n",
      "Epoch [9455/20000], Training Loss: 0.013914214692184552, Validation Loss: 0.012277495382093828\n",
      "Epoch [9456/20000], Training Loss: 0.00537199795196232, Validation Loss: 0.004821658938891158\n",
      "Epoch [9457/20000], Training Loss: 0.007052840714875076, Validation Loss: 0.01206854277895634\n",
      "Epoch [9458/20000], Training Loss: 0.014888059435959024, Validation Loss: 0.038479673741659726\n",
      "Epoch [9459/20000], Training Loss: 0.04841718860448704, Validation Loss: 0.08882413165935889\n",
      "Epoch [9460/20000], Training Loss: 0.05352894396388105, Validation Loss: 0.04607986309255021\n",
      "Epoch [9461/20000], Training Loss: 0.022974911129235158, Validation Loss: 0.011828515670182869\n",
      "Epoch [9462/20000], Training Loss: 0.027077977437459464, Validation Loss: 0.04599408653822528\n",
      "Epoch [9463/20000], Training Loss: 0.02957991059936051, Validation Loss: 0.015720657918965993\n",
      "Epoch [9464/20000], Training Loss: 0.017312828584441116, Validation Loss: 0.011183836472950657\n",
      "Epoch [9465/20000], Training Loss: 0.012639246814485108, Validation Loss: 0.009000780242526167\n",
      "Epoch [9466/20000], Training Loss: 0.008672716716059117, Validation Loss: 0.01322203386111479\n",
      "Epoch [9467/20000], Training Loss: 0.007131266607237714, Validation Loss: 0.0057898429996330325\n",
      "Epoch [9468/20000], Training Loss: 0.005338140308075319, Validation Loss: 0.0071130834032309435\n",
      "Epoch [9469/20000], Training Loss: 0.0076529324786471465, Validation Loss: 0.008669629826158196\n",
      "Epoch [9470/20000], Training Loss: 0.011591152639344468, Validation Loss: 0.014678158398349166\n",
      "Epoch [9471/20000], Training Loss: 0.008881918553795134, Validation Loss: 0.007233059300135142\n",
      "Epoch [9472/20000], Training Loss: 0.004520307465489688, Validation Loss: 0.004520358863470638\n",
      "Epoch [9473/20000], Training Loss: 0.010736939783133235, Validation Loss: 0.007038224551050251\n",
      "Epoch [9474/20000], Training Loss: 0.013153367193548806, Validation Loss: 0.0036938552818782298\n",
      "Epoch [9475/20000], Training Loss: 0.00696002269147097, Validation Loss: 0.004374242964774392\n",
      "Epoch [9476/20000], Training Loss: 0.005930004695723515, Validation Loss: 0.015794210136005033\n",
      "Epoch [9477/20000], Training Loss: 0.012997117477685347, Validation Loss: 0.010079651434824128\n",
      "Epoch [9478/20000], Training Loss: 0.007254818235586364, Validation Loss: 0.004539457300060311\n",
      "Epoch [9479/20000], Training Loss: 0.009217395034543838, Validation Loss: 0.003019665206220033\n",
      "Epoch [9480/20000], Training Loss: 0.008863329735634449, Validation Loss: 0.012915920998451609\n",
      "Epoch [9481/20000], Training Loss: 0.01318468523580967, Validation Loss: 0.01491096456229333\n",
      "Epoch [9482/20000], Training Loss: 0.010004063116087179, Validation Loss: 0.00565512890918769\n",
      "Epoch [9483/20000], Training Loss: 0.004456396966259657, Validation Loss: 0.004393022092172996\n",
      "Epoch [9484/20000], Training Loss: 0.0051037509743991905, Validation Loss: 0.00933366907527435\n",
      "Epoch [9485/20000], Training Loss: 0.01056370247969036, Validation Loss: 0.00675103174816731\n",
      "Epoch [9486/20000], Training Loss: 0.004637122270651162, Validation Loss: 0.0028061891725076854\n",
      "Epoch [9487/20000], Training Loss: 0.006127290939600373, Validation Loss: 0.015773162005132686\n",
      "Epoch [9488/20000], Training Loss: 0.021477869810139185, Validation Loss: 0.0670079516046955\n",
      "Epoch [9489/20000], Training Loss: 0.028141035701749622, Validation Loss: 0.03524043464299493\n",
      "Epoch [9490/20000], Training Loss: 0.02430082280521414, Validation Loss: 0.00972764099706718\n",
      "Epoch [9491/20000], Training Loss: 0.013173037135857157, Validation Loss: 0.009438593681371708\n",
      "Epoch [9492/20000], Training Loss: 0.014304944147754992, Validation Loss: 0.006232450451791343\n",
      "Epoch [9493/20000], Training Loss: 0.012095284881070256, Validation Loss: 0.013587778305792362\n",
      "Epoch [9494/20000], Training Loss: 0.00691155151746768, Validation Loss: 0.00550507670926516\n",
      "Epoch [9495/20000], Training Loss: 0.008834080945234746, Validation Loss: 0.010577909410979025\n",
      "Epoch [9496/20000], Training Loss: 0.01970009560318431, Validation Loss: 0.03379466638138443\n",
      "Epoch [9497/20000], Training Loss: 0.049320588603482714, Validation Loss: 0.07651422363245454\n",
      "Epoch [9498/20000], Training Loss: 0.03584438806865364, Validation Loss: 0.012522711886947932\n",
      "Epoch [9499/20000], Training Loss: 0.01743406358166664, Validation Loss: 0.013367208527532577\n",
      "Epoch [9500/20000], Training Loss: 0.01277871965430677, Validation Loss: 0.011177280577317055\n",
      "Epoch [9501/20000], Training Loss: 0.009003846847917885, Validation Loss: 0.007358339642134329\n",
      "Epoch [9502/20000], Training Loss: 0.00645604204328265, Validation Loss: 0.006882155132205169\n",
      "Epoch [9503/20000], Training Loss: 0.008588436369401669, Validation Loss: 0.005125118594714413\n",
      "Epoch [9504/20000], Training Loss: 0.00623671650412559, Validation Loss: 0.010479390787977614\n",
      "Epoch [9505/20000], Training Loss: 0.007798256751682077, Validation Loss: 0.008927836697127742\n",
      "Epoch [9506/20000], Training Loss: 0.005126158928500705, Validation Loss: 0.005734572452964671\n",
      "Epoch [9507/20000], Training Loss: 0.004210600239665447, Validation Loss: 0.006218028009378556\n",
      "Epoch [9508/20000], Training Loss: 0.027106963481596073, Validation Loss: 0.032835069018870754\n",
      "Epoch [9509/20000], Training Loss: 0.042045028329084744, Validation Loss: 0.027478981771005485\n",
      "Epoch [9510/20000], Training Loss: 0.03326866097631864, Validation Loss: 0.028668801055871742\n",
      "Epoch [9511/20000], Training Loss: 0.018969108034590527, Validation Loss: 0.012184735399062317\n",
      "Epoch [9512/20000], Training Loss: 0.011960953690244682, Validation Loss: 0.009058560923438787\n",
      "Epoch [9513/20000], Training Loss: 0.009978633008099027, Validation Loss: 0.007583556581074902\n",
      "Epoch [9514/20000], Training Loss: 0.008130811021796294, Validation Loss: 0.008240341482815097\n",
      "Epoch [9515/20000], Training Loss: 0.008145064785951815, Validation Loss: 0.009935057949444825\n",
      "Epoch [9516/20000], Training Loss: 0.006655678694577156, Validation Loss: 0.017219628097302302\n",
      "Epoch [9517/20000], Training Loss: 0.008634545044125324, Validation Loss: 0.004929185787554334\n",
      "Epoch [9518/20000], Training Loss: 0.007893101096020214, Validation Loss: 0.0038139994997954873\n",
      "Epoch [9519/20000], Training Loss: 0.012348449111803867, Validation Loss: 0.041300705503133646\n",
      "Epoch [9520/20000], Training Loss: 0.018821730694201375, Validation Loss: 0.019176534253323142\n",
      "Epoch [9521/20000], Training Loss: 0.008312145262607373, Validation Loss: 0.004273701473056978\n",
      "Epoch [9522/20000], Training Loss: 0.011029130436197323, Validation Loss: 0.01213735236242893\n",
      "Epoch [9523/20000], Training Loss: 0.014731328385615987, Validation Loss: 0.007639325947053902\n",
      "Epoch [9524/20000], Training Loss: 0.008836554174974611, Validation Loss: 0.0030444417743958024\n",
      "Epoch [9525/20000], Training Loss: 0.004874292324529961, Validation Loss: 0.009631843642822066\n",
      "Epoch [9526/20000], Training Loss: 0.007404658215818927, Validation Loss: 0.004532176976358069\n",
      "Epoch [9527/20000], Training Loss: 0.00602395204363607, Validation Loss: 0.00890343217386111\n",
      "Epoch [9528/20000], Training Loss: 0.004071046055677081, Validation Loss: 0.0062037646188507645\n",
      "Epoch [9529/20000], Training Loss: 0.008664881187931834, Validation Loss: 0.0029169436356380686\n",
      "Epoch [9530/20000], Training Loss: 0.023930695216612157, Validation Loss: 0.014777433530884636\n",
      "Epoch [9531/20000], Training Loss: 0.023487937928491322, Validation Loss: 0.009645456580565323\n",
      "Epoch [9532/20000], Training Loss: 0.029676479181197073, Validation Loss: 0.015172784623923923\n",
      "Epoch [9533/20000], Training Loss: 0.017130320700484196, Validation Loss: 0.0062772126050479726\n",
      "Epoch [9534/20000], Training Loss: 0.008140087377146952, Validation Loss: 0.011285578090498762\n",
      "Epoch [9535/20000], Training Loss: 0.0072570750988753775, Validation Loss: 0.005028580728516577\n",
      "Epoch [9536/20000], Training Loss: 0.0055265290138777345, Validation Loss: 0.003737755794739228\n",
      "Epoch [9537/20000], Training Loss: 0.009142287261056481, Validation Loss: 0.004159584412478082\n",
      "Epoch [9538/20000], Training Loss: 0.00710577394445977, Validation Loss: 0.004884264764580233\n",
      "Epoch [9539/20000], Training Loss: 0.01424734843814284, Validation Loss: 0.024880176884900202\n",
      "Epoch [9540/20000], Training Loss: 0.01822173208763291, Validation Loss: 0.007389060008506476\n",
      "Epoch [9541/20000], Training Loss: 0.007129140245288974, Validation Loss: 0.006798783643291577\n",
      "Epoch [9542/20000], Training Loss: 0.013152701173177255, Validation Loss: 0.021245055815387253\n",
      "Epoch [9543/20000], Training Loss: 0.01360411450085459, Validation Loss: 0.00559242193056824\n",
      "Epoch [9544/20000], Training Loss: 0.010366363003000296, Validation Loss: 0.00811578541687855\n",
      "Epoch [9545/20000], Training Loss: 0.0076638416082589955, Validation Loss: 0.00431672858463829\n",
      "Epoch [9546/20000], Training Loss: 0.009032501674352846, Validation Loss: 0.006477772297255007\n",
      "Epoch [9547/20000], Training Loss: 0.018224063038360327, Validation Loss: 0.007745046001912012\n",
      "Epoch [9548/20000], Training Loss: 0.019105466792617726, Validation Loss: 0.01941961443092764\n",
      "Epoch [9549/20000], Training Loss: 0.010782348216577833, Validation Loss: 0.007179347287029815\n",
      "Epoch [9550/20000], Training Loss: 0.0141596184361593, Validation Loss: 0.005531990232737728\n",
      "Epoch [9551/20000], Training Loss: 0.01345353344471992, Validation Loss: 0.005010988756923089\n",
      "Epoch [9552/20000], Training Loss: 0.007544057726460908, Validation Loss: 0.004423191662884359\n",
      "Epoch [9553/20000], Training Loss: 0.0046716490552561095, Validation Loss: 0.004359052616091168\n",
      "Epoch [9554/20000], Training Loss: 0.00614823739459374, Validation Loss: 0.00459483155195325\n",
      "Epoch [9555/20000], Training Loss: 0.006114759582227894, Validation Loss: 0.0030579684542882796\n",
      "Epoch [9556/20000], Training Loss: 0.005193178783104356, Validation Loss: 0.005363864042090264\n",
      "Epoch [9557/20000], Training Loss: 0.010627921562575336, Validation Loss: 0.006884109543786059\n",
      "Epoch [9558/20000], Training Loss: 0.02003874886005568, Validation Loss: 0.0046708960467106864\n",
      "Epoch [9559/20000], Training Loss: 0.025983113579448918, Validation Loss: 0.004504546673351635\n",
      "Epoch [9560/20000], Training Loss: 0.010477472524144105, Validation Loss: 0.00829391599380155\n",
      "Epoch [9561/20000], Training Loss: 0.009695098005457632, Validation Loss: 0.005700485689025417\n",
      "Epoch [9562/20000], Training Loss: 0.006876135120234851, Validation Loss: 0.005137231177412461\n",
      "Epoch [9563/20000], Training Loss: 0.006522990444734335, Validation Loss: 0.011088649352149562\n",
      "Epoch [9564/20000], Training Loss: 0.007675084353000524, Validation Loss: 0.009223173101653563\n",
      "Epoch [9565/20000], Training Loss: 0.012044666804805664, Validation Loss: 0.002802428010199825\n",
      "Epoch [9566/20000], Training Loss: 0.006294541546235877, Validation Loss: 0.008047568662866615\n",
      "Epoch [9567/20000], Training Loss: 0.003726644822681432, Validation Loss: 0.003837328331016465\n",
      "Epoch [9568/20000], Training Loss: 0.009532392509364789, Validation Loss: 0.007299934590659315\n",
      "Epoch [9569/20000], Training Loss: 0.00931794012181594, Validation Loss: 0.007976118108079586\n",
      "Epoch [9570/20000], Training Loss: 0.008900713507110984, Validation Loss: 0.004431650390099787\n",
      "Epoch [9571/20000], Training Loss: 0.006082063462080052, Validation Loss: 0.004953135341739642\n",
      "Epoch [9572/20000], Training Loss: 0.009985436705132347, Validation Loss: 0.005690980213560134\n",
      "Epoch [9573/20000], Training Loss: 0.007574531278805807, Validation Loss: 0.004871366544283313\n",
      "Epoch [9574/20000], Training Loss: 0.006701364491683697, Validation Loss: 0.011362444630840431\n",
      "Epoch [9575/20000], Training Loss: 0.010239618858999424, Validation Loss: 0.007447914609730647\n",
      "Epoch [9576/20000], Training Loss: 0.012379302191090704, Validation Loss: 0.004890001510050431\n",
      "Epoch [9577/20000], Training Loss: 0.006544271171014161, Validation Loss: 0.0048031635315069566\n",
      "Epoch [9578/20000], Training Loss: 0.016724882079610585, Validation Loss: 0.0039569706065064126\n",
      "Epoch [9579/20000], Training Loss: 0.00950983991164581, Validation Loss: 0.009850234582117312\n",
      "Epoch [9580/20000], Training Loss: 0.01044123535887463, Validation Loss: 0.02019403439128707\n",
      "Epoch [9581/20000], Training Loss: 0.007272104028288595, Validation Loss: 0.0063491557229326135\n",
      "Epoch [9582/20000], Training Loss: 0.0064054799087378865, Validation Loss: 0.0035720381396203544\n",
      "Epoch [9583/20000], Training Loss: 0.007421614705225542, Validation Loss: 0.006773650737024565\n",
      "Epoch [9584/20000], Training Loss: 0.03259968275046633, Validation Loss: 0.07929544577168411\n",
      "Epoch [9585/20000], Training Loss: 0.04763351355891895, Validation Loss: 0.06543356621517074\n",
      "Epoch [9586/20000], Training Loss: 0.05541692122018763, Validation Loss: 0.017409701862447946\n",
      "Epoch [9587/20000], Training Loss: 0.018082647025169405, Validation Loss: 0.02243040456440732\n",
      "Epoch [9588/20000], Training Loss: 0.014458729137134339, Validation Loss: 0.008192432713258313\n",
      "Epoch [9589/20000], Training Loss: 0.01087986008496955, Validation Loss: 0.012828357582634453\n",
      "Epoch [9590/20000], Training Loss: 0.009539683981399451, Validation Loss: 0.007836384552449869\n",
      "Epoch [9591/20000], Training Loss: 0.006763493461871154, Validation Loss: 0.008078720491117306\n",
      "Epoch [9592/20000], Training Loss: 0.006258685644882332, Validation Loss: 0.0060306571015514366\n",
      "Epoch [9593/20000], Training Loss: 0.006481108247368995, Validation Loss: 0.007922546927099055\n",
      "Epoch [9594/20000], Training Loss: 0.00694737075744862, Validation Loss: 0.005269587185953891\n",
      "Epoch [9595/20000], Training Loss: 0.006244718991053689, Validation Loss: 0.0048370519265940205\n",
      "Epoch [9596/20000], Training Loss: 0.006239624879006962, Validation Loss: 0.008196768091662778\n",
      "Epoch [9597/20000], Training Loss: 0.007348185233110728, Validation Loss: 0.005345110107642524\n",
      "Epoch [9598/20000], Training Loss: 0.008451118707723384, Validation Loss: 0.0045091208277889495\n",
      "Epoch [9599/20000], Training Loss: 0.006483071982594473, Validation Loss: 0.005394414708951396\n",
      "Epoch [9600/20000], Training Loss: 0.003357293079220978, Validation Loss: 0.003929619965357948\n",
      "Epoch [9601/20000], Training Loss: 0.004703077290157255, Validation Loss: 0.007888980160472363\n",
      "Epoch [9602/20000], Training Loss: 0.008634417117199129, Validation Loss: 0.005549365856635521\n",
      "Epoch [9603/20000], Training Loss: 0.006748258281731978, Validation Loss: 0.013769262841375886\n",
      "Epoch [9604/20000], Training Loss: 0.014647724152642436, Validation Loss: 0.012079566066602654\n",
      "Epoch [9605/20000], Training Loss: 0.01539926603384499, Validation Loss: 0.006610400733808268\n",
      "Epoch [9606/20000], Training Loss: 0.020037205515628948, Validation Loss: 0.014839265699742097\n",
      "Epoch [9607/20000], Training Loss: 0.015447143215819128, Validation Loss: 0.041739892776668994\n",
      "Epoch [9608/20000], Training Loss: 0.027685613360323, Validation Loss: 0.007255415723801966\n",
      "Epoch [9609/20000], Training Loss: 0.027772134381978373, Validation Loss: 0.016616237914734353\n",
      "Epoch [9610/20000], Training Loss: 0.038162787081091665, Validation Loss: 0.032665565960737046\n",
      "Epoch [9611/20000], Training Loss: 0.03783973437800471, Validation Loss: 0.015342310053735478\n",
      "Epoch [9612/20000], Training Loss: 0.013325581318765347, Validation Loss: 0.0091751971204498\n",
      "Epoch [9613/20000], Training Loss: 0.009261452709324658, Validation Loss: 0.007063125756238671\n",
      "Epoch [9614/20000], Training Loss: 0.008759945631027222, Validation Loss: 0.006260713472914047\n",
      "Epoch [9615/20000], Training Loss: 0.006633913861410942, Validation Loss: 0.0057270217898154185\n",
      "Epoch [9616/20000], Training Loss: 0.006318265580505665, Validation Loss: 0.006235990637719624\n",
      "Epoch [9617/20000], Training Loss: 0.006201440709576543, Validation Loss: 0.004981941186250489\n",
      "Epoch [9618/20000], Training Loss: 0.006267927200367142, Validation Loss: 0.005022477720363635\n",
      "Epoch [9619/20000], Training Loss: 0.00481331253623856, Validation Loss: 0.00883973531703727\n",
      "Epoch [9620/20000], Training Loss: 0.013966040386419212, Validation Loss: 0.009517854708284241\n",
      "Epoch [9621/20000], Training Loss: 0.012854014868415626, Validation Loss: 0.020971957182025238\n",
      "Epoch [9622/20000], Training Loss: 0.014755339755408516, Validation Loss: 0.004578126023261575\n",
      "Epoch [9623/20000], Training Loss: 0.006707798061792606, Validation Loss: 0.004657015589887895\n",
      "Epoch [9624/20000], Training Loss: 0.005256264540238119, Validation Loss: 0.006332616510958035\n",
      "Epoch [9625/20000], Training Loss: 0.0064441738497111245, Validation Loss: 0.0033705965301112123\n",
      "Epoch [9626/20000], Training Loss: 0.0036278030519107623, Validation Loss: 0.0042025372905471715\n",
      "Epoch [9627/20000], Training Loss: 0.007956134749633748, Validation Loss: 0.002588241187384353\n",
      "Epoch [9628/20000], Training Loss: 0.005629954622626039, Validation Loss: 0.0024853342554524665\n",
      "Epoch [9629/20000], Training Loss: 0.0040329207066471485, Validation Loss: 0.0019573113576940265\n",
      "Epoch [9630/20000], Training Loss: 0.0074814028464191195, Validation Loss: 0.002186453344485782\n",
      "Epoch [9631/20000], Training Loss: 0.004343562614979289, Validation Loss: 0.006079853737992231\n",
      "Epoch [9632/20000], Training Loss: 0.005063765272337507, Validation Loss: 0.002241567047709836\n",
      "Epoch [9633/20000], Training Loss: 0.007939470187661104, Validation Loss: 0.004076431161100997\n",
      "Epoch [9634/20000], Training Loss: 0.004823631699244808, Validation Loss: 0.0037345300519172142\n",
      "Epoch [9635/20000], Training Loss: 0.0029836729166033494, Validation Loss: 0.026353467271744518\n",
      "Epoch [9636/20000], Training Loss: 0.007549528709205333, Validation Loss: 0.004922744600618161\n",
      "Epoch [9637/20000], Training Loss: 0.014088367526320715, Validation Loss: 0.007890445705788332\n",
      "Epoch [9638/20000], Training Loss: 0.018972417226255596, Validation Loss: 0.005594410279431131\n",
      "Epoch [9639/20000], Training Loss: 0.024499966217019522, Validation Loss: 0.021570229463596022\n",
      "Epoch [9640/20000], Training Loss: 0.01661262008045534, Validation Loss: 0.007791833825968248\n",
      "Epoch [9641/20000], Training Loss: 0.007002643648328493, Validation Loss: 0.006809379695945543\n",
      "Epoch [9642/20000], Training Loss: 0.01762128064884954, Validation Loss: 0.015513193658936839\n",
      "Epoch [9643/20000], Training Loss: 0.014672632287589036, Validation Loss: 0.006931326039575835\n",
      "Epoch [9644/20000], Training Loss: 0.01084361275089967, Validation Loss: 0.011657407374246174\n",
      "Epoch [9645/20000], Training Loss: 0.01750133162077613, Validation Loss: 0.008435859383536939\n",
      "Epoch [9646/20000], Training Loss: 0.00807707079079456, Validation Loss: 0.0055011980089858525\n",
      "Epoch [9647/20000], Training Loss: 0.005137589280626084, Validation Loss: 0.006719509021537665\n",
      "Epoch [9648/20000], Training Loss: 0.005806903568619808, Validation Loss: 0.014490083844975743\n",
      "Epoch [9649/20000], Training Loss: 0.008340728704102471, Validation Loss: 0.005783472364518324\n",
      "Epoch [9650/20000], Training Loss: 0.010008977355547748, Validation Loss: 0.026830388345160827\n",
      "Epoch [9651/20000], Training Loss: 0.014936113114734846, Validation Loss: 0.00828216228912737\n",
      "Epoch [9652/20000], Training Loss: 0.01021776484269919, Validation Loss: 0.014897621579571307\n",
      "Epoch [9653/20000], Training Loss: 0.01218571327938532, Validation Loss: 0.0071045926246561175\n",
      "Epoch [9654/20000], Training Loss: 0.005704453894265628, Validation Loss: 0.003649108716554344\n",
      "Epoch [9655/20000], Training Loss: 0.006946315292485191, Validation Loss: 0.0036390806819537958\n",
      "Epoch [9656/20000], Training Loss: 0.007612046887516044, Validation Loss: 0.004141578411570479\n",
      "Epoch [9657/20000], Training Loss: 0.006338628684586313, Validation Loss: 0.012557078298761262\n",
      "Epoch [9658/20000], Training Loss: 0.00811629102632391, Validation Loss: 0.0051050816592209825\n",
      "Epoch [9659/20000], Training Loss: 0.02555390432722174, Validation Loss: 0.013782561514012264\n",
      "Epoch [9660/20000], Training Loss: 0.020857242883234255, Validation Loss: 0.04143139296749559\n",
      "Epoch [9661/20000], Training Loss: 0.03959563671599296, Validation Loss: 0.009348576232879071\n",
      "Epoch [9662/20000], Training Loss: 0.016983383551373014, Validation Loss: 0.026021232197178636\n",
      "Epoch [9663/20000], Training Loss: 0.016140742605784908, Validation Loss: 0.01349004388306899\n",
      "Epoch [9664/20000], Training Loss: 0.0156757861425701, Validation Loss: 0.015530068742399246\n",
      "Epoch [9665/20000], Training Loss: 0.00889742371925552, Validation Loss: 0.004894098766783372\n",
      "Epoch [9666/20000], Training Loss: 0.007411380174208456, Validation Loss: 0.02026041856768796\n",
      "Epoch [9667/20000], Training Loss: 0.010176223945953617, Validation Loss: 0.004887797698647286\n",
      "Epoch [9668/20000], Training Loss: 0.008017900105960913, Validation Loss: 0.004442038836069274\n",
      "Epoch [9669/20000], Training Loss: 0.006295661146785798, Validation Loss: 0.007495742876893997\n",
      "Epoch [9670/20000], Training Loss: 0.006464846806401121, Validation Loss: 0.004818982300126403\n",
      "Epoch [9671/20000], Training Loss: 0.009136882259034402, Validation Loss: 0.0029809638251597293\n",
      "Epoch [9672/20000], Training Loss: 0.006427291088454824, Validation Loss: 0.03490610745227893\n",
      "Epoch [9673/20000], Training Loss: 0.014035372199162208, Validation Loss: 0.012436063056853186\n",
      "Epoch [9674/20000], Training Loss: 0.009348037842983103, Validation Loss: 0.05029499689731968\n",
      "Epoch [9675/20000], Training Loss: 0.03776781757168075, Validation Loss: 0.06878105441319349\n",
      "Epoch [9676/20000], Training Loss: 0.041566330406307576, Validation Loss: 0.05415920806678644\n",
      "Epoch [9677/20000], Training Loss: 0.03856878702728344, Validation Loss: 0.011989402392438413\n",
      "Epoch [9678/20000], Training Loss: 0.013348134234547615, Validation Loss: 0.007821929040021911\n",
      "Epoch [9679/20000], Training Loss: 0.010318820007211928, Validation Loss: 0.007527894437982915\n",
      "Epoch [9680/20000], Training Loss: 0.00814541954813259, Validation Loss: 0.00956310564675457\n",
      "Epoch [9681/20000], Training Loss: 0.007346750725576255, Validation Loss: 0.008711593182852084\n",
      "Epoch [9682/20000], Training Loss: 0.0058530103664712185, Validation Loss: 0.0220812798698655\n",
      "Epoch [9683/20000], Training Loss: 0.013781856155089502, Validation Loss: 0.023410684296159112\n",
      "Epoch [9684/20000], Training Loss: 0.012867184779939375, Validation Loss: 0.004748850118437937\n",
      "Epoch [9685/20000], Training Loss: 0.0053497001936193556, Validation Loss: 0.004269147434166953\n",
      "Epoch [9686/20000], Training Loss: 0.005379249728451084, Validation Loss: 0.008128079193259832\n",
      "Epoch [9687/20000], Training Loss: 0.013589040289843979, Validation Loss: 0.006971893765689234\n",
      "Epoch [9688/20000], Training Loss: 0.01114249278706276, Validation Loss: 0.024430174490713545\n",
      "Epoch [9689/20000], Training Loss: 0.010438478819976029, Validation Loss: 0.004904765569531053\n",
      "Epoch [9690/20000], Training Loss: 0.006433757573436846, Validation Loss: 0.008253804009063639\n",
      "Epoch [9691/20000], Training Loss: 0.0044430562868780854, Validation Loss: 0.0031041175664930126\n",
      "Epoch [9692/20000], Training Loss: 0.009991698698805911, Validation Loss: 0.00816858242618425\n",
      "Epoch [9693/20000], Training Loss: 0.02650142830888009, Validation Loss: 0.04189368510404685\n",
      "Epoch [9694/20000], Training Loss: 0.04369627868124683, Validation Loss: 0.07279311190671121\n",
      "Epoch [9695/20000], Training Loss: 0.04694445265574489, Validation Loss: 0.0686160352287126\n",
      "Epoch [9696/20000], Training Loss: 0.06251396973883468, Validation Loss: 0.058421915005137\n",
      "Epoch [9697/20000], Training Loss: 0.027160690432148322, Validation Loss: 0.014945194792955958\n",
      "Epoch [9698/20000], Training Loss: 0.020159608152295862, Validation Loss: 0.012531188757358385\n",
      "Epoch [9699/20000], Training Loss: 0.012970185955055058, Validation Loss: 0.010910554866809585\n",
      "Epoch [9700/20000], Training Loss: 0.008534853396538113, Validation Loss: 0.008184786191512623\n",
      "Epoch [9701/20000], Training Loss: 0.0076922418000841776, Validation Loss: 0.010322232147822368\n",
      "Epoch [9702/20000], Training Loss: 0.006775202384330571, Validation Loss: 0.008237680497066842\n",
      "Epoch [9703/20000], Training Loss: 0.009631904365960509, Validation Loss: 0.00617036442014702\n",
      "Epoch [9704/20000], Training Loss: 0.006680245141199391, Validation Loss: 0.006049780348265342\n",
      "Epoch [9705/20000], Training Loss: 0.00803809693232844, Validation Loss: 0.005433481977369021\n",
      "Epoch [9706/20000], Training Loss: 0.006559590842308742, Validation Loss: 0.004283267281684695\n",
      "Epoch [9707/20000], Training Loss: 0.013764184375759214, Validation Loss: 0.004735739370748782\n",
      "Epoch [9708/20000], Training Loss: 0.01390525745227933, Validation Loss: 0.009900567394914319\n",
      "Epoch [9709/20000], Training Loss: 0.011617898234232728, Validation Loss: 0.009670695430494694\n",
      "Epoch [9710/20000], Training Loss: 0.0057953049833165005, Validation Loss: 0.005322381092676258\n",
      "Epoch [9711/20000], Training Loss: 0.005138584175646039, Validation Loss: 0.003782368786872294\n",
      "Epoch [9712/20000], Training Loss: 0.0060090079454572076, Validation Loss: 0.0035429394534217196\n",
      "Epoch [9713/20000], Training Loss: 0.010567972199558946, Validation Loss: 0.006921144206848758\n",
      "Epoch [9714/20000], Training Loss: 0.008763813168375887, Validation Loss: 0.012912389955351211\n",
      "Epoch [9715/20000], Training Loss: 0.007580957286856054, Validation Loss: 0.0075731510668585345\n",
      "Epoch [9716/20000], Training Loss: 0.006061167231694396, Validation Loss: 0.0057494494048922285\n",
      "Epoch [9717/20000], Training Loss: 0.00660442934479631, Validation Loss: 0.013702356788736202\n",
      "Epoch [9718/20000], Training Loss: 0.019522543327184394, Validation Loss: 0.005797194251806315\n",
      "Epoch [9719/20000], Training Loss: 0.04429134041569276, Validation Loss: 0.004667917137014922\n",
      "Epoch [9720/20000], Training Loss: 0.02050913398852572, Validation Loss: 0.018164253286034437\n",
      "Epoch [9721/20000], Training Loss: 0.01633255870235319, Validation Loss: 0.030503700338678236\n",
      "Epoch [9722/20000], Training Loss: 0.026484222380012006, Validation Loss: 0.008447406515479868\n",
      "Epoch [9723/20000], Training Loss: 0.017132728905250718, Validation Loss: 0.01735800398251391\n",
      "Epoch [9724/20000], Training Loss: 0.009835183254576154, Validation Loss: 0.004616840834300869\n",
      "Epoch [9725/20000], Training Loss: 0.005588734871707857, Validation Loss: 0.011338551686724279\n",
      "Epoch [9726/20000], Training Loss: 0.008032340012245445, Validation Loss: 0.004440397945988447\n",
      "Epoch [9727/20000], Training Loss: 0.01014142901424618, Validation Loss: 0.010722814527836405\n",
      "Epoch [9728/20000], Training Loss: 0.010870339889411948, Validation Loss: 0.012077359083583306\n",
      "Epoch [9729/20000], Training Loss: 0.007797366407300744, Validation Loss: 0.004009529174676714\n",
      "Epoch [9730/20000], Training Loss: 0.005639835385490447, Validation Loss: 0.0036600049201137097\n",
      "Epoch [9731/20000], Training Loss: 0.004884541269607975, Validation Loss: 0.0035592394704153174\n",
      "Epoch [9732/20000], Training Loss: 0.006704631599859567, Validation Loss: 0.0092607475197458\n",
      "Epoch [9733/20000], Training Loss: 0.006211469380234901, Validation Loss: 0.005435555684860677\n",
      "Epoch [9734/20000], Training Loss: 0.007127310057902443, Validation Loss: 0.005697845248703904\n",
      "Epoch [9735/20000], Training Loss: 0.005655116358021977, Validation Loss: 0.0067137640602028216\n",
      "Epoch [9736/20000], Training Loss: 0.008540859627627486, Validation Loss: 0.003582213872104445\n",
      "Epoch [9737/20000], Training Loss: 0.006049454756998378, Validation Loss: 0.017529982674334747\n",
      "Epoch [9738/20000], Training Loss: 0.013196593819884583, Validation Loss: 0.003746291625953723\n",
      "Epoch [9739/20000], Training Loss: 0.00966551193518431, Validation Loss: 0.027912958015673235\n",
      "Epoch [9740/20000], Training Loss: 0.02081982195938638, Validation Loss: 0.010050031063209579\n",
      "Epoch [9741/20000], Training Loss: 0.011944750594141493, Validation Loss: 0.018785948757467398\n",
      "Epoch [9742/20000], Training Loss: 0.011917812472114113, Validation Loss: 0.011188826656767748\n",
      "Epoch [9743/20000], Training Loss: 0.010125529975604357, Validation Loss: 0.007562069521685792\n",
      "Epoch [9744/20000], Training Loss: 0.012173408516381252, Validation Loss: 0.003913157403524942\n",
      "Epoch [9745/20000], Training Loss: 0.02154011137775212, Validation Loss: 0.02582002298108169\n",
      "Epoch [9746/20000], Training Loss: 0.011012610126921831, Validation Loss: 0.009706006052770886\n",
      "Epoch [9747/20000], Training Loss: 0.00985579722120227, Validation Loss: 0.007840868609295781\n",
      "Epoch [9748/20000], Training Loss: 0.013521673066341984, Validation Loss: 0.00508375276292316\n",
      "Epoch [9749/20000], Training Loss: 0.013041261781056943, Validation Loss: 0.008115384786540883\n",
      "Epoch [9750/20000], Training Loss: 0.009353914185859529, Validation Loss: 0.005188591225551133\n",
      "Epoch [9751/20000], Training Loss: 0.004788337357500235, Validation Loss: 0.004319040079947213\n",
      "Epoch [9752/20000], Training Loss: 0.006340011133163769, Validation Loss: 0.004133188167164481\n",
      "Epoch [9753/20000], Training Loss: 0.006126761273792779, Validation Loss: 0.010228805044400132\n",
      "Epoch [9754/20000], Training Loss: 0.00583809437999402, Validation Loss: 0.003751081224785821\n",
      "Epoch [9755/20000], Training Loss: 0.005453794827189995, Validation Loss: 0.003132656500756814\n",
      "Epoch [9756/20000], Training Loss: 0.005861364587742303, Validation Loss: 0.0028722843282585364\n",
      "Epoch [9757/20000], Training Loss: 0.00540808402937338, Validation Loss: 0.0038220907029630656\n",
      "Epoch [9758/20000], Training Loss: 0.005332398032546085, Validation Loss: 0.004173237131912744\n",
      "Epoch [9759/20000], Training Loss: 0.007166444315678115, Validation Loss: 0.004931070472676972\n",
      "Epoch [9760/20000], Training Loss: 0.008044642090681009, Validation Loss: 0.007384727829981341\n",
      "Epoch [9761/20000], Training Loss: 0.007995092332878682, Validation Loss: 0.0028079007087247385\n",
      "Epoch [9762/20000], Training Loss: 0.006514427445446407, Validation Loss: 0.0037506600586305863\n",
      "Epoch [9763/20000], Training Loss: 0.007283251200744546, Validation Loss: 0.020857892398323332\n",
      "Epoch [9764/20000], Training Loss: 0.010280342897172088, Validation Loss: 0.006021988280865904\n",
      "Epoch [9765/20000], Training Loss: 0.012374346877550124, Validation Loss: 0.009505023381557263\n",
      "Epoch [9766/20000], Training Loss: 0.028601624619146344, Validation Loss: 0.030595117381640842\n",
      "Epoch [9767/20000], Training Loss: 0.03854697910166965, Validation Loss: 0.012269125120448214\n",
      "Epoch [9768/20000], Training Loss: 0.018305014366238277, Validation Loss: 0.005448236052556383\n",
      "Epoch [9769/20000], Training Loss: 0.024513938123813465, Validation Loss: 0.0074495391250967\n",
      "Epoch [9770/20000], Training Loss: 0.014253601165689491, Validation Loss: 0.005493586492182853\n",
      "Epoch [9771/20000], Training Loss: 0.018103081695569147, Validation Loss: 0.014562838029762912\n",
      "Epoch [9772/20000], Training Loss: 0.008592823470633806, Validation Loss: 0.006451492650953128\n",
      "Epoch [9773/20000], Training Loss: 0.008479314756446652, Validation Loss: 0.003948117131520641\n",
      "Epoch [9774/20000], Training Loss: 0.004131542552824742, Validation Loss: 0.004673812412156459\n",
      "Epoch [9775/20000], Training Loss: 0.005841741192853078, Validation Loss: 0.0047664292543621355\n",
      "Epoch [9776/20000], Training Loss: 0.006326833437404795, Validation Loss: 0.0056548492582457754\n",
      "Epoch [9777/20000], Training Loss: 0.004956187290580212, Validation Loss: 0.0065061932229062746\n",
      "Epoch [9778/20000], Training Loss: 0.009916329128216939, Validation Loss: 0.0041276685677024616\n",
      "Epoch [9779/20000], Training Loss: 0.017652271833347704, Validation Loss: 0.006416747828552616\n",
      "Epoch [9780/20000], Training Loss: 0.006954801466365877, Validation Loss: 0.007398543744253162\n",
      "Epoch [9781/20000], Training Loss: 0.015206142663243685, Validation Loss: 0.005894132582546214\n",
      "Epoch [9782/20000], Training Loss: 0.008483704573726365, Validation Loss: 0.005491673903070117\n",
      "Epoch [9783/20000], Training Loss: 0.00809051139575396, Validation Loss: 0.004090819113441414\n",
      "Epoch [9784/20000], Training Loss: 0.007065361462114846, Validation Loss: 0.00404954430634165\n",
      "Epoch [9785/20000], Training Loss: 0.010669254255065295, Validation Loss: 0.005692717022563036\n",
      "Epoch [9786/20000], Training Loss: 0.02714642383840068, Validation Loss: 0.006571932530653405\n",
      "Epoch [9787/20000], Training Loss: 0.04415455867264687, Validation Loss: 0.00576998024150147\n",
      "Epoch [9788/20000], Training Loss: 0.0441210791593351, Validation Loss: 0.06041194767619475\n",
      "Epoch [9789/20000], Training Loss: 0.04867015369485931, Validation Loss: 0.016168140699716917\n",
      "Epoch [9790/20000], Training Loss: 0.013910766675118274, Validation Loss: 0.015056195202674363\n",
      "Epoch [9791/20000], Training Loss: 0.012703175228255401, Validation Loss: 0.006414198539719759\n",
      "Epoch [9792/20000], Training Loss: 0.00802498365270107, Validation Loss: 0.0057229033994060175\n",
      "Epoch [9793/20000], Training Loss: 0.007986871650375958, Validation Loss: 0.005526224758170883\n",
      "Epoch [9794/20000], Training Loss: 0.007209543803972858, Validation Loss: 0.011045275553553122\n",
      "Epoch [9795/20000], Training Loss: 0.00987134022995763, Validation Loss: 0.006382838736962501\n",
      "Epoch [9796/20000], Training Loss: 0.00816883096766625, Validation Loss: 0.006074001700894861\n",
      "Epoch [9797/20000], Training Loss: 0.00799182105194112, Validation Loss: 0.005813530236829976\n",
      "Epoch [9798/20000], Training Loss: 0.007810909276096416, Validation Loss: 0.006463519138216205\n",
      "Epoch [9799/20000], Training Loss: 0.008586906168992365, Validation Loss: 0.006567231842512099\n",
      "Epoch [9800/20000], Training Loss: 0.0077655510727449185, Validation Loss: 0.004759230413193498\n",
      "Epoch [9801/20000], Training Loss: 0.007134787838107773, Validation Loss: 0.004646445415054197\n",
      "Epoch [9802/20000], Training Loss: 0.010819822682866029, Validation Loss: 0.007323502049985109\n",
      "Epoch [9803/20000], Training Loss: 0.006679337225640276, Validation Loss: 0.004653532532367015\n",
      "Epoch [9804/20000], Training Loss: 0.011811175408573555, Validation Loss: 0.004190357005557287\n",
      "Epoch [9805/20000], Training Loss: 0.011482127538848934, Validation Loss: 0.01073392985553515\n",
      "Epoch [9806/20000], Training Loss: 0.01700648492234385, Validation Loss: 0.01640965760707044\n",
      "Epoch [9807/20000], Training Loss: 0.010270939191936381, Validation Loss: 0.013985815794258241\n",
      "Epoch [9808/20000], Training Loss: 0.018116631227063147, Validation Loss: 0.05373487839499701\n",
      "Epoch [9809/20000], Training Loss: 0.0290632369578816, Validation Loss: 0.014570995390905799\n",
      "Epoch [9810/20000], Training Loss: 0.011371617192967929, Validation Loss: 0.009808770613066287\n",
      "Epoch [9811/20000], Training Loss: 0.005804240196344576, Validation Loss: 0.004656769560499795\n",
      "Epoch [9812/20000], Training Loss: 0.00776666561224764, Validation Loss: 0.00372517407605041\n",
      "Epoch [9813/20000], Training Loss: 0.0059911442388381276, Validation Loss: 0.003946128722223615\n",
      "Epoch [9814/20000], Training Loss: 0.00631598267757129, Validation Loss: 0.01326481422697309\n",
      "Epoch [9815/20000], Training Loss: 0.015683181425889155, Validation Loss: 0.052558225180421754\n",
      "Epoch [9816/20000], Training Loss: 0.023806387411793497, Validation Loss: 0.004793066489761648\n",
      "Epoch [9817/20000], Training Loss: 0.009073600786125877, Validation Loss: 0.01007226611493804\n",
      "Epoch [9818/20000], Training Loss: 0.007480832145250004, Validation Loss: 0.004175377080952915\n",
      "Epoch [9819/20000], Training Loss: 0.009080944708882401, Validation Loss: 0.003978628966600744\n",
      "Epoch [9820/20000], Training Loss: 0.005384853731129624, Validation Loss: 0.01868636054682283\n",
      "Epoch [9821/20000], Training Loss: 0.008519745958340406, Validation Loss: 0.004986882101061241\n",
      "Epoch [9822/20000], Training Loss: 0.006900188479814265, Validation Loss: 0.005728620067836841\n",
      "Epoch [9823/20000], Training Loss: 0.004644175606829647, Validation Loss: 0.0036732407597891586\n",
      "Epoch [9824/20000], Training Loss: 0.0074531749082130515, Validation Loss: 0.008052269246910779\n",
      "Epoch [9825/20000], Training Loss: 0.004846956127689087, Validation Loss: 0.004371117759615422\n",
      "Epoch [9826/20000], Training Loss: 0.0072825359716911665, Validation Loss: 0.00467917851337266\n",
      "Epoch [9827/20000], Training Loss: 0.018194772259448655, Validation Loss: 0.005646484641902459\n",
      "Epoch [9828/20000], Training Loss: 0.053632984079221, Validation Loss: 0.012087281241422585\n",
      "Epoch [9829/20000], Training Loss: 0.0403747237391404, Validation Loss: 0.039795186015778326\n",
      "Epoch [9830/20000], Training Loss: 0.027006110363540108, Validation Loss: 0.017593443690982116\n",
      "Epoch [9831/20000], Training Loss: 0.011706575271091424, Validation Loss: 0.015869228398187576\n",
      "Epoch [9832/20000], Training Loss: 0.00956449686457615, Validation Loss: 0.0061087643267196\n",
      "Epoch [9833/20000], Training Loss: 0.009956637837936797, Validation Loss: 0.010667396041428998\n",
      "Epoch [9834/20000], Training Loss: 0.014780709835966783, Validation Loss: 0.018084935086026883\n",
      "Epoch [9835/20000], Training Loss: 0.01101705082276437, Validation Loss: 0.010509535384958977\n",
      "Epoch [9836/20000], Training Loss: 0.012355767403538007, Validation Loss: 0.010898437514892325\n",
      "Epoch [9837/20000], Training Loss: 0.007573074564073514, Validation Loss: 0.0038827141106594354\n",
      "Epoch [9838/20000], Training Loss: 0.005282857077483121, Validation Loss: 0.005932319258830278\n",
      "Epoch [9839/20000], Training Loss: 0.005956584421385612, Validation Loss: 0.006033348378524904\n",
      "Epoch [9840/20000], Training Loss: 0.009101516173749198, Validation Loss: 0.0030834817159351197\n",
      "Epoch [9841/20000], Training Loss: 0.006477235184450235, Validation Loss: 0.0051113894609216\n",
      "Epoch [9842/20000], Training Loss: 0.004590636717454443, Validation Loss: 0.0032550380195581446\n",
      "Epoch [9843/20000], Training Loss: 0.007787034369227642, Validation Loss: 0.0027302682830980174\n",
      "Epoch [9844/20000], Training Loss: 0.006369524039785445, Validation Loss: 0.012191091796957274\n",
      "Epoch [9845/20000], Training Loss: 0.006205715018690431, Validation Loss: 0.005043680016634815\n",
      "Epoch [9846/20000], Training Loss: 0.011274724662793492, Validation Loss: 0.008697168674059088\n",
      "Epoch [9847/20000], Training Loss: 0.006340779626043513, Validation Loss: 0.0030889584758710953\n",
      "Epoch [9848/20000], Training Loss: 0.0069425428311140945, Validation Loss: 0.002665328865926345\n",
      "Epoch [9849/20000], Training Loss: 0.01296159418208325, Validation Loss: 0.026069820060263704\n",
      "Epoch [9850/20000], Training Loss: 0.026299156151903195, Validation Loss: 0.021726766964186166\n",
      "Epoch [9851/20000], Training Loss: 0.022700592682148062, Validation Loss: 0.04790217748833836\n",
      "Epoch [9852/20000], Training Loss: 0.041785537616565956, Validation Loss: 0.03568454991497303\n",
      "Epoch [9853/20000], Training Loss: 0.03447589980454983, Validation Loss: 0.005944901803569483\n",
      "Epoch [9854/20000], Training Loss: 0.020394880318105737, Validation Loss: 0.0781675805795073\n",
      "Epoch [9855/20000], Training Loss: 0.045967185079851855, Validation Loss: 0.03835470238534201\n",
      "Epoch [9856/20000], Training Loss: 0.01652458793666613, Validation Loss: 0.007978291321154731\n",
      "Epoch [9857/20000], Training Loss: 0.006988962124783679, Validation Loss: 0.006281443792741267\n",
      "Epoch [9858/20000], Training Loss: 0.008778874246802713, Validation Loss: 0.006298237106043507\n",
      "Epoch [9859/20000], Training Loss: 0.006833731847744876, Validation Loss: 0.004893114944661444\n",
      "Epoch [9860/20000], Training Loss: 0.0046232693753803945, Validation Loss: 0.004819707911607907\n",
      "Epoch [9861/20000], Training Loss: 0.005622806156623028, Validation Loss: 0.010995157853404833\n",
      "Epoch [9862/20000], Training Loss: 0.012259251425608195, Validation Loss: 0.005544648882932839\n",
      "Epoch [9863/20000], Training Loss: 0.007286825179887403, Validation Loss: 0.006223558228894588\n",
      "Epoch [9864/20000], Training Loss: 0.010200040254208684, Validation Loss: 0.024781551744500013\n",
      "Epoch [9865/20000], Training Loss: 0.026603922971324728, Validation Loss: 0.014885039471836324\n",
      "Epoch [9866/20000], Training Loss: 0.05568612185639462, Validation Loss: 0.013873534011119253\n",
      "Epoch [9867/20000], Training Loss: 0.019554296530259307, Validation Loss: 0.01700056754790629\n",
      "Epoch [9868/20000], Training Loss: 0.013088913717573243, Validation Loss: 0.007336633929298841\n",
      "Epoch [9869/20000], Training Loss: 0.009142927201797388, Validation Loss: 0.006423754077331978\n",
      "Epoch [9870/20000], Training Loss: 0.0059518055557938555, Validation Loss: 0.004519941695174826\n",
      "Epoch [9871/20000], Training Loss: 0.0070813512388018095, Validation Loss: 0.0047074593038424995\n",
      "Epoch [9872/20000], Training Loss: 0.006112893799062087, Validation Loss: 0.007072531523590442\n",
      "Epoch [9873/20000], Training Loss: 0.005886357726662287, Validation Loss: 0.004751946279481801\n",
      "Epoch [9874/20000], Training Loss: 0.005288117865112392, Validation Loss: 0.006181556079679597\n",
      "Epoch [9875/20000], Training Loss: 0.010484215749394414, Validation Loss: 0.0066290674132492965\n",
      "Epoch [9876/20000], Training Loss: 0.036718587769883015, Validation Loss: 0.004349278011789336\n",
      "Epoch [9877/20000], Training Loss: 0.025986588697248538, Validation Loss: 0.01372193480340554\n",
      "Epoch [9878/20000], Training Loss: 0.018655090433769925, Validation Loss: 0.05491784002127982\n",
      "Epoch [9879/20000], Training Loss: 0.027493085377240538, Validation Loss: 0.011061784498191887\n",
      "Epoch [9880/20000], Training Loss: 0.019037728409784904, Validation Loss: 0.03540201994675434\n",
      "Epoch [9881/20000], Training Loss: 0.028649347942389016, Validation Loss: 0.028165265935383235\n",
      "Epoch [9882/20000], Training Loss: 0.0265029676083941, Validation Loss: 0.005289111423573039\n",
      "Epoch [9883/20000], Training Loss: 0.03642423179865416, Validation Loss: 0.0635577686123652\n",
      "Epoch [9884/20000], Training Loss: 0.06795551385896813, Validation Loss: 0.0273145638814357\n",
      "Epoch [9885/20000], Training Loss: 0.038289652326576676, Validation Loss: 0.019373852885321324\n",
      "Epoch [9886/20000], Training Loss: 0.014758762088604271, Validation Loss: 0.012573641477815605\n",
      "Epoch [9887/20000], Training Loss: 0.009272274577857129, Validation Loss: 0.008875822594080722\n",
      "Epoch [9888/20000], Training Loss: 0.00825710907312376, Validation Loss: 0.007633437904409306\n",
      "Epoch [9889/20000], Training Loss: 0.007286965448396846, Validation Loss: 0.006782453202699387\n",
      "Epoch [9890/20000], Training Loss: 0.006800064933486283, Validation Loss: 0.006943092218664658\n",
      "Epoch [9891/20000], Training Loss: 0.006672834603315485, Validation Loss: 0.006010169742158463\n",
      "Epoch [9892/20000], Training Loss: 0.005695836842538223, Validation Loss: 0.004940256280073234\n",
      "Epoch [9893/20000], Training Loss: 0.005176746303082577, Validation Loss: 0.004519265143439303\n",
      "Epoch [9894/20000], Training Loss: 0.0061142333309232655, Validation Loss: 0.005699646868111229\n",
      "Epoch [9895/20000], Training Loss: 0.006551690193841101, Validation Loss: 0.00557939377638864\n",
      "Epoch [9896/20000], Training Loss: 0.005100471404148266, Validation Loss: 0.006484723366416542\n",
      "Epoch [9897/20000], Training Loss: 0.00455489506878491, Validation Loss: 0.015987161729304335\n",
      "Epoch [9898/20000], Training Loss: 0.013710606788371089, Validation Loss: 0.006441117562283287\n",
      "Epoch [9899/20000], Training Loss: 0.011710148576120056, Validation Loss: 0.009539407163824112\n",
      "Epoch [9900/20000], Training Loss: 0.013698572101670184, Validation Loss: 0.0085924666625853\n",
      "Epoch [9901/20000], Training Loss: 0.010458278683862383, Validation Loss: 0.00991376118086659\n",
      "Epoch [9902/20000], Training Loss: 0.007997119973879308, Validation Loss: 0.010023704400542297\n",
      "Epoch [9903/20000], Training Loss: 0.014911572548693844, Validation Loss: 0.015387455612816276\n",
      "Epoch [9904/20000], Training Loss: 0.019007192244836397, Validation Loss: 0.013425257476537158\n",
      "Epoch [9905/20000], Training Loss: 0.00975942404147645, Validation Loss: 0.020033284519822052\n",
      "Epoch [9906/20000], Training Loss: 0.01569109532244641, Validation Loss: 0.003319624826076506\n",
      "Epoch [9907/20000], Training Loss: 0.008492253221837538, Validation Loss: 0.006394793426660986\n",
      "Epoch [9908/20000], Training Loss: 0.007377317599353513, Validation Loss: 0.0039476835888925576\n",
      "Epoch [9909/20000], Training Loss: 0.004683968578839475, Validation Loss: 0.008832007565903919\n",
      "Epoch [9910/20000], Training Loss: 0.004976424983137154, Validation Loss: 0.009216068285224668\n",
      "Epoch [9911/20000], Training Loss: 0.007170229776534682, Validation Loss: 0.0037396447024613216\n",
      "Epoch [9912/20000], Training Loss: 0.00509053969082223, Validation Loss: 0.005013760951125538\n",
      "Epoch [9913/20000], Training Loss: 0.005182402422568495, Validation Loss: 0.0031926711252521856\n",
      "Epoch [9914/20000], Training Loss: 0.0037729769958657145, Validation Loss: 0.003224533079052857\n",
      "Epoch [9915/20000], Training Loss: 0.004413325955413582, Validation Loss: 0.009004025931500894\n",
      "Epoch [9916/20000], Training Loss: 0.005857140734276202, Validation Loss: 0.0025129995644225084\n",
      "Epoch [9917/20000], Training Loss: 0.006785545001808454, Validation Loss: 0.0025283552134242526\n",
      "Epoch [9918/20000], Training Loss: 0.008918423309069112, Validation Loss: 0.0037999520987089647\n",
      "Epoch [9919/20000], Training Loss: 0.03239343085733383, Validation Loss: 0.015396410971581937\n",
      "Epoch [9920/20000], Training Loss: 0.021319027509889565, Validation Loss: 0.009698243398438373\n",
      "Epoch [9921/20000], Training Loss: 0.017064129410763535, Validation Loss: 0.0044407306746490705\n",
      "Epoch [9922/20000], Training Loss: 0.012736492829779828, Validation Loss: 0.015621239161914349\n",
      "Epoch [9923/20000], Training Loss: 0.00621084483451081, Validation Loss: 0.016266913493138548\n",
      "Epoch [9924/20000], Training Loss: 0.01119158833652786, Validation Loss: 0.008114524647591484\n",
      "Epoch [9925/20000], Training Loss: 0.02337754055042751, Validation Loss: 0.004154885445367589\n",
      "Epoch [9926/20000], Training Loss: 0.0487847211409514, Validation Loss: 0.014079231931548617\n",
      "Epoch [9927/20000], Training Loss: 0.01827980840503837, Validation Loss: 0.0070550031102162136\n",
      "Epoch [9928/20000], Training Loss: 0.01131470624490508, Validation Loss: 0.007081523079981318\n",
      "Epoch [9929/20000], Training Loss: 0.0058571036040250745, Validation Loss: 0.007170865638418685\n",
      "Epoch [9930/20000], Training Loss: 0.012681489174220977, Validation Loss: 0.008306943018429333\n",
      "Epoch [9931/20000], Training Loss: 0.020896922672233944, Validation Loss: 0.043065644495547915\n",
      "Epoch [9932/20000], Training Loss: 0.016446673440181518, Validation Loss: 0.006765188668144414\n",
      "Epoch [9933/20000], Training Loss: 0.008785078243818134, Validation Loss: 0.005050707367964995\n",
      "Epoch [9934/20000], Training Loss: 0.007941481050303472, Validation Loss: 0.005087742524827783\n",
      "Epoch [9935/20000], Training Loss: 0.009300000146530303, Validation Loss: 0.005214495902635008\n",
      "Epoch [9936/20000], Training Loss: 0.007704704808410108, Validation Loss: 0.003922518747872671\n",
      "Epoch [9937/20000], Training Loss: 0.006231439360167964, Validation Loss: 0.0042383406130527745\n",
      "Epoch [9938/20000], Training Loss: 0.006464680588188847, Validation Loss: 0.004282727239318059\n",
      "Epoch [9939/20000], Training Loss: 0.006352994303077659, Validation Loss: 0.0031726747771553426\n",
      "Epoch [9940/20000], Training Loss: 0.005025596982769847, Validation Loss: 0.005495821356800791\n",
      "Epoch [9941/20000], Training Loss: 0.005979328919368397, Validation Loss: 0.007218359974428624\n",
      "Epoch [9942/20000], Training Loss: 0.006995834477233335, Validation Loss: 0.030382409445173612\n",
      "Epoch [9943/20000], Training Loss: 0.010362591221304942, Validation Loss: 0.003852749529995338\n",
      "Epoch [9944/20000], Training Loss: 0.006226605881238356, Validation Loss: 0.005137288048079621\n",
      "Epoch [9945/20000], Training Loss: 0.00515532752388026, Validation Loss: 0.0067200423093538575\n",
      "Epoch [9946/20000], Training Loss: 0.00615518903370165, Validation Loss: 0.0056719006044545495\n",
      "Epoch [9947/20000], Training Loss: 0.008065249208243206, Validation Loss: 0.004000902232722378\n",
      "Epoch [9948/20000], Training Loss: 0.006141000427825409, Validation Loss: 0.005736047163716356\n",
      "Epoch [9949/20000], Training Loss: 0.005471179641192846, Validation Loss: 0.004492002537960259\n",
      "Epoch [9950/20000], Training Loss: 0.007946779392243895, Validation Loss: 0.003662879593384056\n",
      "Epoch [9951/20000], Training Loss: 0.004631288830881074, Validation Loss: 0.003699042604860584\n",
      "Epoch [9952/20000], Training Loss: 0.004704324395528862, Validation Loss: 0.013451716676773426\n",
      "Epoch [9953/20000], Training Loss: 0.009209551207667184, Validation Loss: 0.006245738722392551\n",
      "Epoch [9954/20000], Training Loss: 0.01170976107239774, Validation Loss: 0.006162988174929218\n",
      "Epoch [9955/20000], Training Loss: 0.009258183269724083, Validation Loss: 0.004257023857288037\n",
      "Epoch [9956/20000], Training Loss: 0.004435939496033825, Validation Loss: 0.003961458570289876\n",
      "Epoch [9957/20000], Training Loss: 0.004823044754760174, Validation Loss: 0.00291806246162147\n",
      "Epoch [9958/20000], Training Loss: 0.00393876249459676, Validation Loss: 0.004398027703203411\n",
      "Epoch [9959/20000], Training Loss: 0.013003504456719384, Validation Loss: 0.0034083384302801634\n",
      "Epoch [9960/20000], Training Loss: 0.012201850539505748, Validation Loss: 0.0043300750673986544\n",
      "Epoch [9961/20000], Training Loss: 0.016634657757711984, Validation Loss: 0.07330919376441965\n",
      "Epoch [9962/20000], Training Loss: 0.044132475622713434, Validation Loss: 0.042200346610375826\n",
      "Epoch [9963/20000], Training Loss: 0.0173968509272007, Validation Loss: 0.00405757699491005\n",
      "Epoch [9964/20000], Training Loss: 0.004929858631969962, Validation Loss: 0.008061801229879555\n",
      "Epoch [9965/20000], Training Loss: 0.01042130709902267, Validation Loss: 0.003361454864667784\n",
      "Epoch [9966/20000], Training Loss: 0.005558383708245985, Validation Loss: 0.009630672264125093\n",
      "Epoch [9967/20000], Training Loss: 0.007795159700825545, Validation Loss: 0.005762109643098613\n",
      "Epoch [9968/20000], Training Loss: 0.009562973562944015, Validation Loss: 0.0029358468355959694\n",
      "Epoch [9969/20000], Training Loss: 0.006480331138716012, Validation Loss: 0.009828524253416657\n",
      "Epoch [9970/20000], Training Loss: 0.0082458078936075, Validation Loss: 0.003334415295861553\n",
      "Epoch [9971/20000], Training Loss: 0.005426967058156151, Validation Loss: 0.008274049440793923\n",
      "Epoch [9972/20000], Training Loss: 0.010976022141611403, Validation Loss: 0.0059998311209515775\n",
      "Epoch [9973/20000], Training Loss: 0.007652256082760037, Validation Loss: 0.0042527719803778\n",
      "Epoch [9974/20000], Training Loss: 0.011010050415019837, Validation Loss: 0.004779367716115043\n",
      "Epoch [9975/20000], Training Loss: 0.024124155282541842, Validation Loss: 0.031165415687222146\n",
      "Epoch [9976/20000], Training Loss: 0.013911670946981758, Validation Loss: 0.006757690955112139\n",
      "Epoch [9977/20000], Training Loss: 0.007391283062003952, Validation Loss: 0.004986614423517365\n",
      "Epoch [9978/20000], Training Loss: 0.008378328130610109, Validation Loss: 0.004138226218450265\n",
      "Epoch [9979/20000], Training Loss: 0.011362500386342487, Validation Loss: 0.008534428049427691\n",
      "Epoch [9980/20000], Training Loss: 0.01002668954398749, Validation Loss: 0.005840620188272234\n",
      "Epoch [9981/20000], Training Loss: 0.006136689519085589, Validation Loss: 0.003232149970244791\n",
      "Epoch [9982/20000], Training Loss: 0.004383432964719499, Validation Loss: 0.017151514875185504\n",
      "Epoch [9983/20000], Training Loss: 0.0160442545243963, Validation Loss: 0.06496549717017583\n",
      "Epoch [9984/20000], Training Loss: 0.023089999867287197, Validation Loss: 0.012125661704788269\n",
      "Epoch [9985/20000], Training Loss: 0.021672361669646176, Validation Loss: 0.010368045720641967\n",
      "Epoch [9986/20000], Training Loss: 0.015239883517226969, Validation Loss: 0.014494769305786446\n",
      "Epoch [9987/20000], Training Loss: 0.010123158764664757, Validation Loss: 0.005069799554706021\n",
      "Epoch [9988/20000], Training Loss: 0.006740803794985238, Validation Loss: 0.006796067284061077\n",
      "Epoch [9989/20000], Training Loss: 0.008239909380790778, Validation Loss: 0.004111996716346766\n",
      "Epoch [9990/20000], Training Loss: 0.005953690517734296, Validation Loss: 0.007272113124908434\n",
      "Epoch [9991/20000], Training Loss: 0.008356317965080962, Validation Loss: 0.004180919295224937\n",
      "Epoch [9992/20000], Training Loss: 0.018384533769026996, Validation Loss: 0.0055192032421343485\n",
      "Epoch [9993/20000], Training Loss: 0.015012236729879598, Validation Loss: 0.0031373524906907085\n",
      "Epoch [9994/20000], Training Loss: 0.005872173629800922, Validation Loss: 0.005775677747034739\n",
      "Epoch [9995/20000], Training Loss: 0.005360512037961078, Validation Loss: 0.005475531910251773\n",
      "Epoch [9996/20000], Training Loss: 0.007074133418167808, Validation Loss: 0.009793879937907082\n",
      "Epoch [9997/20000], Training Loss: 0.008187787234159518, Validation Loss: 0.007925518310717703\n",
      "Epoch [9998/20000], Training Loss: 0.007821963168680668, Validation Loss: 0.00403075739714203\n",
      "Epoch [9999/20000], Training Loss: 0.02123613190321651, Validation Loss: 0.004767472568549432\n",
      "Epoch [10000/20000], Training Loss: 0.015502877954401941, Validation Loss: 0.005555757372040456\n",
      "Epoch [10001/20000], Training Loss: 0.004989673490789366, Validation Loss: 0.0038902319054971357\n",
      "Epoch [10002/20000], Training Loss: 0.008476589095412887, Validation Loss: 0.011020719771279592\n",
      "Epoch [10003/20000], Training Loss: 0.0061886990089468396, Validation Loss: 0.004825820556135341\n",
      "Epoch [10004/20000], Training Loss: 0.007501053494447011, Validation Loss: 0.004107529069528937\n",
      "Epoch [10005/20000], Training Loss: 0.005462825665225475, Validation Loss: 0.004323445960721983\n",
      "Epoch [10006/20000], Training Loss: 0.0051606079718372454, Validation Loss: 0.0043201096953566775\n",
      "Epoch [10007/20000], Training Loss: 0.005743125693697948, Validation Loss: 0.0032914297829945227\n",
      "Epoch [10008/20000], Training Loss: 0.007601661629970684, Validation Loss: 0.003743411746102408\n",
      "Epoch [10009/20000], Training Loss: 0.0066794920192998165, Validation Loss: 0.009043584057589473\n",
      "Epoch [10010/20000], Training Loss: 0.011230240912742115, Validation Loss: 0.00992073853259561\n",
      "Epoch [10011/20000], Training Loss: 0.015752742557067125, Validation Loss: 0.008148049936624244\n",
      "Epoch [10012/20000], Training Loss: 0.01253116654489921, Validation Loss: 0.03402771100393563\n",
      "Epoch [10013/20000], Training Loss: 0.028503623450108404, Validation Loss: 0.019834603942720314\n",
      "Epoch [10014/20000], Training Loss: 0.010677134394687269, Validation Loss: 0.0050500165875908875\n",
      "Epoch [10015/20000], Training Loss: 0.01566373049536196, Validation Loss: 0.014244293749145851\n",
      "Epoch [10016/20000], Training Loss: 0.010802522856725514, Validation Loss: 0.0031384783545393346\n",
      "Epoch [10017/20000], Training Loss: 0.0036317211275413036, Validation Loss: 0.005588705896039008\n",
      "Epoch [10018/20000], Training Loss: 0.005151876414726887, Validation Loss: 0.004075755723338911\n",
      "Epoch [10019/20000], Training Loss: 0.005271437071249758, Validation Loss: 0.006483436585516422\n",
      "Epoch [10020/20000], Training Loss: 0.00682540767489367, Validation Loss: 0.0033026634884468615\n",
      "Epoch [10021/20000], Training Loss: 0.0046865246768683965, Validation Loss: 0.002799564812481289\n",
      "Epoch [10022/20000], Training Loss: 0.012148840664719631, Validation Loss: 0.0028828599691093715\n",
      "Epoch [10023/20000], Training Loss: 0.0074428705743879876, Validation Loss: 0.007023474909636221\n",
      "Epoch [10024/20000], Training Loss: 0.006035347604535802, Validation Loss: 0.022628185033075374\n",
      "Epoch [10025/20000], Training Loss: 0.015080565538872699, Validation Loss: 0.019753458246338722\n",
      "Epoch [10026/20000], Training Loss: 0.020291265231402837, Validation Loss: 0.03794643708638449\n",
      "Epoch [10027/20000], Training Loss: 0.030511106958978677, Validation Loss: 0.020485422964009712\n",
      "Epoch [10028/20000], Training Loss: 0.04074839539160686, Validation Loss: 0.02534680121711323\n",
      "Epoch [10029/20000], Training Loss: 0.018072924090249996, Validation Loss: 0.03344560868831884\n",
      "Epoch [10030/20000], Training Loss: 0.011372675182168937, Validation Loss: 0.012414684661329634\n",
      "Epoch [10031/20000], Training Loss: 0.011640811792208947, Validation Loss: 0.01440707491052048\n",
      "Epoch [10032/20000], Training Loss: 0.007320689682793662, Validation Loss: 0.00536766557969161\n",
      "Epoch [10033/20000], Training Loss: 0.011763136555340939, Validation Loss: 0.007648591219496633\n",
      "Epoch [10034/20000], Training Loss: 0.01263214279586273, Validation Loss: 0.007538691385548191\n",
      "Epoch [10035/20000], Training Loss: 0.010638977659774744, Validation Loss: 0.006319440194585825\n",
      "Epoch [10036/20000], Training Loss: 0.00907864886250666, Validation Loss: 0.013668393764536038\n",
      "Epoch [10037/20000], Training Loss: 0.011068326033247493, Validation Loss: 0.004093853489483276\n",
      "Epoch [10038/20000], Training Loss: 0.005032382468925789, Validation Loss: 0.006515183194374988\n",
      "Epoch [10039/20000], Training Loss: 0.01011258282025145, Validation Loss: 0.03608873007552978\n",
      "Epoch [10040/20000], Training Loss: 0.012468966702207191, Validation Loss: 0.00862449378437142\n",
      "Epoch [10041/20000], Training Loss: 0.016300617675629576, Validation Loss: 0.004560373791020668\n",
      "Epoch [10042/20000], Training Loss: 0.010947939388902992, Validation Loss: 0.010035183312148937\n",
      "Epoch [10043/20000], Training Loss: 0.01129626390840193, Validation Loss: 0.011665880588225264\n",
      "Epoch [10044/20000], Training Loss: 0.014901212512216131, Validation Loss: 0.004724673631772346\n",
      "Epoch [10045/20000], Training Loss: 0.007539082875155145, Validation Loss: 0.00377177094531232\n",
      "Epoch [10046/20000], Training Loss: 0.013342167227944759, Validation Loss: 0.012591831257789418\n",
      "Epoch [10047/20000], Training Loss: 0.0071779517041120145, Validation Loss: 0.0036547102750351507\n",
      "Epoch [10048/20000], Training Loss: 0.005829592096103754, Validation Loss: 0.0030652581314630617\n",
      "Epoch [10049/20000], Training Loss: 0.005103095066130793, Validation Loss: 0.0029290515805249534\n",
      "Epoch [10050/20000], Training Loss: 0.0059477216501753515, Validation Loss: 0.00514393450721075\n",
      "Epoch [10051/20000], Training Loss: 0.008630394703556834, Validation Loss: 0.004713965261276242\n",
      "Epoch [10052/20000], Training Loss: 0.00923182447149884, Validation Loss: 0.005173707964250492\n",
      "Epoch [10053/20000], Training Loss: 0.01435078029369054, Validation Loss: 0.0044356470426285865\n",
      "Epoch [10054/20000], Training Loss: 0.008260510991801442, Validation Loss: 0.005670732373175724\n",
      "Epoch [10055/20000], Training Loss: 0.004797678293211253, Validation Loss: 0.008973862655823592\n",
      "Epoch [10056/20000], Training Loss: 0.013207976914563753, Validation Loss: 0.008550867279032965\n",
      "Epoch [10057/20000], Training Loss: 0.013674749625693039, Validation Loss: 0.012570781276144543\n",
      "Epoch [10058/20000], Training Loss: 0.009406383056037677, Validation Loss: 0.0052851499664916856\n",
      "Epoch [10059/20000], Training Loss: 0.0067995822654276184, Validation Loss: 0.008556366164579795\n",
      "Epoch [10060/20000], Training Loss: 0.0085681843650361, Validation Loss: 0.0041260290247325014\n",
      "Epoch [10061/20000], Training Loss: 0.006776707650195541, Validation Loss: 0.0046686900898169625\n",
      "Epoch [10062/20000], Training Loss: 0.009436640196197135, Validation Loss: 0.0031233825631724293\n",
      "Epoch [10063/20000], Training Loss: 0.023060912730280898, Validation Loss: 0.007550919833745086\n",
      "Epoch [10064/20000], Training Loss: 0.010449918629352137, Validation Loss: 0.02257513148503176\n",
      "Epoch [10065/20000], Training Loss: 0.011569252428931318, Validation Loss: 0.008008418623243039\n",
      "Epoch [10066/20000], Training Loss: 0.007288163289298869, Validation Loss: 0.009347929193094821\n",
      "Epoch [10067/20000], Training Loss: 0.007743357411170239, Validation Loss: 0.004470075049328314\n",
      "Epoch [10068/20000], Training Loss: 0.007239049416447025, Validation Loss: 0.006770450693392961\n",
      "Epoch [10069/20000], Training Loss: 0.015052029371353066, Validation Loss: 0.0029351746270752494\n",
      "Epoch [10070/20000], Training Loss: 0.005134332450325019, Validation Loss: 0.004795248674714766\n",
      "Epoch [10071/20000], Training Loss: 0.0035278250579722226, Validation Loss: 0.00259059261665584\n",
      "Epoch [10072/20000], Training Loss: 0.003293533725229051, Validation Loss: 0.0035511406675153368\n",
      "Epoch [10073/20000], Training Loss: 0.0054462337107647075, Validation Loss: 0.0033152043055671193\n",
      "Epoch [10074/20000], Training Loss: 0.006297526574469105, Validation Loss: 0.002625447136901065\n",
      "Epoch [10075/20000], Training Loss: 0.005372360126265059, Validation Loss: 0.008235809500496353\n",
      "Epoch [10076/20000], Training Loss: 0.009341075428762582, Validation Loss: 0.005383392905227158\n",
      "Epoch [10077/20000], Training Loss: 0.0144428107357401, Validation Loss: 0.012949673991350111\n",
      "Epoch [10078/20000], Training Loss: 0.03402980777900666, Validation Loss: 0.02137089961169636\n",
      "Epoch [10079/20000], Training Loss: 0.011533960088854656, Validation Loss: 0.016163181114409897\n",
      "Epoch [10080/20000], Training Loss: 0.018362139847470513, Validation Loss: 0.017227005248723216\n",
      "Epoch [10081/20000], Training Loss: 0.011772766909936243, Validation Loss: 0.012057566882757052\n",
      "Epoch [10082/20000], Training Loss: 0.017975061410522488, Validation Loss: 0.011898942969796851\n",
      "Epoch [10083/20000], Training Loss: 0.010807749844389036, Validation Loss: 0.005512401360034248\n",
      "Epoch [10084/20000], Training Loss: 0.004699005271374647, Validation Loss: 0.0041715382686951285\n",
      "Epoch [10085/20000], Training Loss: 0.011702530157403348, Validation Loss: 0.00881577502110489\n",
      "Epoch [10086/20000], Training Loss: 0.006925587786619352, Validation Loss: 0.008588881459916462\n",
      "Epoch [10087/20000], Training Loss: 0.013004228373314877, Validation Loss: 0.005740514403861903\n",
      "Epoch [10088/20000], Training Loss: 0.00846746718187725, Validation Loss: 0.05352934769221714\n",
      "Epoch [10089/20000], Training Loss: 0.015066824506350582, Validation Loss: 0.011872054389862845\n",
      "Epoch [10090/20000], Training Loss: 0.011989849210359742, Validation Loss: 0.011595913536391191\n",
      "Epoch [10091/20000], Training Loss: 0.010835530994102425, Validation Loss: 0.008945596882149156\n",
      "Epoch [10092/20000], Training Loss: 0.006984021051071717, Validation Loss: 0.005418599741103337\n",
      "Epoch [10093/20000], Training Loss: 0.013989885803831774, Validation Loss: 0.0028423525926411586\n",
      "Epoch [10094/20000], Training Loss: 0.009620997992105427, Validation Loss: 0.007776785434771227\n",
      "Epoch [10095/20000], Training Loss: 0.017686633772557667, Validation Loss: 0.03503301580037862\n",
      "Epoch [10096/20000], Training Loss: 0.007790665366233692, Validation Loss: 0.008134453461409072\n",
      "Epoch [10097/20000], Training Loss: 0.007670397674831163, Validation Loss: 0.0039684300799749704\n",
      "Epoch [10098/20000], Training Loss: 0.007752000108147123, Validation Loss: 0.07657273958043984\n",
      "Epoch [10099/20000], Training Loss: 0.036172379945388196, Validation Loss: 0.03450888444038875\n",
      "Epoch [10100/20000], Training Loss: 0.025936074211910767, Validation Loss: 0.030362625213501054\n",
      "Epoch [10101/20000], Training Loss: 0.013283834166941233, Validation Loss: 0.011023133328016119\n",
      "Epoch [10102/20000], Training Loss: 0.011676610232305913, Validation Loss: 0.014268489928919539\n",
      "Epoch [10103/20000], Training Loss: 0.008093596386191036, Validation Loss: 0.013059897100663116\n",
      "Epoch [10104/20000], Training Loss: 0.009516517292857836, Validation Loss: 0.006925367345610746\n",
      "Epoch [10105/20000], Training Loss: 0.007071342764122944, Validation Loss: 0.005213003093730216\n",
      "Epoch [10106/20000], Training Loss: 0.00622327155932518, Validation Loss: 0.0050087813372765655\n",
      "Epoch [10107/20000], Training Loss: 0.009929751517380023, Validation Loss: 0.005073141013460665\n",
      "Epoch [10108/20000], Training Loss: 0.005079976340182059, Validation Loss: 0.007719193019905123\n",
      "Epoch [10109/20000], Training Loss: 0.006419055760488845, Validation Loss: 0.004266660126030069\n",
      "Epoch [10110/20000], Training Loss: 0.012984191411045945, Validation Loss: 0.003254021874403585\n",
      "Epoch [10111/20000], Training Loss: 0.013206571520895645, Validation Loss: 0.006395625193850459\n",
      "Epoch [10112/20000], Training Loss: 0.011931938564105491, Validation Loss: 0.01614067541410285\n",
      "Epoch [10113/20000], Training Loss: 0.019315646560114277, Validation Loss: 0.002754778000867272\n",
      "Epoch [10114/20000], Training Loss: 0.006206887693094488, Validation Loss: 0.0033728924955447326\n",
      "Epoch [10115/20000], Training Loss: 0.005070868515758775, Validation Loss: 0.003560466814519714\n",
      "Epoch [10116/20000], Training Loss: 0.005265269756949108, Validation Loss: 0.0032911195410625688\n",
      "Epoch [10117/20000], Training Loss: 0.005722731198537596, Validation Loss: 0.0187030149889858\n",
      "Epoch [10118/20000], Training Loss: 0.008944594693145649, Validation Loss: 0.003986155487925913\n",
      "Epoch [10119/20000], Training Loss: 0.003797665956621391, Validation Loss: 0.005303109981880986\n",
      "Epoch [10120/20000], Training Loss: 0.004692093590814953, Validation Loss: 0.004555163091765735\n",
      "Epoch [10121/20000], Training Loss: 0.003056212100740855, Validation Loss: 0.009852981864454182\n",
      "Epoch [10122/20000], Training Loss: 0.009635176004979127, Validation Loss: 0.006650062888220858\n",
      "Epoch [10123/20000], Training Loss: 0.009993382012388403, Validation Loss: 0.024387561350262592\n",
      "Epoch [10124/20000], Training Loss: 0.025975118195154728, Validation Loss: 0.004048310516166355\n",
      "Epoch [10125/20000], Training Loss: 0.011328093910248884, Validation Loss: 0.006318941075207363\n",
      "Epoch [10126/20000], Training Loss: 0.006988911671214737, Validation Loss: 0.00417761397677298\n",
      "Epoch [10127/20000], Training Loss: 0.004060905474554082, Validation Loss: 0.0042891149547327045\n",
      "Epoch [10128/20000], Training Loss: 0.00524175344617106, Validation Loss: 0.005400585582480474\n",
      "Epoch [10129/20000], Training Loss: 0.005297981755575165, Validation Loss: 0.009960297371792746\n",
      "Epoch [10130/20000], Training Loss: 0.009671480684898728, Validation Loss: 0.007968098892865696\n",
      "Epoch [10131/20000], Training Loss: 0.008199079868583274, Validation Loss: 0.009767235988564904\n",
      "Epoch [10132/20000], Training Loss: 0.011400079406614947, Validation Loss: 0.0035091784457894704\n",
      "Epoch [10133/20000], Training Loss: 0.008862832960273539, Validation Loss: 0.0026331769802471683\n",
      "Epoch [10134/20000], Training Loss: 0.0045445753504671825, Validation Loss: 0.004684244482876109\n",
      "Epoch [10135/20000], Training Loss: 0.005560025109194352, Validation Loss: 0.00944485059682131\n",
      "Epoch [10136/20000], Training Loss: 0.004732753580928569, Validation Loss: 0.0026087759716748754\n",
      "Epoch [10137/20000], Training Loss: 0.006946516778953082, Validation Loss: 0.00209979172234398\n",
      "Epoch [10138/20000], Training Loss: 0.0038544201732812716, Validation Loss: 0.010791680171897562\n",
      "Epoch [10139/20000], Training Loss: 0.015411646215073103, Validation Loss: 0.0026641261935083094\n",
      "Epoch [10140/20000], Training Loss: 0.012533983456835682, Validation Loss: 0.0064408847006213925\n",
      "Epoch [10141/20000], Training Loss: 0.01827662856313899, Validation Loss: 0.005132288071383583\n",
      "Epoch [10142/20000], Training Loss: 0.029249600308893214, Validation Loss: 0.00782218395056572\n",
      "Epoch [10143/20000], Training Loss: 0.011762906316105517, Validation Loss: 0.006588093088335286\n",
      "Epoch [10144/20000], Training Loss: 0.007628636932037937, Validation Loss: 0.006007416793798633\n",
      "Epoch [10145/20000], Training Loss: 0.008762753689162699, Validation Loss: 0.003290714524247943\n",
      "Epoch [10146/20000], Training Loss: 0.006128762420952886, Validation Loss: 0.01045906583646435\n",
      "Epoch [10147/20000], Training Loss: 0.012747326379212609, Validation Loss: 0.028837124151842936\n",
      "Epoch [10148/20000], Training Loss: 0.012654522170123528, Validation Loss: 0.031046101487485243\n",
      "Epoch [10149/20000], Training Loss: 0.014749097178407413, Validation Loss: 0.031154749116727283\n",
      "Epoch [10150/20000], Training Loss: 0.013874909897562506, Validation Loss: 0.005234674435094102\n",
      "Epoch [10151/20000], Training Loss: 0.009390467778887666, Validation Loss: 0.00538196017033578\n",
      "Epoch [10152/20000], Training Loss: 0.006012022639359722, Validation Loss: 0.00628242728525915\n",
      "Epoch [10153/20000], Training Loss: 0.011934356737193801, Validation Loss: 0.00378340482051084\n",
      "Epoch [10154/20000], Training Loss: 0.010752884410067054, Validation Loss: 0.003507096087720362\n",
      "Epoch [10155/20000], Training Loss: 0.019110336763593035, Validation Loss: 0.006105591984056777\n",
      "Epoch [10156/20000], Training Loss: 0.009205330539511383, Validation Loss: 0.010573118026497599\n",
      "Epoch [10157/20000], Training Loss: 0.012012624523721463, Validation Loss: 0.0036372575424558867\n",
      "Epoch [10158/20000], Training Loss: 0.014484129261940584, Validation Loss: 0.004184522780955903\n",
      "Epoch [10159/20000], Training Loss: 0.021034716663052677, Validation Loss: 0.018256743574656082\n",
      "Epoch [10160/20000], Training Loss: 0.011403525144227647, Validation Loss: 0.025862025365007063\n",
      "Epoch [10161/20000], Training Loss: 0.011396877193895705, Validation Loss: 0.018082724618063945\n",
      "Epoch [10162/20000], Training Loss: 0.01295634972643711, Validation Loss: 0.0072954309708579005\n",
      "Epoch [10163/20000], Training Loss: 0.0082584993547893, Validation Loss: 0.008483317327098965\n",
      "Epoch [10164/20000], Training Loss: 0.00764181703146148, Validation Loss: 0.0050324913371306524\n",
      "Epoch [10165/20000], Training Loss: 0.0070048529090464785, Validation Loss: 0.003216188046682841\n",
      "Epoch [10166/20000], Training Loss: 0.004479421544861647, Validation Loss: 0.005176159067511321\n",
      "Epoch [10167/20000], Training Loss: 0.006605155984808724, Validation Loss: 0.002595201101777102\n",
      "Epoch [10168/20000], Training Loss: 0.004139440165675167, Validation Loss: 0.0036587216325681465\n",
      "Epoch [10169/20000], Training Loss: 0.0077768453879148835, Validation Loss: 0.010398218541274843\n",
      "Epoch [10170/20000], Training Loss: 0.007064682107219207, Validation Loss: 0.007403326287544916\n",
      "Epoch [10171/20000], Training Loss: 0.012184923022037506, Validation Loss: 0.007595164302376872\n",
      "Epoch [10172/20000], Training Loss: 0.025368534951537316, Validation Loss: 0.008971332479665294\n",
      "Epoch [10173/20000], Training Loss: 0.02329044846296061, Validation Loss: 0.01545987771615793\n",
      "Epoch [10174/20000], Training Loss: 0.014155934194734852, Validation Loss: 0.012006675866347547\n",
      "Epoch [10175/20000], Training Loss: 0.013503219165223916, Validation Loss: 0.004820628445000149\n",
      "Epoch [10176/20000], Training Loss: 0.007752873768497791, Validation Loss: 0.006528542305416123\n",
      "Epoch [10177/20000], Training Loss: 0.008373022424556049, Validation Loss: 0.005496750759243823\n",
      "Epoch [10178/20000], Training Loss: 0.022585522185961184, Validation Loss: 0.04078390236411824\n",
      "Epoch [10179/20000], Training Loss: 0.02680920969734351, Validation Loss: 0.03976115477936978\n",
      "Epoch [10180/20000], Training Loss: 0.011337682205651487, Validation Loss: 0.005459485678289562\n",
      "Epoch [10181/20000], Training Loss: 0.009051362915166854, Validation Loss: 0.0045512790818019256\n",
      "Epoch [10182/20000], Training Loss: 0.007046924820835037, Validation Loss: 0.004071250403789417\n",
      "Epoch [10183/20000], Training Loss: 0.00718491538282251, Validation Loss: 0.0037889075639534\n",
      "Epoch [10184/20000], Training Loss: 0.005580102835665457, Validation Loss: 0.003420002974380308\n",
      "Epoch [10185/20000], Training Loss: 0.005829701886146462, Validation Loss: 0.003154737832649777\n",
      "Epoch [10186/20000], Training Loss: 0.005110140865456613, Validation Loss: 0.004411136919890575\n",
      "Epoch [10187/20000], Training Loss: 0.0037293887892571676, Validation Loss: 0.006727369469146398\n",
      "Epoch [10188/20000], Training Loss: 0.004777377860071803, Validation Loss: 0.0033789700958256224\n",
      "Epoch [10189/20000], Training Loss: 0.004041853597820071, Validation Loss: 0.003032486227638615\n",
      "Epoch [10190/20000], Training Loss: 0.004866227821496848, Validation Loss: 0.0035743187947614124\n",
      "Epoch [10191/20000], Training Loss: 0.005432304430152206, Validation Loss: 0.0035804870727140768\n",
      "Epoch [10192/20000], Training Loss: 0.004567895349201275, Validation Loss: 0.015346950984426956\n",
      "Epoch [10193/20000], Training Loss: 0.009661865917483478, Validation Loss: 0.002730480871093209\n",
      "Epoch [10194/20000], Training Loss: 0.005122172748347761, Validation Loss: 0.0033494335031524986\n",
      "Epoch [10195/20000], Training Loss: 0.004934992301574174, Validation Loss: 0.003009592792278621\n",
      "Epoch [10196/20000], Training Loss: 0.005994047894741276, Validation Loss: 0.010379407630068727\n",
      "Epoch [10197/20000], Training Loss: 0.010361832542912452, Validation Loss: 0.01160327996644244\n",
      "Epoch [10198/20000], Training Loss: 0.012894808826136537, Validation Loss: 0.015227188224740404\n",
      "Epoch [10199/20000], Training Loss: 0.01600168471330627, Validation Loss: 0.036850136305604665\n",
      "Epoch [10200/20000], Training Loss: 0.023817225803993227, Validation Loss: 0.007753544917077094\n",
      "Epoch [10201/20000], Training Loss: 0.012915559724660852, Validation Loss: 0.011336823526240294\n",
      "Epoch [10202/20000], Training Loss: 0.02509379682929388, Validation Loss: 0.0031808935624930384\n",
      "Epoch [10203/20000], Training Loss: 0.023066415659351542, Validation Loss: 0.007704399176887841\n",
      "Epoch [10204/20000], Training Loss: 0.027281514278521563, Validation Loss: 0.014674971962839274\n",
      "Epoch [10205/20000], Training Loss: 0.0176229977431441, Validation Loss: 0.009018706512777799\n",
      "Epoch [10206/20000], Training Loss: 0.02639877502120466, Validation Loss: 0.015836647983373302\n",
      "Epoch [10207/20000], Training Loss: 0.008937304774527937, Validation Loss: 0.004466747918147718\n",
      "Epoch [10208/20000], Training Loss: 0.008732374878101317, Validation Loss: 0.006319520632001411\n",
      "Epoch [10209/20000], Training Loss: 0.010752112798010265, Validation Loss: 0.004899372350787174\n",
      "Epoch [10210/20000], Training Loss: 0.011021000879866603, Validation Loss: 0.00575296476647793\n",
      "Epoch [10211/20000], Training Loss: 0.006318249548452773, Validation Loss: 0.003693304949694617\n",
      "Epoch [10212/20000], Training Loss: 0.005074602318927646, Validation Loss: 0.00626422638832277\n",
      "Epoch [10213/20000], Training Loss: 0.006270572816512348, Validation Loss: 0.004266130548641389\n",
      "Epoch [10214/20000], Training Loss: 0.005378461055240028, Validation Loss: 0.003632888037581772\n",
      "Epoch [10215/20000], Training Loss: 0.015189231040884417, Validation Loss: 0.008215136035032667\n",
      "Epoch [10216/20000], Training Loss: 0.005027636909971989, Validation Loss: 0.005445400203382835\n",
      "Epoch [10217/20000], Training Loss: 0.00862273492800471, Validation Loss: 0.003194643758299543\n",
      "Epoch [10218/20000], Training Loss: 0.006913089105052807, Validation Loss: 0.008123556200741535\n",
      "Epoch [10219/20000], Training Loss: 0.01116593028033159, Validation Loss: 0.0038576091999478835\n",
      "Epoch [10220/20000], Training Loss: 0.004652290438701, Validation Loss: 0.004922524693835402\n",
      "Epoch [10221/20000], Training Loss: 0.004901849587830449, Validation Loss: 0.005049119577836539\n",
      "Epoch [10222/20000], Training Loss: 0.006773047305094744, Validation Loss: 0.0023756172429771866\n",
      "Epoch [10223/20000], Training Loss: 0.009773648885649371, Validation Loss: 0.0028508452213175267\n",
      "Epoch [10224/20000], Training Loss: 0.005002800535294227, Validation Loss: 0.007803403639379592\n",
      "Epoch [10225/20000], Training Loss: 0.007115219649027235, Validation Loss: 0.0029642306545610494\n",
      "Epoch [10226/20000], Training Loss: 0.005054577055748918, Validation Loss: 0.009418659936228226\n",
      "Epoch [10227/20000], Training Loss: 0.014209355357577027, Validation Loss: 0.0076265722627058\n",
      "Epoch [10228/20000], Training Loss: 0.008419628144110902, Validation Loss: 0.003375266762083855\n",
      "Epoch [10229/20000], Training Loss: 0.00957719666205646, Validation Loss: 0.014354522306738155\n",
      "Epoch [10230/20000], Training Loss: 0.0104042888816431, Validation Loss: 0.007847332555478021\n",
      "Epoch [10231/20000], Training Loss: 0.011756745363527443, Validation Loss: 0.015524024276861125\n",
      "Epoch [10232/20000], Training Loss: 0.016863393719242076, Validation Loss: 0.012970617841534112\n",
      "Epoch [10233/20000], Training Loss: 0.037894209879303684, Validation Loss: 0.05289408564568408\n",
      "Epoch [10234/20000], Training Loss: 0.06389053035153276, Validation Loss: 0.010161575067462254\n",
      "Epoch [10235/20000], Training Loss: 0.015968161763989235, Validation Loss: 0.010436570078133903\n",
      "Epoch [10236/20000], Training Loss: 0.010835483233677223, Validation Loss: 0.009450583646909014\n",
      "Epoch [10237/20000], Training Loss: 0.010493630460197372, Validation Loss: 0.004908250285387693\n",
      "Epoch [10238/20000], Training Loss: 0.010091557879734734, Validation Loss: 0.00943456644369704\n",
      "Epoch [10239/20000], Training Loss: 0.0039267332135100984, Validation Loss: 0.005510890246737533\n",
      "Epoch [10240/20000], Training Loss: 0.008727069056476466, Validation Loss: 0.004240251985146658\n",
      "Epoch [10241/20000], Training Loss: 0.005963901344720008, Validation Loss: 0.0064441565726070914\n",
      "Epoch [10242/20000], Training Loss: 0.00422614232437419, Validation Loss: 0.004279217904345093\n",
      "Epoch [10243/20000], Training Loss: 0.004138636529685977, Validation Loss: 0.005070901611294753\n",
      "Epoch [10244/20000], Training Loss: 0.005540037870612683, Validation Loss: 0.01675716063025082\n",
      "Epoch [10245/20000], Training Loss: 0.010913746621164526, Validation Loss: 0.004259797679666079\n",
      "Epoch [10246/20000], Training Loss: 0.0077359723873087205, Validation Loss: 0.004452847455941158\n",
      "Epoch [10247/20000], Training Loss: 0.015655773810197258, Validation Loss: 0.003931111874967641\n",
      "Epoch [10248/20000], Training Loss: 0.019150046771626097, Validation Loss: 0.02559259267789977\n",
      "Epoch [10249/20000], Training Loss: 0.012541794413144609, Validation Loss: 0.025404020079888928\n",
      "Epoch [10250/20000], Training Loss: 0.010292685777260755, Validation Loss: 0.005681468290288478\n",
      "Epoch [10251/20000], Training Loss: 0.006347697226114438, Validation Loss: 0.0035461380446601756\n",
      "Epoch [10252/20000], Training Loss: 0.00520067783405206, Validation Loss: 0.008711687488747753\n",
      "Epoch [10253/20000], Training Loss: 0.007320502398213437, Validation Loss: 0.0034796262126843285\n",
      "Epoch [10254/20000], Training Loss: 0.012132228892405854, Validation Loss: 0.0029662480728207875\n",
      "Epoch [10255/20000], Training Loss: 0.025814497969154866, Validation Loss: 0.011248793453058148\n",
      "Epoch [10256/20000], Training Loss: 0.03885936588838896, Validation Loss: 0.04828002090964999\n",
      "Epoch [10257/20000], Training Loss: 0.017304697406611273, Validation Loss: 0.006086211319863423\n",
      "Epoch [10258/20000], Training Loss: 0.009119462184441676, Validation Loss: 0.007103169603010258\n",
      "Epoch [10259/20000], Training Loss: 0.008387150689616516, Validation Loss: 0.002090744316644469\n",
      "Epoch [10260/20000], Training Loss: 0.026086107753518002, Validation Loss: 0.020778811935867583\n",
      "Epoch [10261/20000], Training Loss: 0.033535676537732276, Validation Loss: 0.015274053879206153\n",
      "Epoch [10262/20000], Training Loss: 0.010591050816791332, Validation Loss: 0.011062441355128108\n",
      "Epoch [10263/20000], Training Loss: 0.009070887251125117, Validation Loss: 0.0067374977404678505\n",
      "Epoch [10264/20000], Training Loss: 0.008217393676334592, Validation Loss: 0.006955530089823826\n",
      "Epoch [10265/20000], Training Loss: 0.00954891050891352, Validation Loss: 0.012209347893953495\n",
      "Epoch [10266/20000], Training Loss: 0.011528913222718984, Validation Loss: 0.007188774203815537\n",
      "Epoch [10267/20000], Training Loss: 0.011315233805882079, Validation Loss: 0.021815802330772776\n",
      "Epoch [10268/20000], Training Loss: 0.014542864542038712, Validation Loss: 0.009735811263085658\n",
      "Epoch [10269/20000], Training Loss: 0.008921531717143287, Validation Loss: 0.006875024365048229\n",
      "Epoch [10270/20000], Training Loss: 0.007195262949348294, Validation Loss: 0.007417134283351905\n",
      "Epoch [10271/20000], Training Loss: 0.00824980123538158, Validation Loss: 0.007226590135680995\n",
      "Epoch [10272/20000], Training Loss: 0.00873491696048794, Validation Loss: 0.003577373962136627\n",
      "Epoch [10273/20000], Training Loss: 0.009089315401589764, Validation Loss: 0.011740090337487944\n",
      "Epoch [10274/20000], Training Loss: 0.008099000476899423, Validation Loss: 0.0044833525507641525\n",
      "Epoch [10275/20000], Training Loss: 0.004642933465739978, Validation Loss: 0.004164721248989066\n",
      "Epoch [10276/20000], Training Loss: 0.005665034675205659, Validation Loss: 0.0057430142019013275\n",
      "Epoch [10277/20000], Training Loss: 0.004805029887523103, Validation Loss: 0.0052892140004577315\n",
      "Epoch [10278/20000], Training Loss: 0.004005401181140249, Validation Loss: 0.0035586069666651565\n",
      "Epoch [10279/20000], Training Loss: 0.005158511104777322, Validation Loss: 0.0050531206332331225\n",
      "Epoch [10280/20000], Training Loss: 0.017988897672954147, Validation Loss: 0.005251904493958132\n",
      "Epoch [10281/20000], Training Loss: 0.05091388680895242, Validation Loss: 0.06600563083376214\n",
      "Epoch [10282/20000], Training Loss: 0.03555425669349331, Validation Loss: 0.014023089029121734\n",
      "Epoch [10283/20000], Training Loss: 0.017051156103012284, Validation Loss: 0.008979205519562439\n",
      "Epoch [10284/20000], Training Loss: 0.007883906430963959, Validation Loss: 0.00524617429627077\n",
      "Epoch [10285/20000], Training Loss: 0.010409684054008852, Validation Loss: 0.010890249926108351\n",
      "Epoch [10286/20000], Training Loss: 0.02372952001210901, Validation Loss: 0.018856905824837407\n",
      "Epoch [10287/20000], Training Loss: 0.03390692986015763, Validation Loss: 0.007509941610197325\n",
      "Epoch [10288/20000], Training Loss: 0.020756985753126043, Validation Loss: 0.01663273302926911\n",
      "Epoch [10289/20000], Training Loss: 0.01680937511888, Validation Loss: 0.014161373050850632\n",
      "Epoch [10290/20000], Training Loss: 0.010338726995022236, Validation Loss: 0.005632978591199489\n",
      "Epoch [10291/20000], Training Loss: 0.005965786079676556, Validation Loss: 0.00651268337401234\n",
      "Epoch [10292/20000], Training Loss: 0.005531696660909802, Validation Loss: 0.004020594828631374\n",
      "Epoch [10293/20000], Training Loss: 0.0057746958378369784, Validation Loss: 0.004117763671602389\n",
      "Epoch [10294/20000], Training Loss: 0.005896199916086127, Validation Loss: 0.003912417534983044\n",
      "Epoch [10295/20000], Training Loss: 0.004634401971478448, Validation Loss: 0.007693188664662163\n",
      "Epoch [10296/20000], Training Loss: 0.005817252916001182, Validation Loss: 0.0085371611535103\n",
      "Epoch [10297/20000], Training Loss: 0.01542369972490373, Validation Loss: 0.0047079074783300074\n",
      "Epoch [10298/20000], Training Loss: 0.010905686707701534, Validation Loss: 0.0056724987408137235\n",
      "Epoch [10299/20000], Training Loss: 0.016162875362039943, Validation Loss: 0.0071739989469777705\n",
      "Epoch [10300/20000], Training Loss: 0.019413722134361575, Validation Loss: 0.007202618389557389\n",
      "Epoch [10301/20000], Training Loss: 0.020668893371212756, Validation Loss: 0.0073801830549316645\n",
      "Epoch [10302/20000], Training Loss: 0.005947585527402615, Validation Loss: 0.010023289227112006\n",
      "Epoch [10303/20000], Training Loss: 0.008475584526812392, Validation Loss: 0.011601736503681185\n",
      "Epoch [10304/20000], Training Loss: 0.006871911497520549, Validation Loss: 0.007511152668731924\n",
      "Epoch [10305/20000], Training Loss: 0.008694002724951133, Validation Loss: 0.006623982961948898\n",
      "Epoch [10306/20000], Training Loss: 0.007993318478319062, Validation Loss: 0.007536448072666145\n",
      "Epoch [10307/20000], Training Loss: 0.005745731152793658, Validation Loss: 0.004479640854596401\n",
      "Epoch [10308/20000], Training Loss: 0.0070562597461990405, Validation Loss: 0.004767582391056838\n",
      "Epoch [10309/20000], Training Loss: 0.005218448420886749, Validation Loss: 0.004649701362099422\n",
      "Epoch [10310/20000], Training Loss: 0.004780525524568345, Validation Loss: 0.003079754456220226\n",
      "Epoch [10311/20000], Training Loss: 0.007983479821600017, Validation Loss: 0.005775016778516472\n",
      "Epoch [10312/20000], Training Loss: 0.009313383173970837, Validation Loss: 0.004247278577948472\n",
      "Epoch [10313/20000], Training Loss: 0.00477849583902363, Validation Loss: 0.0039410895819761375\n",
      "Epoch [10314/20000], Training Loss: 0.007199858045039166, Validation Loss: 0.0047451321235892924\n",
      "Epoch [10315/20000], Training Loss: 0.01530743382797287, Validation Loss: 0.007003720782880204\n",
      "Epoch [10316/20000], Training Loss: 0.0054678048847043624, Validation Loss: 0.0045002443782852725\n",
      "Epoch [10317/20000], Training Loss: 0.005293722191709094, Validation Loss: 0.0043381064315406274\n",
      "Epoch [10318/20000], Training Loss: 0.011361053128244489, Validation Loss: 0.013195045824562018\n",
      "Epoch [10319/20000], Training Loss: 0.007864988557620174, Validation Loss: 0.0041016580245809625\n",
      "Epoch [10320/20000], Training Loss: 0.007792087471378701, Validation Loss: 0.0025486393355046116\n",
      "Epoch [10321/20000], Training Loss: 0.013655206144254148, Validation Loss: 0.0041521074849774475\n",
      "Epoch [10322/20000], Training Loss: 0.012026320777424968, Validation Loss: 0.0026645440275539977\n",
      "Epoch [10323/20000], Training Loss: 0.00433948120558674, Validation Loss: 0.01203329036278386\n",
      "Epoch [10324/20000], Training Loss: 0.006104000960736552, Validation Loss: 0.010489207226783039\n",
      "Epoch [10325/20000], Training Loss: 0.00851848971875516, Validation Loss: 0.002054707551538404\n",
      "Epoch [10326/20000], Training Loss: 0.004332256276809078, Validation Loss: 0.0024791710382482757\n",
      "Epoch [10327/20000], Training Loss: 0.007292877057937274, Validation Loss: 0.003649024825057013\n",
      "Epoch [10328/20000], Training Loss: 0.0037449628497207804, Validation Loss: 0.004344877853478516\n",
      "Epoch [10329/20000], Training Loss: 0.004476256012172338, Validation Loss: 0.004784913792396996\n",
      "Epoch [10330/20000], Training Loss: 0.0053096917740081805, Validation Loss: 0.005837322886141852\n",
      "Epoch [10331/20000], Training Loss: 0.01420164795958304, Validation Loss: 0.002788084334917553\n",
      "Epoch [10332/20000], Training Loss: 0.014604156349897883, Validation Loss: 0.013747457166974461\n",
      "Epoch [10333/20000], Training Loss: 0.014302685785488782, Validation Loss: 0.0029752125047837236\n",
      "Epoch [10334/20000], Training Loss: 0.007436670474183822, Validation Loss: 0.0034827839345029613\n",
      "Epoch [10335/20000], Training Loss: 0.00563475061686144, Validation Loss: 0.002343099913912238\n",
      "Epoch [10336/20000], Training Loss: 0.006232438566387698, Validation Loss: 0.0025422284983571994\n",
      "Epoch [10337/20000], Training Loss: 0.005429355673965931, Validation Loss: 0.0025817902892227742\n",
      "Epoch [10338/20000], Training Loss: 0.010815218324751186, Validation Loss: 0.004116579127357213\n",
      "Epoch [10339/20000], Training Loss: 0.01808718532005774, Validation Loss: 0.010967753271543939\n",
      "Epoch [10340/20000], Training Loss: 0.007324780657654628, Validation Loss: 0.008000092044200682\n",
      "Epoch [10341/20000], Training Loss: 0.003939015817325396, Validation Loss: 0.010568139276334222\n",
      "Epoch [10342/20000], Training Loss: 0.007904598454063359, Validation Loss: 0.006044178219675749\n",
      "Epoch [10343/20000], Training Loss: 0.01160164032184444, Validation Loss: 0.015388667099555225\n",
      "Epoch [10344/20000], Training Loss: 0.01356506250574187, Validation Loss: 0.023591579597361294\n",
      "Epoch [10345/20000], Training Loss: 0.0325538418655924, Validation Loss: 0.011435773922130816\n",
      "Epoch [10346/20000], Training Loss: 0.013819997579828072, Validation Loss: 0.0075994848938145355\n",
      "Epoch [10347/20000], Training Loss: 0.007670265494588031, Validation Loss: 0.0046802453796581335\n",
      "Epoch [10348/20000], Training Loss: 0.01006201786542452, Validation Loss: 0.0035058183716242253\n",
      "Epoch [10349/20000], Training Loss: 0.005881783963559428, Validation Loss: 0.0029109112986756636\n",
      "Epoch [10350/20000], Training Loss: 0.005895442872445399, Validation Loss: 0.0046189173957191165\n",
      "Epoch [10351/20000], Training Loss: 0.015016764279737669, Validation Loss: 0.00402647525461897\n",
      "Epoch [10352/20000], Training Loss: 0.007261120965787475, Validation Loss: 0.004069089638316203\n",
      "Epoch [10353/20000], Training Loss: 0.005269603579758301, Validation Loss: 0.003841469571607249\n",
      "Epoch [10354/20000], Training Loss: 0.009075306582547325, Validation Loss: 0.0031635944913320194\n",
      "Epoch [10355/20000], Training Loss: 0.0059191551695870915, Validation Loss: 0.006153087796910614\n",
      "Epoch [10356/20000], Training Loss: 0.006309946476124294, Validation Loss: 0.004233232018145946\n",
      "Epoch [10357/20000], Training Loss: 0.004946877129831202, Validation Loss: 0.0027550608873436427\n",
      "Epoch [10358/20000], Training Loss: 0.006097196196995875, Validation Loss: 0.004489142266709647\n",
      "Epoch [10359/20000], Training Loss: 0.00807140943925333, Validation Loss: 0.012628057439412392\n",
      "Epoch [10360/20000], Training Loss: 0.006705124684361051, Validation Loss: 0.018904197561954123\n",
      "Epoch [10361/20000], Training Loss: 0.007870360813935154, Validation Loss: 0.002784152723128475\n",
      "Epoch [10362/20000], Training Loss: 0.0058412963982326415, Validation Loss: 0.005757524412976929\n",
      "Epoch [10363/20000], Training Loss: 0.006136912329233317, Validation Loss: 0.0044595409625410535\n",
      "Epoch [10364/20000], Training Loss: 0.008862469440958063, Validation Loss: 0.007531514684814563\n",
      "Epoch [10365/20000], Training Loss: 0.012059419299475849, Validation Loss: 0.0023612434415196406\n",
      "Epoch [10366/20000], Training Loss: 0.02274132000746119, Validation Loss: 0.007341589319025843\n",
      "Epoch [10367/20000], Training Loss: 0.01261692714446586, Validation Loss: 0.004484418280690641\n",
      "Epoch [10368/20000], Training Loss: 0.008732609985080282, Validation Loss: 0.0025182725947113654\n",
      "Epoch [10369/20000], Training Loss: 0.00852229140790379, Validation Loss: 0.02250669098326138\n",
      "Epoch [10370/20000], Training Loss: 0.00814457670198213, Validation Loss: 0.004891053774035772\n",
      "Epoch [10371/20000], Training Loss: 0.012194162499196994, Validation Loss: 0.005466288163526274\n",
      "Epoch [10372/20000], Training Loss: 0.023076911485986784, Validation Loss: 0.003688645748855382\n",
      "Epoch [10373/20000], Training Loss: 0.04794185209488384, Validation Loss: 0.008740735872253063\n",
      "Epoch [10374/20000], Training Loss: 0.038028480406085564, Validation Loss: 0.009811877506760987\n",
      "Epoch [10375/20000], Training Loss: 0.0632884723267385, Validation Loss: 0.053207262346404605\n",
      "Epoch [10376/20000], Training Loss: 0.042489502379404645, Validation Loss: 0.014201347487777127\n",
      "Epoch [10377/20000], Training Loss: 0.02006439005136989, Validation Loss: 0.016939527389092084\n",
      "Epoch [10378/20000], Training Loss: 0.014848630268326295, Validation Loss: 0.010217914612246113\n",
      "Epoch [10379/20000], Training Loss: 0.008350807599656816, Validation Loss: 0.00876641128213253\n",
      "Epoch [10380/20000], Training Loss: 0.008921797961063151, Validation Loss: 0.0069496962089797675\n",
      "Epoch [10381/20000], Training Loss: 0.006070135514684287, Validation Loss: 0.005715424256029016\n",
      "Epoch [10382/20000], Training Loss: 0.007144513544127611, Validation Loss: 0.006725633215864946\n",
      "Epoch [10383/20000], Training Loss: 0.006566406510371182, Validation Loss: 0.008499915980278279\n",
      "Epoch [10384/20000], Training Loss: 0.00628582645107859, Validation Loss: 0.004700782109686615\n",
      "Epoch [10385/20000], Training Loss: 0.004595830588576584, Validation Loss: 0.006336839944037236\n",
      "Epoch [10386/20000], Training Loss: 0.007426859445071646, Validation Loss: 0.003425337039130965\n",
      "Epoch [10387/20000], Training Loss: 0.006575616897732418, Validation Loss: 0.003401927151165563\n",
      "Epoch [10388/20000], Training Loss: 0.004597288020054943, Validation Loss: 0.004384615790507589\n",
      "Epoch [10389/20000], Training Loss: 0.007021850421941573, Validation Loss: 0.004636834361889235\n",
      "Epoch [10390/20000], Training Loss: 0.010092650475728857, Validation Loss: 0.010984490625560511\n",
      "Epoch [10391/20000], Training Loss: 0.005040418273503227, Validation Loss: 0.003144487062284711\n",
      "Epoch [10392/20000], Training Loss: 0.006506613767395412, Validation Loss: 0.0028042282645593685\n",
      "Epoch [10393/20000], Training Loss: 0.006476690527538355, Validation Loss: 0.009553980350430282\n",
      "Epoch [10394/20000], Training Loss: 0.010693815076332871, Validation Loss: 0.004038167568881058\n",
      "Epoch [10395/20000], Training Loss: 0.005694785320810041, Validation Loss: 0.004458919857708636\n",
      "Epoch [10396/20000], Training Loss: 0.01309971520406959, Validation Loss: 0.0024627793593687114\n",
      "Epoch [10397/20000], Training Loss: 0.023096102081970975, Validation Loss: 0.0074947942713584435\n",
      "Epoch [10398/20000], Training Loss: 0.00997349460625888, Validation Loss: 0.0030927206230791883\n",
      "Epoch [10399/20000], Training Loss: 0.008281161672909678, Validation Loss: 0.002990421452123211\n",
      "Epoch [10400/20000], Training Loss: 0.007378330749426303, Validation Loss: 0.00479550325952497\n",
      "Epoch [10401/20000], Training Loss: 0.0067962673659037265, Validation Loss: 0.002454810575475439\n",
      "Epoch [10402/20000], Training Loss: 0.0058116865880687586, Validation Loss: 0.009163636847266272\n",
      "Epoch [10403/20000], Training Loss: 0.0062544853452501615, Validation Loss: 0.004739461100819309\n",
      "Epoch [10404/20000], Training Loss: 0.008013556996437339, Validation Loss: 0.0027613598963817\n",
      "Epoch [10405/20000], Training Loss: 0.00410090977051628, Validation Loss: 0.003826118724801272\n",
      "Epoch [10406/20000], Training Loss: 0.004196033189165885, Validation Loss: 0.006759644585794698\n",
      "Epoch [10407/20000], Training Loss: 0.005515892011836903, Validation Loss: 0.006448960131300708\n",
      "Epoch [10408/20000], Training Loss: 0.008891080163136524, Validation Loss: 0.004106544856255219\n",
      "Epoch [10409/20000], Training Loss: 0.015405672718770802, Validation Loss: 0.011070065599467074\n",
      "Epoch [10410/20000], Training Loss: 0.011034422610204533, Validation Loss: 0.022752589206527236\n",
      "Epoch [10411/20000], Training Loss: 0.011771175510148169, Validation Loss: 0.026863139482770748\n",
      "Epoch [10412/20000], Training Loss: 0.01855127765968843, Validation Loss: 0.02569171708408258\n",
      "Epoch [10413/20000], Training Loss: 0.026968729137609313, Validation Loss: 0.006102665744005468\n",
      "Epoch [10414/20000], Training Loss: 0.008343469343214695, Validation Loss: 0.01594080589711695\n",
      "Epoch [10415/20000], Training Loss: 0.00850580757833086, Validation Loss: 0.01952318155339786\n",
      "Epoch [10416/20000], Training Loss: 0.022795630691819366, Validation Loss: 0.00383700166022746\n",
      "Epoch [10417/20000], Training Loss: 0.0107266987449423, Validation Loss: 0.006328477259862111\n",
      "Epoch [10418/20000], Training Loss: 0.007945927022124774, Validation Loss: 0.004747148362148261\n",
      "Epoch [10419/20000], Training Loss: 0.005851733736303036, Validation Loss: 0.009372706845267333\n",
      "Epoch [10420/20000], Training Loss: 0.015766415734203423, Validation Loss: 0.005470512734218344\n",
      "Epoch [10421/20000], Training Loss: 0.010412076305978448, Validation Loss: 0.004941851200015627\n",
      "Epoch [10422/20000], Training Loss: 0.006932035612278144, Validation Loss: 0.020602877650942184\n",
      "Epoch [10423/20000], Training Loss: 0.05110745008901826, Validation Loss: 0.11258519547326225\n",
      "Epoch [10424/20000], Training Loss: 0.04772546978451179, Validation Loss: 0.09740747085639409\n",
      "Epoch [10425/20000], Training Loss: 0.06580607641288745, Validation Loss: 0.0068840919152860025\n",
      "Epoch [10426/20000], Training Loss: 0.03640730902413582, Validation Loss: 0.07308769303608642\n",
      "Epoch [10427/20000], Training Loss: 0.029565195575872037, Validation Loss: 0.015078538661615971\n",
      "Epoch [10428/20000], Training Loss: 0.008719955587626569, Validation Loss: 0.009586513811942865\n",
      "Epoch [10429/20000], Training Loss: 0.007582391676870819, Validation Loss: 0.007553541563524129\n",
      "Epoch [10430/20000], Training Loss: 0.00691208486594925, Validation Loss: 0.007337793191429075\n",
      "Epoch [10431/20000], Training Loss: 0.0060416942641105765, Validation Loss: 0.006354518347181156\n",
      "Epoch [10432/20000], Training Loss: 0.008091427791181818, Validation Loss: 0.004400135128424556\n",
      "Epoch [10433/20000], Training Loss: 0.0061537759694536886, Validation Loss: 0.005217109845673378\n",
      "Epoch [10434/20000], Training Loss: 0.00817425357360792, Validation Loss: 0.006153208270153493\n",
      "Epoch [10435/20000], Training Loss: 0.005853779297987265, Validation Loss: 0.005336089658423855\n",
      "Epoch [10436/20000], Training Loss: 0.006555467364056052, Validation Loss: 0.004115176187576232\n",
      "Epoch [10437/20000], Training Loss: 0.0062774331225747505, Validation Loss: 0.005509749629378757\n",
      "Epoch [10438/20000], Training Loss: 0.005234503390965983, Validation Loss: 0.003930987765290875\n",
      "Epoch [10439/20000], Training Loss: 0.00570915946238009, Validation Loss: 0.003062578849590991\n",
      "Epoch [10440/20000], Training Loss: 0.00937053376401309, Validation Loss: 0.0038763468286869855\n",
      "Epoch [10441/20000], Training Loss: 0.007955699723314271, Validation Loss: 0.012314840873611798\n",
      "Epoch [10442/20000], Training Loss: 0.027995038636228337, Validation Loss: 0.02220395465258506\n",
      "Epoch [10443/20000], Training Loss: 0.012944596978611247, Validation Loss: 0.007398750942752787\n",
      "Epoch [10444/20000], Training Loss: 0.005809183471553426, Validation Loss: 0.0031134929746528374\n",
      "Epoch [10445/20000], Training Loss: 0.006346179147450519, Validation Loss: 0.004005450090146886\n",
      "Epoch [10446/20000], Training Loss: 0.014102102510930439, Validation Loss: 0.005099500170827942\n",
      "Epoch [10447/20000], Training Loss: 0.012113366707386117, Validation Loss: 0.00808318337254096\n",
      "Epoch [10448/20000], Training Loss: 0.007436002109898254, Validation Loss: 0.011716286119605816\n",
      "Epoch [10449/20000], Training Loss: 0.007798180995178784, Validation Loss: 0.003583618091022766\n",
      "Epoch [10450/20000], Training Loss: 0.005254278680825207, Validation Loss: 0.0042860873176617225\n",
      "Epoch [10451/20000], Training Loss: 0.008886873719088076, Validation Loss: 0.004471443327410671\n",
      "Epoch [10452/20000], Training Loss: 0.011448443328845315, Validation Loss: 0.006107584972468119\n",
      "Epoch [10453/20000], Training Loss: 0.046073048689452535, Validation Loss: 0.009180468597280975\n",
      "Epoch [10454/20000], Training Loss: 0.022108685054783046, Validation Loss: 0.005480775556601917\n",
      "Epoch [10455/20000], Training Loss: 0.013892539373565731, Validation Loss: 0.007643867031999564\n",
      "Epoch [10456/20000], Training Loss: 0.0059244612556150445, Validation Loss: 0.005252752504929958\n",
      "Epoch [10457/20000], Training Loss: 0.00552876681467751, Validation Loss: 0.004297724447886659\n",
      "Epoch [10458/20000], Training Loss: 0.005258670384396932, Validation Loss: 0.004308439813939913\n",
      "Epoch [10459/20000], Training Loss: 0.004879506151025582, Validation Loss: 0.004060001488791646\n",
      "Epoch [10460/20000], Training Loss: 0.006042975641321391, Validation Loss: 0.004465118499345928\n",
      "Epoch [10461/20000], Training Loss: 0.006508284028054082, Validation Loss: 0.004353769697640278\n",
      "Epoch [10462/20000], Training Loss: 0.01678405176166312, Validation Loss: 0.028160366233091087\n",
      "Epoch [10463/20000], Training Loss: 0.02479066313909633, Validation Loss: 0.03737816284085836\n",
      "Epoch [10464/20000], Training Loss: 0.02383417370479687, Validation Loss: 0.0064011881193956855\n",
      "Epoch [10465/20000], Training Loss: 0.013026007431368012, Validation Loss: 0.01638316446457923\n",
      "Epoch [10466/20000], Training Loss: 0.008721421705558896, Validation Loss: 0.005616673607359449\n",
      "Epoch [10467/20000], Training Loss: 0.007098832529404068, Validation Loss: 0.005898046492932606\n",
      "Epoch [10468/20000], Training Loss: 0.006651906330944257, Validation Loss: 0.02268282598044737\n",
      "Epoch [10469/20000], Training Loss: 0.013524698791505736, Validation Loss: 0.03272174693688612\n",
      "Epoch [10470/20000], Training Loss: 0.023040687880503747, Validation Loss: 0.030418049329780063\n",
      "Epoch [10471/20000], Training Loss: 0.03488267055529702, Validation Loss: 0.005670947584478979\n",
      "Epoch [10472/20000], Training Loss: 0.028838797260375162, Validation Loss: 0.02283322120612346\n",
      "Epoch [10473/20000], Training Loss: 0.0127687636658363, Validation Loss: 0.006790980849343343\n",
      "Epoch [10474/20000], Training Loss: 0.0116389316182384, Validation Loss: 0.01244420567796364\n",
      "Epoch [10475/20000], Training Loss: 0.005443275284570908, Validation Loss: 0.004971361455773149\n",
      "Epoch [10476/20000], Training Loss: 0.007162826040127713, Validation Loss: 0.004856539939867487\n",
      "Epoch [10477/20000], Training Loss: 0.005994626878029001, Validation Loss: 0.0043779898666603655\n",
      "Epoch [10478/20000], Training Loss: 0.007665275928697416, Validation Loss: 0.003988300017421612\n",
      "Epoch [10479/20000], Training Loss: 0.004570280987536535, Validation Loss: 0.0035916500103212456\n",
      "Epoch [10480/20000], Training Loss: 0.005983090223034131, Validation Loss: 0.007204619157586681\n",
      "Epoch [10481/20000], Training Loss: 0.008887524051325662, Validation Loss: 0.004932746995044519\n",
      "Epoch [10482/20000], Training Loss: 0.005692187096298896, Validation Loss: 0.006053457695920932\n",
      "Epoch [10483/20000], Training Loss: 0.008534842499427344, Validation Loss: 0.0034416057719681703\n",
      "Epoch [10484/20000], Training Loss: 0.006183599338068494, Validation Loss: 0.006531360536178342\n",
      "Epoch [10485/20000], Training Loss: 0.010052573228157209, Validation Loss: 0.004214171144207158\n",
      "Epoch [10486/20000], Training Loss: 0.022840333382168825, Validation Loss: 0.016812300581648287\n",
      "Epoch [10487/20000], Training Loss: 0.021778096305830275, Validation Loss: 0.009300343570722252\n",
      "Epoch [10488/20000], Training Loss: 0.023112128527178095, Validation Loss: 0.006669783639824794\n",
      "Epoch [10489/20000], Training Loss: 0.034313460753050355, Validation Loss: 0.025460738203654337\n",
      "Epoch [10490/20000], Training Loss: 0.05100981295150372, Validation Loss: 0.021963655312708306\n",
      "Epoch [10491/20000], Training Loss: 0.0531321069117569, Validation Loss: 0.03700774859442186\n",
      "Epoch [10492/20000], Training Loss: 0.017500852550646023, Validation Loss: 0.011232948243381391\n",
      "Epoch [10493/20000], Training Loss: 0.016343174581249644, Validation Loss: 0.009154615071435859\n",
      "Epoch [10494/20000], Training Loss: 0.00857804053729134, Validation Loss: 0.006577440672588745\n",
      "Epoch [10495/20000], Training Loss: 0.007497126693903867, Validation Loss: 0.015329640185444185\n",
      "Epoch [10496/20000], Training Loss: 0.013766171490715351, Validation Loss: 0.016170157569506825\n",
      "Epoch [10497/20000], Training Loss: 0.018325275151125555, Validation Loss: 0.0057090825588862314\n",
      "Epoch [10498/20000], Training Loss: 0.008617785902710498, Validation Loss: 0.008884456658051931\n",
      "Epoch [10499/20000], Training Loss: 0.010955773676479501, Validation Loss: 0.005304153392834061\n",
      "Epoch [10500/20000], Training Loss: 0.005116255084950743, Validation Loss: 0.004489888747100069\n",
      "Epoch [10501/20000], Training Loss: 0.004894020776142887, Validation Loss: 0.005357900271617057\n",
      "Epoch [10502/20000], Training Loss: 0.004371363170711058, Validation Loss: 0.003894309062843604\n",
      "Epoch [10503/20000], Training Loss: 0.004455736811256169, Validation Loss: 0.007548551578305965\n",
      "Epoch [10504/20000], Training Loss: 0.008725980657702945, Validation Loss: 0.007147353741856932\n",
      "Epoch [10505/20000], Training Loss: 0.007704828090416933, Validation Loss: 0.0037004939027350963\n",
      "Epoch [10506/20000], Training Loss: 0.005474783163143522, Validation Loss: 0.020316097707369148\n",
      "Epoch [10507/20000], Training Loss: 0.022343186079524457, Validation Loss: 0.009503858290470037\n",
      "Epoch [10508/20000], Training Loss: 0.011203545904468879, Validation Loss: 0.003693734784617066\n",
      "Epoch [10509/20000], Training Loss: 0.004952515537297586, Validation Loss: 0.003548424932627573\n",
      "Epoch [10510/20000], Training Loss: 0.01055790514302706, Validation Loss: 0.011467960421116783\n",
      "Epoch [10511/20000], Training Loss: 0.014646004610602503, Validation Loss: 0.00374351614459556\n",
      "Epoch [10512/20000], Training Loss: 0.030075574709761504, Validation Loss: 0.012280763369545358\n",
      "Epoch [10513/20000], Training Loss: 0.1024865553138495, Validation Loss: 0.0826916426935672\n",
      "Epoch [10514/20000], Training Loss: 0.06845736780555203, Validation Loss: 0.033966174921588835\n",
      "Epoch [10515/20000], Training Loss: 0.02990618072051023, Validation Loss: 0.03368914971804455\n",
      "Epoch [10516/20000], Training Loss: 0.026137579421629198, Validation Loss: 0.01757697908824036\n",
      "Epoch [10517/20000], Training Loss: 0.017795219086110592, Validation Loss: 0.01776388177868934\n",
      "Epoch [10518/20000], Training Loss: 0.013212799028094326, Validation Loss: 0.009022210065657938\n",
      "Epoch [10519/20000], Training Loss: 0.008726042761866535, Validation Loss: 0.0065067456879270525\n",
      "Epoch [10520/20000], Training Loss: 0.009220005645797105, Validation Loss: 0.006456816940304374\n",
      "Epoch [10521/20000], Training Loss: 0.008209966588765383, Validation Loss: 0.009323913092105483\n",
      "Epoch [10522/20000], Training Loss: 0.00930386341393127, Validation Loss: 0.004147705200011842\n",
      "Epoch [10523/20000], Training Loss: 0.007594259987984385, Validation Loss: 0.006021205663533432\n",
      "Epoch [10524/20000], Training Loss: 0.00799556105890328, Validation Loss: 0.0037607187740442377\n",
      "Epoch [10525/20000], Training Loss: 0.01024041539598589, Validation Loss: 0.008245575727284762\n",
      "Epoch [10526/20000], Training Loss: 0.011560304709876488, Validation Loss: 0.0070612888316645694\n",
      "Epoch [10527/20000], Training Loss: 0.013345581030340068, Validation Loss: 0.022949046057939477\n",
      "Epoch [10528/20000], Training Loss: 0.007217873605051344, Validation Loss: 0.019197700932896362\n",
      "Epoch [10529/20000], Training Loss: 0.008767839162557729, Validation Loss: 0.004824859902068316\n",
      "Epoch [10530/20000], Training Loss: 0.005826960038185851, Validation Loss: 0.003812556716833829\n",
      "Epoch [10531/20000], Training Loss: 0.00509168344277506, Validation Loss: 0.004680931461994727\n",
      "Epoch [10532/20000], Training Loss: 0.004593782635181144, Validation Loss: 0.004104961242774152\n",
      "Epoch [10533/20000], Training Loss: 0.00383978080449765, Validation Loss: 0.004540391635081116\n",
      "Epoch [10534/20000], Training Loss: 0.005304788922200844, Validation Loss: 0.0034223926662499643\n",
      "Epoch [10535/20000], Training Loss: 0.005499406364054137, Validation Loss: 0.0030738696570032126\n",
      "Epoch [10536/20000], Training Loss: 0.006947755098475942, Validation Loss: 0.0028980592530518023\n",
      "Epoch [10537/20000], Training Loss: 0.005071559139781, Validation Loss: 0.0030815731532649287\n",
      "Epoch [10538/20000], Training Loss: 0.003763137520374065, Validation Loss: 0.007728052015506494\n",
      "Epoch [10539/20000], Training Loss: 0.006643882831018085, Validation Loss: 0.0028690523935048467\n",
      "Epoch [10540/20000], Training Loss: 0.003653517213284171, Validation Loss: 0.0027552357542585276\n",
      "Epoch [10541/20000], Training Loss: 0.004824389999807214, Validation Loss: 0.007448023740965125\n",
      "Epoch [10542/20000], Training Loss: 0.0073045944607851455, Validation Loss: 0.009258305107388358\n",
      "Epoch [10543/20000], Training Loss: 0.006785791698933151, Validation Loss: 0.004709938145132193\n",
      "Epoch [10544/20000], Training Loss: 0.010711913015256869, Validation Loss: 0.00440050832279277\n",
      "Epoch [10545/20000], Training Loss: 0.007591603876790032, Validation Loss: 0.008954665215985156\n",
      "Epoch [10546/20000], Training Loss: 0.020643358658032542, Validation Loss: 0.008069423313080344\n",
      "Epoch [10547/20000], Training Loss: 0.060249090507568326, Validation Loss: 0.015535108000960918\n",
      "Epoch [10548/20000], Training Loss: 0.08083187869384087, Validation Loss: 0.08198626726313815\n",
      "Epoch [10549/20000], Training Loss: 0.05480263753270265, Validation Loss: 0.033639341059404436\n",
      "Epoch [10550/20000], Training Loss: 0.024365701213745133, Validation Loss: 0.007277692555102736\n",
      "Epoch [10551/20000], Training Loss: 0.015038601010538903, Validation Loss: 0.01131282309506787\n",
      "Epoch [10552/20000], Training Loss: 0.018749377010473318, Validation Loss: 0.018827270399048062\n",
      "Epoch [10553/20000], Training Loss: 0.0126658106206118, Validation Loss: 0.008331102924590337\n",
      "Epoch [10554/20000], Training Loss: 0.00803311197419784, Validation Loss: 0.008022317127532526\n",
      "Epoch [10555/20000], Training Loss: 0.006701287560385286, Validation Loss: 0.00555219415036845\n",
      "Epoch [10556/20000], Training Loss: 0.007534972784014826, Validation Loss: 0.012720291279557776\n",
      "Epoch [10557/20000], Training Loss: 0.008324834518134594, Validation Loss: 0.013416008359594602\n",
      "Epoch [10558/20000], Training Loss: 0.016039544909809984, Validation Loss: 0.00698812224995241\n",
      "Epoch [10559/20000], Training Loss: 0.02243517287375393, Validation Loss: 0.005151678280006828\n",
      "Epoch [10560/20000], Training Loss: 0.013923007595751966, Validation Loss: 0.019859733005042863\n",
      "Epoch [10561/20000], Training Loss: 0.010874049230551464, Validation Loss: 0.004543562455442692\n",
      "Epoch [10562/20000], Training Loss: 0.014306446612733583, Validation Loss: 0.013408926267922132\n",
      "Epoch [10563/20000], Training Loss: 0.011986531670637695, Validation Loss: 0.0043416299518607305\n",
      "Epoch [10564/20000], Training Loss: 0.005390747801616921, Validation Loss: 0.006776091607467346\n",
      "Epoch [10565/20000], Training Loss: 0.005109259862885145, Validation Loss: 0.005178597120837887\n",
      "Epoch [10566/20000], Training Loss: 0.007467499169121895, Validation Loss: 0.003647827207389907\n",
      "Epoch [10567/20000], Training Loss: 0.005703870080261757, Validation Loss: 0.010901258547448833\n",
      "Epoch [10568/20000], Training Loss: 0.008794017406346808, Validation Loss: 0.004190770455352322\n",
      "Epoch [10569/20000], Training Loss: 0.007704016430320085, Validation Loss: 0.008877995964602243\n",
      "Epoch [10570/20000], Training Loss: 0.011798386574290427, Validation Loss: 0.004283812454364774\n",
      "Epoch [10571/20000], Training Loss: 0.011323355009413458, Validation Loss: 0.004565211431879236\n",
      "Epoch [10572/20000], Training Loss: 0.01128648786834674, Validation Loss: 0.0065406549175739724\n",
      "Epoch [10573/20000], Training Loss: 0.005319382130567517, Validation Loss: 0.006191423522052121\n",
      "Epoch [10574/20000], Training Loss: 0.0046821128076512, Validation Loss: 0.006305667505785095\n",
      "Epoch [10575/20000], Training Loss: 0.004811281318974839, Validation Loss: 0.0037723827189698306\n",
      "Epoch [10576/20000], Training Loss: 0.005079205152079729, Validation Loss: 0.005415595939260649\n",
      "Epoch [10577/20000], Training Loss: 0.005725585131325975, Validation Loss: 0.004158207861597215\n",
      "Epoch [10578/20000], Training Loss: 0.00590425171554086, Validation Loss: 0.002925336511030504\n",
      "Epoch [10579/20000], Training Loss: 0.005061099440873866, Validation Loss: 0.0038972043995662326\n",
      "Epoch [10580/20000], Training Loss: 0.011164766278982694, Validation Loss: 0.003502119201617201\n",
      "Epoch [10581/20000], Training Loss: 0.02245096593989209, Validation Loss: 0.004374019918291846\n",
      "Epoch [10582/20000], Training Loss: 0.01349618740329918, Validation Loss: 0.005336247461757791\n",
      "Epoch [10583/20000], Training Loss: 0.011131559516278295, Validation Loss: 0.005213861815507569\n",
      "Epoch [10584/20000], Training Loss: 0.009007538384009552, Validation Loss: 0.0050537755752852066\n",
      "Epoch [10585/20000], Training Loss: 0.008716462420125026, Validation Loss: 0.003854733697786677\n",
      "Epoch [10586/20000], Training Loss: 0.006399484000991963, Validation Loss: 0.00542190350808726\n",
      "Epoch [10587/20000], Training Loss: 0.0058566869452728754, Validation Loss: 0.005438030459929776\n",
      "Epoch [10588/20000], Training Loss: 0.009504649475275073, Validation Loss: 0.013652257840736379\n",
      "Epoch [10589/20000], Training Loss: 0.011421677852922585, Validation Loss: 0.0044097773910851045\n",
      "Epoch [10590/20000], Training Loss: 0.02049123273796535, Validation Loss: 0.01925360496539127\n",
      "Epoch [10591/20000], Training Loss: 0.026837142240505533, Validation Loss: 0.009039459656743734\n",
      "Epoch [10592/20000], Training Loss: 0.017405322716513183, Validation Loss: 0.016864297084130238\n",
      "Epoch [10593/20000], Training Loss: 0.020625991240292154, Validation Loss: 0.004751131746908348\n",
      "Epoch [10594/20000], Training Loss: 0.005396683422856897, Validation Loss: 0.0032999761017760776\n",
      "Epoch [10595/20000], Training Loss: 0.007354896170519558, Validation Loss: 0.009132122967327762\n",
      "Epoch [10596/20000], Training Loss: 0.007475282976104479, Validation Loss: 0.004220328560653083\n",
      "Epoch [10597/20000], Training Loss: 0.007665820224376928, Validation Loss: 0.009431949750803272\n",
      "Epoch [10598/20000], Training Loss: 0.011347184270009945, Validation Loss: 0.00433241795431221\n",
      "Epoch [10599/20000], Training Loss: 0.005865961733174377, Validation Loss: 0.00433106142690219\n",
      "Epoch [10600/20000], Training Loss: 0.004679083200504205, Validation Loss: 0.002986728626410695\n",
      "Epoch [10601/20000], Training Loss: 0.0038012912420007134, Validation Loss: 0.0062491978629079115\n",
      "Epoch [10602/20000], Training Loss: 0.006425455827573907, Validation Loss: 0.004921051019463464\n",
      "Epoch [10603/20000], Training Loss: 0.012294475662721587, Validation Loss: 0.005044623204997022\n",
      "Epoch [10604/20000], Training Loss: 0.005873449955061005, Validation Loss: 0.007364906255023078\n",
      "Epoch [10605/20000], Training Loss: 0.004666284241466201, Validation Loss: 0.007825715327141682\n",
      "Epoch [10606/20000], Training Loss: 0.01615093513308758, Validation Loss: 0.0042503214911318666\n",
      "Epoch [10607/20000], Training Loss: 0.010683776776464324, Validation Loss: 0.005787468200192539\n",
      "Epoch [10608/20000], Training Loss: 0.011527460305062829, Validation Loss: 0.005357414349455374\n",
      "Epoch [10609/20000], Training Loss: 0.016754689886641114, Validation Loss: 0.004767869932726805\n",
      "Epoch [10610/20000], Training Loss: 0.015503960921445728, Validation Loss: 0.007170432657117333\n",
      "Epoch [10611/20000], Training Loss: 0.010109915832539887, Validation Loss: 0.02244709540471896\n",
      "Epoch [10612/20000], Training Loss: 0.011839470578706823, Validation Loss: 0.005655407538314299\n",
      "Epoch [10613/20000], Training Loss: 0.007524298059122104, Validation Loss: 0.003760253376039405\n",
      "Epoch [10614/20000], Training Loss: 0.005751057648532358, Validation Loss: 0.003327607561832987\n",
      "Epoch [10615/20000], Training Loss: 0.004109225164159268, Validation Loss: 0.004143015409307996\n",
      "Epoch [10616/20000], Training Loss: 0.006588780438115853, Validation Loss: 0.004338753738888305\n",
      "Epoch [10617/20000], Training Loss: 0.007827342923389682, Validation Loss: 0.004009698455273484\n",
      "Epoch [10618/20000], Training Loss: 0.008029859289568517, Validation Loss: 0.012586824355691664\n",
      "Epoch [10619/20000], Training Loss: 0.011949918563914252, Validation Loss: 0.02364562212187593\n",
      "Epoch [10620/20000], Training Loss: 0.024765489400514134, Validation Loss: 0.0026702832504789903\n",
      "Epoch [10621/20000], Training Loss: 0.019647695503188085, Validation Loss: 0.02729991001435853\n",
      "Epoch [10622/20000], Training Loss: 0.034772943027616875, Validation Loss: 0.09147707906768156\n",
      "Epoch [10623/20000], Training Loss: 0.03893063705098549, Validation Loss: 0.009619833494351073\n",
      "Epoch [10624/20000], Training Loss: 0.007880765627955302, Validation Loss: 0.004750678705412221\n",
      "Epoch [10625/20000], Training Loss: 0.008547484843542666, Validation Loss: 0.0046094266198386845\n",
      "Epoch [10626/20000], Training Loss: 0.006747733743395656, Validation Loss: 0.006279741292851375\n",
      "Epoch [10627/20000], Training Loss: 0.006938652025252979, Validation Loss: 0.0061910407951342806\n",
      "Epoch [10628/20000], Training Loss: 0.004714037022495177, Validation Loss: 0.00459232003978676\n",
      "Epoch [10629/20000], Training Loss: 0.004260625093593262, Validation Loss: 0.005010226615955357\n",
      "Epoch [10630/20000], Training Loss: 0.005927692443005331, Validation Loss: 0.003326666246721288\n",
      "Epoch [10631/20000], Training Loss: 0.004902507766798537, Validation Loss: 0.003232020873919542\n",
      "Epoch [10632/20000], Training Loss: 0.0052710274093052635, Validation Loss: 0.003268571573812551\n",
      "Epoch [10633/20000], Training Loss: 0.006684934349583013, Validation Loss: 0.003121192383527419\n",
      "Epoch [10634/20000], Training Loss: 0.005524854944854139, Validation Loss: 0.0030070037820451034\n",
      "Epoch [10635/20000], Training Loss: 0.004917188466476675, Validation Loss: 0.006383611014838054\n",
      "Epoch [10636/20000], Training Loss: 0.007849969106619678, Validation Loss: 0.0034407618187515254\n",
      "Epoch [10637/20000], Training Loss: 0.009234400578601967, Validation Loss: 0.01645278990391879\n",
      "Epoch [10638/20000], Training Loss: 0.027132041796383937, Validation Loss: 0.04029430562403442\n",
      "Epoch [10639/20000], Training Loss: 0.015180276583123486, Validation Loss: 0.01053217898983608\n",
      "Epoch [10640/20000], Training Loss: 0.021687193578275452, Validation Loss: 0.005777109427433353\n",
      "Epoch [10641/20000], Training Loss: 0.02175640472096672, Validation Loss: 0.00962471053363905\n",
      "Epoch [10642/20000], Training Loss: 0.014942716672002072, Validation Loss: 0.007816526560348638\n",
      "Epoch [10643/20000], Training Loss: 0.008433968315200348, Validation Loss: 0.004715238579430141\n",
      "Epoch [10644/20000], Training Loss: 0.008481883462601607, Validation Loss: 0.005417886511845081\n",
      "Epoch [10645/20000], Training Loss: 0.010687126933979536, Validation Loss: 0.0035705147897065865\n",
      "Epoch [10646/20000], Training Loss: 0.0056346931710972315, Validation Loss: 0.011652386265655679\n",
      "Epoch [10647/20000], Training Loss: 0.005116658756107435, Validation Loss: 0.003284226956273934\n",
      "Epoch [10648/20000], Training Loss: 0.010037748529742072, Validation Loss: 0.004318880390022579\n",
      "Epoch [10649/20000], Training Loss: 0.02713463676887581, Validation Loss: 0.0060176319383405796\n",
      "Epoch [10650/20000], Training Loss: 0.014433317081836452, Validation Loss: 0.01326490153699998\n",
      "Epoch [10651/20000], Training Loss: 0.008738413141275356, Validation Loss: 0.004716894708191636\n",
      "Epoch [10652/20000], Training Loss: 0.013042121391273602, Validation Loss: 0.006703619756600736\n",
      "Epoch [10653/20000], Training Loss: 0.06079942180908152, Validation Loss: 0.016946734210492594\n",
      "Epoch [10654/20000], Training Loss: 0.04208354598657544, Validation Loss: 0.007013264832494315\n",
      "Epoch [10655/20000], Training Loss: 0.009025228592301053, Validation Loss: 0.02718627586143287\n",
      "Epoch [10656/20000], Training Loss: 0.019933913021564616, Validation Loss: 0.008241025034067206\n",
      "Epoch [10657/20000], Training Loss: 0.014164186806218433, Validation Loss: 0.02203937002930242\n",
      "Epoch [10658/20000], Training Loss: 0.015397427096364222, Validation Loss: 0.004780467827718634\n",
      "Epoch [10659/20000], Training Loss: 0.013040005029844386, Validation Loss: 0.027824643498336497\n",
      "Epoch [10660/20000], Training Loss: 0.012340844376012683, Validation Loss: 0.00541767890212798\n",
      "Epoch [10661/20000], Training Loss: 0.005891470573260449, Validation Loss: 0.011879147366218692\n",
      "Epoch [10662/20000], Training Loss: 0.009651059925090522, Validation Loss: 0.006485094691434767\n",
      "Epoch [10663/20000], Training Loss: 0.005955188967553633, Validation Loss: 0.004713907873784657\n",
      "Epoch [10664/20000], Training Loss: 0.005291215376928449, Validation Loss: 0.0040042839482461135\n",
      "Epoch [10665/20000], Training Loss: 0.0075395495847000605, Validation Loss: 0.0038918742983712817\n",
      "Epoch [10666/20000], Training Loss: 0.007181272419984452, Validation Loss: 0.00371599929642993\n",
      "Epoch [10667/20000], Training Loss: 0.006625365952329178, Validation Loss: 0.027300078521862368\n",
      "Epoch [10668/20000], Training Loss: 0.014428390098536121, Validation Loss: 0.006553697143445193\n",
      "Epoch [10669/20000], Training Loss: 0.026641026975694006, Validation Loss: 0.016732947674297196\n",
      "Epoch [10670/20000], Training Loss: 0.014592116769303434, Validation Loss: 0.005707421690769162\n",
      "Epoch [10671/20000], Training Loss: 0.008364127069528746, Validation Loss: 0.007795137760806473\n",
      "Epoch [10672/20000], Training Loss: 0.011380334044328524, Validation Loss: 0.004257114652773453\n",
      "Epoch [10673/20000], Training Loss: 0.010358890606328584, Validation Loss: 0.011079508868911197\n",
      "Epoch [10674/20000], Training Loss: 0.01635161830407534, Validation Loss: 0.006986270345185857\n",
      "Epoch [10675/20000], Training Loss: 0.012797167886528118, Validation Loss: 0.021834213073527398\n",
      "Epoch [10676/20000], Training Loss: 0.017362234845807376, Validation Loss: 0.007995804766850563\n",
      "Epoch [10677/20000], Training Loss: 0.028129196203605846, Validation Loss: 0.005370773679443021\n",
      "Epoch [10678/20000], Training Loss: 0.015691006403845416, Validation Loss: 0.03529612345820559\n",
      "Epoch [10679/20000], Training Loss: 0.01987197493352661, Validation Loss: 0.009967937136659888\n",
      "Epoch [10680/20000], Training Loss: 0.01462976384625238, Validation Loss: 0.006029137197807748\n",
      "Epoch [10681/20000], Training Loss: 0.012028491502860561, Validation Loss: 0.007103943430437917\n",
      "Epoch [10682/20000], Training Loss: 0.011532841959186564, Validation Loss: 0.006191152566184347\n",
      "Epoch [10683/20000], Training Loss: 0.009207217249370712, Validation Loss: 0.005556062433370812\n",
      "Epoch [10684/20000], Training Loss: 0.006987399039124804, Validation Loss: 0.0055168157672751406\n",
      "Epoch [10685/20000], Training Loss: 0.005871612549526617, Validation Loss: 0.008137413581380315\n",
      "Epoch [10686/20000], Training Loss: 0.009437703469302505, Validation Loss: 0.006187788453523905\n",
      "Epoch [10687/20000], Training Loss: 0.010033051737471916, Validation Loss: 0.016551492014274607\n",
      "Epoch [10688/20000], Training Loss: 0.012120250920166395, Validation Loss: 0.008301513738306074\n",
      "Epoch [10689/20000], Training Loss: 0.011125283511189212, Validation Loss: 0.0035614661428200784\n",
      "Epoch [10690/20000], Training Loss: 0.008252616148508553, Validation Loss: 0.004014765721842117\n",
      "Epoch [10691/20000], Training Loss: 0.0037263074232864062, Validation Loss: 0.005513525220693324\n",
      "Epoch [10692/20000], Training Loss: 0.007018894781140261, Validation Loss: 0.0044432473393370185\n",
      "Epoch [10693/20000], Training Loss: 0.005066332091311259, Validation Loss: 0.009184702195049563\n",
      "Epoch [10694/20000], Training Loss: 0.005770429428853926, Validation Loss: 0.0037253147616306747\n",
      "Epoch [10695/20000], Training Loss: 0.0037372452487553737, Validation Loss: 0.006598032834092009\n",
      "Epoch [10696/20000], Training Loss: 0.006994464940882088, Validation Loss: 0.006010256169497146\n",
      "Epoch [10697/20000], Training Loss: 0.017353370465311206, Validation Loss: 0.019763941751229443\n",
      "Epoch [10698/20000], Training Loss: 0.013462585538426148, Validation Loss: 0.009473207404131092\n",
      "Epoch [10699/20000], Training Loss: 0.005439658862087526, Validation Loss: 0.005226068407384916\n",
      "Epoch [10700/20000], Training Loss: 0.007490818306126, Validation Loss: 0.008063085462760775\n",
      "Epoch [10701/20000], Training Loss: 0.006588860268168771, Validation Loss: 0.005954157606315117\n",
      "Epoch [10702/20000], Training Loss: 0.009502892731656434, Validation Loss: 0.0035724479320314068\n",
      "Epoch [10703/20000], Training Loss: 0.008740315813546269, Validation Loss: 0.003995737440213277\n",
      "Epoch [10704/20000], Training Loss: 0.007277537868503714, Validation Loss: 0.0048652571270686095\n",
      "Epoch [10705/20000], Training Loss: 0.008675160721135658, Validation Loss: 0.0032713790885409316\n",
      "Epoch [10706/20000], Training Loss: 0.01611520898859453, Validation Loss: 0.033248236339586255\n",
      "Epoch [10707/20000], Training Loss: 0.0239489984586336, Validation Loss: 0.02389131430659397\n",
      "Epoch [10708/20000], Training Loss: 0.04855345250221684, Validation Loss: 0.028528245399729663\n",
      "Epoch [10709/20000], Training Loss: 0.03953345626463748, Validation Loss: 0.023640119869992\n",
      "Epoch [10710/20000], Training Loss: 0.027093842658879503, Validation Loss: 0.016753269138887993\n",
      "Epoch [10711/20000], Training Loss: 0.008704176114406437, Validation Loss: 0.0061574175121606855\n",
      "Epoch [10712/20000], Training Loss: 0.007964100077515468, Validation Loss: 0.004715285504546378\n",
      "Epoch [10713/20000], Training Loss: 0.006064527977806782, Validation Loss: 0.005627505880956919\n",
      "Epoch [10714/20000], Training Loss: 0.005410803074482828, Validation Loss: 0.006081836401395099\n",
      "Epoch [10715/20000], Training Loss: 0.009405919398497125, Validation Loss: 0.004140623894602553\n",
      "Epoch [10716/20000], Training Loss: 0.0052773842242978776, Validation Loss: 0.0042737745241499935\n",
      "Epoch [10717/20000], Training Loss: 0.004758922992290796, Validation Loss: 0.00651531172755665\n",
      "Epoch [10718/20000], Training Loss: 0.005734860423087541, Validation Loss: 0.0044197273473175\n",
      "Epoch [10719/20000], Training Loss: 0.014957812182339174, Validation Loss: 0.005063145317501342\n",
      "Epoch [10720/20000], Training Loss: 0.016816377895468446, Validation Loss: 0.012012834306667568\n",
      "Epoch [10721/20000], Training Loss: 0.007680014148978184, Validation Loss: 0.00447825483026392\n",
      "Epoch [10722/20000], Training Loss: 0.006478915011809606, Validation Loss: 0.0054997355803723435\n",
      "Epoch [10723/20000], Training Loss: 0.006789382291442182, Validation Loss: 0.003390730037826893\n",
      "Epoch [10724/20000], Training Loss: 0.004170077596687146, Validation Loss: 0.004110277445756626\n",
      "Epoch [10725/20000], Training Loss: 0.004863678606592917, Validation Loss: 0.0039245678802160965\n",
      "Epoch [10726/20000], Training Loss: 0.005595635430446626, Validation Loss: 0.003331816018742804\n",
      "Epoch [10727/20000], Training Loss: 0.006426742145017832, Validation Loss: 0.005044106934717911\n",
      "Epoch [10728/20000], Training Loss: 0.006610531981615557, Validation Loss: 0.00796015524654203\n",
      "Epoch [10729/20000], Training Loss: 0.011792521899873723, Validation Loss: 0.004424164491657644\n",
      "Epoch [10730/20000], Training Loss: 0.030959236489642144, Validation Loss: 0.025524328820241835\n",
      "Epoch [10731/20000], Training Loss: 0.013610812225254319, Validation Loss: 0.007130922269733934\n",
      "Epoch [10732/20000], Training Loss: 0.0077399973400003675, Validation Loss: 0.003351051581049052\n",
      "Epoch [10733/20000], Training Loss: 0.010396311192640237, Validation Loss: 0.004103811150078708\n",
      "Epoch [10734/20000], Training Loss: 0.011355682177460429, Validation Loss: 0.019102544420158147\n",
      "Epoch [10735/20000], Training Loss: 0.006953113739135526, Validation Loss: 0.008684063338760793\n",
      "Epoch [10736/20000], Training Loss: 0.008232289210874504, Validation Loss: 0.0028244424464137614\n",
      "Epoch [10737/20000], Training Loss: 0.008325303498297996, Validation Loss: 0.004795301132200792\n",
      "Epoch [10738/20000], Training Loss: 0.005627091227714638, Validation Loss: 0.005806031012408052\n",
      "Epoch [10739/20000], Training Loss: 0.010439906957929321, Validation Loss: 0.07220383627074105\n",
      "Epoch [10740/20000], Training Loss: 0.02367066014890692, Validation Loss: 0.005181882810691762\n",
      "Epoch [10741/20000], Training Loss: 0.026952351761532815, Validation Loss: 0.004459806498421065\n",
      "Epoch [10742/20000], Training Loss: 0.012487582628507101, Validation Loss: 0.023071431185697593\n",
      "Epoch [10743/20000], Training Loss: 0.023265944976758744, Validation Loss: 0.009822835445532124\n",
      "Epoch [10744/20000], Training Loss: 0.015620402646683422, Validation Loss: 0.014425629056704256\n",
      "Epoch [10745/20000], Training Loss: 0.014563349704238655, Validation Loss: 0.006884774072107446\n",
      "Epoch [10746/20000], Training Loss: 0.007474105384192496, Validation Loss: 0.006147221018448842\n",
      "Epoch [10747/20000], Training Loss: 0.005716540879802778, Validation Loss: 0.004236104269836005\n",
      "Epoch [10748/20000], Training Loss: 0.005207750580406615, Validation Loss: 0.003999283488663455\n",
      "Epoch [10749/20000], Training Loss: 0.005450096297343927, Validation Loss: 0.005057409482105868\n",
      "Epoch [10750/20000], Training Loss: 0.009759177453815937, Validation Loss: 0.005822274692020203\n",
      "Epoch [10751/20000], Training Loss: 0.008926583449855181, Validation Loss: 0.007270258854257504\n",
      "Epoch [10752/20000], Training Loss: 0.006678670106339268, Validation Loss: 0.011936107245322982\n",
      "Epoch [10753/20000], Training Loss: 0.009213987724251638, Validation Loss: 0.0041402627108969425\n",
      "Epoch [10754/20000], Training Loss: 0.003962510300750312, Validation Loss: 0.004966955264665428\n",
      "Epoch [10755/20000], Training Loss: 0.009612208691877224, Validation Loss: 0.0038344655299868335\n",
      "Epoch [10756/20000], Training Loss: 0.007200495525466977, Validation Loss: 0.004147976892321635\n",
      "Epoch [10757/20000], Training Loss: 0.0041732609115570085, Validation Loss: 0.004187402101479165\n",
      "Epoch [10758/20000], Training Loss: 0.004305374508606162, Validation Loss: 0.008064759332229292\n",
      "Epoch [10759/20000], Training Loss: 0.006667541767196131, Validation Loss: 0.0050487735021371395\n",
      "Epoch [10760/20000], Training Loss: 0.008216239447799114, Validation Loss: 0.010470746420440134\n",
      "Epoch [10761/20000], Training Loss: 0.014220664541686088, Validation Loss: 0.003052760044744624\n",
      "Epoch [10762/20000], Training Loss: 0.010653481671007674, Validation Loss: 0.004674313652170018\n",
      "Epoch [10763/20000], Training Loss: 0.010557295125701265, Validation Loss: 0.027798698682870184\n",
      "Epoch [10764/20000], Training Loss: 0.020870033065615905, Validation Loss: 0.006512862575947663\n",
      "Epoch [10765/20000], Training Loss: 0.04491872716919586, Validation Loss: 0.027982860610391062\n",
      "Epoch [10766/20000], Training Loss: 0.05080623274469482, Validation Loss: 0.012626076091042446\n",
      "Epoch [10767/20000], Training Loss: 0.00949745726913014, Validation Loss: 0.011794255908041253\n",
      "Epoch [10768/20000], Training Loss: 0.00923942654584867, Validation Loss: 0.004586395066456005\n",
      "Epoch [10769/20000], Training Loss: 0.006385759659419169, Validation Loss: 0.007655374231587432\n",
      "Epoch [10770/20000], Training Loss: 0.006432147900376657, Validation Loss: 0.004457491317100846\n",
      "Epoch [10771/20000], Training Loss: 0.004257309546444178, Validation Loss: 0.009812886967158036\n",
      "Epoch [10772/20000], Training Loss: 0.010005458524834207, Validation Loss: 0.003592521085272325\n",
      "Epoch [10773/20000], Training Loss: 0.008342510368882878, Validation Loss: 0.005818756280627099\n",
      "Epoch [10774/20000], Training Loss: 0.011774257779636952, Validation Loss: 0.003874368433495889\n",
      "Epoch [10775/20000], Training Loss: 0.006073537125613727, Validation Loss: 0.005507277990165373\n",
      "Epoch [10776/20000], Training Loss: 0.01093059214846497, Validation Loss: 0.0038812076511245915\n",
      "Epoch [10777/20000], Training Loss: 0.006332562794958514, Validation Loss: 0.005174966196425224\n",
      "Epoch [10778/20000], Training Loss: 0.006368001193030588, Validation Loss: 0.009414058003507853\n",
      "Epoch [10779/20000], Training Loss: 0.006673842275631614, Validation Loss: 0.005837269391116554\n",
      "Epoch [10780/20000], Training Loss: 0.014045325995539315, Validation Loss: 0.012257673378502433\n",
      "Epoch [10781/20000], Training Loss: 0.022070239059449683, Validation Loss: 0.012080941881452301\n",
      "Epoch [10782/20000], Training Loss: 0.03226196535771513, Validation Loss: 0.017705645678298815\n",
      "Epoch [10783/20000], Training Loss: 0.020021316482078482, Validation Loss: 0.050508694989340644\n",
      "Epoch [10784/20000], Training Loss: 0.024371207516716304, Validation Loss: 0.011337981559654675\n",
      "Epoch [10785/20000], Training Loss: 0.07440306276216038, Validation Loss: 0.04437816405801901\n",
      "Epoch [10786/20000], Training Loss: 0.0751891198784246, Validation Loss: 0.039826892769529616\n",
      "Epoch [10787/20000], Training Loss: 0.01813184140649225, Validation Loss: 0.021428858642743926\n",
      "Epoch [10788/20000], Training Loss: 0.01851608921840255, Validation Loss: 0.012212458428832547\n",
      "Epoch [10789/20000], Training Loss: 0.010895769409086955, Validation Loss: 0.007536057935794815\n",
      "Epoch [10790/20000], Training Loss: 0.008628822430702192, Validation Loss: 0.006437957031136777\n",
      "Epoch [10791/20000], Training Loss: 0.006300048636538642, Validation Loss: 0.00832019437231273\n",
      "Epoch [10792/20000], Training Loss: 0.009127161771175452, Validation Loss: 0.006576707250392896\n",
      "Epoch [10793/20000], Training Loss: 0.008066597519375916, Validation Loss: 0.004961512629701507\n",
      "Epoch [10794/20000], Training Loss: 0.0055494963099980464, Validation Loss: 0.011647018635163217\n",
      "Epoch [10795/20000], Training Loss: 0.011289410707831848, Validation Loss: 0.004941043745150507\n",
      "Epoch [10796/20000], Training Loss: 0.007590327711243715, Validation Loss: 0.009823288211789207\n",
      "Epoch [10797/20000], Training Loss: 0.007831412461915275, Validation Loss: 0.008412464399173456\n",
      "Epoch [10798/20000], Training Loss: 0.007673108208109625, Validation Loss: 0.005009090905722198\n",
      "Epoch [10799/20000], Training Loss: 0.011177442976207073, Validation Loss: 0.003923326839607658\n",
      "Epoch [10800/20000], Training Loss: 0.011655442050791212, Validation Loss: 0.005437710089219243\n",
      "Epoch [10801/20000], Training Loss: 0.004668380453949794, Validation Loss: 0.0039639775178719515\n",
      "Epoch [10802/20000], Training Loss: 0.004627105181238481, Validation Loss: 0.005983165256371389\n",
      "Epoch [10803/20000], Training Loss: 0.009121941589650564, Validation Loss: 0.00528668861638185\n",
      "Epoch [10804/20000], Training Loss: 0.004882406422569018, Validation Loss: 0.006561693812597825\n",
      "Epoch [10805/20000], Training Loss: 0.00743936076157427, Validation Loss: 0.004137194796645157\n",
      "Epoch [10806/20000], Training Loss: 0.0035033919328790425, Validation Loss: 0.007809110119719013\n",
      "Epoch [10807/20000], Training Loss: 0.008950051408776614, Validation Loss: 0.004063892781102497\n",
      "Epoch [10808/20000], Training Loss: 0.010732859743140264, Validation Loss: 0.009020085039277806\n",
      "Epoch [10809/20000], Training Loss: 0.00563887385367577, Validation Loss: 0.004747205214893516\n",
      "Epoch [10810/20000], Training Loss: 0.01293683971016435, Validation Loss: 0.008709621107820047\n",
      "Epoch [10811/20000], Training Loss: 0.011922252271428338, Validation Loss: 0.007888559817989258\n",
      "Epoch [10812/20000], Training Loss: 0.00819950505696657, Validation Loss: 0.005004504538451978\n",
      "Epoch [10813/20000], Training Loss: 0.003610227449631306, Validation Loss: 0.007852979480893347\n",
      "Epoch [10814/20000], Training Loss: 0.004492641358020981, Validation Loss: 0.005574185130691538\n",
      "Epoch [10815/20000], Training Loss: 0.005617485529260843, Validation Loss: 0.003195397267323382\n",
      "Epoch [10816/20000], Training Loss: 0.004813849144089285, Validation Loss: 0.0068485735033131535\n",
      "Epoch [10817/20000], Training Loss: 0.004152650823276157, Validation Loss: 0.005500705660519348\n",
      "Epoch [10818/20000], Training Loss: 0.009132388877073285, Validation Loss: 0.002600834823916947\n",
      "Epoch [10819/20000], Training Loss: 0.007434103371321855, Validation Loss: 0.018334342735540434\n",
      "Epoch [10820/20000], Training Loss: 0.015096881159739237, Validation Loss: 0.03796051991201109\n",
      "Epoch [10821/20000], Training Loss: 0.022911656435878416, Validation Loss: 0.03398578539494273\n",
      "Epoch [10822/20000], Training Loss: 0.020040560348206782, Validation Loss: 0.03244111393567956\n",
      "Epoch [10823/20000], Training Loss: 0.028739310849102497, Validation Loss: 0.0318209764806982\n",
      "Epoch [10824/20000], Training Loss: 0.02193488526557173, Validation Loss: 0.004861126719138805\n",
      "Epoch [10825/20000], Training Loss: 0.010358425429460891, Validation Loss: 0.006567316301048517\n",
      "Epoch [10826/20000], Training Loss: 0.0067444516664961285, Validation Loss: 0.011682318319214704\n",
      "Epoch [10827/20000], Training Loss: 0.0078024470759763586, Validation Loss: 0.004525573545054483\n",
      "Epoch [10828/20000], Training Loss: 0.007541879550984731, Validation Loss: 0.03622263325191203\n",
      "Epoch [10829/20000], Training Loss: 0.027057112535528307, Validation Loss: 0.03180158250285915\n",
      "Epoch [10830/20000], Training Loss: 0.03231724852256385, Validation Loss: 0.06572852168911984\n",
      "Epoch [10831/20000], Training Loss: 0.0287159653094672, Validation Loss: 0.015308395127314205\n",
      "Epoch [10832/20000], Training Loss: 0.00748161828128754, Validation Loss: 0.00858092120240274\n",
      "Epoch [10833/20000], Training Loss: 0.009188123084771047, Validation Loss: 0.004523823646845163\n",
      "Epoch [10834/20000], Training Loss: 0.0068294911907287315, Validation Loss: 0.0039881678946065745\n",
      "Epoch [10835/20000], Training Loss: 0.00712643730210922, Validation Loss: 0.0072446866336447\n",
      "Epoch [10836/20000], Training Loss: 0.0046960806648712605, Validation Loss: 0.007375120999248403\n",
      "Epoch [10837/20000], Training Loss: 0.007025279038186584, Validation Loss: 0.0027236582503503215\n",
      "Epoch [10838/20000], Training Loss: 0.004111046251897018, Validation Loss: 0.004831398977103163\n",
      "Epoch [10839/20000], Training Loss: 0.007801170637841486, Validation Loss: 0.003094303663565207\n",
      "Epoch [10840/20000], Training Loss: 0.013319304909340903, Validation Loss: 0.008626779509930722\n",
      "Epoch [10841/20000], Training Loss: 0.014499502268336073, Validation Loss: 0.008062077981695179\n",
      "Epoch [10842/20000], Training Loss: 0.005479258443593997, Validation Loss: 0.012680239415788361\n",
      "Epoch [10843/20000], Training Loss: 0.00702579481678965, Validation Loss: 0.0056904594465631165\n",
      "Epoch [10844/20000], Training Loss: 0.009040156013465353, Validation Loss: 0.022844509527845145\n",
      "Epoch [10845/20000], Training Loss: 0.011448332418824845, Validation Loss: 0.007730083292187828\n",
      "Epoch [10846/20000], Training Loss: 0.010999115416296945, Validation Loss: 0.003629865091836239\n",
      "Epoch [10847/20000], Training Loss: 0.009662119724712934, Validation Loss: 0.0075835509872003316\n",
      "Epoch [10848/20000], Training Loss: 0.00886849235290096, Validation Loss: 0.0031802393177632177\n",
      "Epoch [10849/20000], Training Loss: 0.003406523083478013, Validation Loss: 0.003245634625362267\n",
      "Epoch [10850/20000], Training Loss: 0.0074086391354545155, Validation Loss: 0.0033538032941302326\n",
      "Epoch [10851/20000], Training Loss: 0.007725357829908067, Validation Loss: 0.01328500609893776\n",
      "Epoch [10852/20000], Training Loss: 0.009355007638857517, Validation Loss: 0.0062943414864743575\n",
      "Epoch [10853/20000], Training Loss: 0.007677254313652578, Validation Loss: 0.011694538421579637\n",
      "Epoch [10854/20000], Training Loss: 0.016275133821181953, Validation Loss: 0.04985192460573403\n",
      "Epoch [10855/20000], Training Loss: 0.018272057946627944, Validation Loss: 0.01092595403973259\n",
      "Epoch [10856/20000], Training Loss: 0.006213252181688793, Validation Loss: 0.0037863192360256498\n",
      "Epoch [10857/20000], Training Loss: 0.0037967243231053415, Validation Loss: 0.005204093928187247\n",
      "Epoch [10858/20000], Training Loss: 0.006616981511927277, Validation Loss: 0.0034199196187419795\n",
      "Epoch [10859/20000], Training Loss: 0.013795718261722609, Validation Loss: 0.017861720883132648\n",
      "Epoch [10860/20000], Training Loss: 0.008230530160030216, Validation Loss: 0.006407534362722345\n",
      "Epoch [10861/20000], Training Loss: 0.00741518939737164, Validation Loss: 0.006784395188609389\n",
      "Epoch [10862/20000], Training Loss: 0.008894297032384202, Validation Loss: 0.004976281640190499\n",
      "Epoch [10863/20000], Training Loss: 0.00798235387108954, Validation Loss: 0.005513222903857695\n",
      "Epoch [10864/20000], Training Loss: 0.005651929383069338, Validation Loss: 0.021045330146836135\n",
      "Epoch [10865/20000], Training Loss: 0.007651754724585251, Validation Loss: 0.010024931342980738\n",
      "Epoch [10866/20000], Training Loss: 0.014883971915570069, Validation Loss: 0.028452795435405114\n",
      "Epoch [10867/20000], Training Loss: 0.029524258673648416, Validation Loss: 0.022627059219950292\n",
      "Epoch [10868/20000], Training Loss: 0.019921694461329156, Validation Loss: 0.015788455410367015\n",
      "Epoch [10869/20000], Training Loss: 0.05285685694044722, Validation Loss: 0.013430631569138183\n",
      "Epoch [10870/20000], Training Loss: 0.028903410204553177, Validation Loss: 0.07725279969028113\n",
      "Epoch [10871/20000], Training Loss: 0.05334021245445391, Validation Loss: 0.020177119967414985\n",
      "Epoch [10872/20000], Training Loss: 0.09691612355943237, Validation Loss: 0.10582079215022791\n",
      "Epoch [10873/20000], Training Loss: 0.04219812464933576, Validation Loss: 0.014544215778642908\n",
      "Epoch [10874/20000], Training Loss: 0.019723290926776826, Validation Loss: 0.013825021859735205\n",
      "Epoch [10875/20000], Training Loss: 0.01437566163284438, Validation Loss: 0.013008044325504778\n",
      "Epoch [10876/20000], Training Loss: 0.010378546580406172, Validation Loss: 0.007933491864865314\n",
      "Epoch [10877/20000], Training Loss: 0.00855755336981799, Validation Loss: 0.008897494008514124\n",
      "Epoch [10878/20000], Training Loss: 0.006334898865004236, Validation Loss: 0.005343140049262729\n",
      "Epoch [10879/20000], Training Loss: 0.005412677762381334, Validation Loss: 0.005884599191371568\n",
      "Epoch [10880/20000], Training Loss: 0.005138169934590613, Validation Loss: 0.0051302016942307615\n",
      "Epoch [10881/20000], Training Loss: 0.005298242301380794, Validation Loss: 0.0048560930643231715\n",
      "Epoch [10882/20000], Training Loss: 0.006388934370729008, Validation Loss: 0.005221815843859596\n",
      "Epoch [10883/20000], Training Loss: 0.006125594211750597, Validation Loss: 0.0033949745100991485\n",
      "Epoch [10884/20000], Training Loss: 0.009334948896918962, Validation Loss: 0.003405839668565542\n",
      "Epoch [10885/20000], Training Loss: 0.004502909315630989, Validation Loss: 0.003086782116042741\n",
      "Epoch [10886/20000], Training Loss: 0.0050783786220043635, Validation Loss: 0.006934442228823302\n",
      "Epoch [10887/20000], Training Loss: 0.00639060466034737, Validation Loss: 0.0034811031247435104\n",
      "Epoch [10888/20000], Training Loss: 0.004635980789316818, Validation Loss: 0.00391637006480648\n",
      "Epoch [10889/20000], Training Loss: 0.006611526108047526, Validation Loss: 0.004490307619172589\n",
      "Epoch [10890/20000], Training Loss: 0.012915942341870894, Validation Loss: 0.011474630324398851\n",
      "Epoch [10891/20000], Training Loss: 0.010637099174817064, Validation Loss: 0.01420738184506572\n",
      "Epoch [10892/20000], Training Loss: 0.008393247217749636, Validation Loss: 0.006436304611919955\n",
      "Epoch [10893/20000], Training Loss: 0.009494496880506631, Validation Loss: 0.007347886976698906\n",
      "Epoch [10894/20000], Training Loss: 0.013600085496104189, Validation Loss: 0.06937191316059657\n",
      "Epoch [10895/20000], Training Loss: 0.040209916357915584, Validation Loss: 0.01820421713877504\n",
      "Epoch [10896/20000], Training Loss: 0.04841184983628669, Validation Loss: 0.030682083352335972\n",
      "Epoch [10897/20000], Training Loss: 0.01941444356115036, Validation Loss: 0.005827146844954508\n",
      "Epoch [10898/20000], Training Loss: 0.01074686789486025, Validation Loss: 0.007103969754812819\n",
      "Epoch [10899/20000], Training Loss: 0.00853976890773213, Validation Loss: 0.007582611250187808\n",
      "Epoch [10900/20000], Training Loss: 0.005534693357601229, Validation Loss: 0.004211331140400684\n",
      "Epoch [10901/20000], Training Loss: 0.005383947654406386, Validation Loss: 0.005061315423907737\n",
      "Epoch [10902/20000], Training Loss: 0.005064426839193662, Validation Loss: 0.005097255680604056\n",
      "Epoch [10903/20000], Training Loss: 0.006526575276049178, Validation Loss: 0.0038440907959998833\n",
      "Epoch [10904/20000], Training Loss: 0.006895069261060728, Validation Loss: 0.005830225879598012\n",
      "Epoch [10905/20000], Training Loss: 0.007050132377084343, Validation Loss: 0.004958184506157133\n",
      "Epoch [10906/20000], Training Loss: 0.00460339493500734, Validation Loss: 0.004175402874385067\n",
      "Epoch [10907/20000], Training Loss: 0.004881109314477986, Validation Loss: 0.003948810298002822\n",
      "Epoch [10908/20000], Training Loss: 0.00722896635748579, Validation Loss: 0.0028740543198182422\n",
      "Epoch [10909/20000], Training Loss: 0.004536627211824192, Validation Loss: 0.0036358493262663876\n",
      "Epoch [10910/20000], Training Loss: 0.007411262180539779, Validation Loss: 0.04189579297166403\n",
      "Epoch [10911/20000], Training Loss: 0.023866193903294124, Validation Loss: 0.007211226032950435\n",
      "Epoch [10912/20000], Training Loss: 0.026934099818130823, Validation Loss: 0.015012783246514718\n",
      "Epoch [10913/20000], Training Loss: 0.01647054397160121, Validation Loss: 0.005754785129153918\n",
      "Epoch [10914/20000], Training Loss: 0.012191541735384297, Validation Loss: 0.02583334820977092\n",
      "Epoch [10915/20000], Training Loss: 0.027672044953630705, Validation Loss: 0.010458264827710602\n",
      "Epoch [10916/20000], Training Loss: 0.014405287669173308, Validation Loss: 0.0076167213149541824\n",
      "Epoch [10917/20000], Training Loss: 0.006185194446256251, Validation Loss: 0.004669638094680652\n",
      "Epoch [10918/20000], Training Loss: 0.010486194504275253, Validation Loss: 0.05655578949621746\n",
      "Epoch [10919/20000], Training Loss: 0.03431910857277996, Validation Loss: 0.04782025196722582\n",
      "Epoch [10920/20000], Training Loss: 0.030436918455442146, Validation Loss: 0.009503941494358514\n",
      "Epoch [10921/20000], Training Loss: 0.022526484679409414, Validation Loss: 0.019276773846481125\n",
      "Epoch [10922/20000], Training Loss: 0.011935419626752264, Validation Loss: 0.006159918760788839\n",
      "Epoch [10923/20000], Training Loss: 0.009425820144575223, Validation Loss: 0.006733768721265473\n",
      "Epoch [10924/20000], Training Loss: 0.0071311541043022385, Validation Loss: 0.006128177715171189\n",
      "Epoch [10925/20000], Training Loss: 0.0050768690125551075, Validation Loss: 0.00418547712446785\n",
      "Epoch [10926/20000], Training Loss: 0.005073808736986913, Validation Loss: 0.005862723090558575\n",
      "Epoch [10927/20000], Training Loss: 0.008709374127127896, Validation Loss: 0.004676284314626855\n",
      "Epoch [10928/20000], Training Loss: 0.006813805973121946, Validation Loss: 0.0074951203641622855\n",
      "Epoch [10929/20000], Training Loss: 0.008733316618807814, Validation Loss: 0.0049169714525955244\n",
      "Epoch [10930/20000], Training Loss: 0.007877999887568876, Validation Loss: 0.003593409007001875\n",
      "Epoch [10931/20000], Training Loss: 0.00743463663509049, Validation Loss: 0.004318605210390355\n",
      "Epoch [10932/20000], Training Loss: 0.006363893137729194, Validation Loss: 0.004003502340700444\n",
      "Epoch [10933/20000], Training Loss: 0.0056838848665522945, Validation Loss: 0.0033520551732603315\n",
      "Epoch [10934/20000], Training Loss: 0.007090581286839941, Validation Loss: 0.0032041545565399637\n",
      "Epoch [10935/20000], Training Loss: 0.02226011550685923, Validation Loss: 0.016272206392319695\n",
      "Epoch [10936/20000], Training Loss: 0.02243585509339547, Validation Loss: 0.04254216221826417\n",
      "Epoch [10937/20000], Training Loss: 0.015579243660405544, Validation Loss: 0.009922598324205214\n",
      "Epoch [10938/20000], Training Loss: 0.007847081194215986, Validation Loss: 0.004192233702513347\n",
      "Epoch [10939/20000], Training Loss: 0.004321436668825689, Validation Loss: 0.003876033771843725\n",
      "Epoch [10940/20000], Training Loss: 0.004632135317868753, Validation Loss: 0.006567969180769216\n",
      "Epoch [10941/20000], Training Loss: 0.005423835117004013, Validation Loss: 0.012027210348768287\n",
      "Epoch [10942/20000], Training Loss: 0.00871687461377795, Validation Loss: 0.0032990679026170255\n",
      "Epoch [10943/20000], Training Loss: 0.006462514698016873, Validation Loss: 0.0034535979699837427\n",
      "Epoch [10944/20000], Training Loss: 0.009900526053180718, Validation Loss: 0.010660845791166395\n",
      "Epoch [10945/20000], Training Loss: 0.005862426191535113, Validation Loss: 0.0038319661747690604\n",
      "Epoch [10946/20000], Training Loss: 0.0077962187146373, Validation Loss: 0.0035336678063231375\n",
      "Epoch [10947/20000], Training Loss: 0.006079126948472029, Validation Loss: 0.0027051877378515394\n",
      "Epoch [10948/20000], Training Loss: 0.005215240274472828, Validation Loss: 0.005931652620837542\n",
      "Epoch [10949/20000], Training Loss: 0.010311680164055101, Validation Loss: 0.00810948276088246\n",
      "Epoch [10950/20000], Training Loss: 0.006092618074554983, Validation Loss: 0.004171927319530485\n",
      "Epoch [10951/20000], Training Loss: 0.0043119936869646025, Validation Loss: 0.007539931239230325\n",
      "Epoch [10952/20000], Training Loss: 0.007715880566138367, Validation Loss: 0.005870894801766724\n",
      "Epoch [10953/20000], Training Loss: 0.009284834053427662, Validation Loss: 0.009220013757902126\n",
      "Epoch [10954/20000], Training Loss: 0.01677087516197519, Validation Loss: 0.029370280224546668\n",
      "Epoch [10955/20000], Training Loss: 0.02296663956739004, Validation Loss: 0.03230981015924174\n",
      "Epoch [10956/20000], Training Loss: 0.021606113000806153, Validation Loss: 0.005152624524782505\n",
      "Epoch [10957/20000], Training Loss: 0.005744205664086621, Validation Loss: 0.00886511510377416\n",
      "Epoch [10958/20000], Training Loss: 0.014547047756423126, Validation Loss: 0.004698804013596823\n",
      "Epoch [10959/20000], Training Loss: 0.019698060941924007, Validation Loss: 0.0038929107863257934\n",
      "Epoch [10960/20000], Training Loss: 0.007764019013848156, Validation Loss: 0.0037244168743362544\n",
      "Epoch [10961/20000], Training Loss: 0.004809480159435354, Validation Loss: 0.00480329117156919\n",
      "Epoch [10962/20000], Training Loss: 0.006289562747822076, Validation Loss: 0.004341670351363521\n",
      "Epoch [10963/20000], Training Loss: 0.00983430997751254, Validation Loss: 0.0033289270386241804\n",
      "Epoch [10964/20000], Training Loss: 0.007261183886190078, Validation Loss: 0.005414175839076485\n",
      "Epoch [10965/20000], Training Loss: 0.011322054016220915, Validation Loss: 0.010693619877055685\n",
      "Epoch [10966/20000], Training Loss: 0.01128823216119049, Validation Loss: 0.0036723414688221317\n",
      "Epoch [10967/20000], Training Loss: 0.005016747989689715, Validation Loss: 0.0035192294775409244\n",
      "Epoch [10968/20000], Training Loss: 0.006324403179860383, Validation Loss: 0.005505933047783268\n",
      "Epoch [10969/20000], Training Loss: 0.009612125124216877, Validation Loss: 0.011908783710430053\n",
      "Epoch [10970/20000], Training Loss: 0.009449681514966914, Validation Loss: 0.003223716186507123\n",
      "Epoch [10971/20000], Training Loss: 0.005011989427397826, Validation Loss: 0.0038723961145170787\n",
      "Epoch [10972/20000], Training Loss: 0.0060519604843908125, Validation Loss: 0.004694566119578969\n",
      "Epoch [10973/20000], Training Loss: 0.016642600193043627, Validation Loss: 0.009144502558893606\n",
      "Epoch [10974/20000], Training Loss: 0.041364867780690213, Validation Loss: 0.015155560736145343\n",
      "Epoch [10975/20000], Training Loss: 0.025304847887517617, Validation Loss: 0.01664978115396756\n",
      "Epoch [10976/20000], Training Loss: 0.014392482345881166, Validation Loss: 0.005307563154490579\n",
      "Epoch [10977/20000], Training Loss: 0.0069123631443030065, Validation Loss: 0.009682608775138062\n",
      "Epoch [10978/20000], Training Loss: 0.010329147780794301, Validation Loss: 0.004660261784416824\n",
      "Epoch [10979/20000], Training Loss: 0.007558510067091058, Validation Loss: 0.004865413789913374\n",
      "Epoch [10980/20000], Training Loss: 0.007553192860671386, Validation Loss: 0.01015270609425989\n",
      "Epoch [10981/20000], Training Loss: 0.008864413913605469, Validation Loss: 0.006220270024651927\n",
      "Epoch [10982/20000], Training Loss: 0.0054327383113559335, Validation Loss: 0.00597702250802179\n",
      "Epoch [10983/20000], Training Loss: 0.004636759778285133, Validation Loss: 0.009800157842368571\n",
      "Epoch [10984/20000], Training Loss: 0.004819699343987135, Validation Loss: 0.003914656742417881\n",
      "Epoch [10985/20000], Training Loss: 0.004975948556940628, Validation Loss: 0.0037350410277525725\n",
      "Epoch [10986/20000], Training Loss: 0.003613553853938356, Validation Loss: 0.008902495728258533\n",
      "Epoch [10987/20000], Training Loss: 0.007293449972946421, Validation Loss: 0.003520872791147953\n",
      "Epoch [10988/20000], Training Loss: 0.006489740969852521, Validation Loss: 0.013492186541327555\n",
      "Epoch [10989/20000], Training Loss: 0.00599261269651054, Validation Loss: 0.006399865668726816\n",
      "Epoch [10990/20000], Training Loss: 0.0050676129695992654, Validation Loss: 0.0035815664498467675\n",
      "Epoch [10991/20000], Training Loss: 0.005639122911296519, Validation Loss: 0.004776264000488943\n",
      "Epoch [10992/20000], Training Loss: 0.008390154169839141, Validation Loss: 0.0034474523565678644\n",
      "Epoch [10993/20000], Training Loss: 0.00755751779070124, Validation Loss: 0.00263596902765108\n",
      "Epoch [10994/20000], Training Loss: 0.0055220634247754264, Validation Loss: 0.004968288388069285\n",
      "Epoch [10995/20000], Training Loss: 0.007184532238690216, Validation Loss: 0.06493117340973445\n",
      "Epoch [10996/20000], Training Loss: 0.03186845206463269, Validation Loss: 0.05157143729073683\n",
      "Epoch [10997/20000], Training Loss: 0.033678263659697096, Validation Loss: 0.006342266219974643\n",
      "Epoch [10998/20000], Training Loss: 0.017056160608880027, Validation Loss: 0.008579169084193995\n",
      "Epoch [10999/20000], Training Loss: 0.007751306262174954, Validation Loss: 0.004165732827315761\n",
      "Epoch [11000/20000], Training Loss: 0.007873283654459686, Validation Loss: 0.004349179925725366\n",
      "Epoch [11001/20000], Training Loss: 0.009194434615307858, Validation Loss: 0.015564085915729584\n",
      "Epoch [11002/20000], Training Loss: 0.012680004055645051, Validation Loss: 0.009367208541975184\n",
      "Epoch [11003/20000], Training Loss: 0.0100779664052035, Validation Loss: 0.00558102749645477\n",
      "Epoch [11004/20000], Training Loss: 0.005238554851010642, Validation Loss: 0.004608148333448169\n",
      "Epoch [11005/20000], Training Loss: 0.008171476884464417, Validation Loss: 0.003037542623489082\n",
      "Epoch [11006/20000], Training Loss: 0.007793982744000719, Validation Loss: 0.004286009343987628\n",
      "Epoch [11007/20000], Training Loss: 0.006328978786249146, Validation Loss: 0.0031760318833723273\n",
      "Epoch [11008/20000], Training Loss: 0.005271632926970986, Validation Loss: 0.006400880803295974\n",
      "Epoch [11009/20000], Training Loss: 0.006971575602489922, Validation Loss: 0.004038882052943814\n",
      "Epoch [11010/20000], Training Loss: 0.007758064501103945, Validation Loss: 0.005254127179185529\n",
      "Epoch [11011/20000], Training Loss: 0.014752187348286887, Validation Loss: 0.018119602597811903\n",
      "Epoch [11012/20000], Training Loss: 0.06086041982494602, Validation Loss: 0.02507479608591114\n",
      "Epoch [11013/20000], Training Loss: 0.024958311991732835, Validation Loss: 0.028286988181727275\n",
      "Epoch [11014/20000], Training Loss: 0.024246639237909613, Validation Loss: 0.011551147262939614\n",
      "Epoch [11015/20000], Training Loss: 0.04328966025163287, Validation Loss: 0.008135925131097923\n",
      "Epoch [11016/20000], Training Loss: 0.017241104747102196, Validation Loss: 0.02624994057038878\n",
      "Epoch [11017/20000], Training Loss: 0.013217813909120326, Validation Loss: 0.006093646273257036\n",
      "Epoch [11018/20000], Training Loss: 0.010878952922731904, Validation Loss: 0.01717568854254401\n",
      "Epoch [11019/20000], Training Loss: 0.011552796370649179, Validation Loss: 0.007074473621325679\n",
      "Epoch [11020/20000], Training Loss: 0.008294973074433594, Validation Loss: 0.005100713800629819\n",
      "Epoch [11021/20000], Training Loss: 0.006515826738905162, Validation Loss: 0.005807364641707328\n",
      "Epoch [11022/20000], Training Loss: 0.006919520133773663, Validation Loss: 0.02420787313508647\n",
      "Epoch [11023/20000], Training Loss: 0.009890299518056216, Validation Loss: 0.0051815212000434485\n",
      "Epoch [11024/20000], Training Loss: 0.0061733411759113165, Validation Loss: 0.006823619491147578\n",
      "Epoch [11025/20000], Training Loss: 0.010051439285591966, Validation Loss: 0.003725398978868303\n",
      "Epoch [11026/20000], Training Loss: 0.012401070046637739, Validation Loss: 0.004332079633502938\n",
      "Epoch [11027/20000], Training Loss: 0.007315276465046087, Validation Loss: 0.010610671980496093\n",
      "Epoch [11028/20000], Training Loss: 0.011198524097250941, Validation Loss: 0.007365147594710408\n",
      "Epoch [11029/20000], Training Loss: 0.015498987586754862, Validation Loss: 0.007095005297276221\n",
      "Epoch [11030/20000], Training Loss: 0.012654190207415792, Validation Loss: 0.004450298166398525\n",
      "Epoch [11031/20000], Training Loss: 0.008995252337107169, Validation Loss: 0.006071674869027837\n",
      "Epoch [11032/20000], Training Loss: 0.012978158513890645, Validation Loss: 0.005326782538627951\n",
      "Epoch [11033/20000], Training Loss: 0.01307754430204763, Validation Loss: 0.007651060972037819\n",
      "Epoch [11034/20000], Training Loss: 0.018192371054153358, Validation Loss: 0.014989493414831785\n",
      "Epoch [11035/20000], Training Loss: 0.018134055906557478, Validation Loss: 0.008528168662472123\n",
      "Epoch [11036/20000], Training Loss: 0.006878876965077195, Validation Loss: 0.005165170299078307\n",
      "Epoch [11037/20000], Training Loss: 0.006180809722179349, Validation Loss: 0.0058499364489782895\n",
      "Epoch [11038/20000], Training Loss: 0.0059498651658194445, Validation Loss: 0.003938295810845561\n",
      "Epoch [11039/20000], Training Loss: 0.005208541996710535, Validation Loss: 0.011663858512754961\n",
      "Epoch [11040/20000], Training Loss: 0.004569544325931929, Validation Loss: 0.004506285630227543\n",
      "Epoch [11041/20000], Training Loss: 0.007857930997618365, Validation Loss: 0.01113057636799957\n",
      "Epoch [11042/20000], Training Loss: 0.011515046091517434, Validation Loss: 0.00683402267600286\n",
      "Epoch [11043/20000], Training Loss: 0.017983727281846638, Validation Loss: 0.01480956694909505\n",
      "Epoch [11044/20000], Training Loss: 0.004232140763113941, Validation Loss: 0.005277764607267381\n",
      "Epoch [11045/20000], Training Loss: 0.005950063362563794, Validation Loss: 0.007260416977820634\n",
      "Epoch [11046/20000], Training Loss: 0.007460590597734803, Validation Loss: 0.0045521401183229605\n",
      "Epoch [11047/20000], Training Loss: 0.007705039669027818, Validation Loss: 0.004855742519518215\n",
      "Epoch [11048/20000], Training Loss: 0.006409219484859412, Validation Loss: 0.008315813329756666\n",
      "Epoch [11049/20000], Training Loss: 0.006812397992628112, Validation Loss: 0.003971891939802471\n",
      "Epoch [11050/20000], Training Loss: 0.013811248125940827, Validation Loss: 0.007639781983894652\n",
      "Epoch [11051/20000], Training Loss: 0.008402302044227586, Validation Loss: 0.008203392998763905\n",
      "Epoch [11052/20000], Training Loss: 0.006801101264138814, Validation Loss: 0.014696860020714199\n",
      "Epoch [11053/20000], Training Loss: 0.006499789593882659, Validation Loss: 0.0023128874436758834\n",
      "Epoch [11054/20000], Training Loss: 0.0037307224078436513, Validation Loss: 0.00680171485476787\n",
      "Epoch [11055/20000], Training Loss: 0.0055360308776601285, Validation Loss: 0.0031411820864204743\n",
      "Epoch [11056/20000], Training Loss: 0.004558765173341909, Validation Loss: 0.003070809381553719\n",
      "Epoch [11057/20000], Training Loss: 0.004686123245424564, Validation Loss: 0.003960384503664268\n",
      "Epoch [11058/20000], Training Loss: 0.007087147045240272, Validation Loss: 0.008861616170700541\n",
      "Epoch [11059/20000], Training Loss: 0.01211554644885382, Validation Loss: 0.013603831441806904\n",
      "Epoch [11060/20000], Training Loss: 0.008104591113544493, Validation Loss: 0.002927601740596731\n",
      "Epoch [11061/20000], Training Loss: 0.016505847485923146, Validation Loss: 0.00852272492742975\n",
      "Epoch [11062/20000], Training Loss: 0.005069629999135421, Validation Loss: 0.005737582487947888\n",
      "Epoch [11063/20000], Training Loss: 0.010438985382571835, Validation Loss: 0.009425838877057583\n",
      "Epoch [11064/20000], Training Loss: 0.0353108683048049, Validation Loss: 0.03350650306258883\n",
      "Epoch [11065/20000], Training Loss: 0.03542619558382804, Validation Loss: 0.021606899930962493\n",
      "Epoch [11066/20000], Training Loss: 0.017229273496143702, Validation Loss: 0.04647080813138731\n",
      "Epoch [11067/20000], Training Loss: 0.02177826993699585, Validation Loss: 0.014232235894661592\n",
      "Epoch [11068/20000], Training Loss: 0.012696824046932826, Validation Loss: 0.007065179350125358\n",
      "Epoch [11069/20000], Training Loss: 0.010004800214249241, Validation Loss: 0.007798763994807294\n",
      "Epoch [11070/20000], Training Loss: 0.00709493707108777, Validation Loss: 0.011322137899723486\n",
      "Epoch [11071/20000], Training Loss: 0.006286351474825226, Validation Loss: 0.004056342313164144\n",
      "Epoch [11072/20000], Training Loss: 0.009701660903894143, Validation Loss: 0.004370030390609988\n",
      "Epoch [11073/20000], Training Loss: 0.014904436021294844, Validation Loss: 0.004872080780511268\n",
      "Epoch [11074/20000], Training Loss: 0.011094400596838179, Validation Loss: 0.006059571507315263\n",
      "Epoch [11075/20000], Training Loss: 0.004365038802416977, Validation Loss: 0.00454116022912286\n",
      "Epoch [11076/20000], Training Loss: 0.007682049179233478, Validation Loss: 0.003085815851307199\n",
      "Epoch [11077/20000], Training Loss: 0.00687954256136436, Validation Loss: 0.005346131401746953\n",
      "Epoch [11078/20000], Training Loss: 0.005394301050858173, Validation Loss: 0.004101127416558962\n",
      "Epoch [11079/20000], Training Loss: 0.005549698763648588, Validation Loss: 0.005314803478127723\n",
      "Epoch [11080/20000], Training Loss: 0.007568786400692521, Validation Loss: 0.00683863660685351\n",
      "Epoch [11081/20000], Training Loss: 0.008542066377760449, Validation Loss: 0.0028450234741663116\n",
      "Epoch [11082/20000], Training Loss: 0.005318087911811225, Validation Loss: 0.00561421328902039\n",
      "Epoch [11083/20000], Training Loss: 0.01633185608079657, Validation Loss: 0.026026966316620676\n",
      "Epoch [11084/20000], Training Loss: 0.034264059116269764, Validation Loss: 0.07541787305048533\n",
      "Epoch [11085/20000], Training Loss: 0.03280200655821578, Validation Loss: 0.06763183219092235\n",
      "Epoch [11086/20000], Training Loss: 0.034124143114955326, Validation Loss: 0.04755257282938276\n",
      "Epoch [11087/20000], Training Loss: 0.03308946430583352, Validation Loss: 0.009740262528274408\n",
      "Epoch [11088/20000], Training Loss: 0.010441942191976392, Validation Loss: 0.00657058239110483\n",
      "Epoch [11089/20000], Training Loss: 0.008912545425118878, Validation Loss: 0.006142726989532087\n",
      "Epoch [11090/20000], Training Loss: 0.006171865873121922, Validation Loss: 0.005436871113584263\n",
      "Epoch [11091/20000], Training Loss: 0.0059593793744820035, Validation Loss: 0.00519583345568123\n",
      "Epoch [11092/20000], Training Loss: 0.005512200476784658, Validation Loss: 0.004480083441321233\n",
      "Epoch [11093/20000], Training Loss: 0.004654716392646411, Validation Loss: 0.004631752190653339\n",
      "Epoch [11094/20000], Training Loss: 0.004630990409558373, Validation Loss: 0.00865209262349336\n",
      "Epoch [11095/20000], Training Loss: 0.013410979436084094, Validation Loss: 0.005088804629887345\n",
      "Epoch [11096/20000], Training Loss: 0.011490123514054826, Validation Loss: 0.006072826884171842\n",
      "Epoch [11097/20000], Training Loss: 0.008741438306209335, Validation Loss: 0.004172144493579436\n",
      "Epoch [11098/20000], Training Loss: 0.0041751996031962335, Validation Loss: 0.003740531336067824\n",
      "Epoch [11099/20000], Training Loss: 0.005413372553070823, Validation Loss: 0.011483738198975793\n",
      "Epoch [11100/20000], Training Loss: 0.008688223165726023, Validation Loss: 0.005941618531043754\n",
      "Epoch [11101/20000], Training Loss: 0.005598942507016805, Validation Loss: 0.007453190130575089\n",
      "Epoch [11102/20000], Training Loss: 0.012733152304593074, Validation Loss: 0.004397816175347561\n",
      "Epoch [11103/20000], Training Loss: 0.004079161003963756, Validation Loss: 0.003990194400413931\n",
      "Epoch [11104/20000], Training Loss: 0.005742466871327322, Validation Loss: 0.00748957853353082\n",
      "Epoch [11105/20000], Training Loss: 0.010550551727646962, Validation Loss: 0.01239537068509645\n",
      "Epoch [11106/20000], Training Loss: 0.010272646957738678, Validation Loss: 0.026021501891940652\n",
      "Epoch [11107/20000], Training Loss: 0.019083071226075856, Validation Loss: 0.023567121292791704\n",
      "Epoch [11108/20000], Training Loss: 0.01908332446120247, Validation Loss: 0.009559982248457064\n",
      "Epoch [11109/20000], Training Loss: 0.04381416135999773, Validation Loss: 0.040674605541645814\n",
      "Epoch [11110/20000], Training Loss: 0.010793736123430011, Validation Loss: 0.007097242168281797\n",
      "Epoch [11111/20000], Training Loss: 0.011248770772778829, Validation Loss: 0.007204249594615567\n",
      "Epoch [11112/20000], Training Loss: 0.009537754082818444, Validation Loss: 0.004221436113192917\n",
      "Epoch [11113/20000], Training Loss: 0.022738058036858484, Validation Loss: 0.030312939413956234\n",
      "Epoch [11114/20000], Training Loss: 0.01507023539410771, Validation Loss: 0.012345649435074536\n",
      "Epoch [11115/20000], Training Loss: 0.019708695204047087, Validation Loss: 0.005154892938184055\n",
      "Epoch [11116/20000], Training Loss: 0.01952983972504236, Validation Loss: 0.020271114066499\n",
      "Epoch [11117/20000], Training Loss: 0.014595702422541632, Validation Loss: 0.010536580244981997\n",
      "Epoch [11118/20000], Training Loss: 0.014378861060582235, Validation Loss: 0.004681707815126458\n",
      "Epoch [11119/20000], Training Loss: 0.015029404661618173, Validation Loss: 0.017388285935989452\n",
      "Epoch [11120/20000], Training Loss: 0.009130582476574643, Validation Loss: 0.0057948991892792555\n",
      "Epoch [11121/20000], Training Loss: 0.01088017033846102, Validation Loss: 0.007832646369965494\n",
      "Epoch [11122/20000], Training Loss: 0.007890143881273357, Validation Loss: 0.004760837119621107\n",
      "Epoch [11123/20000], Training Loss: 0.004763753642432026, Validation Loss: 0.005342301513699664\n",
      "Epoch [11124/20000], Training Loss: 0.0061160151090007275, Validation Loss: 0.005338854302559805\n",
      "Epoch [11125/20000], Training Loss: 0.005534504708334355, Validation Loss: 0.014625657882009513\n",
      "Epoch [11126/20000], Training Loss: 0.010508796968289451, Validation Loss: 0.00538801820967202\n",
      "Epoch [11127/20000], Training Loss: 0.0064962951416548875, Validation Loss: 0.006431464438503105\n",
      "Epoch [11128/20000], Training Loss: 0.006376022082154772, Validation Loss: 0.012233122518020223\n",
      "Epoch [11129/20000], Training Loss: 0.008528940778757845, Validation Loss: 0.0032799544453021743\n",
      "Epoch [11130/20000], Training Loss: 0.005314947498845868, Validation Loss: 0.003226674919584798\n",
      "Epoch [11131/20000], Training Loss: 0.004397199258424475, Validation Loss: 0.006550668206614444\n",
      "Epoch [11132/20000], Training Loss: 0.004029374936570613, Validation Loss: 0.007924124147626463\n",
      "Epoch [11133/20000], Training Loss: 0.02096803145832382, Validation Loss: 0.004841159210247107\n",
      "Epoch [11134/20000], Training Loss: 0.017788086812288384, Validation Loss: 0.011540622400519038\n",
      "Epoch [11135/20000], Training Loss: 0.01866380004529284, Validation Loss: 0.006114378373513364\n",
      "Epoch [11136/20000], Training Loss: 0.014898335888574366, Validation Loss: 0.006619876418391522\n",
      "Epoch [11137/20000], Training Loss: 0.010903570526612125, Validation Loss: 0.020456708169409206\n",
      "Epoch [11138/20000], Training Loss: 0.010827533107203635, Validation Loss: 0.014309240019604477\n",
      "Epoch [11139/20000], Training Loss: 0.0070498420412020224, Validation Loss: 0.004362541469385223\n",
      "Epoch [11140/20000], Training Loss: 0.006661684788663739, Validation Loss: 0.01496709736862353\n",
      "Epoch [11141/20000], Training Loss: 0.006871928572114224, Validation Loss: 0.004000177914952421\n",
      "Epoch [11142/20000], Training Loss: 0.008226303574961744, Validation Loss: 0.0032430129377908935\n",
      "Epoch [11143/20000], Training Loss: 0.004193042804087911, Validation Loss: 0.0032568250652388756\n",
      "Epoch [11144/20000], Training Loss: 0.004864386061724092, Validation Loss: 0.005678477198168097\n",
      "Epoch [11145/20000], Training Loss: 0.01516263670906223, Validation Loss: 0.012042348960135252\n",
      "Epoch [11146/20000], Training Loss: 0.019048999670857825, Validation Loss: 0.03338252928889623\n",
      "Epoch [11147/20000], Training Loss: 0.02653075904319329, Validation Loss: 0.02599048137727292\n",
      "Epoch [11148/20000], Training Loss: 0.01159984431225374, Validation Loss: 0.013484929162009416\n",
      "Epoch [11149/20000], Training Loss: 0.013486861218033093, Validation Loss: 0.006291615477420214\n",
      "Epoch [11150/20000], Training Loss: 0.013754650326778315, Validation Loss: 0.0055504676743898885\n",
      "Epoch [11151/20000], Training Loss: 0.008304959377190764, Validation Loss: 0.021172702312469587\n",
      "Epoch [11152/20000], Training Loss: 0.009730080454443981, Validation Loss: 0.0035750611320733835\n",
      "Epoch [11153/20000], Training Loss: 0.010715380130932317, Validation Loss: 0.008731474698729511\n",
      "Epoch [11154/20000], Training Loss: 0.007172793220628851, Validation Loss: 0.0036570126624126055\n",
      "Epoch [11155/20000], Training Loss: 0.008146061886301549, Validation Loss: 0.002670068892535891\n",
      "Epoch [11156/20000], Training Loss: 0.008895376494495264, Validation Loss: 0.0048231980264453024\n",
      "Epoch [11157/20000], Training Loss: 0.01538281830341605, Validation Loss: 0.01068352535367012\n",
      "Epoch [11158/20000], Training Loss: 0.015059392056303165, Validation Loss: 0.03365837250437055\n",
      "Epoch [11159/20000], Training Loss: 0.04849816226513732, Validation Loss: 0.057887873371588024\n",
      "Epoch [11160/20000], Training Loss: 0.045483495108783245, Validation Loss: 0.0222323273778317\n",
      "Epoch [11161/20000], Training Loss: 0.014796318225697698, Validation Loss: 0.011658759917866388\n",
      "Epoch [11162/20000], Training Loss: 0.011023592667437956, Validation Loss: 0.00730867376036518\n",
      "Epoch [11163/20000], Training Loss: 0.008228283275717072, Validation Loss: 0.0062318316245832\n",
      "Epoch [11164/20000], Training Loss: 0.00865117522438855, Validation Loss: 0.007909743732333092\n",
      "Epoch [11165/20000], Training Loss: 0.006328979541909315, Validation Loss: 0.004996310796806418\n",
      "Epoch [11166/20000], Training Loss: 0.006163007695119761, Validation Loss: 0.004890639366619455\n",
      "Epoch [11167/20000], Training Loss: 0.0066291867406107485, Validation Loss: 0.005133980748983699\n",
      "Epoch [11168/20000], Training Loss: 0.006076986273650878, Validation Loss: 0.00472605682719924\n",
      "Epoch [11169/20000], Training Loss: 0.007075296827159556, Validation Loss: 0.004920958170389765\n",
      "Epoch [11170/20000], Training Loss: 0.004442546494829003, Validation Loss: 0.006024758716973467\n",
      "Epoch [11171/20000], Training Loss: 0.004642848090692756, Validation Loss: 0.004145703493480012\n",
      "Epoch [11172/20000], Training Loss: 0.005303965172580709, Validation Loss: 0.0035573228217344877\n",
      "Epoch [11173/20000], Training Loss: 0.004692791116082974, Validation Loss: 0.004253463076565822\n",
      "Epoch [11174/20000], Training Loss: 0.006037810560623517, Validation Loss: 0.003125993795265458\n",
      "Epoch [11175/20000], Training Loss: 0.006768479942755741, Validation Loss: 0.0160344461846762\n",
      "Epoch [11176/20000], Training Loss: 0.008265330140212817, Validation Loss: 0.012757009410277829\n",
      "Epoch [11177/20000], Training Loss: 0.00643742317541702, Validation Loss: 0.005672182165946131\n",
      "Epoch [11178/20000], Training Loss: 0.006501064666995912, Validation Loss: 0.009889893489961261\n",
      "Epoch [11179/20000], Training Loss: 0.006754177619378814, Validation Loss: 0.008259723808009767\n",
      "Epoch [11180/20000], Training Loss: 0.020289111216178362, Validation Loss: 0.0026064109734664142\n",
      "Epoch [11181/20000], Training Loss: 0.034715205913438955, Validation Loss: 0.02627408531095354\n",
      "Epoch [11182/20000], Training Loss: 0.01664704975103502, Validation Loss: 0.047357615636206864\n",
      "Epoch [11183/20000], Training Loss: 0.02133792778684957, Validation Loss: 0.004973757278646319\n",
      "Epoch [11184/20000], Training Loss: 0.008603493959526531, Validation Loss: 0.004959337229885412\n",
      "Epoch [11185/20000], Training Loss: 0.005491467048029465, Validation Loss: 0.006850139798026638\n",
      "Epoch [11186/20000], Training Loss: 0.006611087927012704, Validation Loss: 0.0046715225866721865\n",
      "Epoch [11187/20000], Training Loss: 0.005196604969179524, Validation Loss: 0.003790497843267206\n",
      "Epoch [11188/20000], Training Loss: 0.004640456239160683, Validation Loss: 0.004521162075986597\n",
      "Epoch [11189/20000], Training Loss: 0.008304854006772595, Validation Loss: 0.003799006349481715\n",
      "Epoch [11190/20000], Training Loss: 0.004679608088086492, Validation Loss: 0.006064220995361731\n",
      "Epoch [11191/20000], Training Loss: 0.008588300709171952, Validation Loss: 0.008314790360876552\n",
      "Epoch [11192/20000], Training Loss: 0.008094200048487567, Validation Loss: 0.01883230597897583\n",
      "Epoch [11193/20000], Training Loss: 0.009185681942264117, Validation Loss: 0.004256588520580148\n",
      "Epoch [11194/20000], Training Loss: 0.010999106388356137, Validation Loss: 0.003521081294252326\n",
      "Epoch [11195/20000], Training Loss: 0.006587856830573661, Validation Loss: 0.005193754189284456\n",
      "Epoch [11196/20000], Training Loss: 0.008817599987488782, Validation Loss: 0.008180892231710615\n",
      "Epoch [11197/20000], Training Loss: 0.005282943482078346, Validation Loss: 0.007122728062329641\n",
      "Epoch [11198/20000], Training Loss: 0.009772814984899014, Validation Loss: 0.013427806630893395\n",
      "Epoch [11199/20000], Training Loss: 0.008452856518228405, Validation Loss: 0.0035509298154790614\n",
      "Epoch [11200/20000], Training Loss: 0.027880790499953685, Validation Loss: 0.010300236237713176\n",
      "Epoch [11201/20000], Training Loss: 0.03452951585599554, Validation Loss: 0.015422499426956957\n",
      "Epoch [11202/20000], Training Loss: 0.02413901352026317, Validation Loss: 0.006278130561733113\n",
      "Epoch [11203/20000], Training Loss: 0.009065001284657878, Validation Loss: 0.007589631712757442\n",
      "Epoch [11204/20000], Training Loss: 0.0058117437854109865, Validation Loss: 0.007295790985738547\n",
      "Epoch [11205/20000], Training Loss: 0.005786584108136594, Validation Loss: 0.00378680030322747\n",
      "Epoch [11206/20000], Training Loss: 0.0058193327289391816, Validation Loss: 0.004776436717098217\n",
      "Epoch [11207/20000], Training Loss: 0.0051269410656199655, Validation Loss: 0.00908253870079534\n",
      "Epoch [11208/20000], Training Loss: 0.008195908292691456, Validation Loss: 0.005169070028322105\n",
      "Epoch [11209/20000], Training Loss: 0.007934969609655848, Validation Loss: 0.004293208633725953\n",
      "Epoch [11210/20000], Training Loss: 0.004942030075783675, Validation Loss: 0.006402635573508064\n",
      "Epoch [11211/20000], Training Loss: 0.003981995731010102, Validation Loss: 0.004587986821206869\n",
      "Epoch [11212/20000], Training Loss: 0.008971672992109754, Validation Loss: 0.023604161506901804\n",
      "Epoch [11213/20000], Training Loss: 0.010402948862715025, Validation Loss: 0.00583796156065221\n",
      "Epoch [11214/20000], Training Loss: 0.006545241569775888, Validation Loss: 0.0030155688562639887\n",
      "Epoch [11215/20000], Training Loss: 0.004566488709575164, Validation Loss: 0.00575593985773334\n",
      "Epoch [11216/20000], Training Loss: 0.012782312173840182, Validation Loss: 0.010279990778664276\n",
      "Epoch [11217/20000], Training Loss: 0.01002642338439078, Validation Loss: 0.0042869318094419994\n",
      "Epoch [11218/20000], Training Loss: 0.018365660481029927, Validation Loss: 0.013017501411143582\n",
      "Epoch [11219/20000], Training Loss: 0.014815299453662842, Validation Loss: 0.00817538662613515\n",
      "Epoch [11220/20000], Training Loss: 0.010692603390322932, Validation Loss: 0.01537972998461815\n",
      "Epoch [11221/20000], Training Loss: 0.011083137021549712, Validation Loss: 0.005177314588763693\n",
      "Epoch [11222/20000], Training Loss: 0.006267069428369203, Validation Loss: 0.003348577831080287\n",
      "Epoch [11223/20000], Training Loss: 0.006036130293848275, Validation Loss: 0.0028985862514738464\n",
      "Epoch [11224/20000], Training Loss: 0.005271576195290046, Validation Loss: 0.0032358852150926915\n",
      "Epoch [11225/20000], Training Loss: 0.006516417195029029, Validation Loss: 0.004059995128630233\n",
      "Epoch [11226/20000], Training Loss: 0.008299888091901917, Validation Loss: 0.007406742653821701\n",
      "Epoch [11227/20000], Training Loss: 0.0059654508543514695, Validation Loss: 0.0024875209499961265\n",
      "Epoch [11228/20000], Training Loss: 0.005034474600896439, Validation Loss: 0.003799930920406171\n",
      "Epoch [11229/20000], Training Loss: 0.007534329095506109, Validation Loss: 0.0028391775455247625\n",
      "Epoch [11230/20000], Training Loss: 0.012639427415316666, Validation Loss: 0.004424207343195771\n",
      "Epoch [11231/20000], Training Loss: 0.004123779839314271, Validation Loss: 0.010471033892270934\n",
      "Epoch [11232/20000], Training Loss: 0.011211742969213187, Validation Loss: 0.0036329448098838058\n",
      "Epoch [11233/20000], Training Loss: 0.027085156262923453, Validation Loss: 0.014463132408142851\n",
      "Epoch [11234/20000], Training Loss: 0.01607914607094634, Validation Loss: 0.007447249986738498\n",
      "Epoch [11235/20000], Training Loss: 0.009848296072699927, Validation Loss: 0.002505500120426924\n",
      "Epoch [11236/20000], Training Loss: 0.015524978909525089, Validation Loss: 0.007346531833333765\n",
      "Epoch [11237/20000], Training Loss: 0.00507838882375771, Validation Loss: 0.006963423228528752\n",
      "Epoch [11238/20000], Training Loss: 0.005885658561185535, Validation Loss: 0.0034209240526641743\n",
      "Epoch [11239/20000], Training Loss: 0.008094366261242871, Validation Loss: 0.03170562411040529\n",
      "Epoch [11240/20000], Training Loss: 0.022471133141152677, Validation Loss: 0.005147798616868775\n",
      "Epoch [11241/20000], Training Loss: 0.018897671170204182, Validation Loss: 0.003013089337790526\n",
      "Epoch [11242/20000], Training Loss: 0.016295705019729212, Validation Loss: 0.019561595815633024\n",
      "Epoch [11243/20000], Training Loss: 0.016111424533716803, Validation Loss: 0.008696105663505347\n",
      "Epoch [11244/20000], Training Loss: 0.00644334705014314, Validation Loss: 0.0036645537177751714\n",
      "Epoch [11245/20000], Training Loss: 0.004843221359936122, Validation Loss: 0.006374160298733754\n",
      "Epoch [11246/20000], Training Loss: 0.007513472284439818, Validation Loss: 0.006135462356334236\n",
      "Epoch [11247/20000], Training Loss: 0.00570175666351653, Validation Loss: 0.0026920468980214146\n",
      "Epoch [11248/20000], Training Loss: 0.0039123238919793425, Validation Loss: 0.003102581380582186\n",
      "Epoch [11249/20000], Training Loss: 0.00599735314316214, Validation Loss: 0.003086562910681323\n",
      "Epoch [11250/20000], Training Loss: 0.0068008992613093665, Validation Loss: 0.0030893538488214745\n",
      "Epoch [11251/20000], Training Loss: 0.01089589937224186, Validation Loss: 0.008991385094369824\n",
      "Epoch [11252/20000], Training Loss: 0.009326878462161403, Validation Loss: 0.016193464930568436\n",
      "Epoch [11253/20000], Training Loss: 0.0032207331524237554, Validation Loss: 0.0038652701038342585\n",
      "Epoch [11254/20000], Training Loss: 0.007377707482581692, Validation Loss: 0.020324762281429036\n",
      "Epoch [11255/20000], Training Loss: 0.010975762565586982, Validation Loss: 0.0023847199814050718\n",
      "Epoch [11256/20000], Training Loss: 0.006370217385145744, Validation Loss: 0.03414492628404072\n",
      "Epoch [11257/20000], Training Loss: 0.013206210405249814, Validation Loss: 0.03934010651042092\n",
      "Epoch [11258/20000], Training Loss: 0.021606090523684025, Validation Loss: 0.011565050923668285\n",
      "Epoch [11259/20000], Training Loss: 0.007950197273333157, Validation Loss: 0.007360522230198642\n",
      "Epoch [11260/20000], Training Loss: 0.009560495024613504, Validation Loss: 0.010929293629162982\n",
      "Epoch [11261/20000], Training Loss: 0.010713806493315912, Validation Loss: 0.003972282082713718\n",
      "Epoch [11262/20000], Training Loss: 0.008326959372165479, Validation Loss: 0.004993661559037069\n",
      "Epoch [11263/20000], Training Loss: 0.018855512877053116, Validation Loss: 0.013194927879210033\n",
      "Epoch [11264/20000], Training Loss: 0.031937182926152934, Validation Loss: 0.04368581942149571\n",
      "Epoch [11265/20000], Training Loss: 0.03880614664272538, Validation Loss: 0.034498339252812524\n",
      "Epoch [11266/20000], Training Loss: 0.055930725292585394, Validation Loss: 0.0224617122769197\n",
      "Epoch [11267/20000], Training Loss: 0.03374630457255989, Validation Loss: 0.012240422811633575\n",
      "Epoch [11268/20000], Training Loss: 0.010507636713945041, Validation Loss: 0.011042859955705845\n",
      "Epoch [11269/20000], Training Loss: 0.007983854447957128, Validation Loss: 0.007270136469807247\n",
      "Epoch [11270/20000], Training Loss: 0.007629604321014735, Validation Loss: 0.006272307862640706\n",
      "Epoch [11271/20000], Training Loss: 0.006611252270106759, Validation Loss: 0.008147070821616285\n",
      "Epoch [11272/20000], Training Loss: 0.006527397183162975, Validation Loss: 0.004910904798310055\n",
      "Epoch [11273/20000], Training Loss: 0.006081667824348967, Validation Loss: 0.0044533029220588105\n",
      "Epoch [11274/20000], Training Loss: 0.004603249896068259, Validation Loss: 0.004155820943382642\n",
      "Epoch [11275/20000], Training Loss: 0.00544212616763876, Validation Loss: 0.0056358608962948165\n",
      "Epoch [11276/20000], Training Loss: 0.0072549119234151605, Validation Loss: 0.004602736607851932\n",
      "Epoch [11277/20000], Training Loss: 0.005623726623265871, Validation Loss: 0.013649240646275627\n",
      "Epoch [11278/20000], Training Loss: 0.010524924821636108, Validation Loss: 0.004094365749694922\n",
      "Epoch [11279/20000], Training Loss: 0.007902714471648713, Validation Loss: 0.004469597193632146\n",
      "Epoch [11280/20000], Training Loss: 0.009947841350887237, Validation Loss: 0.007594315468583463\n",
      "Epoch [11281/20000], Training Loss: 0.0059287513036646745, Validation Loss: 0.005024153191508317\n",
      "Epoch [11282/20000], Training Loss: 0.006258034271533168, Validation Loss: 0.003758956396788043\n",
      "Epoch [11283/20000], Training Loss: 0.004621021549350449, Validation Loss: 0.0028965959285262182\n",
      "Epoch [11284/20000], Training Loss: 0.004003142946333226, Validation Loss: 0.0032048888587564556\n",
      "Epoch [11285/20000], Training Loss: 0.003399387833529285, Validation Loss: 0.009945059594300257\n",
      "Epoch [11286/20000], Training Loss: 0.009106060358102954, Validation Loss: 0.006431932055940394\n",
      "Epoch [11287/20000], Training Loss: 0.008092812206346675, Validation Loss: 0.004881858664340881\n",
      "Epoch [11288/20000], Training Loss: 0.004743560853447499, Validation Loss: 0.004439570749452416\n",
      "Epoch [11289/20000], Training Loss: 0.0069737331900146925, Validation Loss: 0.015493033137457044\n",
      "Epoch [11290/20000], Training Loss: 0.012655146130003101, Validation Loss: 0.012873019810204564\n",
      "Epoch [11291/20000], Training Loss: 0.024155480189490066, Validation Loss: 0.004110312209691753\n",
      "Epoch [11292/20000], Training Loss: 0.009330544720455943, Validation Loss: 0.015330063445227493\n",
      "Epoch [11293/20000], Training Loss: 0.010223052926968583, Validation Loss: 0.03546856450183051\n",
      "Epoch [11294/20000], Training Loss: 0.02334853528113204, Validation Loss: 0.004524258545879345\n",
      "Epoch [11295/20000], Training Loss: 0.007435085977444292, Validation Loss: 0.012753170262840285\n",
      "Epoch [11296/20000], Training Loss: 0.01752292165032746, Validation Loss: 0.029138624596535174\n",
      "Epoch [11297/20000], Training Loss: 0.015581086654232681, Validation Loss: 0.01514181853865791\n",
      "Epoch [11298/20000], Training Loss: 0.007768820817416001, Validation Loss: 0.004563838322441054\n",
      "Epoch [11299/20000], Training Loss: 0.010790657733585769, Validation Loss: 0.0033917060698742773\n",
      "Epoch [11300/20000], Training Loss: 0.00826997418090498, Validation Loss: 0.005278252680713724\n",
      "Epoch [11301/20000], Training Loss: 0.011477346787744733, Validation Loss: 0.019959799973195254\n",
      "Epoch [11302/20000], Training Loss: 0.010469664733266524, Validation Loss: 0.007517497720601738\n",
      "Epoch [11303/20000], Training Loss: 0.007685421410964669, Validation Loss: 0.016419813835195172\n",
      "Epoch [11304/20000], Training Loss: 0.019082298161395426, Validation Loss: 0.0040413402188678\n",
      "Epoch [11305/20000], Training Loss: 0.019481324087402236, Validation Loss: 0.0047907286562697705\n",
      "Epoch [11306/20000], Training Loss: 0.01619176095846342, Validation Loss: 0.011184081119219117\n",
      "Epoch [11307/20000], Training Loss: 0.0136321294526819, Validation Loss: 0.010109885297589827\n",
      "Epoch [11308/20000], Training Loss: 0.017214961536020774, Validation Loss: 0.011324230847623934\n",
      "Epoch [11309/20000], Training Loss: 0.01286782078802519, Validation Loss: 0.013120694993574489\n",
      "Epoch [11310/20000], Training Loss: 0.01330759881476037, Validation Loss: 0.0036635001492137625\n",
      "Epoch [11311/20000], Training Loss: 0.00541069405673105, Validation Loss: 0.008600392112781904\n",
      "Epoch [11312/20000], Training Loss: 0.007610992857475399, Validation Loss: 0.004991226883526727\n",
      "Epoch [11313/20000], Training Loss: 0.006257129887152197, Validation Loss: 0.0036354896389302178\n",
      "Epoch [11314/20000], Training Loss: 0.009548378163474678, Validation Loss: 0.006495435591590747\n",
      "Epoch [11315/20000], Training Loss: 0.006181083321050908, Validation Loss: 0.004312933921924045\n",
      "Epoch [11316/20000], Training Loss: 0.006106789714457201, Validation Loss: 0.005249503700621568\n",
      "Epoch [11317/20000], Training Loss: 0.007860424251080467, Validation Loss: 0.003861028402791281\n",
      "Epoch [11318/20000], Training Loss: 0.034294107984351285, Validation Loss: 0.008996648353516434\n",
      "Epoch [11319/20000], Training Loss: 0.028968417476530055, Validation Loss: 0.020010706188910734\n",
      "Epoch [11320/20000], Training Loss: 0.04785344483596938, Validation Loss: 0.006552503376659453\n",
      "Epoch [11321/20000], Training Loss: 0.03889882329843074, Validation Loss: 0.03813076498253005\n",
      "Epoch [11322/20000], Training Loss: 0.02275202183616492, Validation Loss: 0.025804395920515862\n",
      "Epoch [11323/20000], Training Loss: 0.02008490628629391, Validation Loss: 0.009976214754195618\n",
      "Epoch [11324/20000], Training Loss: 0.010703604202717543, Validation Loss: 0.008067673576684363\n",
      "Epoch [11325/20000], Training Loss: 0.009069980392398844, Validation Loss: 0.007725854697265991\n",
      "Epoch [11326/20000], Training Loss: 0.007009163521745775, Validation Loss: 0.006181985699340332\n",
      "Epoch [11327/20000], Training Loss: 0.006355498454045768, Validation Loss: 0.00735987754921706\n",
      "Epoch [11328/20000], Training Loss: 0.007243467695973648, Validation Loss: 0.00613947045257403\n",
      "Epoch [11329/20000], Training Loss: 0.005923939343558361, Validation Loss: 0.004667988528811292\n",
      "Epoch [11330/20000], Training Loss: 0.004728058311489544, Validation Loss: 0.003823322650652169\n",
      "Epoch [11331/20000], Training Loss: 0.0043865109816709135, Validation Loss: 0.007121580453399541\n",
      "Epoch [11332/20000], Training Loss: 0.004565383540466428, Validation Loss: 0.005128767867041465\n",
      "Epoch [11333/20000], Training Loss: 0.004048785889608553, Validation Loss: 0.005247542639139455\n",
      "Epoch [11334/20000], Training Loss: 0.0057385236515464, Validation Loss: 0.007958223244974687\n",
      "Epoch [11335/20000], Training Loss: 0.007409418635298997, Validation Loss: 0.0038369961525294943\n",
      "Epoch [11336/20000], Training Loss: 0.008580308068173639, Validation Loss: 0.003811456204334666\n",
      "Epoch [11337/20000], Training Loss: 0.013384029142668754, Validation Loss: 0.03338092314797336\n",
      "Epoch [11338/20000], Training Loss: 0.01335802423683674, Validation Loss: 0.005808197352986755\n",
      "Epoch [11339/20000], Training Loss: 0.007216620175833148, Validation Loss: 0.004499734679749663\n",
      "Epoch [11340/20000], Training Loss: 0.004620660228413597, Validation Loss: 0.009831936231683778\n",
      "Epoch [11341/20000], Training Loss: 0.011772006936423298, Validation Loss: 0.007003953547367214\n",
      "Epoch [11342/20000], Training Loss: 0.005565561329214168, Validation Loss: 0.008887443664011684\n",
      "Epoch [11343/20000], Training Loss: 0.005828506122530338, Validation Loss: 0.0024204341346666775\n",
      "Epoch [11344/20000], Training Loss: 0.005663386797615593, Validation Loss: 0.003234264445074523\n",
      "Epoch [11345/20000], Training Loss: 0.009191239735108476, Validation Loss: 0.0031022056278594548\n",
      "Epoch [11346/20000], Training Loss: 0.026964958222898922, Validation Loss: 0.005661140927780149\n",
      "Epoch [11347/20000], Training Loss: 0.11022277730808128, Validation Loss: 0.04276857621568654\n",
      "Epoch [11348/20000], Training Loss: 0.0642523998831166, Validation Loss: 0.0512721414956364\n",
      "Epoch [11349/20000], Training Loss: 0.023948330681637993, Validation Loss: 0.009688505106255565\n",
      "Epoch [11350/20000], Training Loss: 0.01128450663028551, Validation Loss: 0.012797777309080669\n",
      "Epoch [11351/20000], Training Loss: 0.009148543405379834, Validation Loss: 0.005633748138409699\n",
      "Epoch [11352/20000], Training Loss: 0.006714374841456967, Validation Loss: 0.005767327032507248\n",
      "Epoch [11353/20000], Training Loss: 0.006195937877886796, Validation Loss: 0.00439614257762904\n",
      "Epoch [11354/20000], Training Loss: 0.007422811561679866, Validation Loss: 0.0033517080134970456\n",
      "Epoch [11355/20000], Training Loss: 0.00922914744348548, Validation Loss: 0.0031271980884233536\n",
      "Epoch [11356/20000], Training Loss: 0.014558393537299708, Validation Loss: 0.005738414449390684\n",
      "Epoch [11357/20000], Training Loss: 0.014704668615844898, Validation Loss: 0.02392721187970892\n",
      "Epoch [11358/20000], Training Loss: 0.03321736510107647, Validation Loss: 0.005185006704972953\n",
      "Epoch [11359/20000], Training Loss: 0.0134705349586771, Validation Loss: 0.007571217564434717\n",
      "Epoch [11360/20000], Training Loss: 0.007129705346804778, Validation Loss: 0.007004049642140282\n",
      "Epoch [11361/20000], Training Loss: 0.005435304551176452, Validation Loss: 0.007479599149324453\n",
      "Epoch [11362/20000], Training Loss: 0.005772562631006751, Validation Loss: 0.012239968465775877\n",
      "Epoch [11363/20000], Training Loss: 0.006581378801326666, Validation Loss: 0.004677857415897181\n",
      "Epoch [11364/20000], Training Loss: 0.007844306285599194, Validation Loss: 0.05102444301722463\n",
      "Epoch [11365/20000], Training Loss: 0.020227794870152138, Validation Loss: 0.007304826245071386\n",
      "Epoch [11366/20000], Training Loss: 0.007699137167167335, Validation Loss: 0.012571621938150201\n",
      "Epoch [11367/20000], Training Loss: 0.00927794465262975, Validation Loss: 0.007638495196975522\n",
      "Epoch [11368/20000], Training Loss: 0.012167389138734766, Validation Loss: 0.0031155326885011292\n",
      "Epoch [11369/20000], Training Loss: 0.005427499423344021, Validation Loss: 0.003961111409807927\n",
      "Epoch [11370/20000], Training Loss: 0.005131249621626921, Validation Loss: 0.009207347262223007\n",
      "Epoch [11371/20000], Training Loss: 0.011990853227741485, Validation Loss: 0.0032172997921163316\n",
      "Epoch [11372/20000], Training Loss: 0.006504284590066943, Validation Loss: 0.008949817804155\n",
      "Epoch [11373/20000], Training Loss: 0.008879319051629864, Validation Loss: 0.022095782343310204\n",
      "Epoch [11374/20000], Training Loss: 0.008227701621113479, Validation Loss: 0.004242821326373912\n",
      "Epoch [11375/20000], Training Loss: 0.007493136147136933, Validation Loss: 0.013073613040650198\n",
      "Epoch [11376/20000], Training Loss: 0.016287383357329027, Validation Loss: 0.006522464108109765\n",
      "Epoch [11377/20000], Training Loss: 0.009707046643680986, Validation Loss: 0.0157152255143639\n",
      "Epoch [11378/20000], Training Loss: 0.01149485449137241, Validation Loss: 0.021324436432303275\n",
      "Epoch [11379/20000], Training Loss: 0.0129250729857761, Validation Loss: 0.0075288300433860355\n",
      "Epoch [11380/20000], Training Loss: 0.012781827957951464, Validation Loss: 0.007998574968405186\n",
      "Epoch [11381/20000], Training Loss: 0.017261170103925645, Validation Loss: 0.024015853468972535\n",
      "Epoch [11382/20000], Training Loss: 0.025519843847307908, Validation Loss: 0.010952957000784278\n",
      "Epoch [11383/20000], Training Loss: 0.01316694350524423, Validation Loss: 0.0049459586287282165\n",
      "Epoch [11384/20000], Training Loss: 0.007335505865090194, Validation Loss: 0.0057702577441075065\n",
      "Epoch [11385/20000], Training Loss: 0.004699366928044972, Validation Loss: 0.006663034140099658\n",
      "Epoch [11386/20000], Training Loss: 0.00456166138116844, Validation Loss: 0.0035492750336609496\n",
      "Epoch [11387/20000], Training Loss: 0.0045612626236106735, Validation Loss: 0.0034602022073784156\n",
      "Epoch [11388/20000], Training Loss: 0.007973047239439828, Validation Loss: 0.00866068604290555\n",
      "Epoch [11389/20000], Training Loss: 0.008394165819221857, Validation Loss: 0.007183465646966576\n",
      "Epoch [11390/20000], Training Loss: 0.008906850061196434, Validation Loss: 0.012690865526768189\n",
      "Epoch [11391/20000], Training Loss: 0.014173784772408129, Validation Loss: 0.00472460471233863\n",
      "Epoch [11392/20000], Training Loss: 0.003755117594014986, Validation Loss: 0.003286620552974081\n",
      "Epoch [11393/20000], Training Loss: 0.005823936257261916, Validation Loss: 0.0033798757194663915\n",
      "Epoch [11394/20000], Training Loss: 0.004853639333353905, Validation Loss: 0.0027855970686638492\n",
      "Epoch [11395/20000], Training Loss: 0.005262130393280781, Validation Loss: 0.0028477018812184595\n",
      "Epoch [11396/20000], Training Loss: 0.005522685108839401, Validation Loss: 0.005566067361611106\n",
      "Epoch [11397/20000], Training Loss: 0.0035351781370991375, Validation Loss: 0.013593504661647517\n",
      "Epoch [11398/20000], Training Loss: 0.005977949863953265, Validation Loss: 0.008276985400693247\n",
      "Epoch [11399/20000], Training Loss: 0.012318872649692432, Validation Loss: 0.003516572898428803\n",
      "Epoch [11400/20000], Training Loss: 0.037064409996414076, Validation Loss: 0.008404908524259522\n",
      "Epoch [11401/20000], Training Loss: 0.019439943242105073, Validation Loss: 0.003310760907820866\n",
      "Epoch [11402/20000], Training Loss: 0.0038867788701151896, Validation Loss: 0.003320705522733241\n",
      "Epoch [11403/20000], Training Loss: 0.006339566032693256, Validation Loss: 0.0037190969152963555\n",
      "Epoch [11404/20000], Training Loss: 0.00518653923895077, Validation Loss: 0.026150801083474562\n",
      "Epoch [11405/20000], Training Loss: 0.017997419031546866, Validation Loss: 0.005007517723151571\n",
      "Epoch [11406/20000], Training Loss: 0.00860923171851383, Validation Loss: 0.002375738538032936\n",
      "Epoch [11407/20000], Training Loss: 0.01202129199191404, Validation Loss: 0.006638880004146586\n",
      "Epoch [11408/20000], Training Loss: 0.011580311126985115, Validation Loss: 0.011532667701317223\n",
      "Epoch [11409/20000], Training Loss: 0.011663347011043308, Validation Loss: 0.003530551459361066\n",
      "Epoch [11410/20000], Training Loss: 0.013371936024798612, Validation Loss: 0.009257851626512905\n",
      "Epoch [11411/20000], Training Loss: 0.007273597641545036, Validation Loss: 0.0058839686123682055\n",
      "Epoch [11412/20000], Training Loss: 0.009372735826348486, Validation Loss: 0.0037395134870743213\n",
      "Epoch [11413/20000], Training Loss: 0.01386355449722032, Validation Loss: 0.0029920110376439523\n",
      "Epoch [11414/20000], Training Loss: 0.012176993963553937, Validation Loss: 0.00562079856437947\n",
      "Epoch [11415/20000], Training Loss: 0.005703840414753358, Validation Loss: 0.007459047564241571\n",
      "Epoch [11416/20000], Training Loss: 0.0052662149007249226, Validation Loss: 0.0069179071658729595\n",
      "Epoch [11417/20000], Training Loss: 0.0097565843144756, Validation Loss: 0.004063895923264583\n",
      "Epoch [11418/20000], Training Loss: 0.007406491304014448, Validation Loss: 0.012733965828197873\n",
      "Epoch [11419/20000], Training Loss: 0.017879944101358496, Validation Loss: 0.013648754161134693\n",
      "Epoch [11420/20000], Training Loss: 0.012670383295958995, Validation Loss: 0.0036647295746133424\n",
      "Epoch [11421/20000], Training Loss: 0.007183944000546815, Validation Loss: 0.027116779958991595\n",
      "Epoch [11422/20000], Training Loss: 0.009644879744882928, Validation Loss: 0.0042923329146020706\n",
      "Epoch [11423/20000], Training Loss: 0.020240903311555973, Validation Loss: 0.0048337281717445125\n",
      "Epoch [11424/20000], Training Loss: 0.010497002924109568, Validation Loss: 0.003738518159735804\n",
      "Epoch [11425/20000], Training Loss: 0.006521362898638472, Validation Loss: 0.0029438358141403243\n",
      "Epoch [11426/20000], Training Loss: 0.015612141395519887, Validation Loss: 0.003180626057463038\n",
      "Epoch [11427/20000], Training Loss: 0.026843530285686614, Validation Loss: 0.0067421266771207555\n",
      "Epoch [11428/20000], Training Loss: 0.026590872481132725, Validation Loss: 0.004190009586839746\n",
      "Epoch [11429/20000], Training Loss: 0.024704553328254924, Validation Loss: 0.014707798936943359\n",
      "Epoch [11430/20000], Training Loss: 0.031558353112944006, Validation Loss: 0.020654173673473712\n",
      "Epoch [11431/20000], Training Loss: 0.016736869525630027, Validation Loss: 0.008284552364297528\n",
      "Epoch [11432/20000], Training Loss: 0.012643116039855937, Validation Loss: 0.007029879677146157\n",
      "Epoch [11433/20000], Training Loss: 0.008628663062933941, Validation Loss: 0.007154261032525255\n",
      "Epoch [11434/20000], Training Loss: 0.006040313583590822, Validation Loss: 0.004291860105752221\n",
      "Epoch [11435/20000], Training Loss: 0.006866696319775656, Validation Loss: 0.0037412250870992076\n",
      "Epoch [11436/20000], Training Loss: 0.00458591542805412, Validation Loss: 0.004762657022792739\n",
      "Epoch [11437/20000], Training Loss: 0.009231939577148296, Validation Loss: 0.0035159127893434417\n",
      "Epoch [11438/20000], Training Loss: 0.008271828244427784, Validation Loss: 0.012650063236595852\n",
      "Epoch [11439/20000], Training Loss: 0.009719188573269224, Validation Loss: 0.004999369172653344\n",
      "Epoch [11440/20000], Training Loss: 0.005009401049132326, Validation Loss: 0.0034023304602795535\n",
      "Epoch [11441/20000], Training Loss: 0.006048728342908102, Validation Loss: 0.0033772037763567042\n",
      "Epoch [11442/20000], Training Loss: 0.0060600425993990415, Validation Loss: 0.0036396443028853093\n",
      "Epoch [11443/20000], Training Loss: 0.008171343152103614, Validation Loss: 0.011773036869066849\n",
      "Epoch [11444/20000], Training Loss: 0.004935309290650431, Validation Loss: 0.012266430710318283\n",
      "Epoch [11445/20000], Training Loss: 0.009908336233430808, Validation Loss: 0.0034073458566723502\n",
      "Epoch [11446/20000], Training Loss: 0.005569872958793505, Validation Loss: 0.0024293797477525914\n",
      "Epoch [11447/20000], Training Loss: 0.006035698502923229, Validation Loss: 0.0024756021484892343\n",
      "Epoch [11448/20000], Training Loss: 0.004757074613215602, Validation Loss: 0.004905425587369093\n",
      "Epoch [11449/20000], Training Loss: 0.004170394115914989, Validation Loss: 0.0025214949528061326\n",
      "Epoch [11450/20000], Training Loss: 0.004786765626964292, Validation Loss: 0.0031047959189872847\n",
      "Epoch [11451/20000], Training Loss: 0.009654768029642062, Validation Loss: 0.010440291053083481\n",
      "Epoch [11452/20000], Training Loss: 0.01576192367727864, Validation Loss: 0.03703335497368067\n",
      "Epoch [11453/20000], Training Loss: 0.030313975692219434, Validation Loss: 0.007915160303002924\n",
      "Epoch [11454/20000], Training Loss: 0.028142794845085257, Validation Loss: 0.006606239021924206\n",
      "Epoch [11455/20000], Training Loss: 0.01878388107797946, Validation Loss: 0.006976239003372646\n",
      "Epoch [11456/20000], Training Loss: 0.006725551084985325, Validation Loss: 0.004792898368998115\n",
      "Epoch [11457/20000], Training Loss: 0.007597712867664086, Validation Loss: 0.05637608445366433\n",
      "Epoch [11458/20000], Training Loss: 0.039465601474928134, Validation Loss: 0.005076665299814717\n",
      "Epoch [11459/20000], Training Loss: 0.03517807487615495, Validation Loss: 0.04860435746136954\n",
      "Epoch [11460/20000], Training Loss: 0.017481674731243402, Validation Loss: 0.01062220119697242\n",
      "Epoch [11461/20000], Training Loss: 0.008578628262122428, Validation Loss: 0.004358420454272911\n",
      "Epoch [11462/20000], Training Loss: 0.009560707524461318, Validation Loss: 0.004677027098495046\n",
      "Epoch [11463/20000], Training Loss: 0.007270122268112443, Validation Loss: 0.005196481297665352\n",
      "Epoch [11464/20000], Training Loss: 0.008838180011870074, Validation Loss: 0.0056694448319863765\n",
      "Epoch [11465/20000], Training Loss: 0.004651219666162173, Validation Loss: 0.005211907503767829\n",
      "Epoch [11466/20000], Training Loss: 0.00685262495868041, Validation Loss: 0.006081721591036577\n",
      "Epoch [11467/20000], Training Loss: 0.007235714512976951, Validation Loss: 0.0039353824071639565\n",
      "Epoch [11468/20000], Training Loss: 0.010946168584528746, Validation Loss: 0.0077029154137834765\n",
      "Epoch [11469/20000], Training Loss: 0.005701289578740086, Validation Loss: 0.005953292242769034\n",
      "Epoch [11470/20000], Training Loss: 0.007982619210711814, Validation Loss: 0.00393985336203733\n",
      "Epoch [11471/20000], Training Loss: 0.004610093680509765, Validation Loss: 0.006044249400738571\n",
      "Epoch [11472/20000], Training Loss: 0.007236346377924617, Validation Loss: 0.003519368373446241\n",
      "Epoch [11473/20000], Training Loss: 0.005546254088195772, Validation Loss: 0.011188669177424717\n",
      "Epoch [11474/20000], Training Loss: 0.00836169936155784, Validation Loss: 0.002990046353533314\n",
      "Epoch [11475/20000], Training Loss: 0.007669976230317843, Validation Loss: 0.01063269966681649\n",
      "Epoch [11476/20000], Training Loss: 0.013963260850037582, Validation Loss: 0.003126334830424428\n",
      "Epoch [11477/20000], Training Loss: 0.023430831647211953, Validation Loss: 0.006095415888832382\n",
      "Epoch [11478/20000], Training Loss: 0.03201956273869655, Validation Loss: 0.01448542239057003\n",
      "Epoch [11479/20000], Training Loss: 0.020262801042658145, Validation Loss: 0.032111892008694146\n",
      "Epoch [11480/20000], Training Loss: 0.01853360145469196, Validation Loss: 0.007944608925356533\n",
      "Epoch [11481/20000], Training Loss: 0.011730147691975747, Validation Loss: 0.0318871821943491\n",
      "Epoch [11482/20000], Training Loss: 0.018359227837728604, Validation Loss: 0.008631760586382597\n",
      "Epoch [11483/20000], Training Loss: 0.012211264800758468, Validation Loss: 0.0071488045369006515\n",
      "Epoch [11484/20000], Training Loss: 0.006011745586874895, Validation Loss: 0.004082596321124145\n",
      "Epoch [11485/20000], Training Loss: 0.004485090854213273, Validation Loss: 0.003933230702383891\n",
      "Epoch [11486/20000], Training Loss: 0.005574097214516119, Validation Loss: 0.004013102725807016\n",
      "Epoch [11487/20000], Training Loss: 0.005793874655085217, Validation Loss: 0.002697160209810891\n",
      "Epoch [11488/20000], Training Loss: 0.006253067298010657, Validation Loss: 0.002548260564923664\n",
      "Epoch [11489/20000], Training Loss: 0.004518397967331111, Validation Loss: 0.002742764304430117\n",
      "Epoch [11490/20000], Training Loss: 0.004426151341119423, Validation Loss: 0.005117604364286876\n",
      "Epoch [11491/20000], Training Loss: 0.006051085471491595, Validation Loss: 0.0030156046722257406\n",
      "Epoch [11492/20000], Training Loss: 0.003906507232646358, Validation Loss: 0.00534517995466948\n",
      "Epoch [11493/20000], Training Loss: 0.007874164376906785, Validation Loss: 0.0021328393001712903\n",
      "Epoch [11494/20000], Training Loss: 0.006768109488932948, Validation Loss: 0.005533308773938269\n",
      "Epoch [11495/20000], Training Loss: 0.005044400855175419, Validation Loss: 0.0021904232707541382\n",
      "Epoch [11496/20000], Training Loss: 0.008607142895406079, Validation Loss: 0.0025241665931053448\n",
      "Epoch [11497/20000], Training Loss: 0.023514765191066545, Validation Loss: 0.00976964094822311\n",
      "Epoch [11498/20000], Training Loss: 0.00924064570374737, Validation Loss: 0.006691218502660539\n",
      "Epoch [11499/20000], Training Loss: 0.008920344781862306, Validation Loss: 0.002422371145324271\n",
      "Epoch [11500/20000], Training Loss: 0.007346932471851427, Validation Loss: 0.003824751656056589\n",
      "Epoch [11501/20000], Training Loss: 0.006211909753541737, Validation Loss: 0.006611206416909202\n",
      "Epoch [11502/20000], Training Loss: 0.0173404336043989, Validation Loss: 0.019376967485604295\n",
      "Epoch [11503/20000], Training Loss: 0.010594691109223018, Validation Loss: 0.012561963335230075\n",
      "Epoch [11504/20000], Training Loss: 0.024534464264953777, Validation Loss: 0.02973545085215206\n",
      "Epoch [11505/20000], Training Loss: 0.02670338730760185, Validation Loss: 0.034733262844386534\n",
      "Epoch [11506/20000], Training Loss: 0.02041283209941217, Validation Loss: 0.0035512266185061697\n",
      "Epoch [11507/20000], Training Loss: 0.003702258500201846, Validation Loss: 0.004039708372553312\n",
      "Epoch [11508/20000], Training Loss: 0.004078811737209824, Validation Loss: 0.0044128493742314435\n",
      "Epoch [11509/20000], Training Loss: 0.0049538607293340775, Validation Loss: 0.0046880176041668165\n",
      "Epoch [11510/20000], Training Loss: 0.007648800802209215, Validation Loss: 0.0029933580350050954\n",
      "Epoch [11511/20000], Training Loss: 0.006455432558141183, Validation Loss: 0.006241473046104927\n",
      "Epoch [11512/20000], Training Loss: 0.00755532856523392, Validation Loss: 0.004021435271799386\n",
      "Epoch [11513/20000], Training Loss: 0.00815256015630439, Validation Loss: 0.006917561575487444\n",
      "Epoch [11514/20000], Training Loss: 0.01068187497522948, Validation Loss: 0.021419994312555195\n",
      "Epoch [11515/20000], Training Loss: 0.013328611803333144, Validation Loss: 0.004732376316852083\n",
      "Epoch [11516/20000], Training Loss: 0.011412258347970041, Validation Loss: 0.009577175580377326\n",
      "Epoch [11517/20000], Training Loss: 0.00698221087730287, Validation Loss: 0.0034991321228708167\n",
      "Epoch [11518/20000], Training Loss: 0.005019244785078821, Validation Loss: 0.004093974062547951\n",
      "Epoch [11519/20000], Training Loss: 0.009790945064326349, Validation Loss: 0.007998645640534667\n",
      "Epoch [11520/20000], Training Loss: 0.01567757953099707, Validation Loss: 0.017054022891573237\n",
      "Epoch [11521/20000], Training Loss: 0.009772103332839574, Validation Loss: 0.0030595361647759284\n",
      "Epoch [11522/20000], Training Loss: 0.010537598561703427, Validation Loss: 0.0033260880420412165\n",
      "Epoch [11523/20000], Training Loss: 0.005808309523282722, Validation Loss: 0.008186767161549011\n",
      "Epoch [11524/20000], Training Loss: 0.010658178819409971, Validation Loss: 0.008164024145710666\n",
      "Epoch [11525/20000], Training Loss: 0.014809301864650999, Validation Loss: 0.009566525397156665\n",
      "Epoch [11526/20000], Training Loss: 0.015214037549282824, Validation Loss: 0.005166280112845811\n",
      "Epoch [11527/20000], Training Loss: 0.029503423672784784, Validation Loss: 0.004114478363236783\n",
      "Epoch [11528/20000], Training Loss: 0.021090139422865053, Validation Loss: 0.007660940184548183\n",
      "Epoch [11529/20000], Training Loss: 0.01011063963921645, Validation Loss: 0.042107806431595965\n",
      "Epoch [11530/20000], Training Loss: 0.014955932557183717, Validation Loss: 0.003981409589804181\n",
      "Epoch [11531/20000], Training Loss: 0.00690980933541141, Validation Loss: 0.00429480973521988\n",
      "Epoch [11532/20000], Training Loss: 0.008538434983555427, Validation Loss: 0.025211455633609603\n",
      "Epoch [11533/20000], Training Loss: 0.01680878814035428, Validation Loss: 0.009999901434846541\n",
      "Epoch [11534/20000], Training Loss: 0.01709240222615855, Validation Loss: 0.016249157546285784\n",
      "Epoch [11535/20000], Training Loss: 0.016890370581807344, Validation Loss: 0.005638165608242421\n",
      "Epoch [11536/20000], Training Loss: 0.0071395553064316376, Validation Loss: 0.012359782311286998\n",
      "Epoch [11537/20000], Training Loss: 0.012201839488365554, Validation Loss: 0.005657829156916705\n",
      "Epoch [11538/20000], Training Loss: 0.008755050649467324, Validation Loss: 0.011740404639750426\n",
      "Epoch [11539/20000], Training Loss: 0.0032390797329883625, Validation Loss: 0.0052979747352200745\n",
      "Epoch [11540/20000], Training Loss: 0.006814308936424952, Validation Loss: 0.004883402419366689\n",
      "Epoch [11541/20000], Training Loss: 0.006602712938079743, Validation Loss: 0.007354420517399619\n",
      "Epoch [11542/20000], Training Loss: 0.005127093068454412, Validation Loss: 0.0030596225012859335\n",
      "Epoch [11543/20000], Training Loss: 0.005325681204989711, Validation Loss: 0.010662375639831063\n",
      "Epoch [11544/20000], Training Loss: 0.009678386465452604, Validation Loss: 0.004279193221545289\n",
      "Epoch [11545/20000], Training Loss: 0.006696277567998875, Validation Loss: 0.003352296018316484\n",
      "Epoch [11546/20000], Training Loss: 0.004425621504196897, Validation Loss: 0.0031282033847607033\n",
      "Epoch [11547/20000], Training Loss: 0.0053570886405956185, Validation Loss: 0.002711088926225053\n",
      "Epoch [11548/20000], Training Loss: 0.00966394134541458, Validation Loss: 0.004237405056109367\n",
      "Epoch [11549/20000], Training Loss: 0.013564599678050269, Validation Loss: 0.015428924273559273\n",
      "Epoch [11550/20000], Training Loss: 0.004843851301952132, Validation Loss: 0.005706068248349945\n",
      "Epoch [11551/20000], Training Loss: 0.005291868713227034, Validation Loss: 0.0036328611306645697\n",
      "Epoch [11552/20000], Training Loss: 0.0076976573464630094, Validation Loss: 0.022147508656075843\n",
      "Epoch [11553/20000], Training Loss: 0.010749703202496417, Validation Loss: 0.004869156204338968\n",
      "Epoch [11554/20000], Training Loss: 0.008426371069487817, Validation Loss: 0.003981220419482108\n",
      "Epoch [11555/20000], Training Loss: 0.005357205136976161, Validation Loss: 0.0029747026978950786\n",
      "Epoch [11556/20000], Training Loss: 0.005117669473033207, Validation Loss: 0.01285058675597688\n",
      "Epoch [11557/20000], Training Loss: 0.02241445589970681, Validation Loss: 0.004370509263662622\n",
      "Epoch [11558/20000], Training Loss: 0.014225773831463633, Validation Loss: 0.00896509221891506\n",
      "Epoch [11559/20000], Training Loss: 0.03205623490742125, Validation Loss: 0.0647289879393611\n",
      "Epoch [11560/20000], Training Loss: 0.03803185036771798, Validation Loss: 0.02047444887479496\n",
      "Epoch [11561/20000], Training Loss: 0.018192513814678284, Validation Loss: 0.014652132985970314\n",
      "Epoch [11562/20000], Training Loss: 0.00845674959210945, Validation Loss: 0.0038557707861132584\n",
      "Epoch [11563/20000], Training Loss: 0.005944139345176934, Validation Loss: 0.003848584670570127\n",
      "Epoch [11564/20000], Training Loss: 0.0033875065356759088, Validation Loss: 0.00940791691408752\n",
      "Epoch [11565/20000], Training Loss: 0.0086506386869587, Validation Loss: 0.0031778865591210854\n",
      "Epoch [11566/20000], Training Loss: 0.01012838824577297, Validation Loss: 0.00943521200984573\n",
      "Epoch [11567/20000], Training Loss: 0.008372059195867873, Validation Loss: 0.005150294095634373\n",
      "Epoch [11568/20000], Training Loss: 0.00550942363458619, Validation Loss: 0.004033374823151803\n",
      "Epoch [11569/20000], Training Loss: 0.007864920916166593, Validation Loss: 0.0056796970958105574\n",
      "Epoch [11570/20000], Training Loss: 0.007742604186725137, Validation Loss: 0.003379185957953723\n",
      "Epoch [11571/20000], Training Loss: 0.015742313584009286, Validation Loss: 0.004573704060295621\n",
      "Epoch [11572/20000], Training Loss: 0.008244992381410807, Validation Loss: 0.011981316590404547\n",
      "Epoch [11573/20000], Training Loss: 0.00708868377010471, Validation Loss: 0.008873378922403141\n",
      "Epoch [11574/20000], Training Loss: 0.015791635881344388, Validation Loss: 0.01030844084004262\n",
      "Epoch [11575/20000], Training Loss: 0.023562662525884143, Validation Loss: 0.007856231199854362\n",
      "Epoch [11576/20000], Training Loss: 0.021324776616113792, Validation Loss: 0.0074194908948633\n",
      "Epoch [11577/20000], Training Loss: 0.01730247883382877, Validation Loss: 0.036407098034292824\n",
      "Epoch [11578/20000], Training Loss: 0.010661622254068399, Validation Loss: 0.004681614752745945\n",
      "Epoch [11579/20000], Training Loss: 0.014315239091437044, Validation Loss: 0.01174818319368011\n",
      "Epoch [11580/20000], Training Loss: 0.006609446323376947, Validation Loss: 0.0046303632981885235\n",
      "Epoch [11581/20000], Training Loss: 0.004612674312007974, Validation Loss: 0.005313341060652994\n",
      "Epoch [11582/20000], Training Loss: 0.0066214844667099116, Validation Loss: 0.015662105341826407\n",
      "Epoch [11583/20000], Training Loss: 0.010437138538933815, Validation Loss: 0.005508018876454616\n",
      "Epoch [11584/20000], Training Loss: 0.008036370836115176, Validation Loss: 0.0037777726876393153\n",
      "Epoch [11585/20000], Training Loss: 0.01582814087695234, Validation Loss: 0.008000479243134708\n",
      "Epoch [11586/20000], Training Loss: 0.011038834846528647, Validation Loss: 0.011210336436067994\n",
      "Epoch [11587/20000], Training Loss: 0.02022442651026982, Validation Loss: 0.01414453847922102\n",
      "Epoch [11588/20000], Training Loss: 0.01159285761864989, Validation Loss: 0.004231281947436319\n",
      "Epoch [11589/20000], Training Loss: 0.004781110598872017, Validation Loss: 0.004790340687967338\n",
      "Epoch [11590/20000], Training Loss: 0.005506394757373657, Validation Loss: 0.003563005868583348\n",
      "Epoch [11591/20000], Training Loss: 0.0041318318305587, Validation Loss: 0.0032169206532809574\n",
      "Epoch [11592/20000], Training Loss: 0.005944010009573374, Validation Loss: 0.018357749604218994\n",
      "Epoch [11593/20000], Training Loss: 0.011497203589117686, Validation Loss: 0.007697123103146123\n",
      "Epoch [11594/20000], Training Loss: 0.00730153643858752, Validation Loss: 0.0066331582642291664\n",
      "Epoch [11595/20000], Training Loss: 0.006131782983305, Validation Loss: 0.007524866162174452\n",
      "Epoch [11596/20000], Training Loss: 0.007190533875838615, Validation Loss: 0.005874682388857355\n",
      "Epoch [11597/20000], Training Loss: 0.005467185626392685, Validation Loss: 0.0033248980689667257\n",
      "Epoch [11598/20000], Training Loss: 0.007560622892896163, Validation Loss: 0.010967433942583973\n",
      "Epoch [11599/20000], Training Loss: 0.020747165319091567, Validation Loss: 0.01122457181074155\n",
      "Epoch [11600/20000], Training Loss: 0.010740724332988196, Validation Loss: 0.018053077395173273\n",
      "Epoch [11601/20000], Training Loss: 0.02560386903600634, Validation Loss: 0.02145630147997101\n",
      "Epoch [11602/20000], Training Loss: 0.018331883923695256, Validation Loss: 0.00424218744038894\n",
      "Epoch [11603/20000], Training Loss: 0.00800403778807127, Validation Loss: 0.009738177414543461\n",
      "Epoch [11604/20000], Training Loss: 0.008159941934406691, Validation Loss: 0.004682512801414772\n",
      "Epoch [11605/20000], Training Loss: 0.0073064483344101715, Validation Loss: 0.00832436552198398\n",
      "Epoch [11606/20000], Training Loss: 0.005036309785542211, Validation Loss: 0.007532798317835646\n",
      "Epoch [11607/20000], Training Loss: 0.005267124293043578, Validation Loss: 0.02620837468153617\n",
      "Epoch [11608/20000], Training Loss: 0.01014418229169678, Validation Loss: 0.004131844388044925\n",
      "Epoch [11609/20000], Training Loss: 0.006162758717567028, Validation Loss: 0.00306779139277426\n",
      "Epoch [11610/20000], Training Loss: 0.007647545004147105, Validation Loss: 0.010822798215215559\n",
      "Epoch [11611/20000], Training Loss: 0.004211010168156852, Validation Loss: 0.003301161780055192\n",
      "Epoch [11612/20000], Training Loss: 0.004548833050648682, Validation Loss: 0.005044240410697398\n",
      "Epoch [11613/20000], Training Loss: 0.008557851257397229, Validation Loss: 0.004864924945048342\n",
      "Epoch [11614/20000], Training Loss: 0.004909732485560069, Validation Loss: 0.014646140215855244\n",
      "Epoch [11615/20000], Training Loss: 0.013323571940304646, Validation Loss: 0.005128533329663502\n",
      "Epoch [11616/20000], Training Loss: 0.008648266164787597, Validation Loss: 0.007607213237802414\n",
      "Epoch [11617/20000], Training Loss: 0.007824089158280654, Validation Loss: 0.011271821055073263\n",
      "Epoch [11618/20000], Training Loss: 0.012474810413550586, Validation Loss: 0.006123761920864723\n",
      "Epoch [11619/20000], Training Loss: 0.01658027037254734, Validation Loss: 0.006132885801806408\n",
      "Epoch [11620/20000], Training Loss: 0.009703508359442432, Validation Loss: 0.0061712202754295375\n",
      "Epoch [11621/20000], Training Loss: 0.008502610418093224, Validation Loss: 0.008402153021347978\n",
      "Epoch [11622/20000], Training Loss: 0.005961618091142944, Validation Loss: 0.005200153456796792\n",
      "Epoch [11623/20000], Training Loss: 0.008720084516815925, Validation Loss: 0.010427340366508784\n",
      "Epoch [11624/20000], Training Loss: 0.013217536896263482, Validation Loss: 0.024987796411421317\n",
      "Epoch [11625/20000], Training Loss: 0.023497237582757537, Validation Loss: 0.007158051425481712\n",
      "Epoch [11626/20000], Training Loss: 0.05537177521702168, Validation Loss: 0.04275792086991714\n",
      "Epoch [11627/20000], Training Loss: 0.02813992105841732, Validation Loss: 0.040247891171020456\n",
      "Epoch [11628/20000], Training Loss: 0.018267328110856136, Validation Loss: 0.00453938268308158\n",
      "Epoch [11629/20000], Training Loss: 0.006838466743439702, Validation Loss: 0.004135465579882488\n",
      "Epoch [11630/20000], Training Loss: 0.005953663182253877, Validation Loss: 0.006609064226865817\n",
      "Epoch [11631/20000], Training Loss: 0.0068112515798109795, Validation Loss: 0.0040457914991398935\n",
      "Epoch [11632/20000], Training Loss: 0.005664734747759732, Validation Loss: 0.006932529485060813\n",
      "Epoch [11633/20000], Training Loss: 0.009596078802132979, Validation Loss: 0.0043437835258726765\n",
      "Epoch [11634/20000], Training Loss: 0.025076164772534475, Validation Loss: 0.016048522313355264\n",
      "Epoch [11635/20000], Training Loss: 0.04719352895959413, Validation Loss: 0.018734956958549494\n",
      "Epoch [11636/20000], Training Loss: 0.01650526930877407, Validation Loss: 0.017147501453328978\n",
      "Epoch [11637/20000], Training Loss: 0.015244008834997658, Validation Loss: 0.007222081641316436\n",
      "Epoch [11638/20000], Training Loss: 0.004938922657207253, Validation Loss: 0.006180743263333304\n",
      "Epoch [11639/20000], Training Loss: 0.005506710691926335, Validation Loss: 0.004180198080518949\n",
      "Epoch [11640/20000], Training Loss: 0.0041998142270521, Validation Loss: 0.005639853529405262\n",
      "Epoch [11641/20000], Training Loss: 0.00494635575367803, Validation Loss: 0.0039475132158709614\n",
      "Epoch [11642/20000], Training Loss: 0.00551675864387237, Validation Loss: 0.004257712946535922\n",
      "Epoch [11643/20000], Training Loss: 0.004146613050368485, Validation Loss: 0.003309092294715908\n",
      "Epoch [11644/20000], Training Loss: 0.0057966245427191355, Validation Loss: 0.019555530947979995\n",
      "Epoch [11645/20000], Training Loss: 0.02162014434108122, Validation Loss: 0.006102255235023358\n",
      "Epoch [11646/20000], Training Loss: 0.018939225730719045, Validation Loss: 0.005017703328007883\n",
      "Epoch [11647/20000], Training Loss: 0.022262526817939943, Validation Loss: 0.007507467441744861\n",
      "Epoch [11648/20000], Training Loss: 0.021327153758777837, Validation Loss: 0.00787973189050766\n",
      "Epoch [11649/20000], Training Loss: 0.012173886643722653, Validation Loss: 0.018137865433870668\n",
      "Epoch [11650/20000], Training Loss: 0.010579160583022582, Validation Loss: 0.006213741412570631\n",
      "Epoch [11651/20000], Training Loss: 0.015089730769562135, Validation Loss: 0.0076608845308702\n",
      "Epoch [11652/20000], Training Loss: 0.012692519519727543, Validation Loss: 0.006003939604615586\n",
      "Epoch [11653/20000], Training Loss: 0.0066960851836483926, Validation Loss: 0.0057982581030266035\n",
      "Epoch [11654/20000], Training Loss: 0.005565459630036328, Validation Loss: 0.00433422355768341\n",
      "Epoch [11655/20000], Training Loss: 0.006627584569287137, Validation Loss: 0.004235763655525407\n",
      "Epoch [11656/20000], Training Loss: 0.00680646273914525, Validation Loss: 0.0044946446680914475\n",
      "Epoch [11657/20000], Training Loss: 0.005374830058177135, Validation Loss: 0.005539009844593831\n",
      "Epoch [11658/20000], Training Loss: 0.005220360182907565, Validation Loss: 0.0036179215368140455\n",
      "Epoch [11659/20000], Training Loss: 0.0045183141774032265, Validation Loss: 0.0032557894266947835\n",
      "Epoch [11660/20000], Training Loss: 0.0037451623564785613, Validation Loss: 0.004976256549475152\n",
      "Epoch [11661/20000], Training Loss: 0.00554369282792831, Validation Loss: 0.0028480530904926938\n",
      "Epoch [11662/20000], Training Loss: 0.006887567901555615, Validation Loss: 0.006617381305164062\n",
      "Epoch [11663/20000], Training Loss: 0.007064908299032169, Validation Loss: 0.002805101505163776\n",
      "Epoch [11664/20000], Training Loss: 0.012698503121100657, Validation Loss: 0.005393886252261179\n",
      "Epoch [11665/20000], Training Loss: 0.009817745654231658, Validation Loss: 0.007872225946234586\n",
      "Epoch [11666/20000], Training Loss: 0.015276616936067253, Validation Loss: 0.03518283432174485\n",
      "Epoch [11667/20000], Training Loss: 0.045934034533144895, Validation Loss: 0.011421820613029052\n",
      "Epoch [11668/20000], Training Loss: 0.034372619097536826, Validation Loss: 0.024377559337874808\n",
      "Epoch [11669/20000], Training Loss: 0.026702662680431137, Validation Loss: 0.029484903173822328\n",
      "Epoch [11670/20000], Training Loss: 0.023929369436310872, Validation Loss: 0.025299186676395254\n",
      "Epoch [11671/20000], Training Loss: 0.0362224314420538, Validation Loss: 0.01115771939520219\n",
      "Epoch [11672/20000], Training Loss: 0.019229412914553125, Validation Loss: 0.012144646301734952\n",
      "Epoch [11673/20000], Training Loss: 0.012361802967331772, Validation Loss: 0.008124103120922297\n",
      "Epoch [11674/20000], Training Loss: 0.020481804067717997, Validation Loss: 0.011404061028651657\n",
      "Epoch [11675/20000], Training Loss: 0.01141225486104044, Validation Loss: 0.016556581528483844\n",
      "Epoch [11676/20000], Training Loss: 0.014145000910502858, Validation Loss: 0.01619636629979076\n",
      "Epoch [11677/20000], Training Loss: 0.014602941943199508, Validation Loss: 0.00750971984408823\n",
      "Epoch [11678/20000], Training Loss: 0.009740776382386684, Validation Loss: 0.00910973238986758\n",
      "Epoch [11679/20000], Training Loss: 0.008259001716006813, Validation Loss: 0.004765977480094615\n",
      "Epoch [11680/20000], Training Loss: 0.007812303185346536, Validation Loss: 0.005150522338799947\n",
      "Epoch [11681/20000], Training Loss: 0.008740440958799158, Validation Loss: 0.006830608515308764\n",
      "Epoch [11682/20000], Training Loss: 0.0053698513573700824, Validation Loss: 0.007326723132650719\n",
      "Epoch [11683/20000], Training Loss: 0.006865388943489441, Validation Loss: 0.005230149434641935\n",
      "Epoch [11684/20000], Training Loss: 0.0067938690429270665, Validation Loss: 0.0039830372545393644\n",
      "Epoch [11685/20000], Training Loss: 0.004952757895352988, Validation Loss: 0.004119554198712397\n",
      "Epoch [11686/20000], Training Loss: 0.004016673187184746, Validation Loss: 0.002926563386805065\n",
      "Epoch [11687/20000], Training Loss: 0.006493779374328109, Validation Loss: 0.002891191499792128\n",
      "Epoch [11688/20000], Training Loss: 0.0066591384712800005, Validation Loss: 0.0036468598653840445\n",
      "Epoch [11689/20000], Training Loss: 0.00558253276436257, Validation Loss: 0.013017487450569482\n",
      "Epoch [11690/20000], Training Loss: 0.00637854020958782, Validation Loss: 0.0025452594512443178\n",
      "Epoch [11691/20000], Training Loss: 0.00324064155574888, Validation Loss: 0.004505349409187842\n",
      "Epoch [11692/20000], Training Loss: 0.0061053611934767105, Validation Loss: 0.0024501032288576203\n",
      "Epoch [11693/20000], Training Loss: 0.012004857514122185, Validation Loss: 0.003991781185018591\n",
      "Epoch [11694/20000], Training Loss: 0.03731938283194073, Validation Loss: 0.009760735296339878\n",
      "Epoch [11695/20000], Training Loss: 0.012403696407740685, Validation Loss: 0.02332227695208238\n",
      "Epoch [11696/20000], Training Loss: 0.009647721802528915, Validation Loss: 0.004547363903584741\n",
      "Epoch [11697/20000], Training Loss: 0.005760185867464835, Validation Loss: 0.007746238243652741\n",
      "Epoch [11698/20000], Training Loss: 0.013671516650512265, Validation Loss: 0.003472881791429318\n",
      "Epoch [11699/20000], Training Loss: 0.01300066679147806, Validation Loss: 0.006927346737281174\n",
      "Epoch [11700/20000], Training Loss: 0.014245700783378976, Validation Loss: 0.006128750260759398\n",
      "Epoch [11701/20000], Training Loss: 0.014010763454410022, Validation Loss: 0.024761851295368485\n",
      "Epoch [11702/20000], Training Loss: 0.038418507685751786, Validation Loss: 0.013698026573519278\n",
      "Epoch [11703/20000], Training Loss: 0.015020633126758704, Validation Loss: 0.011787797675489906\n",
      "Epoch [11704/20000], Training Loss: 0.008818026083255453, Validation Loss: 0.004368954014170511\n",
      "Epoch [11705/20000], Training Loss: 0.005273297167621292, Validation Loss: 0.004480889609149179\n",
      "Epoch [11706/20000], Training Loss: 0.005172417020990646, Validation Loss: 0.004059847512150431\n",
      "Epoch [11707/20000], Training Loss: 0.005175953286587693, Validation Loss: 0.003587792939674626\n",
      "Epoch [11708/20000], Training Loss: 0.0043622125543021995, Validation Loss: 0.0036254897377838396\n",
      "Epoch [11709/20000], Training Loss: 0.007164772643591277, Validation Loss: 0.013367571538576126\n",
      "Epoch [11710/20000], Training Loss: 0.024448187907962295, Validation Loss: 0.0043426573626446086\n",
      "Epoch [11711/20000], Training Loss: 0.03842562649332774, Validation Loss: 0.0180683468173746\n",
      "Epoch [11712/20000], Training Loss: 0.011515756100249876, Validation Loss: 0.0058035406803307\n",
      "Epoch [11713/20000], Training Loss: 0.012725193782119146, Validation Loss: 0.00393927995885081\n",
      "Epoch [11714/20000], Training Loss: 0.00721098400754272, Validation Loss: 0.011794817720361348\n",
      "Epoch [11715/20000], Training Loss: 0.011267705187784876, Validation Loss: 0.004432297704129594\n",
      "Epoch [11716/20000], Training Loss: 0.005736884619961659, Validation Loss: 0.008952910869792012\n",
      "Epoch [11717/20000], Training Loss: 0.0068790823300202775, Validation Loss: 0.004228978702700813\n",
      "Epoch [11718/20000], Training Loss: 0.007184291609779133, Validation Loss: 0.004143391486260433\n",
      "Epoch [11719/20000], Training Loss: 0.00772910344052694, Validation Loss: 0.0036376925330249116\n",
      "Epoch [11720/20000], Training Loss: 0.015796075770465125, Validation Loss: 0.003650937107599767\n",
      "Epoch [11721/20000], Training Loss: 0.007776642010347652, Validation Loss: 0.005766101661546738\n",
      "Epoch [11722/20000], Training Loss: 0.00611077547988056, Validation Loss: 0.008834783490133955\n",
      "Epoch [11723/20000], Training Loss: 0.008363894597485861, Validation Loss: 0.0069483505238281396\n",
      "Epoch [11724/20000], Training Loss: 0.007850979928792055, Validation Loss: 0.009629693283879557\n",
      "Epoch [11725/20000], Training Loss: 0.005680236410366238, Validation Loss: 0.002889049821354612\n",
      "Epoch [11726/20000], Training Loss: 0.005442424065092512, Validation Loss: 0.0028497919970515034\n",
      "Epoch [11727/20000], Training Loss: 0.004783979750105313, Validation Loss: 0.002586765922410044\n",
      "Epoch [11728/20000], Training Loss: 0.005004133093669745, Validation Loss: 0.0028167291701812758\n",
      "Epoch [11729/20000], Training Loss: 0.004215739376377314, Validation Loss: 0.0035359139434447684\n",
      "Epoch [11730/20000], Training Loss: 0.0070029682163814345, Validation Loss: 0.0064870734444259105\n",
      "Epoch [11731/20000], Training Loss: 0.009112819852102152, Validation Loss: 0.01791729037602912\n",
      "Epoch [11732/20000], Training Loss: 0.013083129641017877, Validation Loss: 0.005571233098702777\n",
      "Epoch [11733/20000], Training Loss: 0.007629016529920461, Validation Loss: 0.012433562733109702\n",
      "Epoch [11734/20000], Training Loss: 0.006618642354234388, Validation Loss: 0.0034348303480707465\n",
      "Epoch [11735/20000], Training Loss: 0.00860406592931171, Validation Loss: 0.003649740828646791\n",
      "Epoch [11736/20000], Training Loss: 0.009330385607626113, Validation Loss: 0.010121619611341667\n",
      "Epoch [11737/20000], Training Loss: 0.006173045339112703, Validation Loss: 0.0037151034420880463\n",
      "Epoch [11738/20000], Training Loss: 0.0053076896402899365, Validation Loss: 0.012309116965574114\n",
      "Epoch [11739/20000], Training Loss: 0.020037021683362712, Validation Loss: 0.008301360030316118\n",
      "Epoch [11740/20000], Training Loss: 0.011424802606646673, Validation Loss: 0.006332937426189582\n",
      "Epoch [11741/20000], Training Loss: 0.006486249256080815, Validation Loss: 0.004650960414864157\n",
      "Epoch [11742/20000], Training Loss: 0.0049284102682958875, Validation Loss: 0.003272816902068852\n",
      "Epoch [11743/20000], Training Loss: 0.010098748981233387, Validation Loss: 0.0037835820272143572\n",
      "Epoch [11744/20000], Training Loss: 0.015596498196828179, Validation Loss: 0.005032260242357621\n",
      "Epoch [11745/20000], Training Loss: 0.007168205389461946, Validation Loss: 0.0026713998381597192\n",
      "Epoch [11746/20000], Training Loss: 0.005933007691835103, Validation Loss: 0.003165726166639412\n",
      "Epoch [11747/20000], Training Loss: 0.013228368001624144, Validation Loss: 0.004638749840624338\n",
      "Epoch [11748/20000], Training Loss: 0.021043918299255893, Validation Loss: 0.004688911421076065\n",
      "Epoch [11749/20000], Training Loss: 0.013731013160785161, Validation Loss: 0.00883531927559218\n",
      "Epoch [11750/20000], Training Loss: 0.011818736948563517, Validation Loss: 0.005618850185025741\n",
      "Epoch [11751/20000], Training Loss: 0.016329404586161087, Validation Loss: 0.009870390955247872\n",
      "Epoch [11752/20000], Training Loss: 0.00946516020589375, Validation Loss: 0.0058319132653911355\n",
      "Epoch [11753/20000], Training Loss: 0.013374940122178356, Validation Loss: 0.010918936605744787\n",
      "Epoch [11754/20000], Training Loss: 0.026684140016316502, Validation Loss: 0.010141567040719572\n",
      "Epoch [11755/20000], Training Loss: 0.007768368383819636, Validation Loss: 0.004753448306803624\n",
      "Epoch [11756/20000], Training Loss: 0.009338753422655697, Validation Loss: 0.008608981040550492\n",
      "Epoch [11757/20000], Training Loss: 0.006369402904965682, Validation Loss: 0.008448216030937974\n",
      "Epoch [11758/20000], Training Loss: 0.005276463212794624, Validation Loss: 0.0036288185636306836\n",
      "Epoch [11759/20000], Training Loss: 0.005437129642814398, Validation Loss: 0.006416472229891009\n",
      "Epoch [11760/20000], Training Loss: 0.005896200183867352, Validation Loss: 0.011705724664703019\n",
      "Epoch [11761/20000], Training Loss: 0.005310519806308938, Validation Loss: 0.0038079228456352\n",
      "Epoch [11762/20000], Training Loss: 0.006387793767706691, Validation Loss: 0.028904761026881585\n",
      "Epoch [11763/20000], Training Loss: 0.019921933591311763, Validation Loss: 0.008733494468087038\n",
      "Epoch [11764/20000], Training Loss: 0.020665726050667996, Validation Loss: 0.06656508383297291\n",
      "Epoch [11765/20000], Training Loss: 0.01714559181324368, Validation Loss: 0.007178623183979279\n",
      "Epoch [11766/20000], Training Loss: 0.01783104441398921, Validation Loss: 0.01028595101477767\n",
      "Epoch [11767/20000], Training Loss: 0.006898540881853218, Validation Loss: 0.005501973400989917\n",
      "Epoch [11768/20000], Training Loss: 0.005005392771896108, Validation Loss: 0.0036594848954994825\n",
      "Epoch [11769/20000], Training Loss: 0.006535079007984937, Validation Loss: 0.004459706142477212\n",
      "Epoch [11770/20000], Training Loss: 0.0080582885410487, Validation Loss: 0.013077109588937859\n",
      "Epoch [11771/20000], Training Loss: 0.009163581264406926, Validation Loss: 0.004901880955714454\n",
      "Epoch [11772/20000], Training Loss: 0.014315926881993488, Validation Loss: 0.004280705958200086\n",
      "Epoch [11773/20000], Training Loss: 0.0069017768338588735, Validation Loss: 0.0068743063636783775\n",
      "Epoch [11774/20000], Training Loss: 0.007187787602431074, Validation Loss: 0.004202665743030869\n",
      "Epoch [11775/20000], Training Loss: 0.017491014102469244, Validation Loss: 0.0024076088291228154\n",
      "Epoch [11776/20000], Training Loss: 0.09789229415244856, Validation Loss: 0.018832992169222962\n",
      "Epoch [11777/20000], Training Loss: 0.09739970939075906, Validation Loss: 0.1901069120019656\n",
      "Epoch [11778/20000], Training Loss: 0.061274328152649105, Validation Loss: 0.07663385652267607\n",
      "Epoch [11779/20000], Training Loss: 0.030489092422482957, Validation Loss: 0.017688513150519367\n",
      "Epoch [11780/20000], Training Loss: 0.013239157943254603, Validation Loss: 0.011554678636409053\n",
      "Epoch [11781/20000], Training Loss: 0.010288850637152791, Validation Loss: 0.008675957131539949\n",
      "Epoch [11782/20000], Training Loss: 0.01009041745315439, Validation Loss: 0.0065273136947975475\n",
      "Epoch [11783/20000], Training Loss: 0.013255519035737962, Validation Loss: 0.006641348269965874\n",
      "Epoch [11784/20000], Training Loss: 0.0075359021804095915, Validation Loss: 0.005672742358866246\n",
      "Epoch [11785/20000], Training Loss: 0.006276116336396497, Validation Loss: 0.0051183308314633235\n",
      "Epoch [11786/20000], Training Loss: 0.005720135198706495, Validation Loss: 0.004653673119397604\n",
      "Epoch [11787/20000], Training Loss: 0.008446313097790283, Validation Loss: 0.009522760399212096\n",
      "Epoch [11788/20000], Training Loss: 0.007345574739344686, Validation Loss: 0.011305835160064908\n",
      "Epoch [11789/20000], Training Loss: 0.010154667829088535, Validation Loss: 0.004704894542673675\n",
      "Epoch [11790/20000], Training Loss: 0.006248321665875015, Validation Loss: 0.010592739032771945\n",
      "Epoch [11791/20000], Training Loss: 0.011278721699324836, Validation Loss: 0.005273344472964903\n",
      "Epoch [11792/20000], Training Loss: 0.009960068614288633, Validation Loss: 0.006901841082023\n",
      "Epoch [11793/20000], Training Loss: 0.009554930821260703, Validation Loss: 0.007935768001631366\n",
      "Epoch [11794/20000], Training Loss: 0.005428978920852445, Validation Loss: 0.006025903322325158\n",
      "Epoch [11795/20000], Training Loss: 0.005325430451193824, Validation Loss: 0.003503504907816288\n",
      "Epoch [11796/20000], Training Loss: 0.008510921831462579, Validation Loss: 0.09934844092703646\n",
      "Epoch [11797/20000], Training Loss: 0.04796832134057435, Validation Loss: 0.02693616653264311\n",
      "Epoch [11798/20000], Training Loss: 0.03816365309571016, Validation Loss: 0.06380842413221087\n",
      "Epoch [11799/20000], Training Loss: 0.03618837189813868, Validation Loss: 0.04210527367656198\n",
      "Epoch [11800/20000], Training Loss: 0.023513878870289773, Validation Loss: 0.012291238580138777\n",
      "Epoch [11801/20000], Training Loss: 0.012157149076561577, Validation Loss: 0.006139484417027623\n",
      "Epoch [11802/20000], Training Loss: 0.010344637300087405, Validation Loss: 0.0103034848146178\n",
      "Epoch [11803/20000], Training Loss: 0.012445889371779881, Validation Loss: 0.009759426511833067\n",
      "Epoch [11804/20000], Training Loss: 0.007527173139221434, Validation Loss: 0.0050325614211682735\n",
      "Epoch [11805/20000], Training Loss: 0.007039706721635801, Validation Loss: 0.010745490395536501\n",
      "Epoch [11806/20000], Training Loss: 0.010156198747738796, Validation Loss: 0.00498678901849254\n",
      "Epoch [11807/20000], Training Loss: 0.008047966226253525, Validation Loss: 0.009663683290000336\n",
      "Epoch [11808/20000], Training Loss: 0.008054999909031071, Validation Loss: 0.009662920201729415\n",
      "Epoch [11809/20000], Training Loss: 0.007306368624475519, Validation Loss: 0.00737431817475961\n",
      "Epoch [11810/20000], Training Loss: 0.007788518903128404, Validation Loss: 0.005176837560347687\n",
      "Epoch [11811/20000], Training Loss: 0.005419145492071818, Validation Loss: 0.005472431221640254\n",
      "Epoch [11812/20000], Training Loss: 0.006406308477640518, Validation Loss: 0.01746324268918085\n",
      "Epoch [11813/20000], Training Loss: 0.013401969120682875, Validation Loss: 0.004156058317270046\n",
      "Epoch [11814/20000], Training Loss: 0.012054390614918833, Validation Loss: 0.006672816131268989\n",
      "Epoch [11815/20000], Training Loss: 0.008265643591390212, Validation Loss: 0.008357225913346469\n",
      "Epoch [11816/20000], Training Loss: 0.006912401851877803, Validation Loss: 0.03918120045701861\n",
      "Epoch [11817/20000], Training Loss: 0.014620337311693479, Validation Loss: 0.004147011440157452\n",
      "Epoch [11818/20000], Training Loss: 0.013730694954853018, Validation Loss: 0.01853313836755229\n",
      "Epoch [11819/20000], Training Loss: 0.007735474336576382, Validation Loss: 0.003910325496605083\n",
      "Epoch [11820/20000], Training Loss: 0.009428840837894572, Validation Loss: 0.00317303707854474\n",
      "Epoch [11821/20000], Training Loss: 0.00973303102780067, Validation Loss: 0.008357884171873917\n",
      "Epoch [11822/20000], Training Loss: 0.01725485321575044, Validation Loss: 0.019428128433316618\n",
      "Epoch [11823/20000], Training Loss: 0.02347769367042929, Validation Loss: 0.011925978811112665\n",
      "Epoch [11824/20000], Training Loss: 0.014983310867267261, Validation Loss: 0.004850894578789257\n",
      "Epoch [11825/20000], Training Loss: 0.0059234180942959415, Validation Loss: 0.004564140500702418\n",
      "Epoch [11826/20000], Training Loss: 0.006314982040618945, Validation Loss: 0.006067522305134584\n",
      "Epoch [11827/20000], Training Loss: 0.00730644332101552, Validation Loss: 0.006125745724749062\n",
      "Epoch [11828/20000], Training Loss: 0.0055530545100087435, Validation Loss: 0.020856936861592037\n",
      "Epoch [11829/20000], Training Loss: 0.008863539516044381, Validation Loss: 0.0032557588361186163\n",
      "Epoch [11830/20000], Training Loss: 0.005276343697914854, Validation Loss: 0.004026920280417861\n",
      "Epoch [11831/20000], Training Loss: 0.0050853638190996465, Validation Loss: 0.003627256748196877\n",
      "Epoch [11832/20000], Training Loss: 0.004683776544490164, Validation Loss: 0.005918134744612423\n",
      "Epoch [11833/20000], Training Loss: 0.0056094304828937834, Validation Loss: 0.003256145689951901\n",
      "Epoch [11834/20000], Training Loss: 0.0034671121476484196, Validation Loss: 0.0031805676109176568\n",
      "Epoch [11835/20000], Training Loss: 0.004542810441177737, Validation Loss: 0.004617620066134301\n",
      "Epoch [11836/20000], Training Loss: 0.008883474450807885, Validation Loss: 0.002932035576555287\n",
      "Epoch [11837/20000], Training Loss: 0.007252392297752002, Validation Loss: 0.0036611379491680346\n",
      "Epoch [11838/20000], Training Loss: 0.00331792844164411, Validation Loss: 0.006050139773545159\n",
      "Epoch [11839/20000], Training Loss: 0.010558529311050993, Validation Loss: 0.009747847988785622\n",
      "Epoch [11840/20000], Training Loss: 0.011978793221455166, Validation Loss: 0.0051101926707845125\n",
      "Epoch [11841/20000], Training Loss: 0.01192405525502441, Validation Loss: 0.0036979781277572477\n",
      "Epoch [11842/20000], Training Loss: 0.007932885695481673, Validation Loss: 0.00394946018189668\n",
      "Epoch [11843/20000], Training Loss: 0.018455960172494606, Validation Loss: 0.006894071652346108\n",
      "Epoch [11844/20000], Training Loss: 0.009309793042623329, Validation Loss: 0.012322178649683593\n",
      "Epoch [11845/20000], Training Loss: 0.012399073354380365, Validation Loss: 0.009606693905295418\n",
      "Epoch [11846/20000], Training Loss: 0.030235398634561404, Validation Loss: 0.05267005733081741\n",
      "Epoch [11847/20000], Training Loss: 0.051715886985350935, Validation Loss: 0.1391960084438324\n",
      "Epoch [11848/20000], Training Loss: 0.042923080551970215, Validation Loss: 0.011922542954965008\n",
      "Epoch [11849/20000], Training Loss: 0.023813591904139945, Validation Loss: 0.019195885090312004\n",
      "Epoch [11850/20000], Training Loss: 0.011184593342477456, Validation Loss: 0.006701091212237641\n",
      "Epoch [11851/20000], Training Loss: 0.008108942503375667, Validation Loss: 0.005369243418426452\n",
      "Epoch [11852/20000], Training Loss: 0.008683100419667815, Validation Loss: 0.0049229421669971475\n",
      "Epoch [11853/20000], Training Loss: 0.006378988086778138, Validation Loss: 0.004842293394075016\n",
      "Epoch [11854/20000], Training Loss: 0.007586984350512337, Validation Loss: 0.015660865596626716\n",
      "Epoch [11855/20000], Training Loss: 0.009415419741726185, Validation Loss: 0.004905816870454137\n",
      "Epoch [11856/20000], Training Loss: 0.006556915738786172, Validation Loss: 0.00423396930827989\n",
      "Epoch [11857/20000], Training Loss: 0.00505081878717257, Validation Loss: 0.003963029584620058\n",
      "Epoch [11858/20000], Training Loss: 0.005808186051061577, Validation Loss: 0.003942849413711512\n",
      "Epoch [11859/20000], Training Loss: 0.00639403304064347, Validation Loss: 0.0050779763698471925\n",
      "Epoch [11860/20000], Training Loss: 0.008003399376840597, Validation Loss: 0.0138345312902936\n",
      "Epoch [11861/20000], Training Loss: 0.007140183797933527, Validation Loss: 0.006651061122624924\n",
      "Epoch [11862/20000], Training Loss: 0.005170492725613128, Validation Loss: 0.005165903349326888\n",
      "Epoch [11863/20000], Training Loss: 0.012483530380580175, Validation Loss: 0.009599848412688466\n",
      "Epoch [11864/20000], Training Loss: 0.010035048617282882, Validation Loss: 0.04102904934968267\n",
      "Epoch [11865/20000], Training Loss: 0.023795774066456943, Validation Loss: 0.01992412463892582\n",
      "Epoch [11866/20000], Training Loss: 0.01821448085476212, Validation Loss: 0.014165738014229743\n",
      "Epoch [11867/20000], Training Loss: 0.006511776703908774, Validation Loss: 0.004684124269438682\n",
      "Epoch [11868/20000], Training Loss: 0.004690426981791721, Validation Loss: 0.004869069953212822\n",
      "Epoch [11869/20000], Training Loss: 0.005335474362046304, Validation Loss: 0.004844491259116139\n",
      "Epoch [11870/20000], Training Loss: 0.010058725545344973, Validation Loss: 0.004533490603290399\n",
      "Epoch [11871/20000], Training Loss: 0.0061757998274905345, Validation Loss: 0.005047551841247547\n",
      "Epoch [11872/20000], Training Loss: 0.007097428026359661, Validation Loss: 0.012385037866414952\n",
      "Epoch [11873/20000], Training Loss: 0.00779804920395171, Validation Loss: 0.0036561467083109683\n",
      "Epoch [11874/20000], Training Loss: 0.006910601605860782, Validation Loss: 0.007801959528818665\n",
      "Epoch [11875/20000], Training Loss: 0.015403368532653465, Validation Loss: 0.003686130757782077\n",
      "Epoch [11876/20000], Training Loss: 0.020988923567659055, Validation Loss: 0.006853772812456376\n",
      "Epoch [11877/20000], Training Loss: 0.026328689585040723, Validation Loss: 0.010517393773324541\n",
      "Epoch [11878/20000], Training Loss: 0.006867897054544666, Validation Loss: 0.00957661219123843\n",
      "Epoch [11879/20000], Training Loss: 0.005964139397552312, Validation Loss: 0.007620034975957424\n",
      "Epoch [11880/20000], Training Loss: 0.0091750277081571, Validation Loss: 0.00472658155711977\n",
      "Epoch [11881/20000], Training Loss: 0.011666039788906346, Validation Loss: 0.01778384702736527\n",
      "Epoch [11882/20000], Training Loss: 0.008338996926405733, Validation Loss: 0.007269069083017657\n",
      "Epoch [11883/20000], Training Loss: 0.008677999667790053, Validation Loss: 0.004292943646455731\n",
      "Epoch [11884/20000], Training Loss: 0.0044103699536728, Validation Loss: 0.0034909608707207923\n",
      "Epoch [11885/20000], Training Loss: 0.004949024056259077, Validation Loss: 0.011973324234753197\n",
      "Epoch [11886/20000], Training Loss: 0.009498094749038241, Validation Loss: 0.003876809073480964\n",
      "Epoch [11887/20000], Training Loss: 0.0034724291889421466, Validation Loss: 0.01078342114454205\n",
      "Epoch [11888/20000], Training Loss: 0.006908774454911638, Validation Loss: 0.0029652068323821217\n",
      "Epoch [11889/20000], Training Loss: 0.011433023423056252, Validation Loss: 0.00978192076080932\n",
      "Epoch [11890/20000], Training Loss: 0.015372351091043259, Validation Loss: 0.030080374062687123\n",
      "Epoch [11891/20000], Training Loss: 0.019614426414461508, Validation Loss: 0.01249732195857892\n",
      "Epoch [11892/20000], Training Loss: 0.009989985439460725, Validation Loss: 0.011693034387630797\n",
      "Epoch [11893/20000], Training Loss: 0.009159567506555635, Validation Loss: 0.0034124894274841617\n",
      "Epoch [11894/20000], Training Loss: 0.0070946134131060845, Validation Loss: 0.002674077575764449\n",
      "Epoch [11895/20000], Training Loss: 0.0043222534794559965, Validation Loss: 0.002865378229373177\n",
      "Epoch [11896/20000], Training Loss: 0.004040766303820728, Validation Loss: 0.002653833911464248\n",
      "Epoch [11897/20000], Training Loss: 0.003295272030559967, Validation Loss: 0.00240153533909439\n",
      "Epoch [11898/20000], Training Loss: 0.006079565247293743, Validation Loss: 0.022063277661801457\n",
      "Epoch [11899/20000], Training Loss: 0.008670971101050131, Validation Loss: 0.005401751370453652\n",
      "Epoch [11900/20000], Training Loss: 0.005989408977516827, Validation Loss: 0.004834997946521311\n",
      "Epoch [11901/20000], Training Loss: 0.004472716122173941, Validation Loss: 0.008858969343843828\n",
      "Epoch [11902/20000], Training Loss: 0.005659299643801725, Validation Loss: 0.004311378503718694\n",
      "Epoch [11903/20000], Training Loss: 0.011458930562574616, Validation Loss: 0.004095524316380812\n",
      "Epoch [11904/20000], Training Loss: 0.024546468640827306, Validation Loss: 0.0047909947402148744\n",
      "Epoch [11905/20000], Training Loss: 0.019888400722785655, Validation Loss: 0.01770243305879506\n",
      "Epoch [11906/20000], Training Loss: 0.012945159138845546, Validation Loss: 0.007109657461119452\n",
      "Epoch [11907/20000], Training Loss: 0.007762061343696709, Validation Loss: 0.008409816741607304\n",
      "Epoch [11908/20000], Training Loss: 0.00808012028085032, Validation Loss: 0.004349804059194083\n",
      "Epoch [11909/20000], Training Loss: 0.004764248149253295, Validation Loss: 0.003127747505427242\n",
      "Epoch [11910/20000], Training Loss: 0.00434462631970486, Validation Loss: 0.0033798834617763417\n",
      "Epoch [11911/20000], Training Loss: 0.004982345240022263, Validation Loss: 0.003102735777149725\n",
      "Epoch [11912/20000], Training Loss: 0.006881395128272873, Validation Loss: 0.007285753998435908\n",
      "Epoch [11913/20000], Training Loss: 0.006401097264772814, Validation Loss: 0.004223326388205836\n",
      "Epoch [11914/20000], Training Loss: 0.014056743073784414, Validation Loss: 0.01196677636887776\n",
      "Epoch [11915/20000], Training Loss: 0.012575996027278182, Validation Loss: 0.028193741549801792\n",
      "Epoch [11916/20000], Training Loss: 0.007058424308606196, Validation Loss: 0.005592641555257966\n",
      "Epoch [11917/20000], Training Loss: 0.004670547423627015, Validation Loss: 0.019886943941147477\n",
      "Epoch [11918/20000], Training Loss: 0.008166284226328051, Validation Loss: 0.0026066571160316726\n",
      "Epoch [11919/20000], Training Loss: 0.003790318210251696, Validation Loss: 0.013503633838680609\n",
      "Epoch [11920/20000], Training Loss: 0.015502203591625272, Validation Loss: 0.005981679972949766\n",
      "Epoch [11921/20000], Training Loss: 0.04500494120309538, Validation Loss: 0.08539572730584918\n",
      "Epoch [11922/20000], Training Loss: 0.03665784348792554, Validation Loss: 0.012077077275869439\n",
      "Epoch [11923/20000], Training Loss: 0.02395330510418197, Validation Loss: 0.0041105978601747905\n",
      "Epoch [11924/20000], Training Loss: 0.00624790707817218, Validation Loss: 0.007153642558949506\n",
      "Epoch [11925/20000], Training Loss: 0.017834853391312726, Validation Loss: 0.005309498437750161\n",
      "Epoch [11926/20000], Training Loss: 0.012093604209699802, Validation Loss: 0.01712047601386404\n",
      "Epoch [11927/20000], Training Loss: 0.007849818868895195, Validation Loss: 0.0070519340084729265\n",
      "Epoch [11928/20000], Training Loss: 0.005480975627766124, Validation Loss: 0.00704323655472454\n",
      "Epoch [11929/20000], Training Loss: 0.007558813992155982, Validation Loss: 0.0040128834518072965\n",
      "Epoch [11930/20000], Training Loss: 0.014489430144471729, Validation Loss: 0.0042151268951928144\n",
      "Epoch [11931/20000], Training Loss: 0.01281817168013991, Validation Loss: 0.0036318394958375472\n",
      "Epoch [11932/20000], Training Loss: 0.006716251119671922, Validation Loss: 0.0098083806099528\n",
      "Epoch [11933/20000], Training Loss: 0.009248791943329187, Validation Loss: 0.0066982224562883025\n",
      "Epoch [11934/20000], Training Loss: 0.006944194557166027, Validation Loss: 0.003585111952431116\n",
      "Epoch [11935/20000], Training Loss: 0.005122670705921857, Validation Loss: 0.004575346184401506\n",
      "Epoch [11936/20000], Training Loss: 0.007537024313933216, Validation Loss: 0.0035345939733647685\n",
      "Epoch [11937/20000], Training Loss: 0.009600167392428765, Validation Loss: 0.0066076088629764785\n",
      "Epoch [11938/20000], Training Loss: 0.010588286099456517, Validation Loss: 0.006372467436466416\n",
      "Epoch [11939/20000], Training Loss: 0.010503158459739228, Validation Loss: 0.010382396504397333\n",
      "Epoch [11940/20000], Training Loss: 0.021488834675041955, Validation Loss: 0.0061532204860960905\n",
      "Epoch [11941/20000], Training Loss: 0.007434292097709009, Validation Loss: 0.0037821137617266775\n",
      "Epoch [11942/20000], Training Loss: 0.004860705602498326, Validation Loss: 0.002989048244439526\n",
      "Epoch [11943/20000], Training Loss: 0.006909191584418003, Validation Loss: 0.00269336728799214\n",
      "Epoch [11944/20000], Training Loss: 0.003742696340071104, Validation Loss: 0.0026305147173348864\n",
      "Epoch [11945/20000], Training Loss: 0.005096570958617771, Validation Loss: 0.006871289908160381\n",
      "Epoch [11946/20000], Training Loss: 0.008158364089987507, Validation Loss: 0.0030329826999315112\n",
      "Epoch [11947/20000], Training Loss: 0.00901476054942967, Validation Loss: 0.018163264719273035\n",
      "Epoch [11948/20000], Training Loss: 0.02123816194067071, Validation Loss: 0.009508630337461252\n",
      "Epoch [11949/20000], Training Loss: 0.008259789408303473, Validation Loss: 0.00754078265366423\n",
      "Epoch [11950/20000], Training Loss: 0.00682145099266688, Validation Loss: 0.006775476339764614\n",
      "Epoch [11951/20000], Training Loss: 0.010050047758600808, Validation Loss: 0.005876485384950739\n",
      "Epoch [11952/20000], Training Loss: 0.006522656670962793, Validation Loss: 0.005796594749628593\n",
      "Epoch [11953/20000], Training Loss: 0.008872295195130522, Validation Loss: 0.00894310512087915\n",
      "Epoch [11954/20000], Training Loss: 0.009741389748731828, Validation Loss: 0.004597001823154151\n",
      "Epoch [11955/20000], Training Loss: 0.007045197267871117, Validation Loss: 0.003774430049821686\n",
      "Epoch [11956/20000], Training Loss: 0.006169028994073804, Validation Loss: 0.005445570027343589\n",
      "Epoch [11957/20000], Training Loss: 0.004487516691109964, Validation Loss: 0.006459535954674475\n",
      "Epoch [11958/20000], Training Loss: 0.007211582940466802, Validation Loss: 0.006922212570806907\n",
      "Epoch [11959/20000], Training Loss: 0.012043272054662728, Validation Loss: 0.013995718221988498\n",
      "Epoch [11960/20000], Training Loss: 0.019518630765950156, Validation Loss: 0.006599712519948709\n",
      "Epoch [11961/20000], Training Loss: 0.02046783917103312, Validation Loss: 0.08145582675933845\n",
      "Epoch [11962/20000], Training Loss: 0.08155917187520702, Validation Loss: 0.1295972217835454\n",
      "Epoch [11963/20000], Training Loss: 0.07978614419698715, Validation Loss: 0.028206922323858765\n",
      "Epoch [11964/20000], Training Loss: 0.02938788808283529, Validation Loss: 0.01489949633833021\n",
      "Epoch [11965/20000], Training Loss: 0.0195464718355132, Validation Loss: 0.01523152510916199\n",
      "Epoch [11966/20000], Training Loss: 0.013986164421242262, Validation Loss: 0.01623735838698589\n",
      "Epoch [11967/20000], Training Loss: 0.016138170155630047, Validation Loss: 0.010690037458386672\n",
      "Epoch [11968/20000], Training Loss: 0.010896125849935092, Validation Loss: 0.00937056049016454\n",
      "Epoch [11969/20000], Training Loss: 0.01043754565346587, Validation Loss: 0.0075128809036998844\n",
      "Epoch [11970/20000], Training Loss: 0.007301516481675208, Validation Loss: 0.008831839104686878\n",
      "Epoch [11971/20000], Training Loss: 0.006806454569284272, Validation Loss: 0.006224750390889052\n",
      "Epoch [11972/20000], Training Loss: 0.0062679136587706, Validation Loss: 0.006373976892064093\n",
      "Epoch [11973/20000], Training Loss: 0.005468324471231816, Validation Loss: 0.005662598738230226\n",
      "Epoch [11974/20000], Training Loss: 0.0114797257891094, Validation Loss: 0.0037542169366296158\n",
      "Epoch [11975/20000], Training Loss: 0.022303380580654317, Validation Loss: 0.09550717686111707\n",
      "Epoch [11976/20000], Training Loss: 0.07140688304949501, Validation Loss: 0.04870262788504728\n",
      "Epoch [11977/20000], Training Loss: 0.034764383121260574, Validation Loss: 0.010116445710025346\n",
      "Epoch [11978/20000], Training Loss: 0.009881738190805274, Validation Loss: 0.008591627388568668\n",
      "Epoch [11979/20000], Training Loss: 0.006742971736197693, Validation Loss: 0.006502047058802418\n",
      "Epoch [11980/20000], Training Loss: 0.008068653605213123, Validation Loss: 0.005599934526019622\n",
      "Epoch [11981/20000], Training Loss: 0.00790062457287734, Validation Loss: 0.007049885510631222\n",
      "Epoch [11982/20000], Training Loss: 0.009783221446143995, Validation Loss: 0.005274466262465337\n",
      "Epoch [11983/20000], Training Loss: 0.006186472437808074, Validation Loss: 0.005138197787086288\n",
      "Epoch [11984/20000], Training Loss: 0.007317369009667475, Validation Loss: 0.004457530553151757\n",
      "Epoch [11985/20000], Training Loss: 0.005788611103006198, Validation Loss: 0.006547631612014422\n",
      "Epoch [11986/20000], Training Loss: 0.007845398341877237, Validation Loss: 0.008107603876084113\n",
      "Epoch [11987/20000], Training Loss: 0.008272359755210346, Validation Loss: 0.003265402462212868\n",
      "Epoch [11988/20000], Training Loss: 0.007399533765107792, Validation Loss: 0.007730275595999306\n",
      "Epoch [11989/20000], Training Loss: 0.016679661178256668, Validation Loss: 0.005500814496112331\n",
      "Epoch [11990/20000], Training Loss: 0.0062103035265213945, Validation Loss: 0.0040246257222764014\n",
      "Epoch [11991/20000], Training Loss: 0.004163009783951566, Validation Loss: 0.004096007568315534\n",
      "Epoch [11992/20000], Training Loss: 0.006912419271040042, Validation Loss: 0.005431410877382145\n",
      "Epoch [11993/20000], Training Loss: 0.0036575356477572185, Validation Loss: 0.0029467135018259277\n",
      "Epoch [11994/20000], Training Loss: 0.0035301617754157633, Validation Loss: 0.0031693420832458025\n",
      "Epoch [11995/20000], Training Loss: 0.004087845117153067, Validation Loss: 0.003350237039512649\n",
      "Epoch [11996/20000], Training Loss: 0.00768166479122913, Validation Loss: 0.017599605311391184\n",
      "Epoch [11997/20000], Training Loss: 0.026983249558855147, Validation Loss: 0.052817777033072034\n",
      "Epoch [11998/20000], Training Loss: 0.06858017624426793, Validation Loss: 0.01545928417555485\n",
      "Epoch [11999/20000], Training Loss: 0.020752449145740166, Validation Loss: 0.007941051317743586\n",
      "Epoch [12000/20000], Training Loss: 0.016270322685678757, Validation Loss: 0.015954030149673924\n",
      "Epoch [12001/20000], Training Loss: 0.022180761233487698, Validation Loss: 0.016592651464041604\n",
      "Epoch [12002/20000], Training Loss: 0.011056142085830547, Validation Loss: 0.005587990307698679\n",
      "Epoch [12003/20000], Training Loss: 0.008741213636572607, Validation Loss: 0.009520267673727696\n",
      "Epoch [12004/20000], Training Loss: 0.007964612018050892, Validation Loss: 0.006341316375515557\n",
      "Epoch [12005/20000], Training Loss: 0.006013503645330535, Validation Loss: 0.005680809946982533\n",
      "Epoch [12006/20000], Training Loss: 0.005529761732239942, Validation Loss: 0.00574330292588497\n",
      "Epoch [12007/20000], Training Loss: 0.00709173967100339, Validation Loss: 0.004085716903659886\n",
      "Epoch [12008/20000], Training Loss: 0.012908771509371166, Validation Loss: 0.0036147967065452186\n",
      "Epoch [12009/20000], Training Loss: 0.005520095960361816, Validation Loss: 0.005251672615827374\n",
      "Epoch [12010/20000], Training Loss: 0.013737181893832582, Validation Loss: 0.003970706669724068\n",
      "Epoch [12011/20000], Training Loss: 0.012825119182317784, Validation Loss: 0.017069635185423118\n",
      "Epoch [12012/20000], Training Loss: 0.012081332191493337, Validation Loss: 0.0036409064370072236\n",
      "Epoch [12013/20000], Training Loss: 0.003302955744272497, Validation Loss: 0.0032740361059455025\n",
      "Epoch [12014/20000], Training Loss: 0.00771529489169812, Validation Loss: 0.0028734784958775856\n",
      "Epoch [12015/20000], Training Loss: 0.012920279832282435, Validation Loss: 0.023169966758048943\n",
      "Epoch [12016/20000], Training Loss: 0.010230855935723022, Validation Loss: 0.0362974384153864\n",
      "Epoch [12017/20000], Training Loss: 0.031931216341035906, Validation Loss: 0.04461067249733449\n",
      "Epoch [12018/20000], Training Loss: 0.05492027429422056, Validation Loss: 0.07227207552819223\n",
      "Epoch [12019/20000], Training Loss: 0.050687801176016886, Validation Loss: 0.022421995815031032\n",
      "Epoch [12020/20000], Training Loss: 0.012057550767037486, Validation Loss: 0.00751021606129143\n",
      "Epoch [12021/20000], Training Loss: 0.008841206091996614, Validation Loss: 0.0067965742031706865\n",
      "Epoch [12022/20000], Training Loss: 0.006175879397363003, Validation Loss: 0.007491668245263229\n",
      "Epoch [12023/20000], Training Loss: 0.005714138526595239, Validation Loss: 0.00495749182131736\n",
      "Epoch [12024/20000], Training Loss: 0.005777514882668454, Validation Loss: 0.0050744350735060706\n",
      "Epoch [12025/20000], Training Loss: 0.006953710016075222, Validation Loss: 0.012236364707697638\n",
      "Epoch [12026/20000], Training Loss: 0.011162618944321625, Validation Loss: 0.004847045451892232\n",
      "Epoch [12027/20000], Training Loss: 0.006244869415240828, Validation Loss: 0.0037632191147492578\n",
      "Epoch [12028/20000], Training Loss: 0.00505032403660672, Validation Loss: 0.009127591301222537\n",
      "Epoch [12029/20000], Training Loss: 0.006958237215128195, Validation Loss: 0.003924828301795255\n",
      "Epoch [12030/20000], Training Loss: 0.009534680355240457, Validation Loss: 0.016516481674963352\n",
      "Epoch [12031/20000], Training Loss: 0.016994960531259755, Validation Loss: 0.008785233971918518\n",
      "Epoch [12032/20000], Training Loss: 0.023910235256023173, Validation Loss: 0.014158337088831883\n",
      "Epoch [12033/20000], Training Loss: 0.018194269589002943, Validation Loss: 0.007772764701460996\n",
      "Epoch [12034/20000], Training Loss: 0.01718606538322222, Validation Loss: 0.015254187353148398\n",
      "Epoch [12035/20000], Training Loss: 0.007073338746392567, Validation Loss: 0.0038352520465585777\n",
      "Epoch [12036/20000], Training Loss: 0.007041817643247279, Validation Loss: 0.0035975530899869618\n",
      "Epoch [12037/20000], Training Loss: 0.009205314844556207, Validation Loss: 0.013949276938614983\n",
      "Epoch [12038/20000], Training Loss: 0.01644443082172334, Validation Loss: 0.018135075443903963\n",
      "Epoch [12039/20000], Training Loss: 0.03373210353415094, Validation Loss: 0.009056242125471467\n",
      "Epoch [12040/20000], Training Loss: 0.013442995253821468, Validation Loss: 0.03228904912196283\n",
      "Epoch [12041/20000], Training Loss: 0.013958194421970152, Validation Loss: 0.01350805157697518\n",
      "Epoch [12042/20000], Training Loss: 0.014099041883777577, Validation Loss: 0.008448669658975965\n",
      "Epoch [12043/20000], Training Loss: 0.010340177250327542, Validation Loss: 0.007506268414236469\n",
      "Epoch [12044/20000], Training Loss: 0.010706296867608347, Validation Loss: 0.00734838356090352\n",
      "Epoch [12045/20000], Training Loss: 0.006610658331607867, Validation Loss: 0.004937897624350691\n",
      "Epoch [12046/20000], Training Loss: 0.014234004152967827, Validation Loss: 0.007118694656842958\n",
      "Epoch [12047/20000], Training Loss: 0.007368146878434345, Validation Loss: 0.006547185271236751\n",
      "Epoch [12048/20000], Training Loss: 0.009123717940212828, Validation Loss: 0.009460303392807694\n",
      "Epoch [12049/20000], Training Loss: 0.009622314826369152, Validation Loss: 0.004021783723357787\n",
      "Epoch [12050/20000], Training Loss: 0.005569272383581847, Validation Loss: 0.0034247710770582023\n",
      "Epoch [12051/20000], Training Loss: 0.008272742226836272, Validation Loss: 0.005342014596603154\n",
      "Epoch [12052/20000], Training Loss: 0.010708387201053224, Validation Loss: 0.004002342814016758\n",
      "Epoch [12053/20000], Training Loss: 0.00740853954261362, Validation Loss: 0.006996884114693712\n",
      "Epoch [12054/20000], Training Loss: 0.00572568346771212, Validation Loss: 0.0035954938118517737\n",
      "Epoch [12055/20000], Training Loss: 0.009308414481763196, Validation Loss: 0.029899434230173938\n",
      "Epoch [12056/20000], Training Loss: 0.012761441918168462, Validation Loss: 0.005344527447355014\n",
      "Epoch [12057/20000], Training Loss: 0.006194786821392232, Validation Loss: 0.006259114828578406\n",
      "Epoch [12058/20000], Training Loss: 0.0031678502481164677, Validation Loss: 0.004863239915247318\n",
      "Epoch [12059/20000], Training Loss: 0.005934142682755399, Validation Loss: 0.002767674834192998\n",
      "Epoch [12060/20000], Training Loss: 0.008555933293142257, Validation Loss: 0.005103657175855655\n",
      "Epoch [12061/20000], Training Loss: 0.005412183120954849, Validation Loss: 0.016246015128879212\n",
      "Epoch [12062/20000], Training Loss: 0.012748973448261884, Validation Loss: 0.008108329315303666\n",
      "Epoch [12063/20000], Training Loss: 0.06653109117064636, Validation Loss: 0.008773323934267043\n",
      "Epoch [12064/20000], Training Loss: 0.0091551254247731, Validation Loss: 0.007073786449999326\n",
      "Epoch [12065/20000], Training Loss: 0.011907256883984831, Validation Loss: 0.005583633527878972\n",
      "Epoch [12066/20000], Training Loss: 0.01888371178730657, Validation Loss: 0.02124973919206598\n",
      "Epoch [12067/20000], Training Loss: 0.010748064396986072, Validation Loss: 0.006996513024205342\n",
      "Epoch [12068/20000], Training Loss: 0.005897505828085871, Validation Loss: 0.009496821103864152\n",
      "Epoch [12069/20000], Training Loss: 0.009837615578102745, Validation Loss: 0.0049455754193071955\n",
      "Epoch [12070/20000], Training Loss: 0.006933639850883212, Validation Loss: 0.006467877564326889\n",
      "Epoch [12071/20000], Training Loss: 0.012664462268211147, Validation Loss: 0.007538877989349301\n",
      "Epoch [12072/20000], Training Loss: 0.011702818626612757, Validation Loss: 0.004429259575512593\n",
      "Epoch [12073/20000], Training Loss: 0.005599957035363852, Validation Loss: 0.0036434884360850056\n",
      "Epoch [12074/20000], Training Loss: 0.007649314808077179, Validation Loss: 0.003515782610975293\n",
      "Epoch [12075/20000], Training Loss: 0.013125822301455108, Validation Loss: 0.004795320254856961\n",
      "Epoch [12076/20000], Training Loss: 0.006003042423149704, Validation Loss: 0.007461024446976288\n",
      "Epoch [12077/20000], Training Loss: 0.012232547935647224, Validation Loss: 0.007743719516914739\n",
      "Epoch [12078/20000], Training Loss: 0.013330563653393515, Validation Loss: 0.02061026695122235\n",
      "Epoch [12079/20000], Training Loss: 0.01261146186568242, Validation Loss: 0.021128107504275993\n",
      "Epoch [12080/20000], Training Loss: 0.023116565633764758, Validation Loss: 0.016511286880034146\n",
      "Epoch [12081/20000], Training Loss: 0.01807347010929204, Validation Loss: 0.0058641330341718655\n",
      "Epoch [12082/20000], Training Loss: 0.012171838650829159, Validation Loss: 0.005581478372862746\n",
      "Epoch [12083/20000], Training Loss: 0.018624147304113388, Validation Loss: 0.003759274403949218\n",
      "Epoch [12084/20000], Training Loss: 0.014638530761918187, Validation Loss: 0.005711388528652606\n",
      "Epoch [12085/20000], Training Loss: 0.005728511867346242, Validation Loss: 0.0042188779237579055\n",
      "Epoch [12086/20000], Training Loss: 0.004496611699413019, Validation Loss: 0.007264332403037445\n",
      "Epoch [12087/20000], Training Loss: 0.006785576366902595, Validation Loss: 0.006368857770732055\n",
      "Epoch [12088/20000], Training Loss: 0.005560806056434687, Validation Loss: 0.0048264401666691424\n",
      "Epoch [12089/20000], Training Loss: 0.00730704070052265, Validation Loss: 0.005422995583801692\n",
      "Epoch [12090/20000], Training Loss: 0.007995918917004019, Validation Loss: 0.006887310347403895\n",
      "Epoch [12091/20000], Training Loss: 0.007623260906557659, Validation Loss: 0.011206411247258075\n",
      "Epoch [12092/20000], Training Loss: 0.03185696418313455, Validation Loss: 0.0114528933692262\n",
      "Epoch [12093/20000], Training Loss: 0.0677329738557871, Validation Loss: 0.04935708134425444\n",
      "Epoch [12094/20000], Training Loss: 0.026734466616679647, Validation Loss: 0.0366095347170803\n",
      "Epoch [12095/20000], Training Loss: 0.0257460057951643, Validation Loss: 0.007479499055209869\n",
      "Epoch [12096/20000], Training Loss: 0.01713536158370386, Validation Loss: 0.0276093826729006\n",
      "Epoch [12097/20000], Training Loss: 0.018716875767235512, Validation Loss: 0.006858331253170036\n",
      "Epoch [12098/20000], Training Loss: 0.011848877799431128, Validation Loss: 0.01960510874113197\n",
      "Epoch [12099/20000], Training Loss: 0.02029096643673256, Validation Loss: 0.005722589934261383\n",
      "Epoch [12100/20000], Training Loss: 0.013805078149224366, Validation Loss: 0.024155105717779537\n",
      "Epoch [12101/20000], Training Loss: 0.011754521667691213, Validation Loss: 0.01107132202763442\n",
      "Epoch [12102/20000], Training Loss: 0.01042920324834995, Validation Loss: 0.004651232215435032\n",
      "Epoch [12103/20000], Training Loss: 0.02693721536031392, Validation Loss: 0.034985538095969\n",
      "Epoch [12104/20000], Training Loss: 0.023125527871473293, Validation Loss: 0.06203704970200974\n",
      "Epoch [12105/20000], Training Loss: 0.029890563605087146, Validation Loss: 0.012187009215787319\n",
      "Epoch [12106/20000], Training Loss: 0.016017919048733478, Validation Loss: 0.011338790972201553\n",
      "Epoch [12107/20000], Training Loss: 0.011541572698791112, Validation Loss: 0.008775497022335392\n",
      "Epoch [12108/20000], Training Loss: 0.009694879408925772, Validation Loss: 0.009355811787664996\n",
      "Epoch [12109/20000], Training Loss: 0.011872620200197812, Validation Loss: 0.005798834902764481\n",
      "Epoch [12110/20000], Training Loss: 0.006399358142516576, Validation Loss: 0.005195814216157097\n",
      "Epoch [12111/20000], Training Loss: 0.00519364994917331, Validation Loss: 0.005470854009217874\n",
      "Epoch [12112/20000], Training Loss: 0.007192191121483964, Validation Loss: 0.005269377653998423\n",
      "Epoch [12113/20000], Training Loss: 0.005756155583575102, Validation Loss: 0.006853433829576261\n",
      "Epoch [12114/20000], Training Loss: 0.007693278848559463, Validation Loss: 0.00468843114171185\n",
      "Epoch [12115/20000], Training Loss: 0.005589016595747255, Validation Loss: 0.004987896499439363\n",
      "Epoch [12116/20000], Training Loss: 0.004861667488057719, Validation Loss: 0.00847889470320037\n",
      "Epoch [12117/20000], Training Loss: 0.007521547210801925, Validation Loss: 0.003659886424949426\n",
      "Epoch [12118/20000], Training Loss: 0.006572668324224651, Validation Loss: 0.004159221300105693\n",
      "Epoch [12119/20000], Training Loss: 0.004197063286223316, Validation Loss: 0.003957170250243637\n",
      "Epoch [12120/20000], Training Loss: 0.004606004875053519, Validation Loss: 0.00645015833348087\n",
      "Epoch [12121/20000], Training Loss: 0.012794422552230702, Validation Loss: 0.004874952105384166\n",
      "Epoch [12122/20000], Training Loss: 0.009980286450757245, Validation Loss: 0.0042755251599828625\n",
      "Epoch [12123/20000], Training Loss: 0.010278825783253913, Validation Loss: 0.0039724615815950005\n",
      "Epoch [12124/20000], Training Loss: 0.00795583389325267, Validation Loss: 0.007611768481959912\n",
      "Epoch [12125/20000], Training Loss: 0.008755162965306746, Validation Loss: 0.013579256237140467\n",
      "Epoch [12126/20000], Training Loss: 0.007434556797047013, Validation Loss: 0.0031170226163794723\n",
      "Epoch [12127/20000], Training Loss: 0.003630748631881683, Validation Loss: 0.003063723583043806\n",
      "Epoch [12128/20000], Training Loss: 0.005036834085850777, Validation Loss: 0.002750239832262683\n",
      "Epoch [12129/20000], Training Loss: 0.003191011026371728, Validation Loss: 0.0029086807764875694\n",
      "Epoch [12130/20000], Training Loss: 0.007243286578159314, Validation Loss: 0.00535654240887976\n",
      "Epoch [12131/20000], Training Loss: 0.005233244691300508, Validation Loss: 0.0061719889789121195\n",
      "Epoch [12132/20000], Training Loss: 0.01111599897155559, Validation Loss: 0.014762528706973792\n",
      "Epoch [12133/20000], Training Loss: 0.009151138166868935, Validation Loss: 0.0030104776421921997\n",
      "Epoch [12134/20000], Training Loss: 0.009768719024577877, Validation Loss: 0.01022167248538608\n",
      "Epoch [12135/20000], Training Loss: 0.008147618767647405, Validation Loss: 0.00912328767041221\n",
      "Epoch [12136/20000], Training Loss: 0.007752319804110032, Validation Loss: 0.0037098872579325154\n",
      "Epoch [12137/20000], Training Loss: 0.005588486239893038, Validation Loss: 0.006177649451631494\n",
      "Epoch [12138/20000], Training Loss: 0.007678385196153873, Validation Loss: 0.0037996771410299906\n",
      "Epoch [12139/20000], Training Loss: 0.006065937728895473, Validation Loss: 0.00321925459597594\n",
      "Epoch [12140/20000], Training Loss: 0.008269159202297618, Validation Loss: 0.0024565339410855813\n",
      "Epoch [12141/20000], Training Loss: 0.006541089086178025, Validation Loss: 0.018297151890290513\n",
      "Epoch [12142/20000], Training Loss: 0.013530315731519036, Validation Loss: 0.01973417056397663\n",
      "Epoch [12143/20000], Training Loss: 0.017943698669634096, Validation Loss: 0.012037140493628076\n",
      "Epoch [12144/20000], Training Loss: 0.009423405270484051, Validation Loss: 0.0104027049984239\n",
      "Epoch [12145/20000], Training Loss: 0.005963101976833839, Validation Loss: 0.006335214947723346\n",
      "Epoch [12146/20000], Training Loss: 0.006703433444532233, Validation Loss: 0.004509241289152247\n",
      "Epoch [12147/20000], Training Loss: 0.007226987467480025, Validation Loss: 0.0036416303703106223\n",
      "Epoch [12148/20000], Training Loss: 0.00624309264925874, Validation Loss: 0.0034021331511106772\n",
      "Epoch [12149/20000], Training Loss: 0.008892030877177604, Validation Loss: 0.0024736829389959375\n",
      "Epoch [12150/20000], Training Loss: 0.005106459169058196, Validation Loss: 0.003358282101633644\n",
      "Epoch [12151/20000], Training Loss: 0.003640666911711118, Validation Loss: 0.003395919270773667\n",
      "Epoch [12152/20000], Training Loss: 0.00933707123210427, Validation Loss: 0.0028869430575874894\n",
      "Epoch [12153/20000], Training Loss: 0.009996410611035702, Validation Loss: 0.04884117415973273\n",
      "Epoch [12154/20000], Training Loss: 0.017924862648734625, Validation Loss: 0.005021364188554304\n",
      "Epoch [12155/20000], Training Loss: 0.010873269993636216, Validation Loss: 0.006534686386122043\n",
      "Epoch [12156/20000], Training Loss: 0.008781276512308978, Validation Loss: 0.007131254968985955\n",
      "Epoch [12157/20000], Training Loss: 0.015747681190467638, Validation Loss: 0.04541885214196172\n",
      "Epoch [12158/20000], Training Loss: 0.01598669585551501, Validation Loss: 0.021626691468576754\n",
      "Epoch [12159/20000], Training Loss: 0.015369374743646145, Validation Loss: 0.02788045594466634\n",
      "Epoch [12160/20000], Training Loss: 0.009454193656503256, Validation Loss: 0.005272572627319981\n",
      "Epoch [12161/20000], Training Loss: 0.009010994597775022, Validation Loss: 0.04065487001623551\n",
      "Epoch [12162/20000], Training Loss: 0.019464406740943168, Validation Loss: 0.014283463944473647\n",
      "Epoch [12163/20000], Training Loss: 0.010774902030659308, Validation Loss: 0.008249866930979828\n",
      "Epoch [12164/20000], Training Loss: 0.016561672725531804, Validation Loss: 0.004701941865261817\n",
      "Epoch [12165/20000], Training Loss: 0.009785259097692947, Validation Loss: 0.00524926453014737\n",
      "Epoch [12166/20000], Training Loss: 0.007304869302190907, Validation Loss: 0.0049938869823823295\n",
      "Epoch [12167/20000], Training Loss: 0.019081562173546958, Validation Loss: 0.016358706701020253\n",
      "Epoch [12168/20000], Training Loss: 0.0313556744042996, Validation Loss: 0.007170149311648595\n",
      "Epoch [12169/20000], Training Loss: 0.013156969337225226, Validation Loss: 0.020826269725928017\n",
      "Epoch [12170/20000], Training Loss: 0.008827208634881702, Validation Loss: 0.01464363373818897\n",
      "Epoch [12171/20000], Training Loss: 0.011982536152442793, Validation Loss: 0.006858799187252121\n",
      "Epoch [12172/20000], Training Loss: 0.008866896204251264, Validation Loss: 0.012881672009828449\n",
      "Epoch [12173/20000], Training Loss: 0.009365373199411156, Validation Loss: 0.026932723288025175\n",
      "Epoch [12174/20000], Training Loss: 0.01171224811665945, Validation Loss: 0.03633569264119225\n",
      "Epoch [12175/20000], Training Loss: 0.07714683278739796, Validation Loss: 0.0941684763529338\n",
      "Epoch [12176/20000], Training Loss: 0.03651062913870971, Validation Loss: 0.08084870833590685\n",
      "Epoch [12177/20000], Training Loss: 0.04301699543637889, Validation Loss: 0.028116582701581398\n",
      "Epoch [12178/20000], Training Loss: 0.015358930025416027, Validation Loss: 0.013260496250541473\n",
      "Epoch [12179/20000], Training Loss: 0.012410174816198247, Validation Loss: 0.014145875753442576\n",
      "Epoch [12180/20000], Training Loss: 0.010158309239029353, Validation Loss: 0.006024592571398963\n",
      "Epoch [12181/20000], Training Loss: 0.007551325155613345, Validation Loss: 0.009468173233621511\n",
      "Epoch [12182/20000], Training Loss: 0.008799056090148432, Validation Loss: 0.010035902128708488\n",
      "Epoch [12183/20000], Training Loss: 0.007296868213286091, Validation Loss: 0.005442377664104699\n",
      "Epoch [12184/20000], Training Loss: 0.008588120786108837, Validation Loss: 0.0048089424229129\n",
      "Epoch [12185/20000], Training Loss: 0.00483436175688569, Validation Loss: 0.006759684258473438\n",
      "Epoch [12186/20000], Training Loss: 0.008258189265948854, Validation Loss: 0.012647517025470734\n",
      "Epoch [12187/20000], Training Loss: 0.01395524162658798, Validation Loss: 0.029680542485689666\n",
      "Epoch [12188/20000], Training Loss: 0.030247878288459366, Validation Loss: 0.04542456870578404\n",
      "Epoch [12189/20000], Training Loss: 0.017799806415236423, Validation Loss: 0.012368846860811442\n",
      "Epoch [12190/20000], Training Loss: 0.01472650041354687, Validation Loss: 0.00978617473135403\n",
      "Epoch [12191/20000], Training Loss: 0.010262945850253604, Validation Loss: 0.00700054468169193\n",
      "Epoch [12192/20000], Training Loss: 0.0130671754845285, Validation Loss: 0.017613377121278972\n",
      "Epoch [12193/20000], Training Loss: 0.015855350972352817, Validation Loss: 0.021109066753037302\n",
      "Epoch [12194/20000], Training Loss: 0.01767426925237357, Validation Loss: 0.004810173870802308\n",
      "Epoch [12195/20000], Training Loss: 0.009265588638040103, Validation Loss: 0.015983940064740767\n",
      "Epoch [12196/20000], Training Loss: 0.014212299642427493, Validation Loss: 0.0065357829969018766\n",
      "Epoch [12197/20000], Training Loss: 0.005572313352396512, Validation Loss: 0.011407654257092352\n",
      "Epoch [12198/20000], Training Loss: 0.008874626152516742, Validation Loss: 0.004416980176626696\n",
      "Epoch [12199/20000], Training Loss: 0.004316771993347045, Validation Loss: 0.006873312852121671\n",
      "Epoch [12200/20000], Training Loss: 0.005751032698234277, Validation Loss: 0.003945900191995731\n",
      "Epoch [12201/20000], Training Loss: 0.004964412310593096, Validation Loss: 0.005844049269772508\n",
      "Epoch [12202/20000], Training Loss: 0.005809775769843587, Validation Loss: 0.0043760071282252155\n",
      "Epoch [12203/20000], Training Loss: 0.008886843688186121, Validation Loss: 0.0060912442015868306\n",
      "Epoch [12204/20000], Training Loss: 0.010102720631818687, Validation Loss: 0.0064197255029918255\n",
      "Epoch [12205/20000], Training Loss: 0.008838115700720144, Validation Loss: 0.004743530064193432\n",
      "Epoch [12206/20000], Training Loss: 0.010683653589304802, Validation Loss: 0.007449712692895056\n",
      "Epoch [12207/20000], Training Loss: 0.008978333578462687, Validation Loss: 0.008477308423376176\n",
      "Epoch [12208/20000], Training Loss: 0.00690259737893939, Validation Loss: 0.003881499409544631\n",
      "Epoch [12209/20000], Training Loss: 0.009739489845482499, Validation Loss: 0.0074253564686971784\n",
      "Epoch [12210/20000], Training Loss: 0.012425546383643191, Validation Loss: 0.0039600454838364385\n",
      "Epoch [12211/20000], Training Loss: 0.009129342677104952, Validation Loss: 0.004486604214759739\n",
      "Epoch [12212/20000], Training Loss: 0.019140004880422827, Validation Loss: 0.004158141047680276\n",
      "Epoch [12213/20000], Training Loss: 0.008403496569371782, Validation Loss: 0.04572102626396811\n",
      "Epoch [12214/20000], Training Loss: 0.02951758742191097, Validation Loss: 0.016868693905353536\n",
      "Epoch [12215/20000], Training Loss: 0.014735526052390924, Validation Loss: 0.004421368574315109\n",
      "Epoch [12216/20000], Training Loss: 0.008408323354420386, Validation Loss: 0.003415162278802849\n",
      "Epoch [12217/20000], Training Loss: 0.005727857361697326, Validation Loss: 0.0040548994281510075\n",
      "Epoch [12218/20000], Training Loss: 0.008557629148916541, Validation Loss: 0.004160382878418594\n",
      "Epoch [12219/20000], Training Loss: 0.01155790121577281, Validation Loss: 0.005847780672151462\n",
      "Epoch [12220/20000], Training Loss: 0.011411884728918917, Validation Loss: 0.007798028135129341\n",
      "Epoch [12221/20000], Training Loss: 0.009216095399876525, Validation Loss: 0.006148093385036191\n",
      "Epoch [12222/20000], Training Loss: 0.007663288474889539, Validation Loss: 0.00689752334410595\n",
      "Epoch [12223/20000], Training Loss: 0.008996672483460446, Validation Loss: 0.017357994001161257\n",
      "Epoch [12224/20000], Training Loss: 0.016754258762895397, Validation Loss: 0.005566253887081984\n",
      "Epoch [12225/20000], Training Loss: 0.009498019183021305, Validation Loss: 0.028377834973765396\n",
      "Epoch [12226/20000], Training Loss: 0.014726690720895672, Validation Loss: 0.030539580395303086\n",
      "Epoch [12227/20000], Training Loss: 0.012210282224779283, Validation Loss: 0.00833867553098701\n",
      "Epoch [12228/20000], Training Loss: 0.007856041354118912, Validation Loss: 0.01061789141133269\n",
      "Epoch [12229/20000], Training Loss: 0.007163755319197662, Validation Loss: 0.00663458421789593\n",
      "Epoch [12230/20000], Training Loss: 0.008408935545178662, Validation Loss: 0.004174094401573062\n",
      "Epoch [12231/20000], Training Loss: 0.012171963001102475, Validation Loss: 0.006421682265507148\n",
      "Epoch [12232/20000], Training Loss: 0.008493838791504718, Validation Loss: 0.004564757627641873\n",
      "Epoch [12233/20000], Training Loss: 0.006603353270163227, Validation Loss: 0.013038523563330224\n",
      "Epoch [12234/20000], Training Loss: 0.014846139396725886, Validation Loss: 0.0031440616745450306\n",
      "Epoch [12235/20000], Training Loss: 0.008208093031758576, Validation Loss: 0.0036886372510512694\n",
      "Epoch [12236/20000], Training Loss: 0.008779726439700295, Validation Loss: 0.006616566146289772\n",
      "Epoch [12237/20000], Training Loss: 0.007414906009216793, Validation Loss: 0.006346650807884657\n",
      "Epoch [12238/20000], Training Loss: 0.006868048491534344, Validation Loss: 0.007479863425716969\n",
      "Epoch [12239/20000], Training Loss: 0.008871215250110254, Validation Loss: 0.0075695045738147625\n",
      "Epoch [12240/20000], Training Loss: 0.006505774929662168, Validation Loss: 0.004876545220643688\n",
      "Epoch [12241/20000], Training Loss: 0.007974596629667628, Validation Loss: 0.004402383472897294\n",
      "Epoch [12242/20000], Training Loss: 0.006373738453110686, Validation Loss: 0.004409321921587305\n",
      "Epoch [12243/20000], Training Loss: 0.005272464299715856, Validation Loss: 0.0026093272021268732\n",
      "Epoch [12244/20000], Training Loss: 0.009220582344865542, Validation Loss: 0.02214466758674202\n",
      "Epoch [12245/20000], Training Loss: 0.023079427728358075, Validation Loss: 0.09800439232716533\n",
      "Epoch [12246/20000], Training Loss: 0.1130766218500087, Validation Loss: 0.046173849276133945\n",
      "Epoch [12247/20000], Training Loss: 0.02431287821881207, Validation Loss: 0.03086990524310857\n",
      "Epoch [12248/20000], Training Loss: 0.016618514463646403, Validation Loss: 0.005525027505427715\n",
      "Epoch [12249/20000], Training Loss: 0.006698027481823894, Validation Loss: 0.0054425863459722935\n",
      "Epoch [12250/20000], Training Loss: 0.0059824462784620535, Validation Loss: 0.005003349356749303\n",
      "Epoch [12251/20000], Training Loss: 0.0067108359528772, Validation Loss: 0.004849062322591635\n",
      "Epoch [12252/20000], Training Loss: 0.006211468201529767, Validation Loss: 0.004425518734283936\n",
      "Epoch [12253/20000], Training Loss: 0.006657634367002174, Validation Loss: 0.004585600605683534\n",
      "Epoch [12254/20000], Training Loss: 0.0046021846563754866, Validation Loss: 0.005582963734241275\n",
      "Epoch [12255/20000], Training Loss: 0.008636813756311312, Validation Loss: 0.004093085802456439\n",
      "Epoch [12256/20000], Training Loss: 0.02433763419061766, Validation Loss: 0.005356480205331796\n",
      "Epoch [12257/20000], Training Loss: 0.017685344339042786, Validation Loss: 0.014050342472078878\n",
      "Epoch [12258/20000], Training Loss: 0.00729128041919986, Validation Loss: 0.016314781246470438\n",
      "Epoch [12259/20000], Training Loss: 0.007619703704092119, Validation Loss: 0.004220446215575586\n",
      "Epoch [12260/20000], Training Loss: 0.004877872033310788, Validation Loss: 0.004725834547051428\n",
      "Epoch [12261/20000], Training Loss: 0.010847407018965376, Validation Loss: 0.011983253591876877\n",
      "Epoch [12262/20000], Training Loss: 0.008454625249474443, Validation Loss: 0.00697908136290997\n",
      "Epoch [12263/20000], Training Loss: 0.00923471425319024, Validation Loss: 0.003062279877862498\n",
      "Epoch [12264/20000], Training Loss: 0.00876553357271145, Validation Loss: 0.02326707601140551\n",
      "Epoch [12265/20000], Training Loss: 0.015038510127591767, Validation Loss: 0.006925461535420904\n",
      "Epoch [12266/20000], Training Loss: 0.0075971227155865306, Validation Loss: 0.005691398946510162\n",
      "Epoch [12267/20000], Training Loss: 0.007958191783732868, Validation Loss: 0.0034371501350116368\n",
      "Epoch [12268/20000], Training Loss: 0.00677219751719349, Validation Loss: 0.005933498306425073\n",
      "Epoch [12269/20000], Training Loss: 0.004284772885772067, Validation Loss: 0.005966068897708153\n",
      "Epoch [12270/20000], Training Loss: 0.006635373368875922, Validation Loss: 0.004530378524248947\n",
      "Epoch [12271/20000], Training Loss: 0.0046258299022870885, Validation Loss: 0.011147324281113567\n",
      "Epoch [12272/20000], Training Loss: 0.010452266585031924, Validation Loss: 0.004657514080999395\n",
      "Epoch [12273/20000], Training Loss: 0.010923663755030637, Validation Loss: 0.004731778235639338\n",
      "Epoch [12274/20000], Training Loss: 0.015673632425854782, Validation Loss: 0.006291359900127905\n",
      "Epoch [12275/20000], Training Loss: 0.008903722020997, Validation Loss: 0.02537673629554352\n",
      "Epoch [12276/20000], Training Loss: 0.008287606943213177, Validation Loss: 0.005437043012989307\n",
      "Epoch [12277/20000], Training Loss: 0.00564070087420987, Validation Loss: 0.003887986775794161\n",
      "Epoch [12278/20000], Training Loss: 0.005872932834401061, Validation Loss: 0.002899237195346619\n",
      "Epoch [12279/20000], Training Loss: 0.005327218198285014, Validation Loss: 0.010767522147943964\n",
      "Epoch [12280/20000], Training Loss: 0.01566727480634914, Validation Loss: 0.010262901712719343\n",
      "Epoch [12281/20000], Training Loss: 0.004232885610170862, Validation Loss: 0.004534920502883162\n",
      "Epoch [12282/20000], Training Loss: 0.007269037138420182, Validation Loss: 0.007113102885674445\n",
      "Epoch [12283/20000], Training Loss: 0.01024499119583717, Validation Loss: 0.005776154253283039\n",
      "Epoch [12284/20000], Training Loss: 0.026177553593048027, Validation Loss: 0.01703834014823467\n",
      "Epoch [12285/20000], Training Loss: 0.011655537220316805, Validation Loss: 0.0043894378294291\n",
      "Epoch [12286/20000], Training Loss: 0.00432755359361181, Validation Loss: 0.006477527204737044\n",
      "Epoch [12287/20000], Training Loss: 0.007024485682513111, Validation Loss: 0.008050583988092188\n",
      "Epoch [12288/20000], Training Loss: 0.009395268628533618, Validation Loss: 0.031576274297809244\n",
      "Epoch [12289/20000], Training Loss: 0.04459372518197467, Validation Loss: 0.009055436641917072\n",
      "Epoch [12290/20000], Training Loss: 0.015350222676975786, Validation Loss: 0.006439392941436901\n",
      "Epoch [12291/20000], Training Loss: 0.010832823575973245, Validation Loss: 0.003605929487057275\n",
      "Epoch [12292/20000], Training Loss: 0.0065702053595617016, Validation Loss: 0.008072358452539317\n",
      "Epoch [12293/20000], Training Loss: 0.0070784753524516675, Validation Loss: 0.009827678917150402\n",
      "Epoch [12294/20000], Training Loss: 0.0058174244726875, Validation Loss: 0.00540962754565401\n",
      "Epoch [12295/20000], Training Loss: 0.008312879562644022, Validation Loss: 0.003728446520256057\n",
      "Epoch [12296/20000], Training Loss: 0.004741580552945379, Validation Loss: 0.01410513113140561\n",
      "Epoch [12297/20000], Training Loss: 0.009961262951869847, Validation Loss: 0.00873989626047462\n",
      "Epoch [12298/20000], Training Loss: 0.0037685485575431293, Validation Loss: 0.005288688139834059\n",
      "Epoch [12299/20000], Training Loss: 0.007727946588991992, Validation Loss: 0.011755364035642353\n",
      "Epoch [12300/20000], Training Loss: 0.006443313880091799, Validation Loss: 0.003908548739722002\n",
      "Epoch [12301/20000], Training Loss: 0.006721998542876203, Validation Loss: 0.006531469049406269\n",
      "Epoch [12302/20000], Training Loss: 0.005823729555621477, Validation Loss: 0.003120916647538203\n",
      "Epoch [12303/20000], Training Loss: 0.014878827425751038, Validation Loss: 0.007078027649445825\n",
      "Epoch [12304/20000], Training Loss: 0.0076534938416443765, Validation Loss: 0.0061240387282371755\n",
      "Epoch [12305/20000], Training Loss: 0.006203874494531192, Validation Loss: 0.004512899872582109\n",
      "Epoch [12306/20000], Training Loss: 0.008323798202452184, Validation Loss: 0.0047347198052014915\n",
      "Epoch [12307/20000], Training Loss: 0.004858312313444912, Validation Loss: 0.012831611673106076\n",
      "Epoch [12308/20000], Training Loss: 0.01479397587494142, Validation Loss: 0.02206313684971725\n",
      "Epoch [12309/20000], Training Loss: 0.012955349012502535, Validation Loss: 0.004168696962433874\n",
      "Epoch [12310/20000], Training Loss: 0.00962457595804673, Validation Loss: 0.002494745498617184\n",
      "Epoch [12311/20000], Training Loss: 0.007558068212996919, Validation Loss: 0.0035558290517201385\n",
      "Epoch [12312/20000], Training Loss: 0.007910220161388029, Validation Loss: 0.00429582661693928\n",
      "Epoch [12313/20000], Training Loss: 0.007451703605641212, Validation Loss: 0.0031983348472977013\n",
      "Epoch [12314/20000], Training Loss: 0.0070942060098916826, Validation Loss: 0.0036357786229440797\n",
      "Epoch [12315/20000], Training Loss: 0.005917891370440235, Validation Loss: 0.02735909234355109\n",
      "Epoch [12316/20000], Training Loss: 0.012317539338255301, Validation Loss: 0.015155245024643552\n",
      "Epoch [12317/20000], Training Loss: 0.011207540147422281, Validation Loss: 0.016546908767260445\n",
      "Epoch [12318/20000], Training Loss: 0.033454791052333475, Validation Loss: 0.014359976091506204\n",
      "Epoch [12319/20000], Training Loss: 0.03511666706513746, Validation Loss: 0.07195496163798712\n",
      "Epoch [12320/20000], Training Loss: 0.04129959986520199, Validation Loss: 0.016039146255894008\n",
      "Epoch [12321/20000], Training Loss: 0.023567443174085514, Validation Loss: 0.021461586911235436\n",
      "Epoch [12322/20000], Training Loss: 0.030747051500449225, Validation Loss: 0.027140969543585903\n",
      "Epoch [12323/20000], Training Loss: 0.013180737847246096, Validation Loss: 0.009750158863133558\n",
      "Epoch [12324/20000], Training Loss: 0.011398592744496585, Validation Loss: 0.0067155822663603915\n",
      "Epoch [12325/20000], Training Loss: 0.017856568842294758, Validation Loss: 0.005190802868063267\n",
      "Epoch [12326/20000], Training Loss: 0.02174777746300346, Validation Loss: 0.04232498470186589\n",
      "Epoch [12327/20000], Training Loss: 0.02147407328759852, Validation Loss: 0.012941547703454053\n",
      "Epoch [12328/20000], Training Loss: 0.012096028296842374, Validation Loss: 0.014752011131614142\n",
      "Epoch [12329/20000], Training Loss: 0.01269848532371855, Validation Loss: 0.005446925454695669\n",
      "Epoch [12330/20000], Training Loss: 0.009110851205020611, Validation Loss: 0.006773588747967503\n",
      "Epoch [12331/20000], Training Loss: 0.007392955889892099, Validation Loss: 0.009047743003065989\n",
      "Epoch [12332/20000], Training Loss: 0.007251994170863847, Validation Loss: 0.013340272686427056\n",
      "Epoch [12333/20000], Training Loss: 0.007046537698313061, Validation Loss: 0.0038426567684593594\n",
      "Epoch [12334/20000], Training Loss: 0.005696334063811394, Validation Loss: 0.004418084174172269\n",
      "Epoch [12335/20000], Training Loss: 0.006790584459460141, Validation Loss: 0.0037476026145587377\n",
      "Epoch [12336/20000], Training Loss: 0.005373226465703088, Validation Loss: 0.003588830587809915\n",
      "Epoch [12337/20000], Training Loss: 0.004435897141218741, Validation Loss: 0.002890189371703806\n",
      "Epoch [12338/20000], Training Loss: 0.006960907500927403, Validation Loss: 0.0026319764944925544\n",
      "Epoch [12339/20000], Training Loss: 0.005645489379406042, Validation Loss: 0.004266148362830791\n",
      "Epoch [12340/20000], Training Loss: 0.010454942672367906, Validation Loss: 0.004406321644329937\n",
      "Epoch [12341/20000], Training Loss: 0.01644726243934461, Validation Loss: 0.007762178200810662\n",
      "Epoch [12342/20000], Training Loss: 0.012900648560649383, Validation Loss: 0.007154108070756037\n",
      "Epoch [12343/20000], Training Loss: 0.012382125026176385, Validation Loss: 0.006443184139340831\n",
      "Epoch [12344/20000], Training Loss: 0.008012421784639758, Validation Loss: 0.007773763414265833\n",
      "Epoch [12345/20000], Training Loss: 0.006254100259996319, Validation Loss: 0.0025344025857521046\n",
      "Epoch [12346/20000], Training Loss: 0.012538016002508812, Validation Loss: 0.003815972543764344\n",
      "Epoch [12347/20000], Training Loss: 0.014075186070320862, Validation Loss: 0.009459685630448713\n",
      "Epoch [12348/20000], Training Loss: 0.010487201707811826, Validation Loss: 0.004655252690879154\n",
      "Epoch [12349/20000], Training Loss: 0.017067318658259216, Validation Loss: 0.0036707944756407024\n",
      "Epoch [12350/20000], Training Loss: 0.020298656426319388, Validation Loss: 0.002945467233290953\n",
      "Epoch [12351/20000], Training Loss: 0.009338517741395793, Validation Loss: 0.017669311608251463\n",
      "Epoch [12352/20000], Training Loss: 0.008769281104572915, Validation Loss: 0.006729378283631959\n",
      "Epoch [12353/20000], Training Loss: 0.012342222569790198, Validation Loss: 0.0031024541650270165\n",
      "Epoch [12354/20000], Training Loss: 0.012919759729161992, Validation Loss: 0.003809731825536947\n",
      "Epoch [12355/20000], Training Loss: 0.004381077789210914, Validation Loss: 0.0065581205218937354\n",
      "Epoch [12356/20000], Training Loss: 0.008344062893749131, Validation Loss: 0.004303056625635396\n",
      "Epoch [12357/20000], Training Loss: 0.00809393433155492, Validation Loss: 0.004759297171841663\n",
      "Epoch [12358/20000], Training Loss: 0.004144511346696943, Validation Loss: 0.005591028289452841\n",
      "Epoch [12359/20000], Training Loss: 0.013129666115543972, Validation Loss: 0.003862851315548405\n",
      "Epoch [12360/20000], Training Loss: 0.025425867329041858, Validation Loss: 0.01893203390263157\n",
      "Epoch [12361/20000], Training Loss: 0.009408797549050567, Validation Loss: 0.0064642273338669\n",
      "Epoch [12362/20000], Training Loss: 0.011527241136978514, Validation Loss: 0.004779606036109385\n",
      "Epoch [12363/20000], Training Loss: 0.007130862103752277, Validation Loss: 0.003423620929752807\n",
      "Epoch [12364/20000], Training Loss: 0.013565501715804982, Validation Loss: 0.0086329370556736\n",
      "Epoch [12365/20000], Training Loss: 0.024712167214602232, Validation Loss: 0.011896058248729397\n",
      "Epoch [12366/20000], Training Loss: 0.024533891771820242, Validation Loss: 0.02391325469151135\n",
      "Epoch [12367/20000], Training Loss: 0.017388532784285156, Validation Loss: 0.008214563839561555\n",
      "Epoch [12368/20000], Training Loss: 0.016312468265823554, Validation Loss: 0.034822304866507954\n",
      "Epoch [12369/20000], Training Loss: 0.021560913454907547, Validation Loss: 0.024304265156397824\n",
      "Epoch [12370/20000], Training Loss: 0.015862851411969, Validation Loss: 0.004812665878124075\n",
      "Epoch [12371/20000], Training Loss: 0.00860191680632332, Validation Loss: 0.004955349204093652\n",
      "Epoch [12372/20000], Training Loss: 0.006931735720302511, Validation Loss: 0.004697733493194229\n",
      "Epoch [12373/20000], Training Loss: 0.0048226412836811505, Validation Loss: 0.003371968462620397\n",
      "Epoch [12374/20000], Training Loss: 0.006625882305181907, Validation Loss: 0.0029168279596762303\n",
      "Epoch [12375/20000], Training Loss: 0.005495783473244436, Validation Loss: 0.01386057133265684\n",
      "Epoch [12376/20000], Training Loss: 0.008638849807072444, Validation Loss: 0.006478988559914102\n",
      "Epoch [12377/20000], Training Loss: 0.006881361406288177, Validation Loss: 0.004756080036145249\n",
      "Epoch [12378/20000], Training Loss: 0.006427960166287708, Validation Loss: 0.0037707699944538675\n",
      "Epoch [12379/20000], Training Loss: 0.009262846846208308, Validation Loss: 0.003453365864812089\n",
      "Epoch [12380/20000], Training Loss: 0.004565739422105253, Validation Loss: 0.008708058298459522\n",
      "Epoch [12381/20000], Training Loss: 0.010332123165686167, Validation Loss: 0.0033883180956684506\n",
      "Epoch [12382/20000], Training Loss: 0.005158477632254029, Validation Loss: 0.0028468977423606085\n",
      "Epoch [12383/20000], Training Loss: 0.00792780679122578, Validation Loss: 0.005744219565368017\n",
      "Epoch [12384/20000], Training Loss: 0.008869291307876535, Validation Loss: 0.010176344025629598\n",
      "Epoch [12385/20000], Training Loss: 0.006539698783724036, Validation Loss: 0.009887482440377083\n",
      "Epoch [12386/20000], Training Loss: 0.004088133418463258, Validation Loss: 0.005024500505864385\n",
      "Epoch [12387/20000], Training Loss: 0.005058847349053914, Validation Loss: 0.002965810470686035\n",
      "Epoch [12388/20000], Training Loss: 0.007697989815434474, Validation Loss: 0.0034793711157500456\n",
      "Epoch [12389/20000], Training Loss: 0.004366294770859115, Validation Loss: 0.003187220059378596\n",
      "Epoch [12390/20000], Training Loss: 0.00622198564399566, Validation Loss: 0.00234838637763143\n",
      "Epoch [12391/20000], Training Loss: 0.018753879161002778, Validation Loss: 0.010047906991679614\n",
      "Epoch [12392/20000], Training Loss: 0.019310230765508356, Validation Loss: 0.02314417889075687\n",
      "Epoch [12393/20000], Training Loss: 0.021009254063496234, Validation Loss: 0.06966173226370768\n",
      "Epoch [12394/20000], Training Loss: 0.06468180833118302, Validation Loss: 0.045227893761244586\n",
      "Epoch [12395/20000], Training Loss: 0.0209728652729869, Validation Loss: 0.007311394296331335\n",
      "Epoch [12396/20000], Training Loss: 0.01035828428575769, Validation Loss: 0.005154321789007099\n",
      "Epoch [12397/20000], Training Loss: 0.005242609561719291, Validation Loss: 0.005554771864403563\n",
      "Epoch [12398/20000], Training Loss: 0.005663930159893685, Validation Loss: 0.004451769564767005\n",
      "Epoch [12399/20000], Training Loss: 0.006302837228369234, Validation Loss: 0.006477065449246311\n",
      "Epoch [12400/20000], Training Loss: 0.010565867248390402, Validation Loss: 0.004650743074267082\n",
      "Epoch [12401/20000], Training Loss: 0.010016587000531476, Validation Loss: 0.015036273482698414\n",
      "Epoch [12402/20000], Training Loss: 0.007312030030552664, Validation Loss: 0.005920965758536988\n",
      "Epoch [12403/20000], Training Loss: 0.007344249286688864, Validation Loss: 0.0031465548849922698\n",
      "Epoch [12404/20000], Training Loss: 0.009210660706490412, Validation Loss: 0.005065698158667991\n",
      "Epoch [12405/20000], Training Loss: 0.009993139193933789, Validation Loss: 0.0035612765322941948\n",
      "Epoch [12406/20000], Training Loss: 0.009832168071755274, Validation Loss: 0.03720382494591723\n",
      "Epoch [12407/20000], Training Loss: 0.020164549887925074, Validation Loss: 0.016698293920768558\n",
      "Epoch [12408/20000], Training Loss: 0.010270002763718367, Validation Loss: 0.003120524991246043\n",
      "Epoch [12409/20000], Training Loss: 0.009865765160481845, Validation Loss: 0.00785597975966823\n",
      "Epoch [12410/20000], Training Loss: 0.012830846169630863, Validation Loss: 0.005182650030314103\n",
      "Epoch [12411/20000], Training Loss: 0.013605812369080792, Validation Loss: 0.010898251626105051\n",
      "Epoch [12412/20000], Training Loss: 0.010191370971629763, Validation Loss: 0.006413070524591343\n",
      "Epoch [12413/20000], Training Loss: 0.00912585090970554, Validation Loss: 0.004342018519638875\n",
      "Epoch [12414/20000], Training Loss: 0.01009454527723262, Validation Loss: 0.014613942541701721\n",
      "Epoch [12415/20000], Training Loss: 0.016258670089363086, Validation Loss: 0.005265939782654121\n",
      "Epoch [12416/20000], Training Loss: 0.010423361413164198, Validation Loss: 0.004770016375148807\n",
      "Epoch [12417/20000], Training Loss: 0.006498363103414054, Validation Loss: 0.004897248210720152\n",
      "Epoch [12418/20000], Training Loss: 0.005034815311298806, Validation Loss: 0.0041007214441565664\n",
      "Epoch [12419/20000], Training Loss: 0.0054244873005830285, Validation Loss: 0.003196613225481672\n",
      "Epoch [12420/20000], Training Loss: 0.004215531749650836, Validation Loss: 0.004804051956620419\n",
      "Epoch [12421/20000], Training Loss: 0.006429937128294634, Validation Loss: 0.002826093137181423\n",
      "Epoch [12422/20000], Training Loss: 0.026828200864014513, Validation Loss: 0.01475186818945198\n",
      "Epoch [12423/20000], Training Loss: 0.03895574479663212, Validation Loss: 0.005921953253192415\n",
      "Epoch [12424/20000], Training Loss: 0.06987432205371695, Validation Loss: 0.10919475004787674\n",
      "Epoch [12425/20000], Training Loss: 0.04709722238476388, Validation Loss: 0.016231982002826672\n",
      "Epoch [12426/20000], Training Loss: 0.03429978773679717, Validation Loss: 0.009620757178968884\n",
      "Epoch [12427/20000], Training Loss: 0.02813920146685892, Validation Loss: 0.026903900168170885\n",
      "Epoch [12428/20000], Training Loss: 0.01332981518602797, Validation Loss: 0.015978659600743263\n",
      "Epoch [12429/20000], Training Loss: 0.008981269676171775, Validation Loss: 0.006413059773324546\n",
      "Epoch [12430/20000], Training Loss: 0.007522252737544477, Validation Loss: 0.010454602830252822\n",
      "Epoch [12431/20000], Training Loss: 0.011523415229411122, Validation Loss: 0.004752680214835436\n",
      "Epoch [12432/20000], Training Loss: 0.006473916137890358, Validation Loss: 0.005136225685610896\n",
      "Epoch [12433/20000], Training Loss: 0.005309764990150663, Validation Loss: 0.005489767247086418\n",
      "Epoch [12434/20000], Training Loss: 0.007642993942551714, Validation Loss: 0.0038002226034389457\n",
      "Epoch [12435/20000], Training Loss: 0.005315121749715347, Validation Loss: 0.005893420372930606\n",
      "Epoch [12436/20000], Training Loss: 0.01120462915722393, Validation Loss: 0.004738259686451303\n",
      "Epoch [12437/20000], Training Loss: 0.004743566532852128, Validation Loss: 0.004470833811860987\n",
      "Epoch [12438/20000], Training Loss: 0.005488855042910602, Validation Loss: 0.004208980756532453\n",
      "Epoch [12439/20000], Training Loss: 0.006653050852556979, Validation Loss: 0.004643103120697235\n",
      "Epoch [12440/20000], Training Loss: 0.010457157991676727, Validation Loss: 0.0038945065341509824\n",
      "Epoch [12441/20000], Training Loss: 0.011903456029748278, Validation Loss: 0.016737121822724976\n",
      "Epoch [12442/20000], Training Loss: 0.005500873983172434, Validation Loss: 0.004352090459912424\n",
      "Epoch [12443/20000], Training Loss: 0.0069839695601591045, Validation Loss: 0.008776762342725826\n",
      "Epoch [12444/20000], Training Loss: 0.0048314437595503735, Validation Loss: 0.022464858102178634\n",
      "Epoch [12445/20000], Training Loss: 0.007279663491772226, Validation Loss: 0.002784827391956362\n",
      "Epoch [12446/20000], Training Loss: 0.005707848031550254, Validation Loss: 0.007169948272978185\n",
      "Epoch [12447/20000], Training Loss: 0.014122154096185113, Validation Loss: 0.011339493172467232\n",
      "Epoch [12448/20000], Training Loss: 0.008711170575614753, Validation Loss: 0.004955370512978251\n",
      "Epoch [12449/20000], Training Loss: 0.006711809721309692, Validation Loss: 0.002974974795969056\n",
      "Epoch [12450/20000], Training Loss: 0.011345546183292754, Validation Loss: 0.0096337981111755\n",
      "Epoch [12451/20000], Training Loss: 0.009198142528475728, Validation Loss: 0.0036595732095336636\n",
      "Epoch [12452/20000], Training Loss: 0.007126984107474398, Validation Loss: 0.005421387698371521\n",
      "Epoch [12453/20000], Training Loss: 0.0039018923728560495, Validation Loss: 0.002542009521326232\n",
      "Epoch [12454/20000], Training Loss: 0.004848128216898269, Validation Loss: 0.0036743637635357007\n",
      "Epoch [12455/20000], Training Loss: 0.004320100172273149, Validation Loss: 0.006825598822031644\n",
      "Epoch [12456/20000], Training Loss: 0.0053432306620899385, Validation Loss: 0.002340358650828604\n",
      "Epoch [12457/20000], Training Loss: 0.006969373168041264, Validation Loss: 0.0026225670974975765\n",
      "Epoch [12458/20000], Training Loss: 0.006199746513240305, Validation Loss: 0.005488430048781571\n",
      "Epoch [12459/20000], Training Loss: 0.010659798264636524, Validation Loss: 0.008499071330536083\n",
      "Epoch [12460/20000], Training Loss: 0.007659676756052899, Validation Loss: 0.004147769240781672\n",
      "Epoch [12461/20000], Training Loss: 0.007850024173161987, Validation Loss: 0.0033903448424137356\n",
      "Epoch [12462/20000], Training Loss: 0.004025610389232627, Validation Loss: 0.003020256952756258\n",
      "Epoch [12463/20000], Training Loss: 0.0052966472394473385, Validation Loss: 0.008580847450800328\n",
      "Epoch [12464/20000], Training Loss: 0.008258477648528892, Validation Loss: 0.004415298832831078\n",
      "Epoch [12465/20000], Training Loss: 0.009145316487904762, Validation Loss: 0.006920231599838392\n",
      "Epoch [12466/20000], Training Loss: 0.010209324240966973, Validation Loss: 0.005910880265936505\n",
      "Epoch [12467/20000], Training Loss: 0.007811728989119209, Validation Loss: 0.0052572506753070306\n",
      "Epoch [12468/20000], Training Loss: 0.007137620027996101, Validation Loss: 0.0037034680482755317\n",
      "Epoch [12469/20000], Training Loss: 0.004213171772270081, Validation Loss: 0.002425382917495782\n",
      "Epoch [12470/20000], Training Loss: 0.005513484907818825, Validation Loss: 0.0025892101904531582\n",
      "Epoch [12471/20000], Training Loss: 0.009367834092699923, Validation Loss: 0.0038109953979629874\n",
      "Epoch [12472/20000], Training Loss: 0.012573887932376238, Validation Loss: 0.03131758102352463\n",
      "Epoch [12473/20000], Training Loss: 0.046731226620457686, Validation Loss: 0.05203570425510521\n",
      "Epoch [12474/20000], Training Loss: 0.013357620147871785, Validation Loss: 0.006003948115930476\n",
      "Epoch [12475/20000], Training Loss: 0.01774196701964164, Validation Loss: 0.012029395883506109\n",
      "Epoch [12476/20000], Training Loss: 0.01077955933111038, Validation Loss: 0.012702723618496253\n",
      "Epoch [12477/20000], Training Loss: 0.01251482222245873, Validation Loss: 0.0039026276383620534\n",
      "Epoch [12478/20000], Training Loss: 0.005637539141129569, Validation Loss: 0.004634428874895193\n",
      "Epoch [12479/20000], Training Loss: 0.004128805651687019, Validation Loss: 0.0033364508991109453\n",
      "Epoch [12480/20000], Training Loss: 0.004928459387364066, Validation Loss: 0.0026808402742192683\n",
      "Epoch [12481/20000], Training Loss: 0.007244361989640831, Validation Loss: 0.004759620178065097\n",
      "Epoch [12482/20000], Training Loss: 0.006671038707281696, Validation Loss: 0.003239224171241815\n",
      "Epoch [12483/20000], Training Loss: 0.0065815314088208, Validation Loss: 0.005356472493217488\n",
      "Epoch [12484/20000], Training Loss: 0.00740757396904103, Validation Loss: 0.004210267333928793\n",
      "Epoch [12485/20000], Training Loss: 0.010696365707969693, Validation Loss: 0.008385464056914316\n",
      "Epoch [12486/20000], Training Loss: 0.009583351498284693, Validation Loss: 0.0033593137812480173\n",
      "Epoch [12487/20000], Training Loss: 0.007237001693592252, Validation Loss: 0.005759524672645836\n",
      "Epoch [12488/20000], Training Loss: 0.005223521176958457, Validation Loss: 0.007034636311532593\n",
      "Epoch [12489/20000], Training Loss: 0.005225266810157336, Validation Loss: 0.003731311666708835\n",
      "Epoch [12490/20000], Training Loss: 0.00461892964501333, Validation Loss: 0.005805573359791052\n",
      "Epoch [12491/20000], Training Loss: 0.007642843460566999, Validation Loss: 0.007188183455238442\n",
      "Epoch [12492/20000], Training Loss: 0.007330626038830295, Validation Loss: 0.005573048223265672\n",
      "Epoch [12493/20000], Training Loss: 0.00835501152906407, Validation Loss: 0.002800845667682778\n",
      "Epoch [12494/20000], Training Loss: 0.01333773115571343, Validation Loss: 0.004239087991506054\n",
      "Epoch [12495/20000], Training Loss: 0.006673024893513814, Validation Loss: 0.007458494244511437\n",
      "Epoch [12496/20000], Training Loss: 0.013157907808947909, Validation Loss: 0.011180997027948935\n",
      "Epoch [12497/20000], Training Loss: 0.0239573402536085, Validation Loss: 0.0032917557332487438\n",
      "Epoch [12498/20000], Training Loss: 0.00693382607590008, Validation Loss: 0.015797038475804066\n",
      "Epoch [12499/20000], Training Loss: 0.028131503565418825, Validation Loss: 0.04716988175667731\n",
      "Epoch [12500/20000], Training Loss: 0.05339605376814559, Validation Loss: 0.05068275770744484\n",
      "Epoch [12501/20000], Training Loss: 0.05098264404971685, Validation Loss: 0.00949629503906799\n",
      "Epoch [12502/20000], Training Loss: 0.015259012021098377, Validation Loss: 0.015633432834891776\n",
      "Epoch [12503/20000], Training Loss: 0.007023685013077089, Validation Loss: 0.008799396810770434\n",
      "Epoch [12504/20000], Training Loss: 0.00640070545861298, Validation Loss: 0.004709693763690633\n",
      "Epoch [12505/20000], Training Loss: 0.0060347444752031675, Validation Loss: 0.008222256289495895\n",
      "Epoch [12506/20000], Training Loss: 0.006647780564630271, Validation Loss: 0.00526397890993329\n",
      "Epoch [12507/20000], Training Loss: 0.004200281880912371, Validation Loss: 0.00533414646529987\n",
      "Epoch [12508/20000], Training Loss: 0.006117155532202949, Validation Loss: 0.004353086726556654\n",
      "Epoch [12509/20000], Training Loss: 0.00514735359632011, Validation Loss: 0.005894573180556607\n",
      "Epoch [12510/20000], Training Loss: 0.010705239183153026, Validation Loss: 0.005022977036982671\n",
      "Epoch [12511/20000], Training Loss: 0.012802504102832504, Validation Loss: 0.0055733191168752605\n",
      "Epoch [12512/20000], Training Loss: 0.006766157507497285, Validation Loss: 0.016864244427391466\n",
      "Epoch [12513/20000], Training Loss: 0.012987478385420153, Validation Loss: 0.006664918018236451\n",
      "Epoch [12514/20000], Training Loss: 0.005531665163029434, Validation Loss: 0.004121278442427886\n",
      "Epoch [12515/20000], Training Loss: 0.008007555950566061, Validation Loss: 0.007984186125036149\n",
      "Epoch [12516/20000], Training Loss: 0.008970300461182237, Validation Loss: 0.005010953189464651\n",
      "Epoch [12517/20000], Training Loss: 0.006037137225835717, Validation Loss: 0.003998064311440901\n",
      "Epoch [12518/20000], Training Loss: 0.004779063958267216, Validation Loss: 0.003112938336266437\n",
      "Epoch [12519/20000], Training Loss: 0.00592788818666382, Validation Loss: 0.013148897194613405\n",
      "Epoch [12520/20000], Training Loss: 0.006772830967487867, Validation Loss: 0.018258388950666164\n",
      "Epoch [12521/20000], Training Loss: 0.013576836066412008, Validation Loss: 0.003602297567781219\n",
      "Epoch [12522/20000], Training Loss: 0.006482027912820091, Validation Loss: 0.0033662326808369253\n",
      "Epoch [12523/20000], Training Loss: 0.00813742488909546, Validation Loss: 0.009580378356921366\n",
      "Epoch [12524/20000], Training Loss: 0.00577919233650651, Validation Loss: 0.0040813556359541965\n",
      "Epoch [12525/20000], Training Loss: 0.005518224447899099, Validation Loss: 0.009003441248914723\n",
      "Epoch [12526/20000], Training Loss: 0.006795140481699491, Validation Loss: 0.004199016215403718\n",
      "Epoch [12527/20000], Training Loss: 0.005383811362012888, Validation Loss: 0.006743942894755146\n",
      "Epoch [12528/20000], Training Loss: 0.006623805751067786, Validation Loss: 0.00924483208051403\n",
      "Epoch [12529/20000], Training Loss: 0.011781651417030454, Validation Loss: 0.006709402273312146\n",
      "Epoch [12530/20000], Training Loss: 0.01539337166572555, Validation Loss: 0.006165254766272245\n",
      "Epoch [12531/20000], Training Loss: 0.008229692592846862, Validation Loss: 0.005530010343673749\n",
      "Epoch [12532/20000], Training Loss: 0.009071685599857509, Validation Loss: 0.003037850488013725\n",
      "Epoch [12533/20000], Training Loss: 0.028523214909260526, Validation Loss: 0.01007957459162867\n",
      "Epoch [12534/20000], Training Loss: 0.09049975574141302, Validation Loss: 0.2132515105119554\n",
      "Epoch [12535/20000], Training Loss: 0.0674543902298735, Validation Loss: 0.0751037187803465\n",
      "Epoch [12536/20000], Training Loss: 0.031653178785096055, Validation Loss: 0.01930528719859608\n",
      "Epoch [12537/20000], Training Loss: 0.015388189669465646, Validation Loss: 0.01038431299457443\n",
      "Epoch [12538/20000], Training Loss: 0.009240241700484018, Validation Loss: 0.008147251425855626\n",
      "Epoch [12539/20000], Training Loss: 0.006959232408137593, Validation Loss: 0.01061917000723562\n",
      "Epoch [12540/20000], Training Loss: 0.0080989352843192, Validation Loss: 0.005543334033323585\n",
      "Epoch [12541/20000], Training Loss: 0.005136016355079066, Validation Loss: 0.005289055580630832\n",
      "Epoch [12542/20000], Training Loss: 0.005125164607306942, Validation Loss: 0.004241380208051331\n",
      "Epoch [12543/20000], Training Loss: 0.006335539855561885, Validation Loss: 0.0037404223286492716\n",
      "Epoch [12544/20000], Training Loss: 0.004619677524325587, Validation Loss: 0.0035443518868955187\n",
      "Epoch [12545/20000], Training Loss: 0.0055100962033195955, Validation Loss: 0.0037594280170846366\n",
      "Epoch [12546/20000], Training Loss: 0.004646097804652527, Validation Loss: 0.003185716384645535\n",
      "Epoch [12547/20000], Training Loss: 0.005865535521609543, Validation Loss: 0.006536132495674403\n",
      "Epoch [12548/20000], Training Loss: 0.006062780835366409, Validation Loss: 0.004479665121380094\n",
      "Epoch [12549/20000], Training Loss: 0.005057463617829073, Validation Loss: 0.007339058053081057\n",
      "Epoch [12550/20000], Training Loss: 0.0057909642013588124, Validation Loss: 0.002812487609584683\n",
      "Epoch [12551/20000], Training Loss: 0.011428300319364228, Validation Loss: 0.003173084428064093\n",
      "Epoch [12552/20000], Training Loss: 0.0461913217516537, Validation Loss: 0.07160440793482589\n",
      "Epoch [12553/20000], Training Loss: 0.03237512962160898, Validation Loss: 0.007379969266756754\n",
      "Epoch [12554/20000], Training Loss: 0.02755977469496429, Validation Loss: 0.0142715226895364\n",
      "Epoch [12555/20000], Training Loss: 0.009442198239932103, Validation Loss: 0.03339656714345957\n",
      "Epoch [12556/20000], Training Loss: 0.022969221746149873, Validation Loss: 0.007634195988040509\n",
      "Epoch [12557/20000], Training Loss: 0.013787423692909735, Validation Loss: 0.009546555035610384\n",
      "Epoch [12558/20000], Training Loss: 0.009216765850266841, Validation Loss: 0.005092103919066305\n",
      "Epoch [12559/20000], Training Loss: 0.005275698301764871, Validation Loss: 0.006545324932566083\n",
      "Epoch [12560/20000], Training Loss: 0.006868015750244792, Validation Loss: 0.0037789481957588578\n",
      "Epoch [12561/20000], Training Loss: 0.007609894071070552, Validation Loss: 0.00502216726343769\n",
      "Epoch [12562/20000], Training Loss: 0.0048012400636382934, Validation Loss: 0.0037538733392596313\n",
      "Epoch [12563/20000], Training Loss: 0.006608500725373493, Validation Loss: 0.004115375790206599\n",
      "Epoch [12564/20000], Training Loss: 0.008664186513799774, Validation Loss: 0.003428777323279064\n",
      "Epoch [12565/20000], Training Loss: 0.006891743207233958, Validation Loss: 0.00321999721589756\n",
      "Epoch [12566/20000], Training Loss: 0.005533624252166192, Validation Loss: 0.009949438969280371\n",
      "Epoch [12567/20000], Training Loss: 0.013672038200768708, Validation Loss: 0.005028089927301542\n",
      "Epoch [12568/20000], Training Loss: 0.015159859045492599, Validation Loss: 0.005345425606979656\n",
      "Epoch [12569/20000], Training Loss: 0.01025792696184778, Validation Loss: 0.00853303403891914\n",
      "Epoch [12570/20000], Training Loss: 0.0044067435945700185, Validation Loss: 0.0034789099263434764\n",
      "Epoch [12571/20000], Training Loss: 0.0036799780552558203, Validation Loss: 0.005141075291281335\n",
      "Epoch [12572/20000], Training Loss: 0.0050453819504972285, Validation Loss: 0.004792946362873148\n",
      "Epoch [12573/20000], Training Loss: 0.007235606038843149, Validation Loss: 0.011380295496824797\n",
      "Epoch [12574/20000], Training Loss: 0.014988405821246229, Validation Loss: 0.005059731052142524\n",
      "Epoch [12575/20000], Training Loss: 0.003925948967142696, Validation Loss: 0.005550977698119043\n",
      "Epoch [12576/20000], Training Loss: 0.00713122765695776, Validation Loss: 0.0036080277859582566\n",
      "Epoch [12577/20000], Training Loss: 0.005474741897030201, Validation Loss: 0.004467978523052807\n",
      "Epoch [12578/20000], Training Loss: 0.005954080452543816, Validation Loss: 0.00747040071273984\n",
      "Epoch [12579/20000], Training Loss: 0.004343331547196223, Validation Loss: 0.005990329749693046\n",
      "Epoch [12580/20000], Training Loss: 0.006937270940397866, Validation Loss: 0.014395509314007475\n",
      "Epoch [12581/20000], Training Loss: 0.009493966711618538, Validation Loss: 0.003971772124909226\n",
      "Epoch [12582/20000], Training Loss: 0.0118996002586625, Validation Loss: 0.0036892623578199035\n",
      "Epoch [12583/20000], Training Loss: 0.005765938464069872, Validation Loss: 0.01288900962867144\n",
      "Epoch [12584/20000], Training Loss: 0.02158433106108402, Validation Loss: 0.014267829018405922\n",
      "Epoch [12585/20000], Training Loss: 0.019243604520202746, Validation Loss: 0.007849698672836374\n",
      "Epoch [12586/20000], Training Loss: 0.029907236721815673, Validation Loss: 0.04501518076878191\n",
      "Epoch [12587/20000], Training Loss: 0.02571319776844965, Validation Loss: 0.0068743728302901475\n",
      "Epoch [12588/20000], Training Loss: 0.012526922297703484, Validation Loss: 0.008453027114212221\n",
      "Epoch [12589/20000], Training Loss: 0.007129264076606238, Validation Loss: 0.004644972702148412\n",
      "Epoch [12590/20000], Training Loss: 0.010563720282984182, Validation Loss: 0.012269819382449245\n",
      "Epoch [12591/20000], Training Loss: 0.014038796582359023, Validation Loss: 0.014516911175243357\n",
      "Epoch [12592/20000], Training Loss: 0.053476132169765024, Validation Loss: 0.0443641931491194\n",
      "Epoch [12593/20000], Training Loss: 0.03962396214878287, Validation Loss: 0.037904374532006714\n",
      "Epoch [12594/20000], Training Loss: 0.04329413505287708, Validation Loss: 0.1876039917787336\n",
      "Epoch [12595/20000], Training Loss: 0.07465848605248279, Validation Loss: 0.09936256057246846\n",
      "Epoch [12596/20000], Training Loss: 0.05007681814354977, Validation Loss: 0.012403304085767428\n",
      "Epoch [12597/20000], Training Loss: 0.012426160359089928, Validation Loss: 0.009599472396075726\n",
      "Epoch [12598/20000], Training Loss: 0.008852616066828236, Validation Loss: 0.008044211339438334\n",
      "Epoch [12599/20000], Training Loss: 0.008347217625539218, Validation Loss: 0.008716787949067242\n",
      "Epoch [12600/20000], Training Loss: 0.008182016375940293, Validation Loss: 0.007462789438120255\n",
      "Epoch [12601/20000], Training Loss: 0.00756624569268232, Validation Loss: 0.007010999191282151\n",
      "Epoch [12602/20000], Training Loss: 0.006890582245042813, Validation Loss: 0.009193668418447487\n",
      "Epoch [12603/20000], Training Loss: 0.0079557125365162, Validation Loss: 0.007745921275207885\n",
      "Epoch [12604/20000], Training Loss: 0.006618446014337158, Validation Loss: 0.005894874228293442\n",
      "Epoch [12605/20000], Training Loss: 0.008652922343961629, Validation Loss: 0.017280392557690254\n",
      "Epoch [12606/20000], Training Loss: 0.008493608919837112, Validation Loss: 0.005334806523283727\n",
      "Epoch [12607/20000], Training Loss: 0.005202712129435635, Validation Loss: 0.005156850847795015\n",
      "Epoch [12608/20000], Training Loss: 0.00733364744831176, Validation Loss: 0.004702211645313322\n",
      "Epoch [12609/20000], Training Loss: 0.009681807178172417, Validation Loss: 0.005725828974261406\n",
      "Epoch [12610/20000], Training Loss: 0.008608440843610359, Validation Loss: 0.006150461844234607\n",
      "Epoch [12611/20000], Training Loss: 0.0048028918297079925, Validation Loss: 0.004372925420414374\n",
      "Epoch [12612/20000], Training Loss: 0.006211139403084027, Validation Loss: 0.003860230684040289\n",
      "Epoch [12613/20000], Training Loss: 0.005248387435650719, Validation Loss: 0.0038660147495517905\n",
      "Epoch [12614/20000], Training Loss: 0.009167077298375912, Validation Loss: 0.007300604696411028\n",
      "Epoch [12615/20000], Training Loss: 0.006624924603134527, Validation Loss: 0.003650445020671863\n",
      "Epoch [12616/20000], Training Loss: 0.005677034525433555, Validation Loss: 0.005215304119928987\n",
      "Epoch [12617/20000], Training Loss: 0.006256600151703294, Validation Loss: 0.0033800013939071505\n",
      "Epoch [12618/20000], Training Loss: 0.015740269875215467, Validation Loss: 0.008219551229038708\n",
      "Epoch [12619/20000], Training Loss: 0.01640277098340448, Validation Loss: 0.007871697090860965\n",
      "Epoch [12620/20000], Training Loss: 0.007664804274099879, Validation Loss: 0.004818932137758176\n",
      "Epoch [12621/20000], Training Loss: 0.005969525319025186, Validation Loss: 0.00420308448344388\n",
      "Epoch [12622/20000], Training Loss: 0.009564892979272242, Validation Loss: 0.01700791184025872\n",
      "Epoch [12623/20000], Training Loss: 0.014634165162403536, Validation Loss: 0.011087494714242243\n",
      "Epoch [12624/20000], Training Loss: 0.008752607461896591, Validation Loss: 0.0036623282847228567\n",
      "Epoch [12625/20000], Training Loss: 0.006940886145457625, Validation Loss: 0.0035207796442721572\n",
      "Epoch [12626/20000], Training Loss: 0.005736663413699716, Validation Loss: 0.013164174146701109\n",
      "Epoch [12627/20000], Training Loss: 0.0088190780210685, Validation Loss: 0.007398790812983082\n",
      "Epoch [12628/20000], Training Loss: 0.008107619914101503, Validation Loss: 0.0031213189178548157\n",
      "Epoch [12629/20000], Training Loss: 0.0043575404332451785, Validation Loss: 0.004263986895369142\n",
      "Epoch [12630/20000], Training Loss: 0.00468599602962578, Validation Loss: 0.006731400967692837\n",
      "Epoch [12631/20000], Training Loss: 0.006816778493105501, Validation Loss: 0.004153243085324577\n",
      "Epoch [12632/20000], Training Loss: 0.005879077002256443, Validation Loss: 0.006761274301571315\n",
      "Epoch [12633/20000], Training Loss: 0.008841270980026041, Validation Loss: 0.010029477157201785\n",
      "Epoch [12634/20000], Training Loss: 0.007075066219840664, Validation Loss: 0.007963129248090743\n",
      "Epoch [12635/20000], Training Loss: 0.00834687681552688, Validation Loss: 0.0050972129343855544\n",
      "Epoch [12636/20000], Training Loss: 0.008607119070802582, Validation Loss: 0.005342369315063219\n",
      "Epoch [12637/20000], Training Loss: 0.01194053287625658, Validation Loss: 0.007519730231527839\n",
      "Epoch [12638/20000], Training Loss: 0.00794168906057686, Validation Loss: 0.0074886022292568145\n",
      "Epoch [12639/20000], Training Loss: 0.009311780620399597, Validation Loss: 0.002671379541555195\n",
      "Epoch [12640/20000], Training Loss: 0.0038156229836334076, Validation Loss: 0.00313659556825639\n",
      "Epoch [12641/20000], Training Loss: 0.0037041771865915507, Validation Loss: 0.005448359059199349\n",
      "Epoch [12642/20000], Training Loss: 0.007841744102084445, Validation Loss: 0.002593673903707635\n",
      "Epoch [12643/20000], Training Loss: 0.010291294655481968, Validation Loss: 0.002498832671920224\n",
      "Epoch [12644/20000], Training Loss: 0.012832695742482818, Validation Loss: 0.045927059002614054\n",
      "Epoch [12645/20000], Training Loss: 0.019231993692561185, Validation Loss: 0.005367111799419685\n",
      "Epoch [12646/20000], Training Loss: 0.04067182634129755, Validation Loss: 0.0034437869078699024\n",
      "Epoch [12647/20000], Training Loss: 0.027102635101852814, Validation Loss: 0.0044114841785634394\n",
      "Epoch [12648/20000], Training Loss: 0.008600377424337629, Validation Loss: 0.00804605235402635\n",
      "Epoch [12649/20000], Training Loss: 0.009233293687949689, Validation Loss: 0.011701798765029383\n",
      "Epoch [12650/20000], Training Loss: 0.004853345124632012, Validation Loss: 0.005705105748997344\n",
      "Epoch [12651/20000], Training Loss: 0.0064942741223766075, Validation Loss: 0.0031437583409277586\n",
      "Epoch [12652/20000], Training Loss: 0.004383759772671121, Validation Loss: 0.010405856029271783\n",
      "Epoch [12653/20000], Training Loss: 0.008699998224204007, Validation Loss: 0.0036075181251037846\n",
      "Epoch [12654/20000], Training Loss: 0.007295709046177633, Validation Loss: 0.008222651670231633\n",
      "Epoch [12655/20000], Training Loss: 0.012485633467414508, Validation Loss: 0.006366446044012264\n",
      "Epoch [12656/20000], Training Loss: 0.011404093496301877, Validation Loss: 0.011484527015064876\n",
      "Epoch [12657/20000], Training Loss: 0.0119395535537998, Validation Loss: 0.006359451809527984\n",
      "Epoch [12658/20000], Training Loss: 0.01108622495043424, Validation Loss: 0.003378245451212284\n",
      "Epoch [12659/20000], Training Loss: 0.004741037318516257, Validation Loss: 0.003253158415718969\n",
      "Epoch [12660/20000], Training Loss: 0.003951560859734725, Validation Loss: 0.003239617617249938\n",
      "Epoch [12661/20000], Training Loss: 0.005645351467787155, Validation Loss: 0.002883909356715516\n",
      "Epoch [12662/20000], Training Loss: 0.006597253481491602, Validation Loss: 0.002903915798957054\n",
      "Epoch [12663/20000], Training Loss: 0.004586552650883797, Validation Loss: 0.014591709559612451\n",
      "Epoch [12664/20000], Training Loss: 0.015864771454029584, Validation Loss: 0.00648359501167712\n",
      "Epoch [12665/20000], Training Loss: 0.022570284405425665, Validation Loss: 0.02095437332252053\n",
      "Epoch [12666/20000], Training Loss: 0.017180640350228975, Validation Loss: 0.009519415589733984\n",
      "Epoch [12667/20000], Training Loss: 0.01601616532466973, Validation Loss: 0.004149706364468589\n",
      "Epoch [12668/20000], Training Loss: 0.006306319258458514, Validation Loss: 0.0032384303264702146\n",
      "Epoch [12669/20000], Training Loss: 0.006304247075604508, Validation Loss: 0.005153683554632542\n",
      "Epoch [12670/20000], Training Loss: 0.007523589307051485, Validation Loss: 0.006484738847510536\n",
      "Epoch [12671/20000], Training Loss: 0.007632676054851929, Validation Loss: 0.006171787452842636\n",
      "Epoch [12672/20000], Training Loss: 0.0042781384763657115, Validation Loss: 0.006270791571345009\n",
      "Epoch [12673/20000], Training Loss: 0.01397646585246548, Validation Loss: 0.00507283972485441\n",
      "Epoch [12674/20000], Training Loss: 0.007216125035483856, Validation Loss: 0.003311323283457211\n",
      "Epoch [12675/20000], Training Loss: 0.005635949504461938, Validation Loss: 0.004760274547866824\n",
      "Epoch [12676/20000], Training Loss: 0.003843763408700137, Validation Loss: 0.007664415555672528\n",
      "Epoch [12677/20000], Training Loss: 0.009931189105762834, Validation Loss: 0.007594599133503672\n",
      "Epoch [12678/20000], Training Loss: 0.008687486648861003, Validation Loss: 0.01701131747856063\n",
      "Epoch [12679/20000], Training Loss: 0.008806452323109884, Validation Loss: 0.00702284405254667\n",
      "Epoch [12680/20000], Training Loss: 0.013062720179732423, Validation Loss: 0.006584128969348869\n",
      "Epoch [12681/20000], Training Loss: 0.005506605562784118, Validation Loss: 0.004206709740157878\n",
      "Epoch [12682/20000], Training Loss: 0.004844651348581205, Validation Loss: 0.0071014672837605985\n",
      "Epoch [12683/20000], Training Loss: 0.006738334973176409, Validation Loss: 0.005713698256453118\n",
      "Epoch [12684/20000], Training Loss: 0.004316084225138184, Validation Loss: 0.0031425549715363233\n",
      "Epoch [12685/20000], Training Loss: 0.004916676802427641, Validation Loss: 0.005767708313082868\n",
      "Epoch [12686/20000], Training Loss: 0.010040536279640426, Validation Loss: 0.014846219829191145\n",
      "Epoch [12687/20000], Training Loss: 0.009025986634307108, Validation Loss: 0.005441018030481359\n",
      "Epoch [12688/20000], Training Loss: 0.006785882397577682, Validation Loss: 0.0025754862715976806\n",
      "Epoch [12689/20000], Training Loss: 0.008080087537434468, Validation Loss: 0.0024148459559358215\n",
      "Epoch [12690/20000], Training Loss: 0.0177952636037974, Validation Loss: 0.006270619612288491\n",
      "Epoch [12691/20000], Training Loss: 0.014833710651146248, Validation Loss: 0.010023246207800136\n",
      "Epoch [12692/20000], Training Loss: 0.025355467304637256, Validation Loss: 0.006482493043953192\n",
      "Epoch [12693/20000], Training Loss: 0.017508011475521407, Validation Loss: 0.00613611546749634\n",
      "Epoch [12694/20000], Training Loss: 0.009903596190270036, Validation Loss: 0.04489186482155105\n",
      "Epoch [12695/20000], Training Loss: 0.023421103376936765, Validation Loss: 0.031166669541087395\n",
      "Epoch [12696/20000], Training Loss: 0.020522809934586154, Validation Loss: 0.008307359693487473\n",
      "Epoch [12697/20000], Training Loss: 0.0080818322151442, Validation Loss: 0.00773440625543539\n",
      "Epoch [12698/20000], Training Loss: 0.017053164852776654, Validation Loss: 0.004759343439510693\n",
      "Epoch [12699/20000], Training Loss: 0.01012368636760844, Validation Loss: 0.006428561622832214\n",
      "Epoch [12700/20000], Training Loss: 0.006909763265866786, Validation Loss: 0.009538626963055614\n",
      "Epoch [12701/20000], Training Loss: 0.007797107453890411, Validation Loss: 0.0055022913107336535\n",
      "Epoch [12702/20000], Training Loss: 0.008256423281506744, Validation Loss: 0.04340279700651755\n",
      "Epoch [12703/20000], Training Loss: 0.01875397730327677, Validation Loss: 0.00580307085540913\n",
      "Epoch [12704/20000], Training Loss: 0.00577360618081002, Validation Loss: 0.010727056698247648\n",
      "Epoch [12705/20000], Training Loss: 0.011194504744707956, Validation Loss: 0.010028428625381431\n",
      "Epoch [12706/20000], Training Loss: 0.010602061358018286, Validation Loss: 0.008628621016238671\n",
      "Epoch [12707/20000], Training Loss: 0.006465390808963483, Validation Loss: 0.004922246209434077\n",
      "Epoch [12708/20000], Training Loss: 0.0051419305777277, Validation Loss: 0.005276252391423343\n",
      "Epoch [12709/20000], Training Loss: 0.0052150651167072026, Validation Loss: 0.00514961581259854\n",
      "Epoch [12710/20000], Training Loss: 0.0054398659944528815, Validation Loss: 0.013680188892103777\n",
      "Epoch [12711/20000], Training Loss: 0.010637022027107637, Validation Loss: 0.012351052751936566\n",
      "Epoch [12712/20000], Training Loss: 0.015300532238922773, Validation Loss: 0.049418744899006536\n",
      "Epoch [12713/20000], Training Loss: 0.04104260890328858, Validation Loss: 0.014357594337963877\n",
      "Epoch [12714/20000], Training Loss: 0.051093370062777206, Validation Loss: 0.11801214273587643\n",
      "Epoch [12715/20000], Training Loss: 0.03685678110715734, Validation Loss: 0.007258202588120964\n",
      "Epoch [12716/20000], Training Loss: 0.01961604342172255, Validation Loss: 0.011393292131223072\n",
      "Epoch [12717/20000], Training Loss: 0.009721151577001106, Validation Loss: 0.009162478104826732\n",
      "Epoch [12718/20000], Training Loss: 0.009431209677131847, Validation Loss: 0.005005228019074691\n",
      "Epoch [12719/20000], Training Loss: 0.006285099075674745, Validation Loss: 0.005957833106549515\n",
      "Epoch [12720/20000], Training Loss: 0.005879460014901789, Validation Loss: 0.004847147466924591\n",
      "Epoch [12721/20000], Training Loss: 0.005671218578104994, Validation Loss: 0.005889302798777862\n",
      "Epoch [12722/20000], Training Loss: 0.004960184031395849, Validation Loss: 0.004497216315290383\n",
      "Epoch [12723/20000], Training Loss: 0.006573443359229714, Validation Loss: 0.004197780818864365\n",
      "Epoch [12724/20000], Training Loss: 0.00718991633822595, Validation Loss: 0.00394702834734874\n",
      "Epoch [12725/20000], Training Loss: 0.00431751446837682, Validation Loss: 0.005987655620629084\n",
      "Epoch [12726/20000], Training Loss: 0.0062192964562979925, Validation Loss: 0.005115652944523065\n",
      "Epoch [12727/20000], Training Loss: 0.005039385778218275, Validation Loss: 0.004091675751063966\n",
      "Epoch [12728/20000], Training Loss: 0.004333952594996455, Validation Loss: 0.0037995556229069344\n",
      "Epoch [12729/20000], Training Loss: 0.004507637184620502, Validation Loss: 0.008012223726317253\n",
      "Epoch [12730/20000], Training Loss: 0.004998626120920692, Validation Loss: 0.016601465562761826\n",
      "Epoch [12731/20000], Training Loss: 0.008508194441674277, Validation Loss: 0.004651205463955778\n",
      "Epoch [12732/20000], Training Loss: 0.012927982392804032, Validation Loss: 0.007349264866499132\n",
      "Epoch [12733/20000], Training Loss: 0.009627984519576265, Validation Loss: 0.006479631740083798\n",
      "Epoch [12734/20000], Training Loss: 0.005948686621975087, Validation Loss: 0.005826274438309805\n",
      "Epoch [12735/20000], Training Loss: 0.007380329940393234, Validation Loss: 0.00413871942768245\n",
      "Epoch [12736/20000], Training Loss: 0.010924924381730048, Validation Loss: 0.003004899518730651\n",
      "Epoch [12737/20000], Training Loss: 0.005126957426130373, Validation Loss: 0.0049259295263531\n",
      "Epoch [12738/20000], Training Loss: 0.007436762360157445, Validation Loss: 0.003046343592651673\n",
      "Epoch [12739/20000], Training Loss: 0.02643282957225373, Validation Loss: 0.04558373136179817\n",
      "Epoch [12740/20000], Training Loss: 0.02601003439901563, Validation Loss: 0.01167693256303437\n",
      "Epoch [12741/20000], Training Loss: 0.011323687181434903, Validation Loss: 0.005037605690436716\n",
      "Epoch [12742/20000], Training Loss: 0.005622359474987856, Validation Loss: 0.004828959647381582\n",
      "Epoch [12743/20000], Training Loss: 0.004241250584267878, Validation Loss: 0.0040431475425098794\n",
      "Epoch [12744/20000], Training Loss: 0.006067465911785673, Validation Loss: 0.005638240425396036\n",
      "Epoch [12745/20000], Training Loss: 0.004740430415820031, Validation Loss: 0.003916667868387387\n",
      "Epoch [12746/20000], Training Loss: 0.005557532018948612, Validation Loss: 0.004370377086689002\n",
      "Epoch [12747/20000], Training Loss: 0.0046143994353380026, Validation Loss: 0.005777726587796549\n",
      "Epoch [12748/20000], Training Loss: 0.012012356599110976, Validation Loss: 0.0027592238164321054\n",
      "Epoch [12749/20000], Training Loss: 0.004772868896127745, Validation Loss: 0.014677998656789473\n",
      "Epoch [12750/20000], Training Loss: 0.03135599473171169, Validation Loss: 0.006338743365660093\n",
      "Epoch [12751/20000], Training Loss: 0.027502762355392667, Validation Loss: 0.042426117179275025\n",
      "Epoch [12752/20000], Training Loss: 0.027215141634639752, Validation Loss: 0.020335319094361433\n",
      "Epoch [12753/20000], Training Loss: 0.017396438144455066, Validation Loss: 0.017684150514436885\n",
      "Epoch [12754/20000], Training Loss: 0.017447274347668035, Validation Loss: 0.011491517565186118\n",
      "Epoch [12755/20000], Training Loss: 0.01189123013448677, Validation Loss: 0.007581018443033827\n",
      "Epoch [12756/20000], Training Loss: 0.011058034140400455, Validation Loss: 0.010539439368325734\n",
      "Epoch [12757/20000], Training Loss: 0.00821560193851058, Validation Loss: 0.004357762585332888\n",
      "Epoch [12758/20000], Training Loss: 0.013033352405597855, Validation Loss: 0.003864577592009612\n",
      "Epoch [12759/20000], Training Loss: 0.017712109724608842, Validation Loss: 0.00511553613618584\n",
      "Epoch [12760/20000], Training Loss: 0.007810732723945486, Validation Loss: 0.0039054060091854937\n",
      "Epoch [12761/20000], Training Loss: 0.008126710751509694, Validation Loss: 0.004005290566120883\n",
      "Epoch [12762/20000], Training Loss: 0.008397150715609314, Validation Loss: 0.012137485344080037\n",
      "Epoch [12763/20000], Training Loss: 0.010201749498914328, Validation Loss: 0.01661809568577155\n",
      "Epoch [12764/20000], Training Loss: 0.01016775709428787, Validation Loss: 0.008885312198409727\n",
      "Epoch [12765/20000], Training Loss: 0.017751869575420187, Validation Loss: 0.01087491161833246\n",
      "Epoch [12766/20000], Training Loss: 0.01123200865601705, Validation Loss: 0.01501351797123464\n",
      "Epoch [12767/20000], Training Loss: 0.007216376287812766, Validation Loss: 0.006144692358329199\n",
      "Epoch [12768/20000], Training Loss: 0.010995315782825596, Validation Loss: 0.008723459407779697\n",
      "Epoch [12769/20000], Training Loss: 0.01914336626110266, Validation Loss: 0.00982965393071871\n",
      "Epoch [12770/20000], Training Loss: 0.020197005774077428, Validation Loss: 0.006201023079175424\n",
      "Epoch [12771/20000], Training Loss: 0.013801570662898095, Validation Loss: 0.030372923375190503\n",
      "Epoch [12772/20000], Training Loss: 0.017922023685449467, Validation Loss: 0.015056460806826115\n",
      "Epoch [12773/20000], Training Loss: 0.01654223816668881, Validation Loss: 0.0046219326570735575\n",
      "Epoch [12774/20000], Training Loss: 0.01997017941071785, Validation Loss: 0.010980891543787103\n",
      "Epoch [12775/20000], Training Loss: 0.05588142629546512, Validation Loss: 0.10358715080079581\n",
      "Epoch [12776/20000], Training Loss: 0.04920597990193138, Validation Loss: 0.0480945462820661\n",
      "Epoch [12777/20000], Training Loss: 0.030728391812382534, Validation Loss: 0.02840145377142887\n",
      "Epoch [12778/20000], Training Loss: 0.012095432545590614, Validation Loss: 0.01109881726342467\n",
      "Epoch [12779/20000], Training Loss: 0.01933944464794227, Validation Loss: 0.02827933982820241\n",
      "Epoch [12780/20000], Training Loss: 0.020232891093266, Validation Loss: 0.02820656216837441\n",
      "Epoch [12781/20000], Training Loss: 0.01345522646027218, Validation Loss: 0.005881562270782784\n",
      "Epoch [12782/20000], Training Loss: 0.010142026643734425, Validation Loss: 0.020769933383308268\n",
      "Epoch [12783/20000], Training Loss: 0.01877281074536898, Validation Loss: 0.007015279238917823\n",
      "Epoch [12784/20000], Training Loss: 0.01083983044996525, Validation Loss: 0.00956050921636883\n",
      "Epoch [12785/20000], Training Loss: 0.00945405478705652, Validation Loss: 0.0071972532641260455\n",
      "Epoch [12786/20000], Training Loss: 0.006776005239446802, Validation Loss: 0.0060279793425641924\n",
      "Epoch [12787/20000], Training Loss: 0.00922091100437683, Validation Loss: 0.01441119148862364\n",
      "Epoch [12788/20000], Training Loss: 0.007931036483829043, Validation Loss: 0.004814335223695058\n",
      "Epoch [12789/20000], Training Loss: 0.005398764075445277, Validation Loss: 0.005396127566687677\n",
      "Epoch [12790/20000], Training Loss: 0.0048285728969079044, Validation Loss: 0.00464993595177735\n",
      "Epoch [12791/20000], Training Loss: 0.006222276445311893, Validation Loss: 0.009030402842370222\n",
      "Epoch [12792/20000], Training Loss: 0.0070627233536859945, Validation Loss: 0.004699152998089825\n",
      "Epoch [12793/20000], Training Loss: 0.0061429334683842695, Validation Loss: 0.0047157313020583\n",
      "Epoch [12794/20000], Training Loss: 0.007659383101521858, Validation Loss: 0.004056567905674261\n",
      "Epoch [12795/20000], Training Loss: 0.01620045755407773, Validation Loss: 0.004872827154173365\n",
      "Epoch [12796/20000], Training Loss: 0.012105872588498252, Validation Loss: 0.005267265335171649\n",
      "Epoch [12797/20000], Training Loss: 0.008975616115744092, Validation Loss: 0.011828888860268958\n",
      "Epoch [12798/20000], Training Loss: 0.012122694574437836, Validation Loss: 0.004309811983132151\n",
      "Epoch [12799/20000], Training Loss: 0.005521719912654329, Validation Loss: 0.008727459658400352\n",
      "Epoch [12800/20000], Training Loss: 0.007632387326469013, Validation Loss: 0.0066335625770364\n",
      "Epoch [12801/20000], Training Loss: 0.007922690580016933, Validation Loss: 0.006234403228729336\n",
      "Epoch [12802/20000], Training Loss: 0.005106968028745281, Validation Loss: 0.0038468535574298\n",
      "Epoch [12803/20000], Training Loss: 0.0048260956121209475, Validation Loss: 0.0033849880865050508\n",
      "Epoch [12804/20000], Training Loss: 0.005067528235875736, Validation Loss: 0.006085042869705555\n",
      "Epoch [12805/20000], Training Loss: 0.007465440833974364, Validation Loss: 0.010149874001862307\n",
      "Epoch [12806/20000], Training Loss: 0.008706377272444246, Validation Loss: 0.005720399493286875\n",
      "Epoch [12807/20000], Training Loss: 0.00980012862938955, Validation Loss: 0.003356030443340176\n",
      "Epoch [12808/20000], Training Loss: 0.004219622392806092, Validation Loss: 0.00690410863714216\n",
      "Epoch [12809/20000], Training Loss: 0.007921694268588908, Validation Loss: 0.003037935901082684\n",
      "Epoch [12810/20000], Training Loss: 0.003937937869133228, Validation Loss: 0.003154265299315284\n",
      "Epoch [12811/20000], Training Loss: 0.005954999983909407, Validation Loss: 0.0027201863525557917\n",
      "Epoch [12812/20000], Training Loss: 0.006907570624857077, Validation Loss: 0.007991694094136343\n",
      "Epoch [12813/20000], Training Loss: 0.009467446626981004, Validation Loss: 0.002956909197085419\n",
      "Epoch [12814/20000], Training Loss: 0.009144409406872, Validation Loss: 0.010938875214264954\n",
      "Epoch [12815/20000], Training Loss: 0.011062609077531047, Validation Loss: 0.005309238155713761\n",
      "Epoch [12816/20000], Training Loss: 0.008072692239823352, Validation Loss: 0.003479534699636133\n",
      "Epoch [12817/20000], Training Loss: 0.004546536089557256, Validation Loss: 0.0024170931980356564\n",
      "Epoch [12818/20000], Training Loss: 0.00728747648816546, Validation Loss: 0.02030491753819876\n",
      "Epoch [12819/20000], Training Loss: 0.02760269315747012, Validation Loss: 0.03767967383782369\n",
      "Epoch [12820/20000], Training Loss: 0.030065667073618636, Validation Loss: 0.1056642085313797\n",
      "Epoch [12821/20000], Training Loss: 0.04718893362691493, Validation Loss: 0.08509118686717036\n",
      "Epoch [12822/20000], Training Loss: 0.049210802944539216, Validation Loss: 0.013126117980001644\n",
      "Epoch [12823/20000], Training Loss: 0.010667150096976132, Validation Loss: 0.011211514038271682\n",
      "Epoch [12824/20000], Training Loss: 0.008448053712657253, Validation Loss: 0.015723955712912487\n",
      "Epoch [12825/20000], Training Loss: 0.009892344042392713, Validation Loss: 0.009578120279343239\n",
      "Epoch [12826/20000], Training Loss: 0.011334685126452573, Validation Loss: 0.005254615280145052\n",
      "Epoch [12827/20000], Training Loss: 0.004463362873398832, Validation Loss: 0.005231183215008579\n",
      "Epoch [12828/20000], Training Loss: 0.005205206365124988, Validation Loss: 0.0038886672984972392\n",
      "Epoch [12829/20000], Training Loss: 0.0043363275118671095, Validation Loss: 0.0036860454546570054\n",
      "Epoch [12830/20000], Training Loss: 0.004250540111180661, Validation Loss: 0.004225785920914811\n",
      "Epoch [12831/20000], Training Loss: 0.0062778592909710695, Validation Loss: 0.004146876393621467\n",
      "Epoch [12832/20000], Training Loss: 0.005874908789077641, Validation Loss: 0.004207605851969934\n",
      "Epoch [12833/20000], Training Loss: 0.004903138362351456, Validation Loss: 0.005658896743755447\n",
      "Epoch [12834/20000], Training Loss: 0.00925683938632054, Validation Loss: 0.004001532764608636\n",
      "Epoch [12835/20000], Training Loss: 0.014618277655764749, Validation Loss: 0.005064461969498423\n",
      "Epoch [12836/20000], Training Loss: 0.02015353453498392, Validation Loss: 0.012705222517979564\n",
      "Epoch [12837/20000], Training Loss: 0.019242894316838437, Validation Loss: 0.0085322172989046\n",
      "Epoch [12838/20000], Training Loss: 0.03909525653775096, Validation Loss: 0.008084877370710128\n",
      "Epoch [12839/20000], Training Loss: 0.013776421606183118, Validation Loss: 0.009346151856356804\n",
      "Epoch [12840/20000], Training Loss: 0.008743515353541755, Validation Loss: 0.00425379662281752\n",
      "Epoch [12841/20000], Training Loss: 0.005987262852223856, Validation Loss: 0.004827786537020984\n",
      "Epoch [12842/20000], Training Loss: 0.008430935818393794, Validation Loss: 0.003710628236632374\n",
      "Epoch [12843/20000], Training Loss: 0.005745677585114858, Validation Loss: 0.004213235941948922\n",
      "Epoch [12844/20000], Training Loss: 0.00547204739164694, Validation Loss: 0.003960242526525397\n",
      "Epoch [12845/20000], Training Loss: 0.006695754056896216, Validation Loss: 0.0061727004869323665\n",
      "Epoch [12846/20000], Training Loss: 0.007853760302948234, Validation Loss: 0.004783313307433852\n",
      "Epoch [12847/20000], Training Loss: 0.006608896223562104, Validation Loss: 0.004394335329142304\n",
      "Epoch [12848/20000], Training Loss: 0.006727203423257119, Validation Loss: 0.004798820824313614\n",
      "Epoch [12849/20000], Training Loss: 0.006574057824244457, Validation Loss: 0.004302112243862106\n",
      "Epoch [12850/20000], Training Loss: 0.006558572398976789, Validation Loss: 0.003845215468583011\n",
      "Epoch [12851/20000], Training Loss: 0.0088434654836809, Validation Loss: 0.00599752331250057\n",
      "Epoch [12852/20000], Training Loss: 0.00640549620168583, Validation Loss: 0.01625703160089285\n",
      "Epoch [12853/20000], Training Loss: 0.012508431591413682, Validation Loss: 0.031365766590761686\n",
      "Epoch [12854/20000], Training Loss: 0.01066159681301672, Validation Loss: 0.007412425266545191\n",
      "Epoch [12855/20000], Training Loss: 0.010982281136551007, Validation Loss: 0.005607133645516983\n",
      "Epoch [12856/20000], Training Loss: 0.004239904326823307, Validation Loss: 0.0029693665940462616\n",
      "Epoch [12857/20000], Training Loss: 0.005729369927004362, Validation Loss: 0.0032899374229441287\n",
      "Epoch [12858/20000], Training Loss: 0.005707630869177852, Validation Loss: 0.008693261563873225\n",
      "Epoch [12859/20000], Training Loss: 0.018548700099147806, Validation Loss: 0.013777053752919852\n",
      "Epoch [12860/20000], Training Loss: 0.01575127790212199, Validation Loss: 0.012844899315240355\n",
      "Epoch [12861/20000], Training Loss: 0.013224399198017116, Validation Loss: 0.007022152383497777\n",
      "Epoch [12862/20000], Training Loss: 0.012320739944698289, Validation Loss: 0.0033431879515382385\n",
      "Epoch [12863/20000], Training Loss: 0.012000996168353595, Validation Loss: 0.0040217715635618234\n",
      "Epoch [12864/20000], Training Loss: 0.012850648981319475, Validation Loss: 0.004897869731192169\n",
      "Epoch [12865/20000], Training Loss: 0.01249988551093598, Validation Loss: 0.011302378887359663\n",
      "Epoch [12866/20000], Training Loss: 0.012286564971353593, Validation Loss: 0.003513200990214825\n",
      "Epoch [12867/20000], Training Loss: 0.006107672644272303, Validation Loss: 0.003611700388400355\n",
      "Epoch [12868/20000], Training Loss: 0.004469249126226974, Validation Loss: 0.004886742825856264\n",
      "Epoch [12869/20000], Training Loss: 0.004419618084544449, Validation Loss: 0.002523223582572192\n",
      "Epoch [12870/20000], Training Loss: 0.00391549118752924, Validation Loss: 0.0029928865874252\n",
      "Epoch [12871/20000], Training Loss: 0.004408371141782449, Validation Loss: 0.0028017606383627935\n",
      "Epoch [12872/20000], Training Loss: 0.004093355979421176, Validation Loss: 0.014509373944480026\n",
      "Epoch [12873/20000], Training Loss: 0.01166349347788907, Validation Loss: 0.0037306161147014905\n",
      "Epoch [12874/20000], Training Loss: 0.005642739440580564, Validation Loss: 0.01028642102904444\n",
      "Epoch [12875/20000], Training Loss: 0.013907180089777935, Validation Loss: 0.01346344912864339\n",
      "Epoch [12876/20000], Training Loss: 0.016261567174333486, Validation Loss: 0.036275775464763224\n",
      "Epoch [12877/20000], Training Loss: 0.02007408771169139, Validation Loss: 0.007775298050553658\n",
      "Epoch [12878/20000], Training Loss: 0.019003353480781828, Validation Loss: 0.006706789631839476\n",
      "Epoch [12879/20000], Training Loss: 0.0100089996870208, Validation Loss: 0.0070532001257771326\n",
      "Epoch [12880/20000], Training Loss: 0.005628872017301286, Validation Loss: 0.00306587620470365\n",
      "Epoch [12881/20000], Training Loss: 0.00663615669938216, Validation Loss: 0.0036907404363729157\n",
      "Epoch [12882/20000], Training Loss: 0.014237705573447914, Validation Loss: 0.005157398892144219\n",
      "Epoch [12883/20000], Training Loss: 0.014269836430685245, Validation Loss: 0.00448514408931341\n",
      "Epoch [12884/20000], Training Loss: 0.011178838888907552, Validation Loss: 0.019772572563261242\n",
      "Epoch [12885/20000], Training Loss: 0.014614089645744701, Validation Loss: 0.020393438866449287\n",
      "Epoch [12886/20000], Training Loss: 0.02290332130308213, Validation Loss: 0.01249845909792364\n",
      "Epoch [12887/20000], Training Loss: 0.03241769710548072, Validation Loss: 0.009300117897293856\n",
      "Epoch [12888/20000], Training Loss: 0.030683446729588986, Validation Loss: 0.01602052050698382\n",
      "Epoch [12889/20000], Training Loss: 0.052030063378749346, Validation Loss: 0.06798968870873699\n",
      "Epoch [12890/20000], Training Loss: 0.08358785886749891, Validation Loss: 0.03591678138140456\n",
      "Epoch [12891/20000], Training Loss: 0.06285401200563813, Validation Loss: 0.0436906134855235\n",
      "Epoch [12892/20000], Training Loss: 0.05836295620870909, Validation Loss: 0.023153538391089308\n",
      "Epoch [12893/20000], Training Loss: 0.018581958604045212, Validation Loss: 0.014856564756650061\n",
      "Epoch [12894/20000], Training Loss: 0.01124485271118049, Validation Loss: 0.009673684297029575\n",
      "Epoch [12895/20000], Training Loss: 0.009026456075454397, Validation Loss: 0.00793800216937208\n",
      "Epoch [12896/20000], Training Loss: 0.006881992671456568, Validation Loss: 0.0046860781850292755\n",
      "Epoch [12897/20000], Training Loss: 0.0066377974227569735, Validation Loss: 0.00394802854594088\n",
      "Epoch [12898/20000], Training Loss: 0.005932688149706726, Validation Loss: 0.007134166352622222\n",
      "Epoch [12899/20000], Training Loss: 0.013725812020173644, Validation Loss: 0.012193096727060038\n",
      "Epoch [12900/20000], Training Loss: 0.013034934439929202, Validation Loss: 0.004777194327873856\n",
      "Epoch [12901/20000], Training Loss: 0.01276664559398861, Validation Loss: 0.0061881712559243495\n",
      "Epoch [12902/20000], Training Loss: 0.010797425019388487, Validation Loss: 0.008744575068519111\n",
      "Epoch [12903/20000], Training Loss: 0.007919281255453825, Validation Loss: 0.01485412133809356\n",
      "Epoch [12904/20000], Training Loss: 0.010446055430553056, Validation Loss: 0.003780674370022195\n",
      "Epoch [12905/20000], Training Loss: 0.007202068702879062, Validation Loss: 0.005619043308720555\n",
      "Epoch [12906/20000], Training Loss: 0.00682148699291117, Validation Loss: 0.005314461770648141\n",
      "Epoch [12907/20000], Training Loss: 0.005673327936425007, Validation Loss: 0.002850330532361056\n",
      "Epoch [12908/20000], Training Loss: 0.00444992216580431, Validation Loss: 0.0031766299767598738\n",
      "Epoch [12909/20000], Training Loss: 0.009892542497254908, Validation Loss: 0.007947245984244742\n",
      "Epoch [12910/20000], Training Loss: 0.013234499218274973, Validation Loss: 0.019910762897522107\n",
      "Epoch [12911/20000], Training Loss: 0.014780505484980364, Validation Loss: 0.012402505723442354\n",
      "Epoch [12912/20000], Training Loss: 0.02144366471475223, Validation Loss: 0.011709109712382672\n",
      "Epoch [12913/20000], Training Loss: 0.009220886808049857, Validation Loss: 0.005621495954658583\n",
      "Epoch [12914/20000], Training Loss: 0.0068237294388901705, Validation Loss: 0.005608567571164664\n",
      "Epoch [12915/20000], Training Loss: 0.004945124742724667, Validation Loss: 0.003389104736044146\n",
      "Epoch [12916/20000], Training Loss: 0.009789328377727153, Validation Loss: 0.0038304952949153354\n",
      "Epoch [12917/20000], Training Loss: 0.0077863281860897715, Validation Loss: 0.00618885349417602\n",
      "Epoch [12918/20000], Training Loss: 0.008014270664196894, Validation Loss: 0.0046951899822594055\n",
      "Epoch [12919/20000], Training Loss: 0.008984200124229704, Validation Loss: 0.007055604414071175\n",
      "Epoch [12920/20000], Training Loss: 0.0061427763352445, Validation Loss: 0.011673560812543242\n",
      "Epoch [12921/20000], Training Loss: 0.00910283079013295, Validation Loss: 0.0025976184256931234\n",
      "Epoch [12922/20000], Training Loss: 0.005931921544158415, Validation Loss: 0.004204475219065021\n",
      "Epoch [12923/20000], Training Loss: 0.004189480753016791, Validation Loss: 0.022043688508892403\n",
      "Epoch [12924/20000], Training Loss: 0.010328179175433303, Validation Loss: 0.008212043624975454\n",
      "Epoch [12925/20000], Training Loss: 0.02598559624961386, Validation Loss: 0.01901616322726442\n",
      "Epoch [12926/20000], Training Loss: 0.028189962773987127, Validation Loss: 0.04533320912133933\n",
      "Epoch [12927/20000], Training Loss: 0.045717562168387564, Validation Loss: 0.061063574893639574\n",
      "Epoch [12928/20000], Training Loss: 0.0406225471353017, Validation Loss: 0.008344079225155707\n",
      "Epoch [12929/20000], Training Loss: 0.01574879271044795, Validation Loss: 0.019068642262417206\n",
      "Epoch [12930/20000], Training Loss: 0.01138595598500355, Validation Loss: 0.006665937765319541\n",
      "Epoch [12931/20000], Training Loss: 0.007327150309549779, Validation Loss: 0.008209999296330548\n",
      "Epoch [12932/20000], Training Loss: 0.006668308695744989, Validation Loss: 0.005679048660795161\n",
      "Epoch [12933/20000], Training Loss: 0.006429083229574774, Validation Loss: 0.0052098540362197\n",
      "Epoch [12934/20000], Training Loss: 0.007356915649974586, Validation Loss: 0.004715220697611195\n",
      "Epoch [12935/20000], Training Loss: 0.007811183159771774, Validation Loss: 0.0044686552926610345\n",
      "Epoch [12936/20000], Training Loss: 0.004319122234863114, Validation Loss: 0.006304976694638397\n",
      "Epoch [12937/20000], Training Loss: 0.0060468801311799325, Validation Loss: 0.005886421853639539\n",
      "Epoch [12938/20000], Training Loss: 0.005763867470834937, Validation Loss: 0.0056398698463746245\n",
      "Epoch [12939/20000], Training Loss: 0.008755648504510256, Validation Loss: 0.013761152509072108\n",
      "Epoch [12940/20000], Training Loss: 0.005392704620524975, Validation Loss: 0.004840524426908489\n",
      "Epoch [12941/20000], Training Loss: 0.006719799479469657, Validation Loss: 0.0051894451172387335\n",
      "Epoch [12942/20000], Training Loss: 0.01045752913965511, Validation Loss: 0.00456319566894641\n",
      "Epoch [12943/20000], Training Loss: 0.018298478252420734, Validation Loss: 0.012919685964334349\n",
      "Epoch [12944/20000], Training Loss: 0.008896378517550017, Validation Loss: 0.015815016703493865\n",
      "Epoch [12945/20000], Training Loss: 0.007757440490032812, Validation Loss: 0.00644686575896815\n",
      "Epoch [12946/20000], Training Loss: 0.0054264917181561555, Validation Loss: 0.004845881975202003\n",
      "Epoch [12947/20000], Training Loss: 0.010013257223493253, Validation Loss: 0.003728136591153738\n",
      "Epoch [12948/20000], Training Loss: 0.024158375484863166, Validation Loss: 0.021080943713172928\n",
      "Epoch [12949/20000], Training Loss: 0.01766191527297321, Validation Loss: 0.009926474331671946\n",
      "Epoch [12950/20000], Training Loss: 0.023270011124717018, Validation Loss: 0.008181821152967521\n",
      "Epoch [12951/20000], Training Loss: 0.015802719183349318, Validation Loss: 0.01647392254152982\n",
      "Epoch [12952/20000], Training Loss: 0.010174749092714461, Validation Loss: 0.007254534806439291\n",
      "Epoch [12953/20000], Training Loss: 0.020839197063781154, Validation Loss: 0.016701290397536792\n",
      "Epoch [12954/20000], Training Loss: 0.03114743271934068, Validation Loss: 0.057420462596991614\n",
      "Epoch [12955/20000], Training Loss: 0.02947459800218764, Validation Loss: 0.0096446476043265\n",
      "Epoch [12956/20000], Training Loss: 0.015175915889163403, Validation Loss: 0.00750127697414931\n",
      "Epoch [12957/20000], Training Loss: 0.012335905200286237, Validation Loss: 0.010427404858887712\n",
      "Epoch [12958/20000], Training Loss: 0.007574450381071074, Validation Loss: 0.013665286667056924\n",
      "Epoch [12959/20000], Training Loss: 0.00994422796481688, Validation Loss: 0.004692648583881821\n",
      "Epoch [12960/20000], Training Loss: 0.007369061433044928, Validation Loss: 0.005944402592506647\n",
      "Epoch [12961/20000], Training Loss: 0.012585616446033652, Validation Loss: 0.004443826315634526\n",
      "Epoch [12962/20000], Training Loss: 0.01768473242866873, Validation Loss: 0.005457180956796037\n",
      "Epoch [12963/20000], Training Loss: 0.0055963809385762685, Validation Loss: 0.00964781623918449\n",
      "Epoch [12964/20000], Training Loss: 0.008041228928050259, Validation Loss: 0.00485223671228141\n",
      "Epoch [12965/20000], Training Loss: 0.007428300309194518, Validation Loss: 0.004849675704341345\n",
      "Epoch [12966/20000], Training Loss: 0.005760122791148855, Validation Loss: 0.0075985993238865445\n",
      "Epoch [12967/20000], Training Loss: 0.0045806816945384654, Validation Loss: 0.0036171192315634926\n",
      "Epoch [12968/20000], Training Loss: 0.007396313623758033, Validation Loss: 0.005695461018309597\n",
      "Epoch [12969/20000], Training Loss: 0.010257124432005835, Validation Loss: 0.005191546488094089\n",
      "Epoch [12970/20000], Training Loss: 0.017535986617856127, Validation Loss: 0.005073500733455408\n",
      "Epoch [12971/20000], Training Loss: 0.02625325777834015, Validation Loss: 0.005057282178589568\n",
      "Epoch [12972/20000], Training Loss: 0.011688376178166695, Validation Loss: 0.004950223769828962\n",
      "Epoch [12973/20000], Training Loss: 0.015462834965936574, Validation Loss: 0.005122164787077769\n",
      "Epoch [12974/20000], Training Loss: 0.012720554162764788, Validation Loss: 0.013550035446873008\n",
      "Epoch [12975/20000], Training Loss: 0.011437976061513058, Validation Loss: 0.013217762221220905\n",
      "Epoch [12976/20000], Training Loss: 0.016330071320096198, Validation Loss: 0.007349955236301347\n",
      "Epoch [12977/20000], Training Loss: 0.010362858033788922, Validation Loss: 0.030838250016770985\n",
      "Epoch [12978/20000], Training Loss: 0.01595362078036747, Validation Loss: 0.004146917460965531\n",
      "Epoch [12979/20000], Training Loss: 0.008116089287097696, Validation Loss: 0.00426549871202805\n",
      "Epoch [12980/20000], Training Loss: 0.007445050120752837, Validation Loss: 0.005271540396762475\n",
      "Epoch [12981/20000], Training Loss: 0.0044919738888893335, Validation Loss: 0.003955261392566253\n",
      "Epoch [12982/20000], Training Loss: 0.004649298959910604, Validation Loss: 0.003264021460174555\n",
      "Epoch [12983/20000], Training Loss: 0.005481036172048854, Validation Loss: 0.007720362998879539\n",
      "Epoch [12984/20000], Training Loss: 0.006890306795997146, Validation Loss: 0.005811150477086358\n",
      "Epoch [12985/20000], Training Loss: 0.010040605502061746, Validation Loss: 0.058583113737118504\n",
      "Epoch [12986/20000], Training Loss: 0.0189039716559429, Validation Loss: 0.020460558345923006\n",
      "Epoch [12987/20000], Training Loss: 0.014550969493882349, Validation Loss: 0.028716241962108313\n",
      "Epoch [12988/20000], Training Loss: 0.014018564297917433, Validation Loss: 0.022898822741675318\n",
      "Epoch [12989/20000], Training Loss: 0.019654692504056066, Validation Loss: 0.004171732571823635\n",
      "Epoch [12990/20000], Training Loss: 0.01842262476254421, Validation Loss: 0.04734916530183649\n",
      "Epoch [12991/20000], Training Loss: 0.022140048444530942, Validation Loss: 0.004176115932862347\n",
      "Epoch [12992/20000], Training Loss: 0.019963029347958842, Validation Loss: 0.019189226208934786\n",
      "Epoch [12993/20000], Training Loss: 0.015795529157912824, Validation Loss: 0.01841541384938645\n",
      "Epoch [12994/20000], Training Loss: 0.01530404544817949, Validation Loss: 0.009676738403762946\n",
      "Epoch [12995/20000], Training Loss: 0.009558483451006137, Validation Loss: 0.004175955599438096\n",
      "Epoch [12996/20000], Training Loss: 0.006309481750317251, Validation Loss: 0.008437801601591641\n",
      "Epoch [12997/20000], Training Loss: 0.005684316356824378, Validation Loss: 0.00872532731376298\n",
      "Epoch [12998/20000], Training Loss: 0.006732582152056109, Validation Loss: 0.005882205425505406\n",
      "Epoch [12999/20000], Training Loss: 0.005225799969593936, Validation Loss: 0.003548602896331537\n",
      "Epoch [13000/20000], Training Loss: 0.005318550237071966, Validation Loss: 0.003940329561006349\n",
      "Epoch [13001/20000], Training Loss: 0.0045712067554656444, Validation Loss: 0.003500508425755048\n",
      "Epoch [13002/20000], Training Loss: 0.0077619960190661785, Validation Loss: 0.005091569355389173\n",
      "Epoch [13003/20000], Training Loss: 0.009503311691722567, Validation Loss: 0.00550204480362128\n",
      "Epoch [13004/20000], Training Loss: 0.0034897924960465127, Validation Loss: 0.0027061207589267966\n",
      "Epoch [13005/20000], Training Loss: 0.005450803739742176, Validation Loss: 0.0033909360614491186\n",
      "Epoch [13006/20000], Training Loss: 0.005954214454115052, Validation Loss: 0.0030396572729998184\n",
      "Epoch [13007/20000], Training Loss: 0.008558513805318009, Validation Loss: 0.003447406844563667\n",
      "Epoch [13008/20000], Training Loss: 0.009845310692859195, Validation Loss: 0.010980905990816072\n",
      "Epoch [13009/20000], Training Loss: 0.014447934796667792, Validation Loss: 0.01375991295370633\n",
      "Epoch [13010/20000], Training Loss: 0.007143349108358977, Validation Loss: 0.005069785228055171\n",
      "Epoch [13011/20000], Training Loss: 0.007456423783358852, Validation Loss: 0.004147277594586488\n",
      "Epoch [13012/20000], Training Loss: 0.005326006836444971, Validation Loss: 0.004842893636920158\n",
      "Epoch [13013/20000], Training Loss: 0.0076542018622214, Validation Loss: 0.008571848844428282\n",
      "Epoch [13014/20000], Training Loss: 0.00507971084568583, Validation Loss: 0.004499150387406051\n",
      "Epoch [13015/20000], Training Loss: 0.006742571938697698, Validation Loss: 0.004143031145586308\n",
      "Epoch [13016/20000], Training Loss: 0.00909381861492875, Validation Loss: 0.009472114917490464\n",
      "Epoch [13017/20000], Training Loss: 0.007476302300131822, Validation Loss: 0.0037778169018568724\n",
      "Epoch [13018/20000], Training Loss: 0.008730202622765708, Validation Loss: 0.005837082518663043\n",
      "Epoch [13019/20000], Training Loss: 0.00398905047999522, Validation Loss: 0.003353582009103105\n",
      "Epoch [13020/20000], Training Loss: 0.008298252679066666, Validation Loss: 0.002402833085654318\n",
      "Epoch [13021/20000], Training Loss: 0.0085756592909872, Validation Loss: 0.017658785755434638\n",
      "Epoch [13022/20000], Training Loss: 0.008280264080375284, Validation Loss: 0.007481439678415873\n",
      "Epoch [13023/20000], Training Loss: 0.015996870966254, Validation Loss: 0.005071586256366041\n",
      "Epoch [13024/20000], Training Loss: 0.03488912649444436, Validation Loss: 0.012076708018480329\n",
      "Epoch [13025/20000], Training Loss: 0.022533260153457895, Validation Loss: 0.007451504377392837\n",
      "Epoch [13026/20000], Training Loss: 0.013530886906664819, Validation Loss: 0.0443457963597861\n",
      "Epoch [13027/20000], Training Loss: 0.0199624957252971, Validation Loss: 0.015620316509595358\n",
      "Epoch [13028/20000], Training Loss: 0.010198660512807172, Validation Loss: 0.0065112367806159045\n",
      "Epoch [13029/20000], Training Loss: 0.00671614140238879, Validation Loss: 0.0033961852035580315\n",
      "Epoch [13030/20000], Training Loss: 0.007128675815433131, Validation Loss: 0.006941263364313143\n",
      "Epoch [13031/20000], Training Loss: 0.00500395725040497, Validation Loss: 0.009300839410279593\n",
      "Epoch [13032/20000], Training Loss: 0.008798821735711369, Validation Loss: 0.0029436041787736906\n",
      "Epoch [13033/20000], Training Loss: 0.006015390302532556, Validation Loss: 0.007364327236368849\n",
      "Epoch [13034/20000], Training Loss: 0.0077235301479439454, Validation Loss: 0.003188109346719362\n",
      "Epoch [13035/20000], Training Loss: 0.004568509190383858, Validation Loss: 0.00436404978711583\n",
      "Epoch [13036/20000], Training Loss: 0.009417740187408137, Validation Loss: 0.004435642367914637\n",
      "Epoch [13037/20000], Training Loss: 0.0081389295219846, Validation Loss: 0.00684639371100967\n",
      "Epoch [13038/20000], Training Loss: 0.007760408530592551, Validation Loss: 0.00804406450211305\n",
      "Epoch [13039/20000], Training Loss: 0.007836904321655831, Validation Loss: 0.005969573939073111\n",
      "Epoch [13040/20000], Training Loss: 0.013426754847647058, Validation Loss: 0.05707588974837563\n",
      "Epoch [13041/20000], Training Loss: 0.034320993924146545, Validation Loss: 0.007856852575894127\n",
      "Epoch [13042/20000], Training Loss: 0.012890612092763019, Validation Loss: 0.005678922330446312\n",
      "Epoch [13043/20000], Training Loss: 0.012030593370062499, Validation Loss: 0.011200763602196113\n",
      "Epoch [13044/20000], Training Loss: 0.011957512250319269, Validation Loss: 0.03216982773444954\n",
      "Epoch [13045/20000], Training Loss: 0.020172565534137123, Validation Loss: 0.032722791390770646\n",
      "Epoch [13046/20000], Training Loss: 0.03684151904391391, Validation Loss: 0.051337236495289824\n",
      "Epoch [13047/20000], Training Loss: 0.0312679037464006, Validation Loss: 0.009512508893489471\n",
      "Epoch [13048/20000], Training Loss: 0.009947336021079016, Validation Loss: 0.007999556843101021\n",
      "Epoch [13049/20000], Training Loss: 0.007662071505494948, Validation Loss: 0.005797220617166333\n",
      "Epoch [13050/20000], Training Loss: 0.0059265195013722405, Validation Loss: 0.004489297545790448\n",
      "Epoch [13051/20000], Training Loss: 0.005003579364191475, Validation Loss: 0.004063019975944826\n",
      "Epoch [13052/20000], Training Loss: 0.00524792382670317, Validation Loss: 0.003645872423118947\n",
      "Epoch [13053/20000], Training Loss: 0.004551397955635496, Validation Loss: 0.006846984991824456\n",
      "Epoch [13054/20000], Training Loss: 0.010111155255425988, Validation Loss: 0.006702210141692636\n",
      "Epoch [13055/20000], Training Loss: 0.010595667834942495, Validation Loss: 0.010325145056640395\n",
      "Epoch [13056/20000], Training Loss: 0.017702795722081128, Validation Loss: 0.0068542869299242125\n",
      "Epoch [13057/20000], Training Loss: 0.007977049711501292, Validation Loss: 0.004938110842289234\n",
      "Epoch [13058/20000], Training Loss: 0.006790083749885005, Validation Loss: 0.006855620152925164\n",
      "Epoch [13059/20000], Training Loss: 0.008079759360823249, Validation Loss: 0.004628349538287628\n",
      "Epoch [13060/20000], Training Loss: 0.006435151034695862, Validation Loss: 0.006062363982303103\n",
      "Epoch [13061/20000], Training Loss: 0.009713508508866653, Validation Loss: 0.0028993806873570194\n",
      "Epoch [13062/20000], Training Loss: 0.017262440698686987, Validation Loss: 0.005759932974957646\n",
      "Epoch [13063/20000], Training Loss: 0.008248548318598685, Validation Loss: 0.01961915154870475\n",
      "Epoch [13064/20000], Training Loss: 0.008990706868644338, Validation Loss: 0.0037144926292447572\n",
      "Epoch [13065/20000], Training Loss: 0.004999519442208111, Validation Loss: 0.0036065411465549297\n",
      "Epoch [13066/20000], Training Loss: 0.004775797635115201, Validation Loss: 0.002885740603873493\n",
      "Epoch [13067/20000], Training Loss: 0.003719442114483432, Validation Loss: 0.005492009196849779\n",
      "Epoch [13068/20000], Training Loss: 0.0037240223464323208, Validation Loss: 0.008095492861144749\n",
      "Epoch [13069/20000], Training Loss: 0.005798587657669226, Validation Loss: 0.017899941349748767\n",
      "Epoch [13070/20000], Training Loss: 0.005239560953798771, Validation Loss: 0.010911917972756717\n",
      "Epoch [13071/20000], Training Loss: 0.010059804462798638, Validation Loss: 0.019080768499569394\n",
      "Epoch [13072/20000], Training Loss: 0.010379739690896323, Validation Loss: 0.004934815161971885\n",
      "Epoch [13073/20000], Training Loss: 0.0051531907571091195, Validation Loss: 0.006508351562128415\n",
      "Epoch [13074/20000], Training Loss: 0.007579647451491057, Validation Loss: 0.0036901831304051924\n",
      "Epoch [13075/20000], Training Loss: 0.010826876915026722, Validation Loss: 0.007466264892264414\n",
      "Epoch [13076/20000], Training Loss: 0.0066580778979446874, Validation Loss: 0.004892496260493674\n",
      "Epoch [13077/20000], Training Loss: 0.005826327936152406, Validation Loss: 0.025164983102441436\n",
      "Epoch [13078/20000], Training Loss: 0.009602331156153403, Validation Loss: 0.0033430675359963197\n",
      "Epoch [13079/20000], Training Loss: 0.010847817019047008, Validation Loss: 0.020484216511275882\n",
      "Epoch [13080/20000], Training Loss: 0.055363605942212804, Validation Loss: 0.12940940260887146\n",
      "Epoch [13081/20000], Training Loss: 0.02708530191531671, Validation Loss: 0.06801955189034474\n",
      "Epoch [13082/20000], Training Loss: 0.023887944233138114, Validation Loss: 0.007032566134215911\n",
      "Epoch [13083/20000], Training Loss: 0.008480839950997117, Validation Loss: 0.007332711102780617\n",
      "Epoch [13084/20000], Training Loss: 0.01301988576804953, Validation Loss: 0.008048049336910676\n",
      "Epoch [13085/20000], Training Loss: 0.007320184421717256, Validation Loss: 0.00873602636638233\n",
      "Epoch [13086/20000], Training Loss: 0.010059221958438716, Validation Loss: 0.005551667929893256\n",
      "Epoch [13087/20000], Training Loss: 0.00698718216153793, Validation Loss: 0.005558898566537859\n",
      "Epoch [13088/20000], Training Loss: 0.005562583297432866, Validation Loss: 0.0038646250201931514\n",
      "Epoch [13089/20000], Training Loss: 0.004506455247922402, Validation Loss: 0.0035424683592383743\n",
      "Epoch [13090/20000], Training Loss: 0.004420924931764603, Validation Loss: 0.006059599516041609\n",
      "Epoch [13091/20000], Training Loss: 0.0040652894490733615, Validation Loss: 0.010856459664994742\n",
      "Epoch [13092/20000], Training Loss: 0.011836192396742717, Validation Loss: 0.006967056165745841\n",
      "Epoch [13093/20000], Training Loss: 0.007588969681819435, Validation Loss: 0.006815827116132108\n",
      "Epoch [13094/20000], Training Loss: 0.010572092759373066, Validation Loss: 0.004170736871697857\n",
      "Epoch [13095/20000], Training Loss: 0.01549314265607141, Validation Loss: 0.008900368539001795\n",
      "Epoch [13096/20000], Training Loss: 0.01314027403960186, Validation Loss: 0.0037491084160389426\n",
      "Epoch [13097/20000], Training Loss: 0.01497229774499179, Validation Loss: 0.014691747267339192\n",
      "Epoch [13098/20000], Training Loss: 0.05490082684419966, Validation Loss: 0.024999692808179746\n",
      "Epoch [13099/20000], Training Loss: 0.03615643188303303, Validation Loss: 0.017779506627578354\n",
      "Epoch [13100/20000], Training Loss: 0.023310436389043128, Validation Loss: 0.0084226951547797\n",
      "Epoch [13101/20000], Training Loss: 0.009186040622028355, Validation Loss: 0.012300950229095926\n",
      "Epoch [13102/20000], Training Loss: 0.012350845741041536, Validation Loss: 0.005716773838847595\n",
      "Epoch [13103/20000], Training Loss: 0.00871527666043091, Validation Loss: 0.005720925854088169\n",
      "Epoch [13104/20000], Training Loss: 0.006979732192121446, Validation Loss: 0.005199841121525749\n",
      "Epoch [13105/20000], Training Loss: 0.006405788186904309, Validation Loss: 0.0051191875275061905\n",
      "Epoch [13106/20000], Training Loss: 0.005811790349460872, Validation Loss: 0.0046873668097353926\n",
      "Epoch [13107/20000], Training Loss: 0.006986733106064743, Validation Loss: 0.007672730889116305\n",
      "Epoch [13108/20000], Training Loss: 0.006607447409935828, Validation Loss: 0.0040759793442768045\n",
      "Epoch [13109/20000], Training Loss: 0.005172102799406275, Validation Loss: 0.00665771034469539\n",
      "Epoch [13110/20000], Training Loss: 0.004700106072204692, Validation Loss: 0.00885968488451486\n",
      "Epoch [13111/20000], Training Loss: 0.005667773144425935, Validation Loss: 0.021718289744577696\n",
      "Epoch [13112/20000], Training Loss: 0.015262105026132693, Validation Loss: 0.007749527754136969\n",
      "Epoch [13113/20000], Training Loss: 0.008131074735242041, Validation Loss: 0.005445258950398316\n",
      "Epoch [13114/20000], Training Loss: 0.006737966631979881, Validation Loss: 0.006243561964473494\n",
      "Epoch [13115/20000], Training Loss: 0.011372926696432322, Validation Loss: 0.008520327689520204\n",
      "Epoch [13116/20000], Training Loss: 0.010281184043768528, Validation Loss: 0.0058052277015739106\n",
      "Epoch [13117/20000], Training Loss: 0.01255073662157104, Validation Loss: 0.0044549647143575\n",
      "Epoch [13118/20000], Training Loss: 0.005158090302886974, Validation Loss: 0.004582901168804808\n",
      "Epoch [13119/20000], Training Loss: 0.008510583151032083, Validation Loss: 0.011962934833413439\n",
      "Epoch [13120/20000], Training Loss: 0.01943253683293733, Validation Loss: 0.004085804814394578\n",
      "Epoch [13121/20000], Training Loss: 0.06152709060916095, Validation Loss: 0.013969055852612655\n",
      "Epoch [13122/20000], Training Loss: 0.02978054219524243, Validation Loss: 0.019017519601478287\n",
      "Epoch [13123/20000], Training Loss: 0.016887159341763436, Validation Loss: 0.008026334169065714\n",
      "Epoch [13124/20000], Training Loss: 0.008220592062571086, Validation Loss: 0.006014171812604933\n",
      "Epoch [13125/20000], Training Loss: 0.006743514983099885, Validation Loss: 0.0059391610692581965\n",
      "Epoch [13126/20000], Training Loss: 0.006155392254835793, Validation Loss: 0.004850569100101764\n",
      "Epoch [13127/20000], Training Loss: 0.006692895044190144, Validation Loss: 0.004945718553286531\n",
      "Epoch [13128/20000], Training Loss: 0.004620589282629746, Validation Loss: 0.004958083027752406\n",
      "Epoch [13129/20000], Training Loss: 0.0054507013867675725, Validation Loss: 0.00488424478942205\n",
      "Epoch [13130/20000], Training Loss: 0.008297597491702748, Validation Loss: 0.003836713328592742\n",
      "Epoch [13131/20000], Training Loss: 0.0055819359757671395, Validation Loss: 0.011742477970463894\n",
      "Epoch [13132/20000], Training Loss: 0.0076141428424827086, Validation Loss: 0.004308217072061132\n",
      "Epoch [13133/20000], Training Loss: 0.020412715807002053, Validation Loss: 0.021978724747896982\n",
      "Epoch [13134/20000], Training Loss: 0.014626631529868714, Validation Loss: 0.004534123665629457\n",
      "Epoch [13135/20000], Training Loss: 0.004747175975873168, Validation Loss: 0.006357069320769515\n",
      "Epoch [13136/20000], Training Loss: 0.005341045643685253, Validation Loss: 0.00806596988487789\n",
      "Epoch [13137/20000], Training Loss: 0.008934289287676509, Validation Loss: 0.004097088283431661\n",
      "Epoch [13138/20000], Training Loss: 0.006400271771001696, Validation Loss: 0.00517429810646064\n",
      "Epoch [13139/20000], Training Loss: 0.006279722319699691, Validation Loss: 0.003366250251742388\n",
      "Epoch [13140/20000], Training Loss: 0.0070262825673645625, Validation Loss: 0.0077814300548197934\n",
      "Epoch [13141/20000], Training Loss: 0.007345349004846672, Validation Loss: 0.010057904758583622\n",
      "Epoch [13142/20000], Training Loss: 0.0044002120900716235, Validation Loss: 0.004906988894439175\n",
      "Epoch [13143/20000], Training Loss: 0.0051452929453392115, Validation Loss: 0.004202587095378415\n",
      "Epoch [13144/20000], Training Loss: 0.004831453753078157, Validation Loss: 0.0030172250120910704\n",
      "Epoch [13145/20000], Training Loss: 0.0053070588411563745, Validation Loss: 0.003036965234481873\n",
      "Epoch [13146/20000], Training Loss: 0.009101597214924238, Validation Loss: 0.0033093615476519184\n",
      "Epoch [13147/20000], Training Loss: 0.013803943609153586, Validation Loss: 0.004801832189148679\n",
      "Epoch [13148/20000], Training Loss: 0.006292990027239804, Validation Loss: 0.006394175248402587\n",
      "Epoch [13149/20000], Training Loss: 0.006101759665987012, Validation Loss: 0.010314545191606937\n",
      "Epoch [13150/20000], Training Loss: 0.008582231613607811, Validation Loss: 0.005410489883404349\n",
      "Epoch [13151/20000], Training Loss: 0.01263950403241714, Validation Loss: 0.003978485735790881\n",
      "Epoch [13152/20000], Training Loss: 0.015663018182489656, Validation Loss: 0.006318232859810026\n",
      "Epoch [13153/20000], Training Loss: 0.00609429573523812, Validation Loss: 0.008314160353198414\n",
      "Epoch [13154/20000], Training Loss: 0.011851215387391026, Validation Loss: 0.008669902121527647\n",
      "Epoch [13155/20000], Training Loss: 0.007419501874153411, Validation Loss: 0.009679190684607196\n",
      "Epoch [13156/20000], Training Loss: 0.005732711948179973, Validation Loss: 0.004208633356458285\n",
      "Epoch [13157/20000], Training Loss: 0.007176462373796052, Validation Loss: 0.0027173507674653763\n",
      "Epoch [13158/20000], Training Loss: 0.005102859772575487, Validation Loss: 0.00625196479744058\n",
      "Epoch [13159/20000], Training Loss: 0.011405103053740666, Validation Loss: 0.019051238646397906\n",
      "Epoch [13160/20000], Training Loss: 0.020153734011858302, Validation Loss: 0.006756113784626118\n",
      "Epoch [13161/20000], Training Loss: 0.011052272191201544, Validation Loss: 0.008549571702714108\n",
      "Epoch [13162/20000], Training Loss: 0.010173259964046468, Validation Loss: 0.0030290276646932463\n",
      "Epoch [13163/20000], Training Loss: 0.01092699852545463, Validation Loss: 0.010379163196830632\n",
      "Epoch [13164/20000], Training Loss: 0.022251448935613944, Validation Loss: 0.006079913230499707\n",
      "Epoch [13165/20000], Training Loss: 0.019795506658348522, Validation Loss: 0.012560635272945741\n",
      "Epoch [13166/20000], Training Loss: 0.012842978999417807, Validation Loss: 0.0056431936232965085\n",
      "Epoch [13167/20000], Training Loss: 0.01170021087247213, Validation Loss: 0.04155721834727696\n",
      "Epoch [13168/20000], Training Loss: 0.020391884114360437, Validation Loss: 0.02373794146979177\n",
      "Epoch [13169/20000], Training Loss: 0.012776086888542133, Validation Loss: 0.005874298345856589\n",
      "Epoch [13170/20000], Training Loss: 0.00868863009132578, Validation Loss: 0.011113992088082471\n",
      "Epoch [13171/20000], Training Loss: 0.009656106155099613, Validation Loss: 0.0038584850500071243\n",
      "Epoch [13172/20000], Training Loss: 0.00667364357436132, Validation Loss: 0.00430778156685336\n",
      "Epoch [13173/20000], Training Loss: 0.008028243562356303, Validation Loss: 0.005394185470797207\n",
      "Epoch [13174/20000], Training Loss: 0.0073090988545508094, Validation Loss: 0.016722065796084998\n",
      "Epoch [13175/20000], Training Loss: 0.019083310136011278, Validation Loss: 0.02166515984950885\n",
      "Epoch [13176/20000], Training Loss: 0.027398862456071323, Validation Loss: 0.006566329607721855\n",
      "Epoch [13177/20000], Training Loss: 0.009659578033965286, Validation Loss: 0.04631820681242971\n",
      "Epoch [13178/20000], Training Loss: 0.03866959646154149, Validation Loss: 0.04309285704231262\n",
      "Epoch [13179/20000], Training Loss: 0.026504148816457018, Validation Loss: 0.047462365456994234\n",
      "Epoch [13180/20000], Training Loss: 0.026162322556566715, Validation Loss: 0.008547017472669996\n",
      "Epoch [13181/20000], Training Loss: 0.028876082896853665, Validation Loss: 0.054297916815163\n",
      "Epoch [13182/20000], Training Loss: 0.018420858204730654, Validation Loss: 0.0063749924503976475\n",
      "Epoch [13183/20000], Training Loss: 0.01483690325403586, Validation Loss: 0.016813121737078867\n",
      "Epoch [13184/20000], Training Loss: 0.009302094861465906, Validation Loss: 0.0052183391817866975\n",
      "Epoch [13185/20000], Training Loss: 0.007664496598798516, Validation Loss: 0.005491046465600391\n",
      "Epoch [13186/20000], Training Loss: 0.006702541022345291, Validation Loss: 0.006913735677094445\n",
      "Epoch [13187/20000], Training Loss: 0.005993743156847943, Validation Loss: 0.004706877897276307\n",
      "Epoch [13188/20000], Training Loss: 0.005934490568636518, Validation Loss: 0.004669061369913693\n",
      "Epoch [13189/20000], Training Loss: 0.006879541061997381, Validation Loss: 0.004777540945285962\n",
      "Epoch [13190/20000], Training Loss: 0.011153139207246048, Validation Loss: 0.018528688166830374\n",
      "Epoch [13191/20000], Training Loss: 0.010235869883867313, Validation Loss: 0.00399848794974592\n",
      "Epoch [13192/20000], Training Loss: 0.005256566553725861, Validation Loss: 0.0040205696544037195\n",
      "Epoch [13193/20000], Training Loss: 0.00636006414840397, Validation Loss: 0.003888094002710269\n",
      "Epoch [13194/20000], Training Loss: 0.007367601262688238, Validation Loss: 0.00456113096228792\n",
      "Epoch [13195/20000], Training Loss: 0.004133824202913924, Validation Loss: 0.003291982521361092\n",
      "Epoch [13196/20000], Training Loss: 0.004725764892230343, Validation Loss: 0.0032258958522938413\n",
      "Epoch [13197/20000], Training Loss: 0.0040905327290862, Validation Loss: 0.003212778241503741\n",
      "Epoch [13198/20000], Training Loss: 0.005724966164312458, Validation Loss: 0.0034136684001993623\n",
      "Epoch [13199/20000], Training Loss: 0.0049075724631880546, Validation Loss: 0.01368950427110721\n",
      "Epoch [13200/20000], Training Loss: 0.005209097430841731, Validation Loss: 0.0046945371801641045\n",
      "Epoch [13201/20000], Training Loss: 0.004342243015084283, Validation Loss: 0.008200622762433574\n",
      "Epoch [13202/20000], Training Loss: 0.0038036958980488373, Validation Loss: 0.006484542351951664\n",
      "Epoch [13203/20000], Training Loss: 0.006182213355454483, Validation Loss: 0.0023036519570219078\n",
      "Epoch [13204/20000], Training Loss: 0.010723221279996713, Validation Loss: 0.005027619697916298\n",
      "Epoch [13205/20000], Training Loss: 0.016569615172491985, Validation Loss: 0.005376835985650246\n",
      "Epoch [13206/20000], Training Loss: 0.0045978459256730275, Validation Loss: 0.002547872905605139\n",
      "Epoch [13207/20000], Training Loss: 0.003784535210278201, Validation Loss: 0.0028769995576523283\n",
      "Epoch [13208/20000], Training Loss: 0.00795839187740707, Validation Loss: 0.008992414413634313\n",
      "Epoch [13209/20000], Training Loss: 0.01005089990628351, Validation Loss: 0.0032732240016964767\n",
      "Epoch [13210/20000], Training Loss: 0.00435049607559839, Validation Loss: 0.0026952090639772172\n",
      "Epoch [13211/20000], Training Loss: 0.0055505884259088945, Validation Loss: 0.01616996061056853\n",
      "Epoch [13212/20000], Training Loss: 0.015158726857147744, Validation Loss: 0.041508427155869346\n",
      "Epoch [13213/20000], Training Loss: 0.019032937061634607, Validation Loss: 0.005589521290479638\n",
      "Epoch [13214/20000], Training Loss: 0.04302081839601409, Validation Loss: 0.015421119884531214\n",
      "Epoch [13215/20000], Training Loss: 0.04292706734018533, Validation Loss: 0.0069110110554966775\n",
      "Epoch [13216/20000], Training Loss: 0.005659815190093858, Validation Loss: 0.0060448707894886445\n",
      "Epoch [13217/20000], Training Loss: 0.006138649661027428, Validation Loss: 0.004700643469703637\n",
      "Epoch [13218/20000], Training Loss: 0.006317056715488434, Validation Loss: 0.004337778069427145\n",
      "Epoch [13219/20000], Training Loss: 0.0053131458683180555, Validation Loss: 0.003953479780860136\n",
      "Epoch [13220/20000], Training Loss: 0.006370222093242514, Validation Loss: 0.02192349641596333\n",
      "Epoch [13221/20000], Training Loss: 0.014907353172020521, Validation Loss: 0.004477824628645518\n",
      "Epoch [13222/20000], Training Loss: 0.013369063135801948, Validation Loss: 0.008550967555530191\n",
      "Epoch [13223/20000], Training Loss: 0.011194798509222892, Validation Loss: 0.02162844128911043\n",
      "Epoch [13224/20000], Training Loss: 0.017088625461578237, Validation Loss: 0.0048150096581189876\n",
      "Epoch [13225/20000], Training Loss: 0.006913130038550922, Validation Loss: 0.005634601387165665\n",
      "Epoch [13226/20000], Training Loss: 0.006742056422029107, Validation Loss: 0.0031612494855123457\n",
      "Epoch [13227/20000], Training Loss: 0.004187746587766534, Validation Loss: 0.007266681774411733\n",
      "Epoch [13228/20000], Training Loss: 0.007906765388075396, Validation Loss: 0.003970902525333\n",
      "Epoch [13229/20000], Training Loss: 0.005905160403146275, Validation Loss: 0.004405550593568956\n",
      "Epoch [13230/20000], Training Loss: 0.0049135397587503704, Validation Loss: 0.010008299670859446\n",
      "Epoch [13231/20000], Training Loss: 0.00887025250995066, Validation Loss: 0.0032137171561588446\n",
      "Epoch [13232/20000], Training Loss: 0.005589927314344679, Validation Loss: 0.019062481953629428\n",
      "Epoch [13233/20000], Training Loss: 0.02141029215467175, Validation Loss: 0.01100725873207661\n",
      "Epoch [13234/20000], Training Loss: 0.04549863611139569, Validation Loss: 0.017575501139877554\n",
      "Epoch [13235/20000], Training Loss: 0.03126840917316948, Validation Loss: 0.03917401154114967\n",
      "Epoch [13236/20000], Training Loss: 0.028612774800941616, Validation Loss: 0.007116425104381311\n",
      "Epoch [13237/20000], Training Loss: 0.01434940787294181, Validation Loss: 0.010954869112931488\n",
      "Epoch [13238/20000], Training Loss: 0.01054895511853309, Validation Loss: 0.00639534270994433\n",
      "Epoch [13239/20000], Training Loss: 0.007138101749920419, Validation Loss: 0.0045924814819171544\n",
      "Epoch [13240/20000], Training Loss: 0.011652202407796202, Validation Loss: 0.009592239878805673\n",
      "Epoch [13241/20000], Training Loss: 0.01116677360031255, Validation Loss: 0.008126661714823877\n",
      "Epoch [13242/20000], Training Loss: 0.009892728155916432, Validation Loss: 0.004493354194997222\n",
      "Epoch [13243/20000], Training Loss: 0.005791110215276214, Validation Loss: 0.004594672224799314\n",
      "Epoch [13244/20000], Training Loss: 0.005944881789868565, Validation Loss: 0.00568569063445125\n",
      "Epoch [13245/20000], Training Loss: 0.009142590236089225, Validation Loss: 0.004790832002266677\n",
      "Epoch [13246/20000], Training Loss: 0.006536365991618704, Validation Loss: 0.009764218436842243\n",
      "Epoch [13247/20000], Training Loss: 0.006194115428407849, Validation Loss: 0.003952248826948238\n",
      "Epoch [13248/20000], Training Loss: 0.0038314331873802337, Validation Loss: 0.004424628208279592\n",
      "Epoch [13249/20000], Training Loss: 0.004452573015571813, Validation Loss: 0.003501602493171282\n",
      "Epoch [13250/20000], Training Loss: 0.004212917670104487, Validation Loss: 0.003869103236968737\n",
      "Epoch [13251/20000], Training Loss: 0.004311942401857648, Validation Loss: 0.0030397157559515337\n",
      "Epoch [13252/20000], Training Loss: 0.004902023506084723, Validation Loss: 0.010800626927191097\n",
      "Epoch [13253/20000], Training Loss: 0.011084355775632762, Validation Loss: 0.003641549565529638\n",
      "Epoch [13254/20000], Training Loss: 0.013482715911127994, Validation Loss: 0.006253701454239392\n",
      "Epoch [13255/20000], Training Loss: 0.010217875498970639, Validation Loss: 0.004336414010874032\n",
      "Epoch [13256/20000], Training Loss: 0.009633472756831907, Validation Loss: 0.0033095065423315853\n",
      "Epoch [13257/20000], Training Loss: 0.008989418168701897, Validation Loss: 0.018650345038622618\n",
      "Epoch [13258/20000], Training Loss: 0.011760221318192115, Validation Loss: 0.003664585928404991\n",
      "Epoch [13259/20000], Training Loss: 0.007792370776639602, Validation Loss: 0.009996864700953114\n",
      "Epoch [13260/20000], Training Loss: 0.011893485565342945, Validation Loss: 0.00869430968147579\n",
      "Epoch [13261/20000], Training Loss: 0.023657782036545023, Validation Loss: 0.0167910297020962\n",
      "Epoch [13262/20000], Training Loss: 0.03119047569842743, Validation Loss: 0.019457988181396026\n",
      "Epoch [13263/20000], Training Loss: 0.021313998113328125, Validation Loss: 0.01706972119531461\n",
      "Epoch [13264/20000], Training Loss: 0.012462474673156976, Validation Loss: 0.0079654875902817\n",
      "Epoch [13265/20000], Training Loss: 0.005696866759015913, Validation Loss: 0.0038737922986762058\n",
      "Epoch [13266/20000], Training Loss: 0.005416282826835024, Validation Loss: 0.009568547563894034\n",
      "Epoch [13267/20000], Training Loss: 0.016803456443345306, Validation Loss: 0.008917758973049339\n",
      "Epoch [13268/20000], Training Loss: 0.016735684688813177, Validation Loss: 0.036368684873019745\n",
      "Epoch [13269/20000], Training Loss: 0.03667165480692347, Validation Loss: 0.10368508770937167\n",
      "Epoch [13270/20000], Training Loss: 0.04774686629701007, Validation Loss: 0.03747294544083652\n",
      "Epoch [13271/20000], Training Loss: 0.02620345988358557, Validation Loss: 0.007100552571162425\n",
      "Epoch [13272/20000], Training Loss: 0.01403742270739323, Validation Loss: 0.010798715464531483\n",
      "Epoch [13273/20000], Training Loss: 0.01060134580828682, Validation Loss: 0.012514878787895003\n",
      "Epoch [13274/20000], Training Loss: 0.0070644685391536245, Validation Loss: 0.017599928020736736\n",
      "Epoch [13275/20000], Training Loss: 0.010121081440177346, Validation Loss: 0.008801043665889503\n",
      "Epoch [13276/20000], Training Loss: 0.010802014656552015, Validation Loss: 0.004684254057610003\n",
      "Epoch [13277/20000], Training Loss: 0.008030126315134112, Validation Loss: 0.008056650869613652\n",
      "Epoch [13278/20000], Training Loss: 0.008699775145422401, Validation Loss: 0.006156110074625797\n",
      "Epoch [13279/20000], Training Loss: 0.008592164892304157, Validation Loss: 0.005956016158224158\n",
      "Epoch [13280/20000], Training Loss: 0.009793911821101833, Validation Loss: 0.007065082768806939\n",
      "Epoch [13281/20000], Training Loss: 0.006078622416160735, Validation Loss: 0.004196114347995851\n",
      "Epoch [13282/20000], Training Loss: 0.004866837604952577, Validation Loss: 0.0037239536190602574\n",
      "Epoch [13283/20000], Training Loss: 0.007988956525098598, Validation Loss: 0.010447493163167922\n",
      "Epoch [13284/20000], Training Loss: 0.007722544584664449, Validation Loss: 0.003548169486249229\n",
      "Epoch [13285/20000], Training Loss: 0.0047350556425434275, Validation Loss: 0.00716050883888147\n",
      "Epoch [13286/20000], Training Loss: 0.006350880139507353, Validation Loss: 0.0078049232917307265\n",
      "Epoch [13287/20000], Training Loss: 0.007753495355302675, Validation Loss: 0.003774513789936703\n",
      "Epoch [13288/20000], Training Loss: 0.004308335832320154, Validation Loss: 0.006465010497037278\n",
      "Epoch [13289/20000], Training Loss: 0.00875343941256038, Validation Loss: 0.003188941387227991\n",
      "Epoch [13290/20000], Training Loss: 0.0046368955425870285, Validation Loss: 0.0029900928508614644\n",
      "Epoch [13291/20000], Training Loss: 0.004070439404261249, Validation Loss: 0.0026236923203129697\n",
      "Epoch [13292/20000], Training Loss: 0.0036709904634335544, Validation Loss: 0.004481538664562858\n",
      "Epoch [13293/20000], Training Loss: 0.00971339708485175, Validation Loss: 0.0024959684067516696\n",
      "Epoch [13294/20000], Training Loss: 0.003984811011053223, Validation Loss: 0.0024661282923407845\n",
      "Epoch [13295/20000], Training Loss: 0.004397014832647983, Validation Loss: 0.002269381324848916\n",
      "Epoch [13296/20000], Training Loss: 0.005978275809346607, Validation Loss: 0.028600351685392007\n",
      "Epoch [13297/20000], Training Loss: 0.009388948151126897, Validation Loss: 0.011096890049539001\n",
      "Epoch [13298/20000], Training Loss: 0.006899080388393486, Validation Loss: 0.013491547932582242\n",
      "Epoch [13299/20000], Training Loss: 0.009570482574807622, Validation Loss: 0.008015607849686352\n",
      "Epoch [13300/20000], Training Loss: 0.01197750637215603, Validation Loss: 0.00634369703974313\n",
      "Epoch [13301/20000], Training Loss: 0.005055470230798521, Validation Loss: 0.00300629363380992\n",
      "Epoch [13302/20000], Training Loss: 0.004943726087991048, Validation Loss: 0.010687658230641581\n",
      "Epoch [13303/20000], Training Loss: 0.03388454154147829, Validation Loss: 0.04186500289610454\n",
      "Epoch [13304/20000], Training Loss: 0.030018229578802545, Validation Loss: 0.01024177271340608\n",
      "Epoch [13305/20000], Training Loss: 0.00820769325517696, Validation Loss: 0.004297637091516534\n",
      "Epoch [13306/20000], Training Loss: 0.007117097222013464, Validation Loss: 0.0036711305974307874\n",
      "Epoch [13307/20000], Training Loss: 0.008621646481125416, Validation Loss: 0.007189255606992055\n",
      "Epoch [13308/20000], Training Loss: 0.007432867446498547, Validation Loss: 0.004624655038388083\n",
      "Epoch [13309/20000], Training Loss: 0.0054483089469223546, Validation Loss: 0.003803222274121402\n",
      "Epoch [13310/20000], Training Loss: 0.005301388036709146, Validation Loss: 0.0028916990464386305\n",
      "Epoch [13311/20000], Training Loss: 0.007195748359663412, Validation Loss: 0.005108236156332221\n",
      "Epoch [13312/20000], Training Loss: 0.005733594296803598, Validation Loss: 0.005676231066720292\n",
      "Epoch [13313/20000], Training Loss: 0.013093803001895919, Validation Loss: 0.0032241070212727222\n",
      "Epoch [13314/20000], Training Loss: 0.0038485517131885416, Validation Loss: 0.006282999612657975\n",
      "Epoch [13315/20000], Training Loss: 0.006027570671514175, Validation Loss: 0.0028924360641953395\n",
      "Epoch [13316/20000], Training Loss: 0.015492373861239426, Validation Loss: 0.013898705265351747\n",
      "Epoch [13317/20000], Training Loss: 0.00888069705199866, Validation Loss: 0.002340035399820506\n",
      "Epoch [13318/20000], Training Loss: 0.004219596112047189, Validation Loss: 0.0029565340211322112\n",
      "Epoch [13319/20000], Training Loss: 0.0074541287218120745, Validation Loss: 0.0028479742887016083\n",
      "Epoch [13320/20000], Training Loss: 0.004727326405502806, Validation Loss: 0.004277810587423738\n",
      "Epoch [13321/20000], Training Loss: 0.007490245872239549, Validation Loss: 0.006639813671687813\n",
      "Epoch [13322/20000], Training Loss: 0.005892778822661577, Validation Loss: 0.0027310582551672985\n",
      "Epoch [13323/20000], Training Loss: 0.01058762239491833, Validation Loss: 0.002732673544270222\n",
      "Epoch [13324/20000], Training Loss: 0.021610266698449516, Validation Loss: 0.005901892961401367\n",
      "Epoch [13325/20000], Training Loss: 0.011030401898486681, Validation Loss: 0.004445791244624406\n",
      "Epoch [13326/20000], Training Loss: 0.013752486302174865, Validation Loss: 0.02319065774125712\n",
      "Epoch [13327/20000], Training Loss: 0.03253245519590564, Validation Loss: 0.02889237872191838\n",
      "Epoch [13328/20000], Training Loss: 0.023414270461736515, Validation Loss: 0.004980987576700687\n",
      "Epoch [13329/20000], Training Loss: 0.008691996152525203, Validation Loss: 0.00629310546661177\n",
      "Epoch [13330/20000], Training Loss: 0.009790728180502941, Validation Loss: 0.006473344285049059\n",
      "Epoch [13331/20000], Training Loss: 0.010442485179477703, Validation Loss: 0.007654418570125369\n",
      "Epoch [13332/20000], Training Loss: 0.006701664107043014, Validation Loss: 0.010891973324354305\n",
      "Epoch [13333/20000], Training Loss: 0.01033057404336952, Validation Loss: 0.006650102359864312\n",
      "Epoch [13334/20000], Training Loss: 0.01629651603486439, Validation Loss: 0.013807869516313251\n",
      "Epoch [13335/20000], Training Loss: 0.020881502780996795, Validation Loss: 0.009737277137381732\n",
      "Epoch [13336/20000], Training Loss: 0.03338699837695458, Validation Loss: 0.035910482917513184\n",
      "Epoch [13337/20000], Training Loss: 0.022537435134706487, Validation Loss: 0.012838045667325076\n",
      "Epoch [13338/20000], Training Loss: 0.010676598771863206, Validation Loss: 0.006398445485236322\n",
      "Epoch [13339/20000], Training Loss: 0.007416665379423648, Validation Loss: 0.02220690334668518\n",
      "Epoch [13340/20000], Training Loss: 0.008873557281081699, Validation Loss: 0.011782712702375627\n",
      "Epoch [13341/20000], Training Loss: 0.009853055856832984, Validation Loss: 0.019100380529250425\n",
      "Epoch [13342/20000], Training Loss: 0.014114525734579988, Validation Loss: 0.004242923843939337\n",
      "Epoch [13343/20000], Training Loss: 0.01329861098825599, Validation Loss: 0.0052812795401095\n",
      "Epoch [13344/20000], Training Loss: 0.005568935158342876, Validation Loss: 0.011394638169024866\n",
      "Epoch [13345/20000], Training Loss: 0.00736384407667044, Validation Loss: 0.006667689042735165\n",
      "Epoch [13346/20000], Training Loss: 0.020828612643526867, Validation Loss: 0.022042150131288248\n",
      "Epoch [13347/20000], Training Loss: 0.01280390103836128, Validation Loss: 0.0060254461797283245\n",
      "Epoch [13348/20000], Training Loss: 0.00895620119143002, Validation Loss: 0.004348891088899421\n",
      "Epoch [13349/20000], Training Loss: 0.008806568150508351, Validation Loss: 0.023503313462794852\n",
      "Epoch [13350/20000], Training Loss: 0.014587014982160846, Validation Loss: 0.003810299295735437\n",
      "Epoch [13351/20000], Training Loss: 0.015336638222964081, Validation Loss: 0.004728092221945867\n",
      "Epoch [13352/20000], Training Loss: 0.021039176707355573, Validation Loss: 0.0447441360780171\n",
      "Epoch [13353/20000], Training Loss: 0.015067782058200205, Validation Loss: 0.009025013896202186\n",
      "Epoch [13354/20000], Training Loss: 0.010786565106952497, Validation Loss: 0.012713548625760487\n",
      "Epoch [13355/20000], Training Loss: 0.012012908974221708, Validation Loss: 0.006020891483147673\n",
      "Epoch [13356/20000], Training Loss: 0.007685119430139561, Validation Loss: 0.008470344922638415\n",
      "Epoch [13357/20000], Training Loss: 0.009061915974598378, Validation Loss: 0.0038584817408979515\n",
      "Epoch [13358/20000], Training Loss: 0.009268027484982408, Validation Loss: 0.004554905795774523\n",
      "Epoch [13359/20000], Training Loss: 0.006500944101259977, Validation Loss: 0.007650622765400837\n",
      "Epoch [13360/20000], Training Loss: 0.0067591480910778046, Validation Loss: 0.003188242147867395\n",
      "Epoch [13361/20000], Training Loss: 0.005266137321346572, Validation Loss: 0.0034776239190570097\n",
      "Epoch [13362/20000], Training Loss: 0.00797767297081009, Validation Loss: 0.023968075002942766\n",
      "Epoch [13363/20000], Training Loss: 0.028754316943897202, Validation Loss: 0.013907086902431086\n",
      "Epoch [13364/20000], Training Loss: 0.024138459424388463, Validation Loss: 0.00789881783670891\n",
      "Epoch [13365/20000], Training Loss: 0.025920469530059824, Validation Loss: 0.009102250028428484\n",
      "Epoch [13366/20000], Training Loss: 0.008033375600332095, Validation Loss: 0.0071667172168028105\n",
      "Epoch [13367/20000], Training Loss: 0.00759111054607534, Validation Loss: 0.012302145095808212\n",
      "Epoch [13368/20000], Training Loss: 0.009558235943716551, Validation Loss: 0.004246304643260443\n",
      "Epoch [13369/20000], Training Loss: 0.013612567792214187, Validation Loss: 0.004240984335369639\n",
      "Epoch [13370/20000], Training Loss: 0.011550281513076957, Validation Loss: 0.009511306705148532\n",
      "Epoch [13371/20000], Training Loss: 0.008319615394741828, Validation Loss: 0.006143442488146266\n",
      "Epoch [13372/20000], Training Loss: 0.005076237196330372, Validation Loss: 0.006911440020693799\n",
      "Epoch [13373/20000], Training Loss: 0.007145045641144472, Validation Loss: 0.004647547141624878\n",
      "Epoch [13374/20000], Training Loss: 0.00853624920481317, Validation Loss: 0.006471957346159109\n",
      "Epoch [13375/20000], Training Loss: 0.004312902715615304, Validation Loss: 0.01035703261618343\n",
      "Epoch [13376/20000], Training Loss: 0.011492615541491042, Validation Loss: 0.004183069836520166\n",
      "Epoch [13377/20000], Training Loss: 0.008959627118786426, Validation Loss: 0.021119567432573864\n",
      "Epoch [13378/20000], Training Loss: 0.011855847381910272, Validation Loss: 0.0053824524927769445\n",
      "Epoch [13379/20000], Training Loss: 0.010787313273275296, Validation Loss: 0.007096111268091512\n",
      "Epoch [13380/20000], Training Loss: 0.009927846885797667, Validation Loss: 0.007848556412318249\n",
      "Epoch [13381/20000], Training Loss: 0.018612408095837703, Validation Loss: 0.002956397175166694\n",
      "Epoch [13382/20000], Training Loss: 0.010461995423871226, Validation Loss: 0.00917765644512034\n",
      "Epoch [13383/20000], Training Loss: 0.004704771436601212, Validation Loss: 0.006542635216777212\n",
      "Epoch [13384/20000], Training Loss: 0.007065125083866189, Validation Loss: 0.007419420971668205\n",
      "Epoch [13385/20000], Training Loss: 0.013223898625126042, Validation Loss: 0.029341365609850203\n",
      "Epoch [13386/20000], Training Loss: 0.02006909376775314, Validation Loss: 0.012979228182562775\n",
      "Epoch [13387/20000], Training Loss: 0.01449096501996142, Validation Loss: 0.004556233121741027\n",
      "Epoch [13388/20000], Training Loss: 0.006466771405290014, Validation Loss: 0.0029365204812013246\n",
      "Epoch [13389/20000], Training Loss: 0.004254727657098556, Validation Loss: 0.0030758843640195264\n",
      "Epoch [13390/20000], Training Loss: 0.004336889145114193, Validation Loss: 0.0029779467609498283\n",
      "Epoch [13391/20000], Training Loss: 0.005327802957686296, Validation Loss: 0.010117696465126114\n",
      "Epoch [13392/20000], Training Loss: 0.008275247950710505, Validation Loss: 0.030277314462832043\n",
      "Epoch [13393/20000], Training Loss: 0.018794570982988392, Validation Loss: 0.005937688877484877\n",
      "Epoch [13394/20000], Training Loss: 0.007040757362639332, Validation Loss: 0.004713974332702796\n",
      "Epoch [13395/20000], Training Loss: 0.007861686716604968, Validation Loss: 0.00799668620207482\n",
      "Epoch [13396/20000], Training Loss: 0.007860094177885912, Validation Loss: 0.00322433048869641\n",
      "Epoch [13397/20000], Training Loss: 0.007639677589785216, Validation Loss: 0.0032433438048528316\n",
      "Epoch [13398/20000], Training Loss: 0.007583475964825733, Validation Loss: 0.0033135904566548922\n",
      "Epoch [13399/20000], Training Loss: 0.007475169049160156, Validation Loss: 0.008074306546111467\n",
      "Epoch [13400/20000], Training Loss: 0.009862699483814399, Validation Loss: 0.005858674505299271\n",
      "Epoch [13401/20000], Training Loss: 0.023302620977899226, Validation Loss: 0.006165683934731324\n",
      "Epoch [13402/20000], Training Loss: 0.05120760834792496, Validation Loss: 0.030131489570651735\n",
      "Epoch [13403/20000], Training Loss: 0.058715208865968246, Validation Loss: 0.10858784204437769\n",
      "Epoch [13404/20000], Training Loss: 0.04721823679782184, Validation Loss: 0.013819885877086055\n",
      "Epoch [13405/20000], Training Loss: 0.01997972187486344, Validation Loss: 0.014880505983455805\n",
      "Epoch [13406/20000], Training Loss: 0.015677368875393376, Validation Loss: 0.009841783170405485\n",
      "Epoch [13407/20000], Training Loss: 0.00878068177761244, Validation Loss: 0.007814291157243096\n",
      "Epoch [13408/20000], Training Loss: 0.008039602361220335, Validation Loss: 0.007285566028786304\n",
      "Epoch [13409/20000], Training Loss: 0.009588164219167084, Validation Loss: 0.0064392834009692835\n",
      "Epoch [13410/20000], Training Loss: 0.006641380765358917, Validation Loss: 0.005534319061604874\n",
      "Epoch [13411/20000], Training Loss: 0.00753237444691227, Validation Loss: 0.005143734928899667\n",
      "Epoch [13412/20000], Training Loss: 0.00643253837668973, Validation Loss: 0.005495589834125578\n",
      "Epoch [13413/20000], Training Loss: 0.005118259972992486, Validation Loss: 0.005541583156394608\n",
      "Epoch [13414/20000], Training Loss: 0.006294111222814536, Validation Loss: 0.0037715693332888434\n",
      "Epoch [13415/20000], Training Loss: 0.005522014905831644, Validation Loss: 0.0036932227057862427\n",
      "Epoch [13416/20000], Training Loss: 0.005901538841758988, Validation Loss: 0.004914469354555783\n",
      "Epoch [13417/20000], Training Loss: 0.006628490718347686, Validation Loss: 0.004310899515512254\n",
      "Epoch [13418/20000], Training Loss: 0.004977942614849391, Validation Loss: 0.0032754211838900904\n",
      "Epoch [13419/20000], Training Loss: 0.005873975509140499, Validation Loss: 0.0033369411785215686\n",
      "Epoch [13420/20000], Training Loss: 0.006915726886031085, Validation Loss: 0.006781176863514498\n",
      "Epoch [13421/20000], Training Loss: 0.004125117792682431, Validation Loss: 0.003503268530362938\n",
      "Epoch [13422/20000], Training Loss: 0.008571450237078742, Validation Loss: 0.0029737582014332424\n",
      "Epoch [13423/20000], Training Loss: 0.005575759610759893, Validation Loss: 0.0037139753953589574\n",
      "Epoch [13424/20000], Training Loss: 0.020779265705381737, Validation Loss: 0.005674439796836239\n",
      "Epoch [13425/20000], Training Loss: 0.016058973419213935, Validation Loss: 0.023858586358073808\n",
      "Epoch [13426/20000], Training Loss: 0.012295176871703006, Validation Loss: 0.007523203418994056\n",
      "Epoch [13427/20000], Training Loss: 0.036778502184461104, Validation Loss: 0.05321189570994606\n",
      "Epoch [13428/20000], Training Loss: 0.030257653162282492, Validation Loss: 0.029810580824106432\n",
      "Epoch [13429/20000], Training Loss: 0.02568108548543283, Validation Loss: 0.015650975341940494\n",
      "Epoch [13430/20000], Training Loss: 0.026195478841795454, Validation Loss: 0.01916633090682548\n",
      "Epoch [13431/20000], Training Loss: 0.012717530181232308, Validation Loss: 0.011341235735088337\n",
      "Epoch [13432/20000], Training Loss: 0.007070947228515122, Validation Loss: 0.005916205852697333\n",
      "Epoch [13433/20000], Training Loss: 0.008360214797513825, Validation Loss: 0.012896357902698316\n",
      "Epoch [13434/20000], Training Loss: 0.006913327200371506, Validation Loss: 0.004945846469841213\n",
      "Epoch [13435/20000], Training Loss: 0.006395013853242355, Validation Loss: 0.004994200747644958\n",
      "Epoch [13436/20000], Training Loss: 0.0067876226255196214, Validation Loss: 0.004675272633742698\n",
      "Epoch [13437/20000], Training Loss: 0.005211460440477822, Validation Loss: 0.005267898401995303\n",
      "Epoch [13438/20000], Training Loss: 0.010635886291441108, Validation Loss: 0.008105362510393234\n",
      "Epoch [13439/20000], Training Loss: 0.005541924657622742, Validation Loss: 0.004578772469800594\n",
      "Epoch [13440/20000], Training Loss: 0.006083094563759265, Validation Loss: 0.004915587687154118\n",
      "Epoch [13441/20000], Training Loss: 0.005818784012392696, Validation Loss: 0.01224756493632333\n",
      "Epoch [13442/20000], Training Loss: 0.006791613718913935, Validation Loss: 0.004277512532192021\n",
      "Epoch [13443/20000], Training Loss: 0.00568755848715747, Validation Loss: 0.004309185824448913\n",
      "Epoch [13444/20000], Training Loss: 0.006548035508797868, Validation Loss: 0.004631295057889276\n",
      "Epoch [13445/20000], Training Loss: 0.0074516284390223776, Validation Loss: 0.018580045018877302\n",
      "Epoch [13446/20000], Training Loss: 0.00976768930217377, Validation Loss: 0.004858454696871127\n",
      "Epoch [13447/20000], Training Loss: 0.006285929178488914, Validation Loss: 0.00719631251194259\n",
      "Epoch [13448/20000], Training Loss: 0.011747583640888999, Validation Loss: 0.007599493986844278\n",
      "Epoch [13449/20000], Training Loss: 0.011488846179937744, Validation Loss: 0.00433887917297945\n",
      "Epoch [13450/20000], Training Loss: 0.009559975616347012, Validation Loss: 0.004589707058468039\n",
      "Epoch [13451/20000], Training Loss: 0.003971411574249422, Validation Loss: 0.004822813627539351\n",
      "Epoch [13452/20000], Training Loss: 0.010444294673756562, Validation Loss: 0.003998768625853182\n",
      "Epoch [13453/20000], Training Loss: 0.027743855350308877, Validation Loss: 0.020773120490568026\n",
      "Epoch [13454/20000], Training Loss: 0.029172702164942166, Validation Loss: 0.10373670501368386\n",
      "Epoch [13455/20000], Training Loss: 0.04616728654114662, Validation Loss: 0.03249841206131787\n",
      "Epoch [13456/20000], Training Loss: 0.03202119456805771, Validation Loss: 0.007282877472156153\n",
      "Epoch [13457/20000], Training Loss: 0.011911036750201933, Validation Loss: 0.008980593500741778\n",
      "Epoch [13458/20000], Training Loss: 0.00981390249632698, Validation Loss: 0.008501260608300715\n",
      "Epoch [13459/20000], Training Loss: 0.006844362052756229, Validation Loss: 0.005841618282487344\n",
      "Epoch [13460/20000], Training Loss: 0.008750210031783874, Validation Loss: 0.006214865975376175\n",
      "Epoch [13461/20000], Training Loss: 0.006707899180241839, Validation Loss: 0.00537197240365282\n",
      "Epoch [13462/20000], Training Loss: 0.005912965882868905, Validation Loss: 0.0090868213613432\n",
      "Epoch [13463/20000], Training Loss: 0.00658460903962675, Validation Loss: 0.006241021117156663\n",
      "Epoch [13464/20000], Training Loss: 0.00621093665124915, Validation Loss: 0.0046596361271609155\n",
      "Epoch [13465/20000], Training Loss: 0.005515408576424566, Validation Loss: 0.004168036860220111\n",
      "Epoch [13466/20000], Training Loss: 0.0056527049382566474, Validation Loss: 0.005418507776941084\n",
      "Epoch [13467/20000], Training Loss: 0.012401458973792094, Validation Loss: 0.009657638147473982\n",
      "Epoch [13468/20000], Training Loss: 0.004067972265017618, Validation Loss: 0.008107595678038787\n",
      "Epoch [13469/20000], Training Loss: 0.006203504341751146, Validation Loss: 0.007459194634632228\n",
      "Epoch [13470/20000], Training Loss: 0.008521512689899933, Validation Loss: 0.00392423116859408\n",
      "Epoch [13471/20000], Training Loss: 0.008698343171480312, Validation Loss: 0.002770299649155245\n",
      "Epoch [13472/20000], Training Loss: 0.007471070016371024, Validation Loss: 0.011759740908698848\n",
      "Epoch [13473/20000], Training Loss: 0.012601263441584314, Validation Loss: 0.005752441473305279\n",
      "Epoch [13474/20000], Training Loss: 0.005541919275336633, Validation Loss: 0.002946563368576999\n",
      "Epoch [13475/20000], Training Loss: 0.004275396072314054, Validation Loss: 0.0042256144806744585\n",
      "Epoch [13476/20000], Training Loss: 0.012639851442405156, Validation Loss: 0.007605011346775746\n",
      "Epoch [13477/20000], Training Loss: 0.031706710448426226, Validation Loss: 0.03606160996566489\n",
      "Epoch [13478/20000], Training Loss: 0.026377238891395142, Validation Loss: 0.003457013180910248\n",
      "Epoch [13479/20000], Training Loss: 0.010502208483688134, Validation Loss: 0.02427974975486385\n",
      "Epoch [13480/20000], Training Loss: 0.02025982373327549, Validation Loss: 0.008627469602511642\n",
      "Epoch [13481/20000], Training Loss: 0.008961341576650739, Validation Loss: 0.004384734200747127\n",
      "Epoch [13482/20000], Training Loss: 0.005754727508506871, Validation Loss: 0.006557682433448723\n",
      "Epoch [13483/20000], Training Loss: 0.005544899060477552, Validation Loss: 0.0035883443120578056\n",
      "Epoch [13484/20000], Training Loss: 0.004867819559876807, Validation Loss: 0.006156936710340233\n",
      "Epoch [13485/20000], Training Loss: 0.007055592107852655, Validation Loss: 0.006557105247630091\n",
      "Epoch [13486/20000], Training Loss: 0.008095576385260626, Validation Loss: 0.004456483689344688\n",
      "Epoch [13487/20000], Training Loss: 0.005563420897877742, Validation Loss: 0.003463278868833543\n",
      "Epoch [13488/20000], Training Loss: 0.010178274880412832, Validation Loss: 0.0071512585858461665\n",
      "Epoch [13489/20000], Training Loss: 0.008930403385810288, Validation Loss: 0.003482712290539171\n",
      "Epoch [13490/20000], Training Loss: 0.007359950895728876, Validation Loss: 0.0028745135352065054\n",
      "Epoch [13491/20000], Training Loss: 0.010738385561852835, Validation Loss: 0.006424065024113444\n",
      "Epoch [13492/20000], Training Loss: 0.006940145888968947, Validation Loss: 0.003095588569369756\n",
      "Epoch [13493/20000], Training Loss: 0.014273188272454718, Validation Loss: 0.0016942624448395322\n",
      "Epoch [13494/20000], Training Loss: 0.030261795516707934, Validation Loss: 0.013867918963358403\n",
      "Epoch [13495/20000], Training Loss: 0.01272599579117793, Validation Loss: 0.022118515292342624\n",
      "Epoch [13496/20000], Training Loss: 0.008336168079040362, Validation Loss: 0.015598145914435488\n",
      "Epoch [13497/20000], Training Loss: 0.005525177202278948, Validation Loss: 0.009928052906827986\n",
      "Epoch [13498/20000], Training Loss: 0.008903838744767458, Validation Loss: 0.00450555551724482\n",
      "Epoch [13499/20000], Training Loss: 0.00540018446814169, Validation Loss: 0.008056354534671937\n",
      "Epoch [13500/20000], Training Loss: 0.010570703752039532, Validation Loss: 0.007369412380296897\n",
      "Epoch [13501/20000], Training Loss: 0.006582262919047415, Validation Loss: 0.01245281552660468\n",
      "Epoch [13502/20000], Training Loss: 0.0074689083116287035, Validation Loss: 0.003129858703199546\n",
      "Epoch [13503/20000], Training Loss: 0.00816301004592138, Validation Loss: 0.003602087635286469\n",
      "Epoch [13504/20000], Training Loss: 0.006338614556755472, Validation Loss: 0.004138905161124018\n",
      "Epoch [13505/20000], Training Loss: 0.007680869676343198, Validation Loss: 0.008703071957925121\n",
      "Epoch [13506/20000], Training Loss: 0.03330233586114316, Validation Loss: 0.018141389102051368\n",
      "Epoch [13507/20000], Training Loss: 0.03882953023847741, Validation Loss: 0.005939662845516328\n",
      "Epoch [13508/20000], Training Loss: 0.03570503372299884, Validation Loss: 0.007477980230554492\n",
      "Epoch [13509/20000], Training Loss: 0.012413622187783144, Validation Loss: 0.012784583280356239\n",
      "Epoch [13510/20000], Training Loss: 0.009865464941997613, Validation Loss: 0.006063504571955544\n",
      "Epoch [13511/20000], Training Loss: 0.006320731566769869, Validation Loss: 0.006066978428342347\n",
      "Epoch [13512/20000], Training Loss: 0.008664454190043866, Validation Loss: 0.004995401520765037\n",
      "Epoch [13513/20000], Training Loss: 0.022172045378413583, Validation Loss: 0.03200831318069992\n",
      "Epoch [13514/20000], Training Loss: 0.01625633363021604, Validation Loss: 0.008478879592036985\n",
      "Epoch [13515/20000], Training Loss: 0.007907468863127829, Validation Loss: 0.009254597038014773\n",
      "Epoch [13516/20000], Training Loss: 0.011461809722407321, Validation Loss: 0.00786365554978514\n",
      "Epoch [13517/20000], Training Loss: 0.010587906356834407, Validation Loss: 0.012277542294712205\n",
      "Epoch [13518/20000], Training Loss: 0.01176865428403419, Validation Loss: 0.017469049866381932\n",
      "Epoch [13519/20000], Training Loss: 0.01617671011574982, Validation Loss: 0.019289020936347306\n",
      "Epoch [13520/20000], Training Loss: 0.017284017954287783, Validation Loss: 0.004389547022135787\n",
      "Epoch [13521/20000], Training Loss: 0.010149035313052732, Validation Loss: 0.00789000446622141\n",
      "Epoch [13522/20000], Training Loss: 0.011714737554679491, Validation Loss: 0.008579793537974568\n",
      "Epoch [13523/20000], Training Loss: 0.009223032291629352, Validation Loss: 0.014701523054181312\n",
      "Epoch [13524/20000], Training Loss: 0.009066940667772931, Validation Loss: 0.004236976198485802\n",
      "Epoch [13525/20000], Training Loss: 0.0033858670358313248, Validation Loss: 0.0038072664848641962\n",
      "Epoch [13526/20000], Training Loss: 0.004049061355804692, Validation Loss: 0.008327752227119125\n",
      "Epoch [13527/20000], Training Loss: 0.008961786417525477, Validation Loss: 0.0034922546944746247\n",
      "Epoch [13528/20000], Training Loss: 0.01726088666224054, Validation Loss: 0.008027644876215894\n",
      "Epoch [13529/20000], Training Loss: 0.009033193468764824, Validation Loss: 0.00341054975252218\n",
      "Epoch [13530/20000], Training Loss: 0.006672081418011138, Validation Loss: 0.01477718220512548\n",
      "Epoch [13531/20000], Training Loss: 0.010394430265088366, Validation Loss: 0.004653590678460947\n",
      "Epoch [13532/20000], Training Loss: 0.008701142580581031, Validation Loss: 0.00340523557849234\n",
      "Epoch [13533/20000], Training Loss: 0.008263000248131114, Validation Loss: 0.005193375388806487\n",
      "Epoch [13534/20000], Training Loss: 0.008484837004549004, Validation Loss: 0.002911016217009177\n",
      "Epoch [13535/20000], Training Loss: 0.03682385528944516, Validation Loss: 0.008075194221061923\n",
      "Epoch [13536/20000], Training Loss: 0.014763049668545136, Validation Loss: 0.014400022636628653\n",
      "Epoch [13537/20000], Training Loss: 0.015741431583592203, Validation Loss: 0.03662582596994842\n",
      "Epoch [13538/20000], Training Loss: 0.031980023586324284, Validation Loss: 0.020687250717790397\n",
      "Epoch [13539/20000], Training Loss: 0.022886153210752776, Validation Loss: 0.020818040866058953\n",
      "Epoch [13540/20000], Training Loss: 0.01693364604060272, Validation Loss: 0.006150626899032626\n",
      "Epoch [13541/20000], Training Loss: 0.007922261034504377, Validation Loss: 0.005011224460923407\n",
      "Epoch [13542/20000], Training Loss: 0.00566909209322018, Validation Loss: 0.008367390412904222\n",
      "Epoch [13543/20000], Training Loss: 0.007518969125937604, Validation Loss: 0.004373719768825676\n",
      "Epoch [13544/20000], Training Loss: 0.011146853058432628, Validation Loss: 0.0061952120647253495\n",
      "Epoch [13545/20000], Training Loss: 0.006830916062296767, Validation Loss: 0.0077335281369991\n",
      "Epoch [13546/20000], Training Loss: 0.006839718652762323, Validation Loss: 0.004321002801014677\n",
      "Epoch [13547/20000], Training Loss: 0.004799063830952426, Validation Loss: 0.013640727316417358\n",
      "Epoch [13548/20000], Training Loss: 0.01108130565769118, Validation Loss: 0.007576670938492823\n",
      "Epoch [13549/20000], Training Loss: 0.007531806832828026, Validation Loss: 0.005211392733635157\n",
      "Epoch [13550/20000], Training Loss: 0.009194117811046043, Validation Loss: 0.012143787397047181\n",
      "Epoch [13551/20000], Training Loss: 0.010778188656591478, Validation Loss: 0.011127785447049319\n",
      "Epoch [13552/20000], Training Loss: 0.016530895013212494, Validation Loss: 0.005369555566854319\n",
      "Epoch [13553/20000], Training Loss: 0.00888046646131053, Validation Loss: 0.010932414035565077\n",
      "Epoch [13554/20000], Training Loss: 0.006218106096761338, Validation Loss: 0.005354859684263205\n",
      "Epoch [13555/20000], Training Loss: 0.00664630694414622, Validation Loss: 0.003669995503283888\n",
      "Epoch [13556/20000], Training Loss: 0.011505117856099136, Validation Loss: 0.0101276402176284\n",
      "Epoch [13557/20000], Training Loss: 0.012441394595676376, Validation Loss: 0.01930344751651441\n",
      "Epoch [13558/20000], Training Loss: 0.031628601889159266, Validation Loss: 0.014804814626632301\n",
      "Epoch [13559/20000], Training Loss: 0.017190972042174377, Validation Loss: 0.011194268132220892\n",
      "Epoch [13560/20000], Training Loss: 0.008684906322741881, Validation Loss: 0.004409711169864399\n",
      "Epoch [13561/20000], Training Loss: 0.004425458664107802, Validation Loss: 0.004840130350383046\n",
      "Epoch [13562/20000], Training Loss: 0.006757326550931404, Validation Loss: 0.00765685105594939\n",
      "Epoch [13563/20000], Training Loss: 0.0057193271478398955, Validation Loss: 0.0036992380786459405\n",
      "Epoch [13564/20000], Training Loss: 0.004634466195836596, Validation Loss: 0.003973950951517377\n",
      "Epoch [13565/20000], Training Loss: 0.005520638361174081, Validation Loss: 0.007191169359107945\n",
      "Epoch [13566/20000], Training Loss: 0.005836361723985257, Validation Loss: 0.006062618858564195\n",
      "Epoch [13567/20000], Training Loss: 0.005766041133028921, Validation Loss: 0.003748473756366638\n",
      "Epoch [13568/20000], Training Loss: 0.00451645659839934, Validation Loss: 0.0039772787361237304\n",
      "Epoch [13569/20000], Training Loss: 0.004589027769335579, Validation Loss: 0.0106517772988591\n",
      "Epoch [13570/20000], Training Loss: 0.005329335729974056, Validation Loss: 0.004092873304362714\n",
      "Epoch [13571/20000], Training Loss: 0.010229429911955126, Validation Loss: 0.003474376710011862\n",
      "Epoch [13572/20000], Training Loss: 0.018324801546571377, Validation Loss: 0.01633166288674905\n",
      "Epoch [13573/20000], Training Loss: 0.04329425347324494, Validation Loss: 0.021489757878276592\n",
      "Epoch [13574/20000], Training Loss: 0.015397723421171707, Validation Loss: 0.006521795292435069\n",
      "Epoch [13575/20000], Training Loss: 0.008759969235792855, Validation Loss: 0.004241619100686275\n",
      "Epoch [13576/20000], Training Loss: 0.007777642911865509, Validation Loss: 0.008810662421820683\n",
      "Epoch [13577/20000], Training Loss: 0.011751530207610423, Validation Loss: 0.015270164236880351\n",
      "Epoch [13578/20000], Training Loss: 0.007821133917397154, Validation Loss: 0.0050249686571785124\n",
      "Epoch [13579/20000], Training Loss: 0.00714330879847174, Validation Loss: 0.004117565733599385\n",
      "Epoch [13580/20000], Training Loss: 0.0066283708230392745, Validation Loss: 0.0027592951489554224\n",
      "Epoch [13581/20000], Training Loss: 0.004782734966999895, Validation Loss: 0.003253763066876608\n",
      "Epoch [13582/20000], Training Loss: 0.003833833806441232, Validation Loss: 0.0027328822380387552\n",
      "Epoch [13583/20000], Training Loss: 0.005977451620180675, Validation Loss: 0.004274570951966515\n",
      "Epoch [13584/20000], Training Loss: 0.011457293934600139, Validation Loss: 0.00603887836189492\n",
      "Epoch [13585/20000], Training Loss: 0.005545062033434078, Validation Loss: 0.007488765142446547\n",
      "Epoch [13586/20000], Training Loss: 0.008360779395853715, Validation Loss: 0.008240056473669367\n",
      "Epoch [13587/20000], Training Loss: 0.010296266270285872, Validation Loss: 0.0028884906075586953\n",
      "Epoch [13588/20000], Training Loss: 0.00845958418040027, Validation Loss: 0.003951290870780166\n",
      "Epoch [13589/20000], Training Loss: 0.00582562994428112, Validation Loss: 0.0028260525008558584\n",
      "Epoch [13590/20000], Training Loss: 0.005837150682574637, Validation Loss: 0.0030712107209870296\n",
      "Epoch [13591/20000], Training Loss: 0.003952977062903683, Validation Loss: 0.0029307657386772323\n",
      "Epoch [13592/20000], Training Loss: 0.008527930697060324, Validation Loss: 0.0025126021691354067\n",
      "Epoch [13593/20000], Training Loss: 0.0044246454622875065, Validation Loss: 0.007939908391162702\n",
      "Epoch [13594/20000], Training Loss: 0.007262079244997634, Validation Loss: 0.0032344886107956786\n",
      "Epoch [13595/20000], Training Loss: 0.006956543228755306, Validation Loss: 0.010213666852171736\n",
      "Epoch [13596/20000], Training Loss: 0.0051579638093244284, Validation Loss: 0.021191908844879696\n",
      "Epoch [13597/20000], Training Loss: 0.006683652125171223, Validation Loss: 0.010310787582886125\n",
      "Epoch [13598/20000], Training Loss: 0.004409109225629696, Validation Loss: 0.01369030500895242\n",
      "Epoch [13599/20000], Training Loss: 0.006051618255631703, Validation Loss: 0.010778166075049318\n",
      "Epoch [13600/20000], Training Loss: 0.006841980753709176, Validation Loss: 0.00991108225259445\n",
      "Epoch [13601/20000], Training Loss: 0.014844499889087663, Validation Loss: 0.004231488149451608\n",
      "Epoch [13602/20000], Training Loss: 0.006572770991234782, Validation Loss: 0.007918595241156932\n",
      "Epoch [13603/20000], Training Loss: 0.0280751654388171, Validation Loss: 0.013239653648010833\n",
      "Epoch [13604/20000], Training Loss: 0.005162641290745744, Validation Loss: 0.013682913567339151\n",
      "Epoch [13605/20000], Training Loss: 0.016708273035354586, Validation Loss: 0.09052774097238268\n",
      "Epoch [13606/20000], Training Loss: 0.02864549289058362, Validation Loss: 0.01959684332144567\n",
      "Epoch [13607/20000], Training Loss: 0.018243162040042598, Validation Loss: 0.018155733385445978\n",
      "Epoch [13608/20000], Training Loss: 0.017345730275597555, Validation Loss: 0.009854360511131486\n",
      "Epoch [13609/20000], Training Loss: 0.012092235286088129, Validation Loss: 0.010188203457265186\n",
      "Epoch [13610/20000], Training Loss: 0.006191355593078437, Validation Loss: 0.0054966069968876196\n",
      "Epoch [13611/20000], Training Loss: 0.004650015513464366, Validation Loss: 0.004226554919081796\n",
      "Epoch [13612/20000], Training Loss: 0.007478448283239102, Validation Loss: 0.005165239293091872\n",
      "Epoch [13613/20000], Training Loss: 0.022603671838753923, Validation Loss: 0.04067921930592482\n",
      "Epoch [13614/20000], Training Loss: 0.027966180728635135, Validation Loss: 0.019093547171855556\n",
      "Epoch [13615/20000], Training Loss: 0.015068468172103167, Validation Loss: 0.005703190782566837\n",
      "Epoch [13616/20000], Training Loss: 0.007794956077954599, Validation Loss: 0.00557752845822636\n",
      "Epoch [13617/20000], Training Loss: 0.007533623299033414, Validation Loss: 0.00356341246541498\n",
      "Epoch [13618/20000], Training Loss: 0.004784859434169318, Validation Loss: 0.004380103954255381\n",
      "Epoch [13619/20000], Training Loss: 0.00577434504498342, Validation Loss: 0.003335696677368105\n",
      "Epoch [13620/20000], Training Loss: 0.006870113200420747, Validation Loss: 0.0039204077106985225\n",
      "Epoch [13621/20000], Training Loss: 0.004680506120036755, Validation Loss: 0.0037723465989724253\n",
      "Epoch [13622/20000], Training Loss: 0.004507283798635139, Validation Loss: 0.0050639387051368044\n",
      "Epoch [13623/20000], Training Loss: 0.004827886431095456, Validation Loss: 0.008290766077412368\n",
      "Epoch [13624/20000], Training Loss: 0.01694776339510489, Validation Loss: 0.04081524072950872\n",
      "Epoch [13625/20000], Training Loss: 0.027372214818439846, Validation Loss: 0.00672297617087939\n",
      "Epoch [13626/20000], Training Loss: 0.0082153218910597, Validation Loss: 0.0032678658429696433\n",
      "Epoch [13627/20000], Training Loss: 0.006684197204387081, Validation Loss: 0.007307697795048236\n",
      "Epoch [13628/20000], Training Loss: 0.008482991482846305, Validation Loss: 0.015714390065698223\n",
      "Epoch [13629/20000], Training Loss: 0.011236239724480259, Validation Loss: 0.006276980067598613\n",
      "Epoch [13630/20000], Training Loss: 0.010299887267840242, Validation Loss: 0.008940030384240214\n",
      "Epoch [13631/20000], Training Loss: 0.0069258829254457465, Validation Loss: 0.00345276970516092\n",
      "Epoch [13632/20000], Training Loss: 0.006729326335646745, Validation Loss: 0.010898036316979593\n",
      "Epoch [13633/20000], Training Loss: 0.005918476149547912, Validation Loss: 0.002791088676612422\n",
      "Epoch [13634/20000], Training Loss: 0.009253834057744825, Validation Loss: 0.01425144874545887\n",
      "Epoch [13635/20000], Training Loss: 0.016546618181954336, Validation Loss: 0.005715816482246997\n",
      "Epoch [13636/20000], Training Loss: 0.0066914749821600185, Validation Loss: 0.004424393127188913\n",
      "Epoch [13637/20000], Training Loss: 0.006726877923938446, Validation Loss: 0.008441916082509644\n",
      "Epoch [13638/20000], Training Loss: 0.007159368579907875, Validation Loss: 0.017689303508947467\n",
      "Epoch [13639/20000], Training Loss: 0.008198385996885398, Validation Loss: 0.004165363341387872\n",
      "Epoch [13640/20000], Training Loss: 0.015926263635005204, Validation Loss: 0.028103634356394482\n",
      "Epoch [13641/20000], Training Loss: 0.011790036489920957, Validation Loss: 0.006013208282394901\n",
      "Epoch [13642/20000], Training Loss: 0.011077960166273573, Validation Loss: 0.030054842780857984\n",
      "Epoch [13643/20000], Training Loss: 0.02905272103946897, Validation Loss: 0.029473119503750565\n",
      "Epoch [13644/20000], Training Loss: 0.033837116824413115, Validation Loss: 0.0038618969008439308\n",
      "Epoch [13645/20000], Training Loss: 0.043814836211302985, Validation Loss: 0.04543979865931241\n",
      "Epoch [13646/20000], Training Loss: 0.031206027533958798, Validation Loss: 0.016086711927065216\n",
      "Epoch [13647/20000], Training Loss: 0.017018777393137237, Validation Loss: 0.02918952430219766\n",
      "Epoch [13648/20000], Training Loss: 0.023464360054016913, Validation Loss: 0.0061842782449405475\n",
      "Epoch [13649/20000], Training Loss: 0.012000527609156311, Validation Loss: 0.006038115213284202\n",
      "Epoch [13650/20000], Training Loss: 0.007249453296286187, Validation Loss: 0.008203662464455062\n",
      "Epoch [13651/20000], Training Loss: 0.006882134941406548, Validation Loss: 0.004182986948080725\n",
      "Epoch [13652/20000], Training Loss: 0.004880776805553718, Validation Loss: 0.0038244240339996133\n",
      "Epoch [13653/20000], Training Loss: 0.005628268375793206, Validation Loss: 0.004339677357977832\n",
      "Epoch [13654/20000], Training Loss: 0.0071606591456137335, Validation Loss: 0.004638312621799352\n",
      "Epoch [13655/20000], Training Loss: 0.007301912956920985, Validation Loss: 0.0060809691016012425\n",
      "Epoch [13656/20000], Training Loss: 0.004577736150132054, Validation Loss: 0.014300429567070085\n",
      "Epoch [13657/20000], Training Loss: 0.01503378437025406, Validation Loss: 0.01426398082808841\n",
      "Epoch [13658/20000], Training Loss: 0.02746719417960516, Validation Loss: 0.03822368330104707\n",
      "Epoch [13659/20000], Training Loss: 0.026504848302075907, Validation Loss: 0.007381060198050753\n",
      "Epoch [13660/20000], Training Loss: 0.04325546763201211, Validation Loss: 0.0852932163698056\n",
      "Epoch [13661/20000], Training Loss: 0.02946002738566936, Validation Loss: 0.005280535023757693\n",
      "Epoch [13662/20000], Training Loss: 0.00861432705784994, Validation Loss: 0.00799279616174837\n",
      "Epoch [13663/20000], Training Loss: 0.007057991302904806, Validation Loss: 0.012561772115451017\n",
      "Epoch [13664/20000], Training Loss: 0.00818073607765005, Validation Loss: 0.005251082492577552\n",
      "Epoch [13665/20000], Training Loss: 0.00636108558059537, Validation Loss: 0.005771511516253928\n",
      "Epoch [13666/20000], Training Loss: 0.007464425729787243, Validation Loss: 0.0042166677533259644\n",
      "Epoch [13667/20000], Training Loss: 0.01866482259772186, Validation Loss: 0.007639517535657043\n",
      "Epoch [13668/20000], Training Loss: 0.02549143318070232, Validation Loss: 0.012708603829586147\n",
      "Epoch [13669/20000], Training Loss: 0.01505554428357365, Validation Loss: 0.006010975242134009\n",
      "Epoch [13670/20000], Training Loss: 0.0064138234229176305, Validation Loss: 0.00518524731450855\n",
      "Epoch [13671/20000], Training Loss: 0.005334530387439632, Validation Loss: 0.004364125596566737\n",
      "Epoch [13672/20000], Training Loss: 0.004540683903111845, Validation Loss: 0.005771894069507003\n",
      "Epoch [13673/20000], Training Loss: 0.005270016080300722, Validation Loss: 0.003904943500580365\n",
      "Epoch [13674/20000], Training Loss: 0.00432949688651466, Validation Loss: 0.003463363548868464\n",
      "Epoch [13675/20000], Training Loss: 0.0043429657583016834, Validation Loss: 0.00323479938176011\n",
      "Epoch [13676/20000], Training Loss: 0.004044622705025956, Validation Loss: 0.0029879822693114227\n",
      "Epoch [13677/20000], Training Loss: 0.004435234581511135, Validation Loss: 0.0030881765268223455\n",
      "Epoch [13678/20000], Training Loss: 0.0041706017842183685, Validation Loss: 0.0030889698881537697\n",
      "Epoch [13679/20000], Training Loss: 0.0037999836474357706, Validation Loss: 0.009291892261111505\n",
      "Epoch [13680/20000], Training Loss: 0.0037216949084332945, Validation Loss: 0.005640206408868055\n",
      "Epoch [13681/20000], Training Loss: 0.005863684136914214, Validation Loss: 0.0031457317329812084\n",
      "Epoch [13682/20000], Training Loss: 0.004323179214095164, Validation Loss: 0.0041078350190722245\n",
      "Epoch [13683/20000], Training Loss: 0.004030192834241981, Validation Loss: 0.014029442529044494\n",
      "Epoch [13684/20000], Training Loss: 0.009274389228169509, Validation Loss: 0.008877185523435796\n",
      "Epoch [13685/20000], Training Loss: 0.011187847005723077, Validation Loss: 0.00835766872325751\n",
      "Epoch [13686/20000], Training Loss: 0.014454038182001179, Validation Loss: 0.006204522555021542\n",
      "Epoch [13687/20000], Training Loss: 0.014392407988842544, Validation Loss: 0.01630893586064173\n",
      "Epoch [13688/20000], Training Loss: 0.027878797414554617, Validation Loss: 0.013275540239858203\n",
      "Epoch [13689/20000], Training Loss: 0.009853865530723331, Validation Loss: 0.009234446422238664\n",
      "Epoch [13690/20000], Training Loss: 0.007275012668937312, Validation Loss: 0.0037944415338212628\n",
      "Epoch [13691/20000], Training Loss: 0.007921726054129457, Validation Loss: 0.007089277224683396\n",
      "Epoch [13692/20000], Training Loss: 0.013167721815989353, Validation Loss: 0.017912913249914977\n",
      "Epoch [13693/20000], Training Loss: 0.016720917863754688, Validation Loss: 0.004901430294694364\n",
      "Epoch [13694/20000], Training Loss: 0.010767597404992557, Validation Loss: 0.003986979109052007\n",
      "Epoch [13695/20000], Training Loss: 0.010673332558196438, Validation Loss: 0.007709470128442523\n",
      "Epoch [13696/20000], Training Loss: 0.03141937312154498, Validation Loss: 0.008091448835963482\n",
      "Epoch [13697/20000], Training Loss: 0.02934713101629833, Validation Loss: 0.008756314961491032\n",
      "Epoch [13698/20000], Training Loss: 0.02367229480955757, Validation Loss: 0.01876839026510067\n",
      "Epoch [13699/20000], Training Loss: 0.011650788892958579, Validation Loss: 0.005161660163334351\n",
      "Epoch [13700/20000], Training Loss: 0.007440382344481934, Validation Loss: 0.011702318444503947\n",
      "Epoch [13701/20000], Training Loss: 0.012223444928947304, Validation Loss: 0.004071979860343943\n",
      "Epoch [13702/20000], Training Loss: 0.005774054807199518, Validation Loss: 0.004647624091311238\n",
      "Epoch [13703/20000], Training Loss: 0.005562496891278508, Validation Loss: 0.0038225094837278967\n",
      "Epoch [13704/20000], Training Loss: 0.005274707800708711, Validation Loss: 0.005200274766780565\n",
      "Epoch [13705/20000], Training Loss: 0.00504196057279062, Validation Loss: 0.0035131583912613807\n",
      "Epoch [13706/20000], Training Loss: 0.005913460384525544, Validation Loss: 0.0034494114516405716\n",
      "Epoch [13707/20000], Training Loss: 0.008653487686972636, Validation Loss: 0.010611787209305541\n",
      "Epoch [13708/20000], Training Loss: 0.008296728274798073, Validation Loss: 0.009949174044290235\n",
      "Epoch [13709/20000], Training Loss: 0.01692007114602997, Validation Loss: 0.004322064828906085\n",
      "Epoch [13710/20000], Training Loss: 0.012631718742860747, Validation Loss: 0.015608478736957554\n",
      "Epoch [13711/20000], Training Loss: 0.013503553815618423, Validation Loss: 0.008224076128397277\n",
      "Epoch [13712/20000], Training Loss: 0.014040607069286384, Validation Loss: 0.005806933446206942\n",
      "Epoch [13713/20000], Training Loss: 0.008925282892521971, Validation Loss: 0.006930708013864465\n",
      "Epoch [13714/20000], Training Loss: 0.009015815569520262, Validation Loss: 0.0076394629322733875\n",
      "Epoch [13715/20000], Training Loss: 0.011465344153943338, Validation Loss: 0.027544840638551105\n",
      "Epoch [13716/20000], Training Loss: 0.01808999843966116, Validation Loss: 0.0044322783078558\n",
      "Epoch [13717/20000], Training Loss: 0.015147842558560245, Validation Loss: 0.017074898573875025\n",
      "Epoch [13718/20000], Training Loss: 0.01640975015236888, Validation Loss: 0.024517587501999775\n",
      "Epoch [13719/20000], Training Loss: 0.016399186247976365, Validation Loss: 0.003914763189941693\n",
      "Epoch [13720/20000], Training Loss: 0.015861057364548157, Validation Loss: 0.009303965314655573\n",
      "Epoch [13721/20000], Training Loss: 0.007011542281751255, Validation Loss: 0.009644303876159808\n",
      "Epoch [13722/20000], Training Loss: 0.0063265065920339635, Validation Loss: 0.004853974515616041\n",
      "Epoch [13723/20000], Training Loss: 0.004997723153792322, Validation Loss: 0.004269897330412878\n",
      "Epoch [13724/20000], Training Loss: 0.0041145415244890115, Validation Loss: 0.005726164663144573\n",
      "Epoch [13725/20000], Training Loss: 0.008801290034170961, Validation Loss: 0.017811432484900195\n",
      "Epoch [13726/20000], Training Loss: 0.014274520354644795, Validation Loss: 0.014328328028115535\n",
      "Epoch [13727/20000], Training Loss: 0.0071374522451930844, Validation Loss: 0.007715393262190316\n",
      "Epoch [13728/20000], Training Loss: 0.012410111226927256, Validation Loss: 0.06112288850876472\n",
      "Epoch [13729/20000], Training Loss: 0.029758595589685553, Validation Loss: 0.004855139525115452\n",
      "Epoch [13730/20000], Training Loss: 0.01600657187684971, Validation Loss: 0.019162639185554196\n",
      "Epoch [13731/20000], Training Loss: 0.02189067146952896, Validation Loss: 0.013093242532607161\n",
      "Epoch [13732/20000], Training Loss: 0.012909919164875256, Validation Loss: 0.004138469742794574\n",
      "Epoch [13733/20000], Training Loss: 0.008754159328670344, Validation Loss: 0.0038301435601714013\n",
      "Epoch [13734/20000], Training Loss: 0.00542939712925415, Validation Loss: 0.003316558825657369\n",
      "Epoch [13735/20000], Training Loss: 0.005593767315856114, Validation Loss: 0.0032061670052720664\n",
      "Epoch [13736/20000], Training Loss: 0.014763163565865918, Validation Loss: 0.00593846147174507\n",
      "Epoch [13737/20000], Training Loss: 0.014079114675821205, Validation Loss: 0.009314658682588612\n",
      "Epoch [13738/20000], Training Loss: 0.008893183328577184, Validation Loss: 0.005279122012260424\n",
      "Epoch [13739/20000], Training Loss: 0.005029045873795569, Validation Loss: 0.00380038339863274\n",
      "Epoch [13740/20000], Training Loss: 0.005717002075730956, Validation Loss: 0.00482170233764074\n",
      "Epoch [13741/20000], Training Loss: 0.0054416287062589875, Validation Loss: 0.023459011654787382\n",
      "Epoch [13742/20000], Training Loss: 0.012307662818009086, Validation Loss: 0.013000834866622702\n",
      "Epoch [13743/20000], Training Loss: 0.013753983305832662, Validation Loss: 0.020824806765413872\n",
      "Epoch [13744/20000], Training Loss: 0.01204208493540396, Validation Loss: 0.009360145849029777\n",
      "Epoch [13745/20000], Training Loss: 0.0340550020253951, Validation Loss: 0.02238065627443575\n",
      "Epoch [13746/20000], Training Loss: 0.04004349226436586, Validation Loss: 0.040632721568910325\n",
      "Epoch [13747/20000], Training Loss: 0.039341072468752306, Validation Loss: 0.054771340320600034\n",
      "Epoch [13748/20000], Training Loss: 0.04945884676167874, Validation Loss: 0.00685866309515047\n",
      "Epoch [13749/20000], Training Loss: 0.018032514042577726, Validation Loss: 0.025181146769032887\n",
      "Epoch [13750/20000], Training Loss: 0.013623208183811844, Validation Loss: 0.010591890983440473\n",
      "Epoch [13751/20000], Training Loss: 0.009705452084225337, Validation Loss: 0.009723648000983433\n",
      "Epoch [13752/20000], Training Loss: 0.00894157905713655, Validation Loss: 0.015412378293409088\n",
      "Epoch [13753/20000], Training Loss: 0.01337755746291285, Validation Loss: 0.010129080889754505\n",
      "Epoch [13754/20000], Training Loss: 0.007858285404446568, Validation Loss: 0.006009608058434424\n",
      "Epoch [13755/20000], Training Loss: 0.008430394612592604, Validation Loss: 0.005763051314391175\n",
      "Epoch [13756/20000], Training Loss: 0.006183540390338749, Validation Loss: 0.005924697821699608\n",
      "Epoch [13757/20000], Training Loss: 0.005391231264054243, Validation Loss: 0.004649495721342386\n",
      "Epoch [13758/20000], Training Loss: 0.004843314806099183, Validation Loss: 0.004463636535263049\n",
      "Epoch [13759/20000], Training Loss: 0.005588966373969535, Validation Loss: 0.005655506600565526\n",
      "Epoch [13760/20000], Training Loss: 0.00520596540549637, Validation Loss: 0.0036840557475719443\n",
      "Epoch [13761/20000], Training Loss: 0.005364664543386815, Validation Loss: 0.005827720481647702\n",
      "Epoch [13762/20000], Training Loss: 0.00852129454987757, Validation Loss: 0.0030221259641791676\n",
      "Epoch [13763/20000], Training Loss: 0.014776589232496917, Validation Loss: 0.004189114880931909\n",
      "Epoch [13764/20000], Training Loss: 0.01320168761386802, Validation Loss: 0.003676831315301245\n",
      "Epoch [13765/20000], Training Loss: 0.007609791974053743, Validation Loss: 0.02835291304758697\n",
      "Epoch [13766/20000], Training Loss: 0.021704722269013082, Validation Loss: 0.0036034222964969687\n",
      "Epoch [13767/20000], Training Loss: 0.01907064107113651, Validation Loss: 0.005776491607231752\n",
      "Epoch [13768/20000], Training Loss: 0.006398040075769781, Validation Loss: 0.004623981876306971\n",
      "Epoch [13769/20000], Training Loss: 0.0051607337206535575, Validation Loss: 0.004049329397373315\n",
      "Epoch [13770/20000], Training Loss: 0.006935577529865051, Validation Loss: 0.006579037500415552\n",
      "Epoch [13771/20000], Training Loss: 0.004231148973953428, Validation Loss: 0.004253615990856068\n",
      "Epoch [13772/20000], Training Loss: 0.005286720383862432, Validation Loss: 0.0030837731628220615\n",
      "Epoch [13773/20000], Training Loss: 0.006181838378584611, Validation Loss: 0.002739860161836662\n",
      "Epoch [13774/20000], Training Loss: 0.003674524766081179, Validation Loss: 0.002838925033748702\n",
      "Epoch [13775/20000], Training Loss: 0.007938067451634976, Validation Loss: 0.00387557216146612\n",
      "Epoch [13776/20000], Training Loss: 0.006455508117142017, Validation Loss: 0.005166763073977977\n",
      "Epoch [13777/20000], Training Loss: 0.006081203820161006, Validation Loss: 0.009473992374353582\n",
      "Epoch [13778/20000], Training Loss: 0.013791447163904584, Validation Loss: 0.004102223598887836\n",
      "Epoch [13779/20000], Training Loss: 0.05563252321319721, Validation Loss: 0.007311725521386084\n",
      "Epoch [13780/20000], Training Loss: 0.026785324444063008, Validation Loss: 0.006073057835822381\n",
      "Epoch [13781/20000], Training Loss: 0.024878666051725822, Validation Loss: 0.0516555702199964\n",
      "Epoch [13782/20000], Training Loss: 0.04324744222269926, Validation Loss: 0.009870022673778425\n",
      "Epoch [13783/20000], Training Loss: 0.008419634402214018, Validation Loss: 0.012473581222780337\n",
      "Epoch [13784/20000], Training Loss: 0.015511656778731517, Validation Loss: 0.014790545627095395\n",
      "Epoch [13785/20000], Training Loss: 0.011296490186306787, Validation Loss: 0.006233299750744793\n",
      "Epoch [13786/20000], Training Loss: 0.009952192209246797, Validation Loss: 0.009059150999308128\n",
      "Epoch [13787/20000], Training Loss: 0.008115526829247497, Validation Loss: 0.010129038419092597\n",
      "Epoch [13788/20000], Training Loss: 0.008913808161326284, Validation Loss: 0.004945840921712781\n",
      "Epoch [13789/20000], Training Loss: 0.006338027966973771, Validation Loss: 0.004266571074822423\n",
      "Epoch [13790/20000], Training Loss: 0.007616619256168633, Validation Loss: 0.010978382132666045\n",
      "Epoch [13791/20000], Training Loss: 0.0039782887782036725, Validation Loss: 0.014949957425674907\n",
      "Epoch [13792/20000], Training Loss: 0.015357138862278978, Validation Loss: 0.006702683857960372\n",
      "Epoch [13793/20000], Training Loss: 0.006484850835736974, Validation Loss: 0.01678385838815339\n",
      "Epoch [13794/20000], Training Loss: 0.012996930004322036, Validation Loss: 0.005325868504054857\n",
      "Epoch [13795/20000], Training Loss: 0.005313178439142315, Validation Loss: 0.007120261400392077\n",
      "Epoch [13796/20000], Training Loss: 0.006531153398619998, Validation Loss: 0.012076589027488208\n",
      "Epoch [13797/20000], Training Loss: 0.009972318069880462, Validation Loss: 0.0067120568045433105\n",
      "Epoch [13798/20000], Training Loss: 0.018023205721484765, Validation Loss: 0.01002133054423585\n",
      "Epoch [13799/20000], Training Loss: 0.022342595720277, Validation Loss: 0.01752026173674689\n",
      "Epoch [13800/20000], Training Loss: 0.012892455306428019, Validation Loss: 0.004047423231278506\n",
      "Epoch [13801/20000], Training Loss: 0.005843320930060665, Validation Loss: 0.003871750619753454\n",
      "Epoch [13802/20000], Training Loss: 0.004731227733178197, Validation Loss: 0.0038043912140080594\n",
      "Epoch [13803/20000], Training Loss: 0.007895932848831373, Validation Loss: 0.0038641699696069865\n",
      "Epoch [13804/20000], Training Loss: 0.015450032486114651, Validation Loss: 0.006114916933354054\n",
      "Epoch [13805/20000], Training Loss: 0.005634733625421566, Validation Loss: 0.004424647973369896\n",
      "Epoch [13806/20000], Training Loss: 0.00387469112060249, Validation Loss: 0.004296531289436903\n",
      "Epoch [13807/20000], Training Loss: 0.006269594531269311, Validation Loss: 0.004744729847120409\n",
      "Epoch [13808/20000], Training Loss: 0.0047830545143889526, Validation Loss: 0.0033165577017492914\n",
      "Epoch [13809/20000], Training Loss: 0.0075742503192616694, Validation Loss: 0.004773709663969125\n",
      "Epoch [13810/20000], Training Loss: 0.005840389807417523, Validation Loss: 0.0038942453992884146\n",
      "Epoch [13811/20000], Training Loss: 0.007444348298512133, Validation Loss: 0.004533092491483005\n",
      "Epoch [13812/20000], Training Loss: 0.004782683134960409, Validation Loss: 0.007367288665239867\n",
      "Epoch [13813/20000], Training Loss: 0.011976945579850249, Validation Loss: 0.007686596206617935\n",
      "Epoch [13814/20000], Training Loss: 0.013439180496724086, Validation Loss: 0.00453157799985401\n",
      "Epoch [13815/20000], Training Loss: 0.008972424873373288, Validation Loss: 0.003469472683996904\n",
      "Epoch [13816/20000], Training Loss: 0.008112453323389803, Validation Loss: 0.013127096276726114\n",
      "Epoch [13817/20000], Training Loss: 0.006389885853169399, Validation Loss: 0.004363912217822222\n",
      "Epoch [13818/20000], Training Loss: 0.005093241303256946, Validation Loss: 0.026923017080654126\n",
      "Epoch [13819/20000], Training Loss: 0.011776659472941122, Validation Loss: 0.003817523393970695\n",
      "Epoch [13820/20000], Training Loss: 0.005580418811275324, Validation Loss: 0.011391186207057924\n",
      "Epoch [13821/20000], Training Loss: 0.011581901207441498, Validation Loss: 0.006254943721048353\n",
      "Epoch [13822/20000], Training Loss: 0.007293206310610653, Validation Loss: 0.0033159571838500108\n",
      "Epoch [13823/20000], Training Loss: 0.0055891178552493715, Validation Loss: 0.013507169593198627\n",
      "Epoch [13824/20000], Training Loss: 0.010539616549587143, Validation Loss: 0.0028268423554566524\n",
      "Epoch [13825/20000], Training Loss: 0.032604013075407626, Validation Loss: 0.05408169994962009\n",
      "Epoch [13826/20000], Training Loss: 0.02159508107901534, Validation Loss: 0.007302467779794597\n",
      "Epoch [13827/20000], Training Loss: 0.014258021845307667, Validation Loss: 0.007019013775675376\n",
      "Epoch [13828/20000], Training Loss: 0.006617401682888158, Validation Loss: 0.003539001385486732\n",
      "Epoch [13829/20000], Training Loss: 0.006484177558637124, Validation Loss: 0.004995754673196018\n",
      "Epoch [13830/20000], Training Loss: 0.007254952041486311, Validation Loss: 0.0109338294572667\n",
      "Epoch [13831/20000], Training Loss: 0.005396215559033278, Validation Loss: 0.007559320819542644\n",
      "Epoch [13832/20000], Training Loss: 0.006897006342894331, Validation Loss: 0.005713616534395426\n",
      "Epoch [13833/20000], Training Loss: 0.007426470918029996, Validation Loss: 0.02773856118853629\n",
      "Epoch [13834/20000], Training Loss: 0.0212540249839159, Validation Loss: 0.006934641439387284\n",
      "Epoch [13835/20000], Training Loss: 0.011012392303590397, Validation Loss: 0.04668554876531873\n",
      "Epoch [13836/20000], Training Loss: 0.017464944667283686, Validation Loss: 0.019286267901893615\n",
      "Epoch [13837/20000], Training Loss: 0.02926117379314113, Validation Loss: 0.008329165034132708\n",
      "Epoch [13838/20000], Training Loss: 0.012873971502943147, Validation Loss: 0.004382709531049451\n",
      "Epoch [13839/20000], Training Loss: 0.012418126968311429, Validation Loss: 0.009709199062496867\n",
      "Epoch [13840/20000], Training Loss: 0.025893697182774695, Validation Loss: 0.010284451865887615\n",
      "Epoch [13841/20000], Training Loss: 0.017296432706643827, Validation Loss: 0.023161494022037488\n",
      "Epoch [13842/20000], Training Loss: 0.01563653874476196, Validation Loss: 0.005893952629711359\n",
      "Epoch [13843/20000], Training Loss: 0.004414472525240853, Validation Loss: 0.004171964763551321\n",
      "Epoch [13844/20000], Training Loss: 0.005841599046042185, Validation Loss: 0.00812807905452725\n",
      "Epoch [13845/20000], Training Loss: 0.004845021331545597, Validation Loss: 0.004701190127580698\n",
      "Epoch [13846/20000], Training Loss: 0.00426317063102033, Validation Loss: 0.0026211992818697843\n",
      "Epoch [13847/20000], Training Loss: 0.004212830148130057, Validation Loss: 0.007079985170998425\n",
      "Epoch [13848/20000], Training Loss: 0.0046898695878293695, Validation Loss: 0.009869394828163942\n",
      "Epoch [13849/20000], Training Loss: 0.009124261234059563, Validation Loss: 0.004339534069799811\n",
      "Epoch [13850/20000], Training Loss: 0.008681947034866815, Validation Loss: 0.00246679953127682\n",
      "Epoch [13851/20000], Training Loss: 0.06168592289398183, Validation Loss: 0.08175361277738245\n",
      "Epoch [13852/20000], Training Loss: 0.06702398093433917, Validation Loss: 0.04132982024125103\n",
      "Epoch [13853/20000], Training Loss: 0.03319526939386768, Validation Loss: 0.009475668945486284\n",
      "Epoch [13854/20000], Training Loss: 0.012324621634823936, Validation Loss: 0.011003472102726976\n",
      "Epoch [13855/20000], Training Loss: 0.008193010256426143, Validation Loss: 0.007858088812294486\n",
      "Epoch [13856/20000], Training Loss: 0.009371725291527713, Validation Loss: 0.005207984833711115\n",
      "Epoch [13857/20000], Training Loss: 0.00894560790110128, Validation Loss: 0.004677264141621761\n",
      "Epoch [13858/20000], Training Loss: 0.006458154927323838, Validation Loss: 0.007770847163718437\n",
      "Epoch [13859/20000], Training Loss: 0.007901451764545138, Validation Loss: 0.004309624658430623\n",
      "Epoch [13860/20000], Training Loss: 0.007127821732345703, Validation Loss: 0.003988189912856147\n",
      "Epoch [13861/20000], Training Loss: 0.009265842925222907, Validation Loss: 0.003982436239287576\n",
      "Epoch [13862/20000], Training Loss: 0.0094992417476273, Validation Loss: 0.0037359597212197775\n",
      "Epoch [13863/20000], Training Loss: 0.005072443324024789, Validation Loss: 0.004763595003091885\n",
      "Epoch [13864/20000], Training Loss: 0.006162145808048081, Validation Loss: 0.003169425330976797\n",
      "Epoch [13865/20000], Training Loss: 0.009545774993187348, Validation Loss: 0.005421620066316889\n",
      "Epoch [13866/20000], Training Loss: 0.020966247514089837, Validation Loss: 0.013389147246178774\n",
      "Epoch [13867/20000], Training Loss: 0.024476786932999466, Validation Loss: 0.06567185353696914\n",
      "Epoch [13868/20000], Training Loss: 0.05151729031682147, Validation Loss: 0.008461354423015368\n",
      "Epoch [13869/20000], Training Loss: 0.01021585726929126, Validation Loss: 0.01254945866914373\n",
      "Epoch [13870/20000], Training Loss: 0.008357435015828482, Validation Loss: 0.006917197735317261\n",
      "Epoch [13871/20000], Training Loss: 0.00904559477307235, Validation Loss: 0.004854735100642367\n",
      "Epoch [13872/20000], Training Loss: 0.006274999709733363, Validation Loss: 0.010150092738038146\n",
      "Epoch [13873/20000], Training Loss: 0.008832148970603677, Validation Loss: 0.0040615202792001325\n",
      "Epoch [13874/20000], Training Loss: 0.007473305716952642, Validation Loss: 0.004178388266162918\n",
      "Epoch [13875/20000], Training Loss: 0.007279470136771644, Validation Loss: 0.0038628810620561387\n",
      "Epoch [13876/20000], Training Loss: 0.005792801083381554, Validation Loss: 0.005294007892319185\n",
      "Epoch [13877/20000], Training Loss: 0.007883862290847381, Validation Loss: 0.003365213647579627\n",
      "Epoch [13878/20000], Training Loss: 0.008582571600917228, Validation Loss: 0.01466448382793065\n",
      "Epoch [13879/20000], Training Loss: 0.00829770395336839, Validation Loss: 0.014985810433409774\n",
      "Epoch [13880/20000], Training Loss: 0.008483226701563191, Validation Loss: 0.0034703921543268734\n",
      "Epoch [13881/20000], Training Loss: 0.004924280726949551, Validation Loss: 0.005195984950514035\n",
      "Epoch [13882/20000], Training Loss: 0.004590514076491152, Validation Loss: 0.010708880175112004\n",
      "Epoch [13883/20000], Training Loss: 0.00608157919227129, Validation Loss: 0.004740954001744869\n",
      "Epoch [13884/20000], Training Loss: 0.004407232318434191, Validation Loss: 0.004462931127487683\n",
      "Epoch [13885/20000], Training Loss: 0.007697988413253499, Validation Loss: 0.007695411650733571\n",
      "Epoch [13886/20000], Training Loss: 0.006515140603629074, Validation Loss: 0.005084115992646941\n",
      "Epoch [13887/20000], Training Loss: 0.00616124777713724, Validation Loss: 0.0030561918638990653\n",
      "Epoch [13888/20000], Training Loss: 0.0061625357328531595, Validation Loss: 0.0023767475050492847\n",
      "Epoch [13889/20000], Training Loss: 0.006504641628583029, Validation Loss: 0.01636018683881663\n",
      "Epoch [13890/20000], Training Loss: 0.015419579187307266, Validation Loss: 0.0052751866968573525\n",
      "Epoch [13891/20000], Training Loss: 0.03724008965738384, Validation Loss: 0.006852506042767865\n",
      "Epoch [13892/20000], Training Loss: 0.028649690652465715, Validation Loss: 0.02699016753051962\n",
      "Epoch [13893/20000], Training Loss: 0.011707014071297246, Validation Loss: 0.007942250265078396\n",
      "Epoch [13894/20000], Training Loss: 0.009290956333643408, Validation Loss: 0.007231244367631007\n",
      "Epoch [13895/20000], Training Loss: 0.011255793686359539, Validation Loss: 0.007011716079822301\n",
      "Epoch [13896/20000], Training Loss: 0.009315221313694824, Validation Loss: 0.005959825667429998\n",
      "Epoch [13897/20000], Training Loss: 0.00942765600484563, Validation Loss: 0.007435600775026379\n",
      "Epoch [13898/20000], Training Loss: 0.005724450951674953, Validation Loss: 0.003941436462209172\n",
      "Epoch [13899/20000], Training Loss: 0.0058974174867866425, Validation Loss: 0.005247683616756506\n",
      "Epoch [13900/20000], Training Loss: 0.006888949420369629, Validation Loss: 0.004324041218410457\n",
      "Epoch [13901/20000], Training Loss: 0.005629261535692162, Validation Loss: 0.004717949647493518\n",
      "Epoch [13902/20000], Training Loss: 0.011267651886945325, Validation Loss: 0.00851106059625544\n",
      "Epoch [13903/20000], Training Loss: 0.012871166325307317, Validation Loss: 0.008422243413016466\n",
      "Epoch [13904/20000], Training Loss: 0.010709183317625761, Validation Loss: 0.026517939620784376\n",
      "Epoch [13905/20000], Training Loss: 0.031385462076286785, Validation Loss: 0.004566731180537837\n",
      "Epoch [13906/20000], Training Loss: 0.015571265770891582, Validation Loss: 0.00992722603898853\n",
      "Epoch [13907/20000], Training Loss: 0.013566161476774141, Validation Loss: 0.008162111191722943\n",
      "Epoch [13908/20000], Training Loss: 0.005751084669360093, Validation Loss: 0.00335457899443879\n",
      "Epoch [13909/20000], Training Loss: 0.0074590053081919905, Validation Loss: 0.006269631707384958\n",
      "Epoch [13910/20000], Training Loss: 0.008730274419576745, Validation Loss: 0.01983149881885435\n",
      "Epoch [13911/20000], Training Loss: 0.016304002070683055, Validation Loss: 0.009989823894034291\n",
      "Epoch [13912/20000], Training Loss: 0.01916142589262953, Validation Loss: 0.010145418702969826\n",
      "Epoch [13913/20000], Training Loss: 0.021159939175309513, Validation Loss: 0.009835675116197049\n",
      "Epoch [13914/20000], Training Loss: 0.016619963222183287, Validation Loss: 0.010506561742220453\n",
      "Epoch [13915/20000], Training Loss: 0.021711704573038202, Validation Loss: 0.006808504388378164\n",
      "Epoch [13916/20000], Training Loss: 0.015770349816323557, Validation Loss: 0.004207155329530516\n",
      "Epoch [13917/20000], Training Loss: 0.012095576453637997, Validation Loss: 0.004133061559782293\n",
      "Epoch [13918/20000], Training Loss: 0.004553171723923047, Validation Loss: 0.008012973186709413\n",
      "Epoch [13919/20000], Training Loss: 0.006017138571353696, Validation Loss: 0.0055362221024941915\n",
      "Epoch [13920/20000], Training Loss: 0.00930653416747061, Validation Loss: 0.015062620046836703\n",
      "Epoch [13921/20000], Training Loss: 0.008709405632024365, Validation Loss: 0.004448240925829136\n",
      "Epoch [13922/20000], Training Loss: 0.007533807847981474, Validation Loss: 0.0034046247616450792\n",
      "Epoch [13923/20000], Training Loss: 0.00811693276045844, Validation Loss: 0.004100439666152462\n",
      "Epoch [13924/20000], Training Loss: 0.005024218981686447, Validation Loss: 0.004012697520337838\n",
      "Epoch [13925/20000], Training Loss: 0.00435544374548564, Validation Loss: 0.0030400462313114724\n",
      "Epoch [13926/20000], Training Loss: 0.005147885472979397, Validation Loss: 0.01892078516601328\n",
      "Epoch [13927/20000], Training Loss: 0.011019615074993843, Validation Loss: 0.0036521852960455753\n",
      "Epoch [13928/20000], Training Loss: 0.011833110587239, Validation Loss: 0.009002359682946397\n",
      "Epoch [13929/20000], Training Loss: 0.00735902160106759, Validation Loss: 0.0036304085133776554\n",
      "Epoch [13930/20000], Training Loss: 0.0038529084024401883, Validation Loss: 0.00521312667261891\n",
      "Epoch [13931/20000], Training Loss: 0.006213223233187039, Validation Loss: 0.0031474219749908017\n",
      "Epoch [13932/20000], Training Loss: 0.008488359177135862, Validation Loss: 0.004365488488640855\n",
      "Epoch [13933/20000], Training Loss: 0.008403770033120444, Validation Loss: 0.0034666170720275397\n",
      "Epoch [13934/20000], Training Loss: 0.005755518818789694, Validation Loss: 0.019148220108555897\n",
      "Epoch [13935/20000], Training Loss: 0.011597994647672749, Validation Loss: 0.007645668411246983\n",
      "Epoch [13936/20000], Training Loss: 0.009098824146868927, Validation Loss: 0.003373497617888331\n",
      "Epoch [13937/20000], Training Loss: 0.006045128848831334, Validation Loss: 0.005902702874169655\n",
      "Epoch [13938/20000], Training Loss: 0.007908438731517111, Validation Loss: 0.004000784782874689\n",
      "Epoch [13939/20000], Training Loss: 0.007490441613689265, Validation Loss: 0.004097915797922054\n",
      "Epoch [13940/20000], Training Loss: 0.013277991763378136, Validation Loss: 0.004199924412919894\n",
      "Epoch [13941/20000], Training Loss: 0.01680340534242727, Validation Loss: 0.011418800801644044\n",
      "Epoch [13942/20000], Training Loss: 0.023496948214934883, Validation Loss: 0.0054408799760880455\n",
      "Epoch [13943/20000], Training Loss: 0.012289036175778685, Validation Loss: 0.004876321651047582\n",
      "Epoch [13944/20000], Training Loss: 0.010017070576265854, Validation Loss: 0.006206686708048993\n",
      "Epoch [13945/20000], Training Loss: 0.009105658868585513, Validation Loss: 0.005462838809757413\n",
      "Epoch [13946/20000], Training Loss: 0.004753712352144898, Validation Loss: 0.004085057765000296\n",
      "Epoch [13947/20000], Training Loss: 0.005312875503217843, Validation Loss: 0.0027439873064573866\n",
      "Epoch [13948/20000], Training Loss: 0.0048731307542766444, Validation Loss: 0.0043014856288684355\n",
      "Epoch [13949/20000], Training Loss: 0.007239827719916191, Validation Loss: 0.0025046153000849764\n",
      "Epoch [13950/20000], Training Loss: 0.01158799633101028, Validation Loss: 0.003882163919975312\n",
      "Epoch [13951/20000], Training Loss: 0.01573055336159866, Validation Loss: 0.0058267782170661675\n",
      "Epoch [13952/20000], Training Loss: 0.0076812512966820835, Validation Loss: 0.005826465630104646\n",
      "Epoch [13953/20000], Training Loss: 0.011070062156899698, Validation Loss: 0.009709412991907942\n",
      "Epoch [13954/20000], Training Loss: 0.006339406931082132, Validation Loss: 0.0040905136954363696\n",
      "Epoch [13955/20000], Training Loss: 0.010539977880918221, Validation Loss: 0.002467771977630865\n",
      "Epoch [13956/20000], Training Loss: 0.013415099876250938, Validation Loss: 0.048263299678054074\n",
      "Epoch [13957/20000], Training Loss: 0.04341789309413408, Validation Loss: 0.017985919091318397\n",
      "Epoch [13958/20000], Training Loss: 0.013457668421324342, Validation Loss: 0.023056596551603952\n",
      "Epoch [13959/20000], Training Loss: 0.02289195174879361, Validation Loss: 0.014655132606679087\n",
      "Epoch [13960/20000], Training Loss: 0.025237541626016276, Validation Loss: 0.09619499121171364\n",
      "Epoch [13961/20000], Training Loss: 0.047029347869673596, Validation Loss: 0.008917661096997367\n",
      "Epoch [13962/20000], Training Loss: 0.02138387815544515, Validation Loss: 0.02114889811013906\n",
      "Epoch [13963/20000], Training Loss: 0.01374224157604788, Validation Loss: 0.004942341723644209\n",
      "Epoch [13964/20000], Training Loss: 0.009508843080506526, Validation Loss: 0.008192597100211125\n",
      "Epoch [13965/20000], Training Loss: 0.008172937914163672, Validation Loss: 0.006346785797802568\n",
      "Epoch [13966/20000], Training Loss: 0.008656888794316078, Validation Loss: 0.006733207317401886\n",
      "Epoch [13967/20000], Training Loss: 0.005133245818017583, Validation Loss: 0.005436136931653631\n",
      "Epoch [13968/20000], Training Loss: 0.005013521166152454, Validation Loss: 0.004106875822287748\n",
      "Epoch [13969/20000], Training Loss: 0.005614516312821901, Validation Loss: 0.005243923006178518\n",
      "Epoch [13970/20000], Training Loss: 0.005903626870154507, Validation Loss: 0.0038785316484433808\n",
      "Epoch [13971/20000], Training Loss: 0.005396333376276223, Validation Loss: 0.004094592725934376\n",
      "Epoch [13972/20000], Training Loss: 0.005691324986401014, Validation Loss: 0.006165384690134549\n",
      "Epoch [13973/20000], Training Loss: 0.007065730914811346, Validation Loss: 0.0031083135204020046\n",
      "Epoch [13974/20000], Training Loss: 0.007031134261394202, Validation Loss: 0.0033303199011953666\n",
      "Epoch [13975/20000], Training Loss: 0.004669198976314094, Validation Loss: 0.004219456572512469\n",
      "Epoch [13976/20000], Training Loss: 0.00653456165931792, Validation Loss: 0.006204815002580284\n",
      "Epoch [13977/20000], Training Loss: 0.0045815601053098886, Validation Loss: 0.004447687350260513\n",
      "Epoch [13978/20000], Training Loss: 0.008921302318672783, Validation Loss: 0.014048819333051401\n",
      "Epoch [13979/20000], Training Loss: 0.02115835159662132, Validation Loss: 0.035388321749656344\n",
      "Epoch [13980/20000], Training Loss: 0.0527000774496368, Validation Loss: 0.09817492536136149\n",
      "Epoch [13981/20000], Training Loss: 0.042566016440365013, Validation Loss: 0.03106332809815331\n",
      "Epoch [13982/20000], Training Loss: 0.0323453704809903, Validation Loss: 0.03874863131464475\n",
      "Epoch [13983/20000], Training Loss: 0.014561476431221567, Validation Loss: 0.009385789599438763\n",
      "Epoch [13984/20000], Training Loss: 0.007566502450832299, Validation Loss: 0.0068147484222436105\n",
      "Epoch [13985/20000], Training Loss: 0.009408390781443034, Validation Loss: 0.005919390733960687\n",
      "Epoch [13986/20000], Training Loss: 0.008307131356559694, Validation Loss: 0.0050686598401219795\n",
      "Epoch [13987/20000], Training Loss: 0.006175931759311685, Validation Loss: 0.007437030335950112\n",
      "Epoch [13988/20000], Training Loss: 0.007392360660430443, Validation Loss: 0.006387047834729336\n",
      "Epoch [13989/20000], Training Loss: 0.00792848403671087, Validation Loss: 0.006081744364211415\n",
      "Epoch [13990/20000], Training Loss: 0.005364030083001126, Validation Loss: 0.0038354954671474062\n",
      "Epoch [13991/20000], Training Loss: 0.005019494197313179, Validation Loss: 0.013196519075815363\n",
      "Epoch [13992/20000], Training Loss: 0.00782641484901043, Validation Loss: 0.004935205084897072\n",
      "Epoch [13993/20000], Training Loss: 0.010248447728453616, Validation Loss: 0.008487149713368185\n",
      "Epoch [13994/20000], Training Loss: 0.005868169385524068, Validation Loss: 0.003695714369736639\n",
      "Epoch [13995/20000], Training Loss: 0.006197515733967001, Validation Loss: 0.005441559235273579\n",
      "Epoch [13996/20000], Training Loss: 0.006652487756582559, Validation Loss: 0.0027092927729484018\n",
      "Epoch [13997/20000], Training Loss: 0.007151741085782045, Validation Loss: 0.005382375538512666\n",
      "Epoch [13998/20000], Training Loss: 0.009230209737974551, Validation Loss: 0.004114990432974253\n",
      "Epoch [13999/20000], Training Loss: 0.005689007473327885, Validation Loss: 0.0033459924372810485\n",
      "Epoch [14000/20000], Training Loss: 0.006669676320078517, Validation Loss: 0.006820958002272702\n",
      "Epoch [14001/20000], Training Loss: 0.007336595353470849, Validation Loss: 0.004852838886636489\n",
      "Epoch [14002/20000], Training Loss: 0.011528626831631885, Validation Loss: 0.022788216299302024\n",
      "Epoch [14003/20000], Training Loss: 0.00960594698150479, Validation Loss: 0.0055764594155687875\n",
      "Epoch [14004/20000], Training Loss: 0.010237235974532919, Validation Loss: 0.004897495339055321\n",
      "Epoch [14005/20000], Training Loss: 0.01218287888748039, Validation Loss: 0.005373744593932932\n",
      "Epoch [14006/20000], Training Loss: 0.01288355895771279, Validation Loss: 0.003171237562733203\n",
      "Epoch [14007/20000], Training Loss: 0.018258192618044893, Validation Loss: 0.006117444686931809\n",
      "Epoch [14008/20000], Training Loss: 0.04047512451480933, Validation Loss: 0.0037591272391495067\n",
      "Epoch [14009/20000], Training Loss: 0.030638068746027005, Validation Loss: 0.033327163596238406\n",
      "Epoch [14010/20000], Training Loss: 0.014610841828731022, Validation Loss: 0.00866412873970138\n",
      "Epoch [14011/20000], Training Loss: 0.01099616930254602, Validation Loss: 0.006916233001671149\n",
      "Epoch [14012/20000], Training Loss: 0.008372025805458958, Validation Loss: 0.00620938636119409\n",
      "Epoch [14013/20000], Training Loss: 0.008687688433513228, Validation Loss: 0.005546586790480035\n",
      "Epoch [14014/20000], Training Loss: 0.004420439899799281, Validation Loss: 0.004392872548379501\n",
      "Epoch [14015/20000], Training Loss: 0.008263433909243239, Validation Loss: 0.009497114025136406\n",
      "Epoch [14016/20000], Training Loss: 0.00444894596512313, Validation Loss: 0.0051712197725513375\n",
      "Epoch [14017/20000], Training Loss: 0.004944229153417317, Validation Loss: 0.003667599261707518\n",
      "Epoch [14018/20000], Training Loss: 0.004130370190458572, Validation Loss: 0.0031632512239018035\n",
      "Epoch [14019/20000], Training Loss: 0.0064901507295352144, Validation Loss: 0.0084149175741004\n",
      "Epoch [14020/20000], Training Loss: 0.00668041960712275, Validation Loss: 0.0036666440136790307\n",
      "Epoch [14021/20000], Training Loss: 0.005504807577900854, Validation Loss: 0.005155303688102971\n",
      "Epoch [14022/20000], Training Loss: 0.0041545541289695264, Validation Loss: 0.003427381983040537\n",
      "Epoch [14023/20000], Training Loss: 0.004319701461129429, Validation Loss: 0.005177396678474722\n",
      "Epoch [14024/20000], Training Loss: 0.0052585421048466485, Validation Loss: 0.0078101092915687466\n",
      "Epoch [14025/20000], Training Loss: 0.009160911967878096, Validation Loss: 0.002856104154802657\n",
      "Epoch [14026/20000], Training Loss: 0.005696012776293661, Validation Loss: 0.010704580813459026\n",
      "Epoch [14027/20000], Training Loss: 0.018602539695815982, Validation Loss: 0.002630695765859059\n",
      "Epoch [14028/20000], Training Loss: 0.01151198402346511, Validation Loss: 0.01027313411371519\n",
      "Epoch [14029/20000], Training Loss: 0.013710127966727928, Validation Loss: 0.004334181323309697\n",
      "Epoch [14030/20000], Training Loss: 0.0076198464895631856, Validation Loss: 0.0027441804325738822\n",
      "Epoch [14031/20000], Training Loss: 0.004527043215473116, Validation Loss: 0.01047817105554876\n",
      "Epoch [14032/20000], Training Loss: 0.021333997080676324, Validation Loss: 0.00757256484929522\n",
      "Epoch [14033/20000], Training Loss: 0.013346666382728602, Validation Loss: 0.019679561789547356\n",
      "Epoch [14034/20000], Training Loss: 0.01290716286166571, Validation Loss: 0.0042903350798176075\n",
      "Epoch [14035/20000], Training Loss: 0.012243397359270602, Validation Loss: 0.008370426807417683\n",
      "Epoch [14036/20000], Training Loss: 0.005639116219494359, Validation Loss: 0.004796145604077167\n",
      "Epoch [14037/20000], Training Loss: 0.005078726699658935, Validation Loss: 0.010735283906979634\n",
      "Epoch [14038/20000], Training Loss: 0.007351585350209332, Validation Loss: 0.009021536416102453\n",
      "Epoch [14039/20000], Training Loss: 0.013637011754326522, Validation Loss: 0.016032348073460007\n",
      "Epoch [14040/20000], Training Loss: 0.015163946661881969, Validation Loss: 0.009300243100526018\n",
      "Epoch [14041/20000], Training Loss: 0.01273408268012385, Validation Loss: 0.016885720975454075\n",
      "Epoch [14042/20000], Training Loss: 0.010175130712533636, Validation Loss: 0.01518820452371993\n",
      "Epoch [14043/20000], Training Loss: 0.017117456395811, Validation Loss: 0.0060048730422417\n",
      "Epoch [14044/20000], Training Loss: 0.015035123431256838, Validation Loss: 0.006031380343827664\n",
      "Epoch [14045/20000], Training Loss: 0.009229660382386231, Validation Loss: 0.00410212506695424\n",
      "Epoch [14046/20000], Training Loss: 0.00480166300881787, Validation Loss: 0.005703751853868511\n",
      "Epoch [14047/20000], Training Loss: 0.005457362234925053, Validation Loss: 0.0034338411837150523\n",
      "Epoch [14048/20000], Training Loss: 0.005236175868568742, Validation Loss: 0.003842917608100801\n",
      "Epoch [14049/20000], Training Loss: 0.007621175865226958, Validation Loss: 0.004265637121672431\n",
      "Epoch [14050/20000], Training Loss: 0.006653461147347376, Validation Loss: 0.0047648695631252735\n",
      "Epoch [14051/20000], Training Loss: 0.005512580154962572, Validation Loss: 0.006214861436343596\n",
      "Epoch [14052/20000], Training Loss: 0.005109111335286798, Validation Loss: 0.004589358676269306\n",
      "Epoch [14053/20000], Training Loss: 0.004951819441235524, Validation Loss: 0.00403757835009141\n",
      "Epoch [14054/20000], Training Loss: 0.005158981599379331, Validation Loss: 0.004917429396005225\n",
      "Epoch [14055/20000], Training Loss: 0.004532486355206596, Validation Loss: 0.0024374138549896024\n",
      "Epoch [14056/20000], Training Loss: 0.007906994122354913, Validation Loss: 0.010580744356949319\n",
      "Epoch [14057/20000], Training Loss: 0.007580271178864807, Validation Loss: 0.0037924211701709154\n",
      "Epoch [14058/20000], Training Loss: 0.0047602015963223365, Validation Loss: 0.0033637946188301688\n",
      "Epoch [14059/20000], Training Loss: 0.0029455996309401306, Validation Loss: 0.00535117908440189\n",
      "Epoch [14060/20000], Training Loss: 0.006221239841709446, Validation Loss: 0.00246981759507595\n",
      "Epoch [14061/20000], Training Loss: 0.008303946880914737, Validation Loss: 0.003257795046439923\n",
      "Epoch [14062/20000], Training Loss: 0.005792348680967864, Validation Loss: 0.002433600194186195\n",
      "Epoch [14063/20000], Training Loss: 0.003643594668043537, Validation Loss: 0.002766701079346084\n",
      "Epoch [14064/20000], Training Loss: 0.0065002739158574385, Validation Loss: 0.0028584361808451524\n",
      "Epoch [14065/20000], Training Loss: 0.004769175974730018, Validation Loss: 0.00554289453883529\n",
      "Epoch [14066/20000], Training Loss: 0.00953457804741577, Validation Loss: 0.009178249051228058\n",
      "Epoch [14067/20000], Training Loss: 0.009210907234643986, Validation Loss: 0.0023309205728130017\n",
      "Epoch [14068/20000], Training Loss: 0.006766707898740216, Validation Loss: 0.0031471229192635724\n",
      "Epoch [14069/20000], Training Loss: 0.027038565792931228, Validation Loss: 0.017666759860757036\n",
      "Epoch [14070/20000], Training Loss: 0.014492684537929432, Validation Loss: 0.006429723599784458\n",
      "Epoch [14071/20000], Training Loss: 0.01282493847731254, Validation Loss: 0.02082577222209281\n",
      "Epoch [14072/20000], Training Loss: 0.01912312458911661, Validation Loss: 0.01041485376021563\n",
      "Epoch [14073/20000], Training Loss: 0.00843445846943983, Validation Loss: 0.00801081796748144\n",
      "Epoch [14074/20000], Training Loss: 0.012680806700830414, Validation Loss: 0.013388015063871049\n",
      "Epoch [14075/20000], Training Loss: 0.008086339504058872, Validation Loss: 0.004569124498802678\n",
      "Epoch [14076/20000], Training Loss: 0.005239056818806732, Validation Loss: 0.0029479321859091\n",
      "Epoch [14077/20000], Training Loss: 0.0046687828353567185, Validation Loss: 0.0037695981504071796\n",
      "Epoch [14078/20000], Training Loss: 0.0038455084515070276, Validation Loss: 0.003507990724732021\n",
      "Epoch [14079/20000], Training Loss: 0.005019946692397882, Validation Loss: 0.011658381205114154\n",
      "Epoch [14080/20000], Training Loss: 0.006971152424479702, Validation Loss: 0.0031309301401965923\n",
      "Epoch [14081/20000], Training Loss: 0.012088872052377806, Validation Loss: 0.006799213942458171\n",
      "Epoch [14082/20000], Training Loss: 0.012269952330305907, Validation Loss: 0.015249866262999083\n",
      "Epoch [14083/20000], Training Loss: 0.01379354684935866, Validation Loss: 0.008911364808124997\n",
      "Epoch [14084/20000], Training Loss: 0.00823961417076394, Validation Loss: 0.010489117815860663\n",
      "Epoch [14085/20000], Training Loss: 0.014338685375877156, Validation Loss: 0.007384881628306671\n",
      "Epoch [14086/20000], Training Loss: 0.009866955060195843, Validation Loss: 0.009141457356168075\n",
      "Epoch [14087/20000], Training Loss: 0.007555918953715134, Validation Loss: 0.005610464692713281\n",
      "Epoch [14088/20000], Training Loss: 0.006768989237441149, Validation Loss: 0.01016284999125326\n",
      "Epoch [14089/20000], Training Loss: 0.004919254565347079, Validation Loss: 0.002801591126028719\n",
      "Epoch [14090/20000], Training Loss: 0.004694046340383855, Validation Loss: 0.0027985544500843673\n",
      "Epoch [14091/20000], Training Loss: 0.004147631797552874, Validation Loss: 0.002858495633588579\n",
      "Epoch [14092/20000], Training Loss: 0.006522311209534694, Validation Loss: 0.0042028491154774295\n",
      "Epoch [14093/20000], Training Loss: 0.010539116477957577, Validation Loss: 0.07819666181291852\n",
      "Epoch [14094/20000], Training Loss: 0.020741386447817995, Validation Loss: 0.010590289927725982\n",
      "Epoch [14095/20000], Training Loss: 0.01415094664220565, Validation Loss: 0.0041394440621697216\n",
      "Epoch [14096/20000], Training Loss: 0.014715130172394961, Validation Loss: 0.005215974319393928\n",
      "Epoch [14097/20000], Training Loss: 0.007951623771042193, Validation Loss: 0.004750424822720589\n",
      "Epoch [14098/20000], Training Loss: 0.013178399188875898, Validation Loss: 0.004445822735437048\n",
      "Epoch [14099/20000], Training Loss: 0.011350700520519499, Validation Loss: 0.012238114018712814\n",
      "Epoch [14100/20000], Training Loss: 0.007649398103239946, Validation Loss: 0.005089403795688087\n",
      "Epoch [14101/20000], Training Loss: 0.008744044617092836, Validation Loss: 0.007104042114204472\n",
      "Epoch [14102/20000], Training Loss: 0.007211256433038216, Validation Loss: 0.003261055043394277\n",
      "Epoch [14103/20000], Training Loss: 0.004511963785002341, Validation Loss: 0.005173080881927409\n",
      "Epoch [14104/20000], Training Loss: 0.004979825410590608, Validation Loss: 0.002619112053176157\n",
      "Epoch [14105/20000], Training Loss: 0.003914494243352757, Validation Loss: 0.011512445552662318\n",
      "Epoch [14106/20000], Training Loss: 0.007473039601921171, Validation Loss: 0.002456972351088993\n",
      "Epoch [14107/20000], Training Loss: 0.007998082232786276, Validation Loss: 0.01006906747498856\n",
      "Epoch [14108/20000], Training Loss: 0.00895927816366436, Validation Loss: 0.0035498299182127164\n",
      "Epoch [14109/20000], Training Loss: 0.006813011478048533, Validation Loss: 0.005651887212648278\n",
      "Epoch [14110/20000], Training Loss: 0.013264962283691213, Validation Loss: 0.0031459117601968195\n",
      "Epoch [14111/20000], Training Loss: 0.054047426589347324, Validation Loss: 0.03647816819804205\n",
      "Epoch [14112/20000], Training Loss: 0.0772060309162563, Validation Loss: 0.02402167734570477\n",
      "Epoch [14113/20000], Training Loss: 0.055595566873047834, Validation Loss: 0.012565991299977344\n",
      "Epoch [14114/20000], Training Loss: 0.01336892496328801, Validation Loss: 0.009726328333552406\n",
      "Epoch [14115/20000], Training Loss: 0.009756849798057894, Validation Loss: 0.007597586862695087\n",
      "Epoch [14116/20000], Training Loss: 0.007491512490170342, Validation Loss: 0.0052152459660175865\n",
      "Epoch [14117/20000], Training Loss: 0.005609724117675796, Validation Loss: 0.009094613742760771\n",
      "Epoch [14118/20000], Training Loss: 0.006656973579335574, Validation Loss: 0.006040174047240855\n",
      "Epoch [14119/20000], Training Loss: 0.00425954219962169, Validation Loss: 0.0070486314534810345\n",
      "Epoch [14120/20000], Training Loss: 0.004437274985580838, Validation Loss: 0.003353781960661146\n",
      "Epoch [14121/20000], Training Loss: 0.004941780239408088, Validation Loss: 0.004955140814563005\n",
      "Epoch [14122/20000], Training Loss: 0.005532604292966425, Validation Loss: 0.0032388790861231443\n",
      "Epoch [14123/20000], Training Loss: 0.004377496671168046, Validation Loss: 0.03618292031543597\n",
      "Epoch [14124/20000], Training Loss: 0.011555682628698247, Validation Loss: 0.004115293994740199\n",
      "Epoch [14125/20000], Training Loss: 0.010076471480195843, Validation Loss: 0.011814639949417735\n",
      "Epoch [14126/20000], Training Loss: 0.027036522287224734, Validation Loss: 0.042154318819484615\n",
      "Epoch [14127/20000], Training Loss: 0.01592718597599482, Validation Loss: 0.009266225251566427\n",
      "Epoch [14128/20000], Training Loss: 0.010070182738542956, Validation Loss: 0.00550781939306942\n",
      "Epoch [14129/20000], Training Loss: 0.006192771726806053, Validation Loss: 0.008906584736293435\n",
      "Epoch [14130/20000], Training Loss: 0.007002455049327442, Validation Loss: 0.004535713983318804\n",
      "Epoch [14131/20000], Training Loss: 0.003970860321065369, Validation Loss: 0.0066705996239190745\n",
      "Epoch [14132/20000], Training Loss: 0.006138050465389304, Validation Loss: 0.0037224037817955896\n",
      "Epoch [14133/20000], Training Loss: 0.006282393271508876, Validation Loss: 0.003292022969601151\n",
      "Epoch [14134/20000], Training Loss: 0.003950836264786111, Validation Loss: 0.005858784326215957\n",
      "Epoch [14135/20000], Training Loss: 0.010827876640129424, Validation Loss: 0.002739977794134469\n",
      "Epoch [14136/20000], Training Loss: 0.00916060962377482, Validation Loss: 0.017549764540108623\n",
      "Epoch [14137/20000], Training Loss: 0.01021866869268706, Validation Loss: 0.0033465154253521845\n",
      "Epoch [14138/20000], Training Loss: 0.00784047704655677, Validation Loss: 0.009272124418771887\n",
      "Epoch [14139/20000], Training Loss: 0.0045562588160724515, Validation Loss: 0.0029457977484333192\n",
      "Epoch [14140/20000], Training Loss: 0.005783171371928931, Validation Loss: 0.002714368977356076\n",
      "Epoch [14141/20000], Training Loss: 0.005423551052964675, Validation Loss: 0.004385753473926509\n",
      "Epoch [14142/20000], Training Loss: 0.005342036223867451, Validation Loss: 0.005105572937643846\n",
      "Epoch [14143/20000], Training Loss: 0.011361903333295231, Validation Loss: 0.006628396977343394\n",
      "Epoch [14144/20000], Training Loss: 0.021551711687931987, Validation Loss: 0.008060262940990779\n",
      "Epoch [14145/20000], Training Loss: 0.048294307136010114, Validation Loss: 0.007268543682248639\n",
      "Epoch [14146/20000], Training Loss: 0.012895383266108442, Validation Loss: 0.015327611938788814\n",
      "Epoch [14147/20000], Training Loss: 0.009385379650796364, Validation Loss: 0.005986161790805308\n",
      "Epoch [14148/20000], Training Loss: 0.0073965675432451205, Validation Loss: 0.004256768165756597\n",
      "Epoch [14149/20000], Training Loss: 0.003274335324022104, Validation Loss: 0.004231140529548887\n",
      "Epoch [14150/20000], Training Loss: 0.006070857389464176, Validation Loss: 0.005353273077162157\n",
      "Epoch [14151/20000], Training Loss: 0.006953461622418088, Validation Loss: 0.0050319433409014335\n",
      "Epoch [14152/20000], Training Loss: 0.00375526758684178, Validation Loss: 0.004313184600281009\n",
      "Epoch [14153/20000], Training Loss: 0.005432361739216114, Validation Loss: 0.0060853979682501235\n",
      "Epoch [14154/20000], Training Loss: 0.00492533254042168, Validation Loss: 0.0033566546693393207\n",
      "Epoch [14155/20000], Training Loss: 0.006398727806559431, Validation Loss: 0.0023225005384195996\n",
      "Epoch [14156/20000], Training Loss: 0.004021914867085538, Validation Loss: 0.00576050177417572\n",
      "Epoch [14157/20000], Training Loss: 0.004688716908276547, Validation Loss: 0.0056042536059674065\n",
      "Epoch [14158/20000], Training Loss: 0.008799822892927165, Validation Loss: 0.003911850382315103\n",
      "Epoch [14159/20000], Training Loss: 0.007136402366865825, Validation Loss: 0.0076375079537145275\n",
      "Epoch [14160/20000], Training Loss: 0.008063234226678364, Validation Loss: 0.005002424145210079\n",
      "Epoch [14161/20000], Training Loss: 0.005901811926508215, Validation Loss: 0.008422625226898057\n",
      "Epoch [14162/20000], Training Loss: 0.011201005693042785, Validation Loss: 0.003400647139295554\n",
      "Epoch [14163/20000], Training Loss: 0.012898930121601941, Validation Loss: 0.004936161100984826\n",
      "Epoch [14164/20000], Training Loss: 0.006596055675929945, Validation Loss: 0.01260748758380427\n",
      "Epoch [14165/20000], Training Loss: 0.005582125283021965, Validation Loss: 0.0029409388006682896\n",
      "Epoch [14166/20000], Training Loss: 0.0043893178570166514, Validation Loss: 0.002711534510926888\n",
      "Epoch [14167/20000], Training Loss: 0.004085923718646102, Validation Loss: 0.0040763901972549355\n",
      "Epoch [14168/20000], Training Loss: 0.008305573891448148, Validation Loss: 0.0041686571289644625\n",
      "Epoch [14169/20000], Training Loss: 0.00579764320718823, Validation Loss: 0.0034924876652386227\n",
      "Epoch [14170/20000], Training Loss: 0.00529361973531195, Validation Loss: 0.007164764054028946\n",
      "Epoch [14171/20000], Training Loss: 0.014554219764249865, Validation Loss: 0.018513553867934313\n",
      "Epoch [14172/20000], Training Loss: 0.03434338426002016, Validation Loss: 0.021939362439931238\n",
      "Epoch [14173/20000], Training Loss: 0.03094856749001857, Validation Loss: 0.028511357110736362\n",
      "Epoch [14174/20000], Training Loss: 0.05771768394648851, Validation Loss: 0.023045622077006556\n",
      "Epoch [14175/20000], Training Loss: 0.022797934835710163, Validation Loss: 0.005141735181825068\n",
      "Epoch [14176/20000], Training Loss: 0.009890729253779032, Validation Loss: 0.014154684580793133\n",
      "Epoch [14177/20000], Training Loss: 0.012590330222987436, Validation Loss: 0.00634986801804465\n",
      "Epoch [14178/20000], Training Loss: 0.007504121914930043, Validation Loss: 0.006301421458017232\n",
      "Epoch [14179/20000], Training Loss: 0.007428312205973953, Validation Loss: 0.004473761281977724\n",
      "Epoch [14180/20000], Training Loss: 0.006458387078185167, Validation Loss: 0.00872668947295401\n",
      "Epoch [14181/20000], Training Loss: 0.008851710940299589, Validation Loss: 0.0075952900424109005\n",
      "Epoch [14182/20000], Training Loss: 0.007119356057151955, Validation Loss: 0.005342022270870724\n",
      "Epoch [14183/20000], Training Loss: 0.00718882615944137, Validation Loss: 0.004285832839215686\n",
      "Epoch [14184/20000], Training Loss: 0.005772923229544956, Validation Loss: 0.005408588301431649\n",
      "Epoch [14185/20000], Training Loss: 0.004030545973884208, Validation Loss: 0.004369419041803927\n",
      "Epoch [14186/20000], Training Loss: 0.01111402051973138, Validation Loss: 0.010455703512822798\n",
      "Epoch [14187/20000], Training Loss: 0.01434613877375211, Validation Loss: 0.008771131904332334\n",
      "Epoch [14188/20000], Training Loss: 0.009976408682226585, Validation Loss: 0.004435117237424687\n",
      "Epoch [14189/20000], Training Loss: 0.004807283590447956, Validation Loss: 0.0037964166183042736\n",
      "Epoch [14190/20000], Training Loss: 0.006035310182986515, Validation Loss: 0.004934930569011493\n",
      "Epoch [14191/20000], Training Loss: 0.0043019143964297, Validation Loss: 0.002903957549979249\n",
      "Epoch [14192/20000], Training Loss: 0.00468603017049775, Validation Loss: 0.007375457080781546\n",
      "Epoch [14193/20000], Training Loss: 0.009715158413330625, Validation Loss: 0.0025843871269525414\n",
      "Epoch [14194/20000], Training Loss: 0.0049532032634098345, Validation Loss: 0.0027127736708304644\n",
      "Epoch [14195/20000], Training Loss: 0.007617299839954025, Validation Loss: 0.0034060138662783046\n",
      "Epoch [14196/20000], Training Loss: 0.0049659080520671394, Validation Loss: 0.00458309947531786\n",
      "Epoch [14197/20000], Training Loss: 0.008710152340816941, Validation Loss: 0.0035432442404369014\n",
      "Epoch [14198/20000], Training Loss: 0.013451780076918243, Validation Loss: 0.01138391006471959\n",
      "Epoch [14199/20000], Training Loss: 0.012644145923931356, Validation Loss: 0.011994476629689886\n",
      "Epoch [14200/20000], Training Loss: 0.005684516699277863, Validation Loss: 0.004525195976697205\n",
      "Epoch [14201/20000], Training Loss: 0.006599927971235177, Validation Loss: 0.0057132807527052265\n",
      "Epoch [14202/20000], Training Loss: 0.019482822754154076, Validation Loss: 0.0184029449500654\n",
      "Epoch [14203/20000], Training Loss: 0.02920527335403936, Validation Loss: 0.01712344032278214\n",
      "Epoch [14204/20000], Training Loss: 0.046308891067123374, Validation Loss: 0.004055519656304796\n",
      "Epoch [14205/20000], Training Loss: 0.018621246787785952, Validation Loss: 0.00892221600463878\n",
      "Epoch [14206/20000], Training Loss: 0.008014596599553312, Validation Loss: 0.006487483392642649\n",
      "Epoch [14207/20000], Training Loss: 0.006945948933467402, Validation Loss: 0.005337457075906319\n",
      "Epoch [14208/20000], Training Loss: 0.008168480878729107, Validation Loss: 0.0041913761568659015\n",
      "Epoch [14209/20000], Training Loss: 0.009734633347501844, Validation Loss: 0.005608449842481734\n",
      "Epoch [14210/20000], Training Loss: 0.010087875212775543, Validation Loss: 0.009426063195470695\n",
      "Epoch [14211/20000], Training Loss: 0.00883479889361232, Validation Loss: 0.01050237375354828\n",
      "Epoch [14212/20000], Training Loss: 0.011761711796030536, Validation Loss: 0.004073477895721973\n",
      "Epoch [14213/20000], Training Loss: 0.007005488882766388, Validation Loss: 0.009515810923858226\n",
      "Epoch [14214/20000], Training Loss: 0.011231691239767574, Validation Loss: 0.004606068577838026\n",
      "Epoch [14215/20000], Training Loss: 0.026533045573160052, Validation Loss: 0.0181729232145774\n",
      "Epoch [14216/20000], Training Loss: 0.010353930886984537, Validation Loss: 0.024909135353352343\n",
      "Epoch [14217/20000], Training Loss: 0.016021550533228686, Validation Loss: 0.006914380958851314\n",
      "Epoch [14218/20000], Training Loss: 0.0100171808943352, Validation Loss: 0.006615070593929089\n",
      "Epoch [14219/20000], Training Loss: 0.006803513280049499, Validation Loss: 0.005414184161486154\n",
      "Epoch [14220/20000], Training Loss: 0.009334109303641267, Validation Loss: 0.00654923366308172\n",
      "Epoch [14221/20000], Training Loss: 0.01346448694260159, Validation Loss: 0.033941192169939835\n",
      "Epoch [14222/20000], Training Loss: 0.015698109653645327, Validation Loss: 0.004152847953552614\n",
      "Epoch [14223/20000], Training Loss: 0.00755961908881935, Validation Loss: 0.02919952225472246\n",
      "Epoch [14224/20000], Training Loss: 0.01452636722310022, Validation Loss: 0.006460426663345448\n",
      "Epoch [14225/20000], Training Loss: 0.007952995443442237, Validation Loss: 0.005745177718520522\n",
      "Epoch [14226/20000], Training Loss: 0.007974784901274947, Validation Loss: 0.004657876112365297\n",
      "Epoch [14227/20000], Training Loss: 0.014226678824408217, Validation Loss: 0.0067608202173399445\n",
      "Epoch [14228/20000], Training Loss: 0.019508589869864017, Validation Loss: 0.005831606070823067\n",
      "Epoch [14229/20000], Training Loss: 0.008951855133675832, Validation Loss: 0.014687906761797162\n",
      "Epoch [14230/20000], Training Loss: 0.015285717959549012, Validation Loss: 0.005425910027788916\n",
      "Epoch [14231/20000], Training Loss: 0.026369009547384588, Validation Loss: 0.035530957260302136\n",
      "Epoch [14232/20000], Training Loss: 0.03493870778142342, Validation Loss: 0.01503971832784247\n",
      "Epoch [14233/20000], Training Loss: 0.027933249607615705, Validation Loss: 0.01831623107055234\n",
      "Epoch [14234/20000], Training Loss: 0.015997026117215034, Validation Loss: 0.017801470722916286\n",
      "Epoch [14235/20000], Training Loss: 0.012644853043769087, Validation Loss: 0.005529991203567468\n",
      "Epoch [14236/20000], Training Loss: 0.004762479172703544, Validation Loss: 0.005221172974561991\n",
      "Epoch [14237/20000], Training Loss: 0.0060652913983046475, Validation Loss: 0.005753876621400066\n",
      "Epoch [14238/20000], Training Loss: 0.00573845285491968, Validation Loss: 0.004463154796383151\n",
      "Epoch [14239/20000], Training Loss: 0.004370887747167477, Validation Loss: 0.003933013202380867\n",
      "Epoch [14240/20000], Training Loss: 0.004371821992598208, Validation Loss: 0.0037156426783196495\n",
      "Epoch [14241/20000], Training Loss: 0.0045680275403096205, Validation Loss: 0.00494288352950077\n",
      "Epoch [14242/20000], Training Loss: 0.006124396602834687, Validation Loss: 0.009526220807797784\n",
      "Epoch [14243/20000], Training Loss: 0.006628922573457073, Validation Loss: 0.010294833155757094\n",
      "Epoch [14244/20000], Training Loss: 0.00797544491900583, Validation Loss: 0.0038049602446008635\n",
      "Epoch [14245/20000], Training Loss: 0.003997698873515786, Validation Loss: 0.00312380070370669\n",
      "Epoch [14246/20000], Training Loss: 0.00556503023030278, Validation Loss: 0.005326663091870592\n",
      "Epoch [14247/20000], Training Loss: 0.005284468703653796, Validation Loss: 0.004081927152764818\n",
      "Epoch [14248/20000], Training Loss: 0.004545847088836906, Validation Loss: 0.02415850785471774\n",
      "Epoch [14249/20000], Training Loss: 0.016688052840306358, Validation Loss: 0.051945948176282154\n",
      "Epoch [14250/20000], Training Loss: 0.028116567864344688, Validation Loss: 0.025033258937972796\n",
      "Epoch [14251/20000], Training Loss: 0.010506332419546587, Validation Loss: 0.010510288247084889\n",
      "Epoch [14252/20000], Training Loss: 0.006760450934962137, Validation Loss: 0.0052536657668288255\n",
      "Epoch [14253/20000], Training Loss: 0.005694571405179261, Validation Loss: 0.007633441065279538\n",
      "Epoch [14254/20000], Training Loss: 0.005050596415198275, Validation Loss: 0.002739078473518524\n",
      "Epoch [14255/20000], Training Loss: 0.004475733462771002, Validation Loss: 0.006480183477958849\n",
      "Epoch [14256/20000], Training Loss: 0.004261915555356869, Validation Loss: 0.006964265766271183\n",
      "Epoch [14257/20000], Training Loss: 0.00967690517115573, Validation Loss: 0.004948861800024454\n",
      "Epoch [14258/20000], Training Loss: 0.005238937289916075, Validation Loss: 0.01042235699393364\n",
      "Epoch [14259/20000], Training Loss: 0.011841129147731928, Validation Loss: 0.0027092034674617133\n",
      "Epoch [14260/20000], Training Loss: 0.005989000032578977, Validation Loss: 0.0025553308046174345\n",
      "Epoch [14261/20000], Training Loss: 0.011594911241055732, Validation Loss: 0.0036468643692816804\n",
      "Epoch [14262/20000], Training Loss: 0.015591257541408205, Validation Loss: 0.011516006942504228\n",
      "Epoch [14263/20000], Training Loss: 0.009409614833462651, Validation Loss: 0.01128847489600192\n",
      "Epoch [14264/20000], Training Loss: 0.012772550287861253, Validation Loss: 0.009853564208892098\n",
      "Epoch [14265/20000], Training Loss: 0.009152610094003779, Validation Loss: 0.00916795839915961\n",
      "Epoch [14266/20000], Training Loss: 0.01053973722725589, Validation Loss: 0.021559211093257806\n",
      "Epoch [14267/20000], Training Loss: 0.036390862810159366, Validation Loss: 0.01436731673666206\n",
      "Epoch [14268/20000], Training Loss: 0.03780819547578825, Validation Loss: 0.012808416712557249\n",
      "Epoch [14269/20000], Training Loss: 0.029852488248122557, Validation Loss: 0.015158401076539069\n",
      "Epoch [14270/20000], Training Loss: 0.016209991164422326, Validation Loss: 0.018201227712259942\n",
      "Epoch [14271/20000], Training Loss: 0.008754969327128492, Validation Loss: 0.00922230555621338\n",
      "Epoch [14272/20000], Training Loss: 0.008769848350701588, Validation Loss: 0.004556625859472823\n",
      "Epoch [14273/20000], Training Loss: 0.005500765529827082, Validation Loss: 0.003662381793203297\n",
      "Epoch [14274/20000], Training Loss: 0.005733327302579028, Validation Loss: 0.009921572423690397\n",
      "Epoch [14275/20000], Training Loss: 0.008159369322519134, Validation Loss: 0.0063008613233494415\n",
      "Epoch [14276/20000], Training Loss: 0.004177611649668377, Validation Loss: 0.007302765478594002\n",
      "Epoch [14277/20000], Training Loss: 0.006554849880298467, Validation Loss: 0.0037250374096739086\n",
      "Epoch [14278/20000], Training Loss: 0.008917002410661163, Validation Loss: 0.006860458077052759\n",
      "Epoch [14279/20000], Training Loss: 0.008172938577315238, Validation Loss: 0.0043303046996341675\n",
      "Epoch [14280/20000], Training Loss: 0.008434603919990227, Validation Loss: 0.008459101560499874\n",
      "Epoch [14281/20000], Training Loss: 0.012742652391482676, Validation Loss: 0.0068263217093890004\n",
      "Epoch [14282/20000], Training Loss: 0.007962562118856502, Validation Loss: 0.008104812156618988\n",
      "Epoch [14283/20000], Training Loss: 0.00679789097288059, Validation Loss: 0.0038172328869242917\n",
      "Epoch [14284/20000], Training Loss: 0.004874556901735819, Validation Loss: 0.004534834257304142\n",
      "Epoch [14285/20000], Training Loss: 0.0064919280775939114, Validation Loss: 0.004590406826439773\n",
      "Epoch [14286/20000], Training Loss: 0.004330139085990376, Validation Loss: 0.0057699233403773935\n",
      "Epoch [14287/20000], Training Loss: 0.00529080491218435, Validation Loss: 0.003905114280029122\n",
      "Epoch [14288/20000], Training Loss: 0.007213761975955484, Validation Loss: 0.002526104297758902\n",
      "Epoch [14289/20000], Training Loss: 0.004623946746245825, Validation Loss: 0.005649840465601838\n",
      "Epoch [14290/20000], Training Loss: 0.008745389568990245, Validation Loss: 0.004460985225398062\n",
      "Epoch [14291/20000], Training Loss: 0.005763577448565879, Validation Loss: 0.0033034889720376143\n",
      "Epoch [14292/20000], Training Loss: 0.004794601908075029, Validation Loss: 0.0031418094853035783\n",
      "Epoch [14293/20000], Training Loss: 0.004961382888723165, Validation Loss: 0.00721131464930322\n",
      "Epoch [14294/20000], Training Loss: 0.004264887009390135, Validation Loss: 0.006536266301790593\n",
      "Epoch [14295/20000], Training Loss: 0.009661990763301478, Validation Loss: 0.0061941131252152915\n",
      "Epoch [14296/20000], Training Loss: 0.007914900919006738, Validation Loss: 0.0024298805458630645\n",
      "Epoch [14297/20000], Training Loss: 0.005675085496347622, Validation Loss: 0.008690231080575015\n",
      "Epoch [14298/20000], Training Loss: 0.008701015531869416, Validation Loss: 0.0031559889598965335\n",
      "Epoch [14299/20000], Training Loss: 0.003086777979790765, Validation Loss: 0.005504165768333956\n",
      "Epoch [14300/20000], Training Loss: 0.009574308969604317, Validation Loss: 0.020224531846387468\n",
      "Epoch [14301/20000], Training Loss: 0.027337294179492995, Validation Loss: 0.18422435436929976\n",
      "Epoch [14302/20000], Training Loss: 0.03073877227309953, Validation Loss: 0.013490296268303479\n",
      "Epoch [14303/20000], Training Loss: 0.019799730995795732, Validation Loss: 0.00899470052282582\n",
      "Epoch [14304/20000], Training Loss: 0.026390036358082267, Validation Loss: 0.007840484080540071\n",
      "Epoch [14305/20000], Training Loss: 0.015090671234897204, Validation Loss: 0.033760019723165016\n",
      "Epoch [14306/20000], Training Loss: 0.01775136320169882, Validation Loss: 0.005085065742129302\n",
      "Epoch [14307/20000], Training Loss: 0.00859848956176482, Validation Loss: 0.008652611832482624\n",
      "Epoch [14308/20000], Training Loss: 0.01012452163109856, Validation Loss: 0.011846395874665796\n",
      "Epoch [14309/20000], Training Loss: 0.007249708706824874, Validation Loss: 0.007436249328872943\n",
      "Epoch [14310/20000], Training Loss: 0.008014356517898185, Validation Loss: 0.004440608809595943\n",
      "Epoch [14311/20000], Training Loss: 0.008735599919288819, Validation Loss: 0.01123186068016356\n",
      "Epoch [14312/20000], Training Loss: 0.009517699660958405, Validation Loss: 0.005786771328919217\n",
      "Epoch [14313/20000], Training Loss: 0.004328315535281442, Validation Loss: 0.009105478795914787\n",
      "Epoch [14314/20000], Training Loss: 0.006068639282602817, Validation Loss: 0.003992667974679628\n",
      "Epoch [14315/20000], Training Loss: 0.00746254286787007, Validation Loss: 0.003688637086472078\n",
      "Epoch [14316/20000], Training Loss: 0.005990499400858036, Validation Loss: 0.005095038445504534\n",
      "Epoch [14317/20000], Training Loss: 0.004332676801206097, Validation Loss: 0.007216293565863348\n",
      "Epoch [14318/20000], Training Loss: 0.006258442948949648, Validation Loss: 0.004657966183501117\n",
      "Epoch [14319/20000], Training Loss: 0.004525344602630607, Validation Loss: 0.029398286981242108\n",
      "Epoch [14320/20000], Training Loss: 0.00858774639771452, Validation Loss: 0.05677858740650014\n",
      "Epoch [14321/20000], Training Loss: 0.058305889645453135, Validation Loss: 0.07914930969127454\n",
      "Epoch [14322/20000], Training Loss: 0.057695759065349454, Validation Loss: 0.017419837034399928\n",
      "Epoch [14323/20000], Training Loss: 0.0172921814217781, Validation Loss: 0.027367748852292713\n",
      "Epoch [14324/20000], Training Loss: 0.014411776389741655, Validation Loss: 0.006524147773908291\n",
      "Epoch [14325/20000], Training Loss: 0.008472132745997183, Validation Loss: 0.010184306286607889\n",
      "Epoch [14326/20000], Training Loss: 0.009136229790913473, Validation Loss: 0.0046161099547004725\n",
      "Epoch [14327/20000], Training Loss: 0.008281865624927118, Validation Loss: 0.005001749941649385\n",
      "Epoch [14328/20000], Training Loss: 0.01771120322583946, Validation Loss: 0.031045643878850393\n",
      "Epoch [14329/20000], Training Loss: 0.013697389729973761, Validation Loss: 0.0066865420028244015\n",
      "Epoch [14330/20000], Training Loss: 0.008063379562892286, Validation Loss: 0.005147196045291886\n",
      "Epoch [14331/20000], Training Loss: 0.0061831673145726586, Validation Loss: 0.005762180968693948\n",
      "Epoch [14332/20000], Training Loss: 0.009649368256629844, Validation Loss: 0.005941418092512711\n",
      "Epoch [14333/20000], Training Loss: 0.008366648694001404, Validation Loss: 0.009250266792559803\n",
      "Epoch [14334/20000], Training Loss: 0.006376012973695262, Validation Loss: 0.005055822535593636\n",
      "Epoch [14335/20000], Training Loss: 0.006959894128418195, Validation Loss: 0.005873558957419224\n",
      "Epoch [14336/20000], Training Loss: 0.006512609857184414, Validation Loss: 0.0036901750130411415\n",
      "Epoch [14337/20000], Training Loss: 0.007062062488070556, Validation Loss: 0.003484930216696317\n",
      "Epoch [14338/20000], Training Loss: 0.010518645764802517, Validation Loss: 0.015704436323988245\n",
      "Epoch [14339/20000], Training Loss: 0.041161210014316954, Validation Loss: 0.007568981775316492\n",
      "Epoch [14340/20000], Training Loss: 0.018412676234480126, Validation Loss: 0.020488558617613308\n",
      "Epoch [14341/20000], Training Loss: 0.007148597781155429, Validation Loss: 0.004302684341228412\n",
      "Epoch [14342/20000], Training Loss: 0.0068954909838794265, Validation Loss: 0.003942374231489053\n",
      "Epoch [14343/20000], Training Loss: 0.005487842393839466, Validation Loss: 0.00336904743860243\n",
      "Epoch [14344/20000], Training Loss: 0.005120269584170144, Validation Loss: 0.0031109222442791762\n",
      "Epoch [14345/20000], Training Loss: 0.005158503244664254, Validation Loss: 0.007248647059927913\n",
      "Epoch [14346/20000], Training Loss: 0.007192803028114473, Validation Loss: 0.005205381088482371\n",
      "Epoch [14347/20000], Training Loss: 0.0059165716935721336, Validation Loss: 0.011712163801738435\n",
      "Epoch [14348/20000], Training Loss: 0.024868855668630983, Validation Loss: 0.00607492567904241\n",
      "Epoch [14349/20000], Training Loss: 0.0154238594155426, Validation Loss: 0.025365850223003424\n",
      "Epoch [14350/20000], Training Loss: 0.011445655723641377, Validation Loss: 0.010532534154561133\n",
      "Epoch [14351/20000], Training Loss: 0.013627512527818908, Validation Loss: 0.0048309539744926565\n",
      "Epoch [14352/20000], Training Loss: 0.006784675576324973, Validation Loss: 0.00555615146166539\n",
      "Epoch [14353/20000], Training Loss: 0.010134416126155494, Validation Loss: 0.004355600974946646\n",
      "Epoch [14354/20000], Training Loss: 0.0043395073558453335, Validation Loss: 0.004071199726074123\n",
      "Epoch [14355/20000], Training Loss: 0.004753724784157904, Validation Loss: 0.004235598406045151\n",
      "Epoch [14356/20000], Training Loss: 0.005099696248570191, Validation Loss: 0.003170908333591476\n",
      "Epoch [14357/20000], Training Loss: 0.004935594987655024, Validation Loss: 0.0029394734546803907\n",
      "Epoch [14358/20000], Training Loss: 0.004331632012118851, Validation Loss: 0.005066984076185983\n",
      "Epoch [14359/20000], Training Loss: 0.008271488525289377, Validation Loss: 0.0034542542996730064\n",
      "Epoch [14360/20000], Training Loss: 0.009701451711797355, Validation Loss: 0.003486414907095733\n",
      "Epoch [14361/20000], Training Loss: 0.008086015411078864, Validation Loss: 0.006961859298954555\n",
      "Epoch [14362/20000], Training Loss: 0.006740391709692111, Validation Loss: 0.006000733383415938\n",
      "Epoch [14363/20000], Training Loss: 0.011171747030208021, Validation Loss: 0.01465016996807224\n",
      "Epoch [14364/20000], Training Loss: 0.02074781868681644, Validation Loss: 0.025023393858516232\n",
      "Epoch [14365/20000], Training Loss: 0.01797412542587803, Validation Loss: 0.013982037880591339\n",
      "Epoch [14366/20000], Training Loss: 0.008855483510550195, Validation Loss: 0.006517023214651403\n",
      "Epoch [14367/20000], Training Loss: 0.007238651420006396, Validation Loss: 0.005309340701362686\n",
      "Epoch [14368/20000], Training Loss: 0.00645024113224021, Validation Loss: 0.0043060416789556044\n",
      "Epoch [14369/20000], Training Loss: 0.005047742884406554, Validation Loss: 0.003972883389167237\n",
      "Epoch [14370/20000], Training Loss: 0.006198884563803274, Validation Loss: 0.0038733823661049526\n",
      "Epoch [14371/20000], Training Loss: 0.01281814711858585, Validation Loss: 0.009264341289810712\n",
      "Epoch [14372/20000], Training Loss: 0.005848848629946295, Validation Loss: 0.008080068380342557\n",
      "Epoch [14373/20000], Training Loss: 0.009589956974001066, Validation Loss: 0.009708708618134094\n",
      "Epoch [14374/20000], Training Loss: 0.005955196087597869, Validation Loss: 0.008640562418577247\n",
      "Epoch [14375/20000], Training Loss: 0.02075650232902377, Validation Loss: 0.07755788108407355\n",
      "Epoch [14376/20000], Training Loss: 0.027228280367190955, Validation Loss: 0.00949647493590484\n",
      "Epoch [14377/20000], Training Loss: 0.01471139379594076, Validation Loss: 0.010513093329369116\n",
      "Epoch [14378/20000], Training Loss: 0.008103490685925863, Validation Loss: 0.005781700132003716\n",
      "Epoch [14379/20000], Training Loss: 0.006583404230436177, Validation Loss: 0.00398951587968825\n",
      "Epoch [14380/20000], Training Loss: 0.007842340152413791, Validation Loss: 0.005163548177201692\n",
      "Epoch [14381/20000], Training Loss: 0.008214123101164919, Validation Loss: 0.004956517127612087\n",
      "Epoch [14382/20000], Training Loss: 0.005255310913298672, Validation Loss: 0.004652941535658682\n",
      "Epoch [14383/20000], Training Loss: 0.008435062952620294, Validation Loss: 0.003946891547018946\n",
      "Epoch [14384/20000], Training Loss: 0.006006319603682998, Validation Loss: 0.0034845850615051505\n",
      "Epoch [14385/20000], Training Loss: 0.008844874757674656, Validation Loss: 0.0038595342602144966\n",
      "Epoch [14386/20000], Training Loss: 0.022830611194227197, Validation Loss: 0.010375810955488467\n",
      "Epoch [14387/20000], Training Loss: 0.01732590462571742, Validation Loss: 0.004589247072853654\n",
      "Epoch [14388/20000], Training Loss: 0.008633950477815233, Validation Loss: 0.008218683455351896\n",
      "Epoch [14389/20000], Training Loss: 0.009198143055462944, Validation Loss: 0.005129049240713256\n",
      "Epoch [14390/20000], Training Loss: 0.006072954046041039, Validation Loss: 0.013284922179909937\n",
      "Epoch [14391/20000], Training Loss: 0.014823218119480381, Validation Loss: 0.030589796602725986\n",
      "Epoch [14392/20000], Training Loss: 0.011979384451738692, Validation Loss: 0.008447939416653292\n",
      "Epoch [14393/20000], Training Loss: 0.00962604057298019, Validation Loss: 0.02720434095512506\n",
      "Epoch [14394/20000], Training Loss: 0.017135245305585807, Validation Loss: 0.009849242367172998\n",
      "Epoch [14395/20000], Training Loss: 0.00745033302456477, Validation Loss: 0.014178899516464593\n",
      "Epoch [14396/20000], Training Loss: 0.009844931391333895, Validation Loss: 0.004364303017088146\n",
      "Epoch [14397/20000], Training Loss: 0.005594412508798996, Validation Loss: 0.0033325663928661492\n",
      "Epoch [14398/20000], Training Loss: 0.008369812149404814, Validation Loss: 0.00787393905114898\n",
      "Epoch [14399/20000], Training Loss: 0.00538007786963135, Validation Loss: 0.007545271156749993\n",
      "Epoch [14400/20000], Training Loss: 0.008050907922941926, Validation Loss: 0.005584198849986228\n",
      "Epoch [14401/20000], Training Loss: 0.007700199339264405, Validation Loss: 0.004688841124427851\n",
      "Epoch [14402/20000], Training Loss: 0.00571773826694409, Validation Loss: 0.0037090554798154224\n",
      "Epoch [14403/20000], Training Loss: 0.009145163197120252, Validation Loss: 0.005202638121188202\n",
      "Epoch [14404/20000], Training Loss: 0.011351728270426808, Validation Loss: 0.0038680178631956435\n",
      "Epoch [14405/20000], Training Loss: 0.0074332903532194905, Validation Loss: 0.0034265746840365097\n",
      "Epoch [14406/20000], Training Loss: 0.004960663348486248, Validation Loss: 0.0038731585438095367\n",
      "Epoch [14407/20000], Training Loss: 0.004156488991741624, Validation Loss: 0.0034148497621119744\n",
      "Epoch [14408/20000], Training Loss: 0.006860172860407536, Validation Loss: 0.005201386866190205\n",
      "Epoch [14409/20000], Training Loss: 0.01825601504866167, Validation Loss: 0.005953584176148326\n",
      "Epoch [14410/20000], Training Loss: 0.01193681024804911, Validation Loss: 0.0025365878767108336\n",
      "Epoch [14411/20000], Training Loss: 0.005032294988718584, Validation Loss: 0.006446834129982508\n",
      "Epoch [14412/20000], Training Loss: 0.015702853003494326, Validation Loss: 0.005417177154799801\n",
      "Epoch [14413/20000], Training Loss: 0.010087900701494488, Validation Loss: 0.004400100012899041\n",
      "Epoch [14414/20000], Training Loss: 0.007288623256109921, Validation Loss: 0.004498486594124489\n",
      "Epoch [14415/20000], Training Loss: 0.004551750557896282, Validation Loss: 0.004238058190143842\n",
      "Epoch [14416/20000], Training Loss: 0.003407621163530296, Validation Loss: 0.008578899582965534\n",
      "Epoch [14417/20000], Training Loss: 0.006167198668533404, Validation Loss: 0.00445371916448684\n",
      "Epoch [14418/20000], Training Loss: 0.011705672929427702, Validation Loss: 0.004487828999392436\n",
      "Epoch [14419/20000], Training Loss: 0.00615309124259511, Validation Loss: 0.0041925375696934\n",
      "Epoch [14420/20000], Training Loss: 0.003111719916237884, Validation Loss: 0.004582183846294304\n",
      "Epoch [14421/20000], Training Loss: 0.005798229791253107, Validation Loss: 0.011484008348946684\n",
      "Epoch [14422/20000], Training Loss: 0.007799292423604649, Validation Loss: 0.007574237686622688\n",
      "Epoch [14423/20000], Training Loss: 0.007931527090469324, Validation Loss: 0.011640469626805887\n",
      "Epoch [14424/20000], Training Loss: 0.010874231930107012, Validation Loss: 0.0033479975217365976\n",
      "Epoch [14425/20000], Training Loss: 0.0057502236637187055, Validation Loss: 0.002894615410611629\n",
      "Epoch [14426/20000], Training Loss: 0.009043667933272255, Validation Loss: 0.002489831343444493\n",
      "Epoch [14427/20000], Training Loss: 0.006064232222602836, Validation Loss: 0.0225793432533224\n",
      "Epoch [14428/20000], Training Loss: 0.009484617738053203, Validation Loss: 0.004971799077338801\n",
      "Epoch [14429/20000], Training Loss: 0.007866437939810567, Validation Loss: 0.009400153967443916\n",
      "Epoch [14430/20000], Training Loss: 0.007362191207773451, Validation Loss: 0.005866319377076934\n",
      "Epoch [14431/20000], Training Loss: 0.008856930006848935, Validation Loss: 0.0031684180981006093\n",
      "Epoch [14432/20000], Training Loss: 0.00960336500838821, Validation Loss: 0.0027465925432544097\n",
      "Epoch [14433/20000], Training Loss: 0.006521491735059369, Validation Loss: 0.004756081484238722\n",
      "Epoch [14434/20000], Training Loss: 0.006331665688031987, Validation Loss: 0.009095805564100315\n",
      "Epoch [14435/20000], Training Loss: 0.006935902278874957, Validation Loss: 0.007472535342387232\n",
      "Epoch [14436/20000], Training Loss: 0.007383461866993457, Validation Loss: 0.0027865705099770588\n",
      "Epoch [14437/20000], Training Loss: 0.008732260891900882, Validation Loss: 0.003509550307069769\n",
      "Epoch [14438/20000], Training Loss: 0.012402933537876899, Validation Loss: 0.0024839032872982897\n",
      "Epoch [14439/20000], Training Loss: 0.01066666495859993, Validation Loss: 0.025613207902315794\n",
      "Epoch [14440/20000], Training Loss: 0.02398401392409661, Validation Loss: 0.06323677248188428\n",
      "Epoch [14441/20000], Training Loss: 0.03632818445631918, Validation Loss: 0.022061679247324308\n",
      "Epoch [14442/20000], Training Loss: 0.015780612472220974, Validation Loss: 0.025680169510999617\n",
      "Epoch [14443/20000], Training Loss: 0.01673638411427549, Validation Loss: 0.0032058992588628954\n",
      "Epoch [14444/20000], Training Loss: 0.009088067936577968, Validation Loss: 0.006322088753677235\n",
      "Epoch [14445/20000], Training Loss: 0.01066725211317784, Validation Loss: 0.006858401843928798\n",
      "Epoch [14446/20000], Training Loss: 0.023181406131438313, Validation Loss: 0.008137411875850376\n",
      "Epoch [14447/20000], Training Loss: 0.008269976167606987, Validation Loss: 0.005713313453804942\n",
      "Epoch [14448/20000], Training Loss: 0.008393294219526328, Validation Loss: 0.0047986582510150455\n",
      "Epoch [14449/20000], Training Loss: 0.009303499809383149, Validation Loss: 0.020312211420429906\n",
      "Epoch [14450/20000], Training Loss: 0.04588990739596609, Validation Loss: 0.024524367414719306\n",
      "Epoch [14451/20000], Training Loss: 0.024186101478075477, Validation Loss: 0.04210519471338873\n",
      "Epoch [14452/20000], Training Loss: 0.022465682280848602, Validation Loss: 0.00877729888714418\n",
      "Epoch [14453/20000], Training Loss: 0.013025681014239256, Validation Loss: 0.016159591885956344\n",
      "Epoch [14454/20000], Training Loss: 0.015445772537272464, Validation Loss: 0.007164056230976712\n",
      "Epoch [14455/20000], Training Loss: 0.006281753972871229, Validation Loss: 0.0052029813405819626\n",
      "Epoch [14456/20000], Training Loss: 0.006988370747421868, Validation Loss: 0.007598052885171569\n",
      "Epoch [14457/20000], Training Loss: 0.007504856692061627, Validation Loss: 0.003968339352411476\n",
      "Epoch [14458/20000], Training Loss: 0.006834917196101742, Validation Loss: 0.0051760837850451935\n",
      "Epoch [14459/20000], Training Loss: 0.0057834156032186, Validation Loss: 0.0037701878855256965\n",
      "Epoch [14460/20000], Training Loss: 0.005273732439880925, Validation Loss: 0.006550431648845135\n",
      "Epoch [14461/20000], Training Loss: 0.008341559986607397, Validation Loss: 0.010200584574801854\n",
      "Epoch [14462/20000], Training Loss: 0.02824404860179389, Validation Loss: 0.06744761345168114\n",
      "Epoch [14463/20000], Training Loss: 0.031585458487305526, Validation Loss: 0.015605030165586101\n",
      "Epoch [14464/20000], Training Loss: 0.017816600314940194, Validation Loss: 0.008291063846398121\n",
      "Epoch [14465/20000], Training Loss: 0.009711551475837561, Validation Loss: 0.005763326501096344\n",
      "Epoch [14466/20000], Training Loss: 0.007611678008938075, Validation Loss: 0.004799914329867404\n",
      "Epoch [14467/20000], Training Loss: 0.005623511926517872, Validation Loss: 0.004737043075222809\n",
      "Epoch [14468/20000], Training Loss: 0.008033349525898561, Validation Loss: 0.009703357977937256\n",
      "Epoch [14469/20000], Training Loss: 0.0081811777367063, Validation Loss: 0.007157739809177396\n",
      "Epoch [14470/20000], Training Loss: 0.005237962061073631, Validation Loss: 0.007980482553012968\n",
      "Epoch [14471/20000], Training Loss: 0.005284886392473709, Validation Loss: 0.004458148139584799\n",
      "Epoch [14472/20000], Training Loss: 0.004848154681634956, Validation Loss: 0.003249480063427005\n",
      "Epoch [14473/20000], Training Loss: 0.004824407182500831, Validation Loss: 0.004878232337147332\n",
      "Epoch [14474/20000], Training Loss: 0.0061517640902561, Validation Loss: 0.004177450879449873\n",
      "Epoch [14475/20000], Training Loss: 0.004031818132976436, Validation Loss: 0.012889075227084924\n",
      "Epoch [14476/20000], Training Loss: 0.010023698894656263, Validation Loss: 0.013821452504284284\n",
      "Epoch [14477/20000], Training Loss: 0.01395907332230958, Validation Loss: 0.006692002036857048\n",
      "Epoch [14478/20000], Training Loss: 0.014733950301596061, Validation Loss: 0.00838596532520752\n",
      "Epoch [14479/20000], Training Loss: 0.02075983644837399, Validation Loss: 0.031617993755000844\n",
      "Epoch [14480/20000], Training Loss: 0.01579645122339051, Validation Loss: 0.02626845030478516\n",
      "Epoch [14481/20000], Training Loss: 0.015581635874696076, Validation Loss: 0.010130425765784823\n",
      "Epoch [14482/20000], Training Loss: 0.01062107520405269, Validation Loss: 0.005356782306028152\n",
      "Epoch [14483/20000], Training Loss: 0.017180257158802954, Validation Loss: 0.007352297782636035\n",
      "Epoch [14484/20000], Training Loss: 0.02386798720596874, Validation Loss: 0.004559436068493421\n",
      "Epoch [14485/20000], Training Loss: 0.010353558017283961, Validation Loss: 0.012076104060984758\n",
      "Epoch [14486/20000], Training Loss: 0.03190670085106311, Validation Loss: 0.021145572826887633\n",
      "Epoch [14487/20000], Training Loss: 0.06978540514994945, Validation Loss: 0.020373260345118622\n",
      "Epoch [14488/20000], Training Loss: 0.018694648482778575, Validation Loss: 0.014384776221103134\n",
      "Epoch [14489/20000], Training Loss: 0.010959063920641452, Validation Loss: 0.00675101953191058\n",
      "Epoch [14490/20000], Training Loss: 0.010538710940246736, Validation Loss: 0.009279963887170848\n",
      "Epoch [14491/20000], Training Loss: 0.009445982921793725, Validation Loss: 0.005467979238218845\n",
      "Epoch [14492/20000], Training Loss: 0.005629413021129689, Validation Loss: 0.006162930026497053\n",
      "Epoch [14493/20000], Training Loss: 0.008279613959270396, Validation Loss: 0.005395242222275546\n",
      "Epoch [14494/20000], Training Loss: 0.005956216045888141, Validation Loss: 0.004811572982287008\n",
      "Epoch [14495/20000], Training Loss: 0.004364895409837898, Validation Loss: 0.0042702832448858275\n",
      "Epoch [14496/20000], Training Loss: 0.00518415085757949, Validation Loss: 0.004508005168023228\n",
      "Epoch [14497/20000], Training Loss: 0.004203192236933059, Validation Loss: 0.0037102554178147784\n",
      "Epoch [14498/20000], Training Loss: 0.003957426007608384, Validation Loss: 0.00402042294990835\n",
      "Epoch [14499/20000], Training Loss: 0.004629047585694934, Validation Loss: 0.0032005616843438145\n",
      "Epoch [14500/20000], Training Loss: 0.005619951018681084, Validation Loss: 0.007287329552517414\n",
      "Epoch [14501/20000], Training Loss: 0.009606280382675842, Validation Loss: 0.004126661033640312\n",
      "Epoch [14502/20000], Training Loss: 0.00754942732705136, Validation Loss: 0.006259347113214645\n",
      "Epoch [14503/20000], Training Loss: 0.012424072347844881, Validation Loss: 0.027337138141904558\n",
      "Epoch [14504/20000], Training Loss: 0.012680846685855483, Validation Loss: 0.007181837756502413\n",
      "Epoch [14505/20000], Training Loss: 0.0053029198898002505, Validation Loss: 0.002947537948039754\n",
      "Epoch [14506/20000], Training Loss: 0.004476769294602231, Validation Loss: 0.005905158214544974\n",
      "Epoch [14507/20000], Training Loss: 0.015353464581754192, Validation Loss: 0.006677204725276299\n",
      "Epoch [14508/20000], Training Loss: 0.006686845191714903, Validation Loss: 0.004472240424372866\n",
      "Epoch [14509/20000], Training Loss: 0.004882228146016132, Validation Loss: 0.00350267717492828\n",
      "Epoch [14510/20000], Training Loss: 0.009659173798484386, Validation Loss: 0.0057077554296777345\n",
      "Epoch [14511/20000], Training Loss: 0.011219033568132935, Validation Loss: 0.0021329082236615704\n",
      "Epoch [14512/20000], Training Loss: 0.005488194631843564, Validation Loss: 0.005839634237766872\n",
      "Epoch [14513/20000], Training Loss: 0.009903851880227623, Validation Loss: 0.01602645138544715\n",
      "Epoch [14514/20000], Training Loss: 0.011416448003209163, Validation Loss: 0.0027147081365511477\n",
      "Epoch [14515/20000], Training Loss: 0.024252991603914, Validation Loss: 0.007784250475135978\n",
      "Epoch [14516/20000], Training Loss: 0.017537980747874826, Validation Loss: 0.015363185787074534\n",
      "Epoch [14517/20000], Training Loss: 0.007799159984902612, Validation Loss: 0.007258626727526216\n",
      "Epoch [14518/20000], Training Loss: 0.007201990715673544, Validation Loss: 0.010402874156300616\n",
      "Epoch [14519/20000], Training Loss: 0.006168739312020729, Validation Loss: 0.010828557185924634\n",
      "Epoch [14520/20000], Training Loss: 0.006498002948189553, Validation Loss: 0.011313014996371175\n",
      "Epoch [14521/20000], Training Loss: 0.008655577748364911, Validation Loss: 0.0031254275094132924\n",
      "Epoch [14522/20000], Training Loss: 0.006936635203601327, Validation Loss: 0.009912160890497798\n",
      "Epoch [14523/20000], Training Loss: 0.009897499891654402, Validation Loss: 0.04611541617009607\n",
      "Epoch [14524/20000], Training Loss: 0.046361171389954894, Validation Loss: 0.020522744011746457\n",
      "Epoch [14525/20000], Training Loss: 0.030136757413856685, Validation Loss: 0.010733221871695215\n",
      "Epoch [14526/20000], Training Loss: 0.018042440490327345, Validation Loss: 0.006585909152056083\n",
      "Epoch [14527/20000], Training Loss: 0.015504719446263542, Validation Loss: 0.010132077821643699\n",
      "Epoch [14528/20000], Training Loss: 0.012534984254671144, Validation Loss: 0.0037163376812122196\n",
      "Epoch [14529/20000], Training Loss: 0.004126854820892082, Validation Loss: 0.0034644407328522525\n",
      "Epoch [14530/20000], Training Loss: 0.00530921774874774, Validation Loss: 0.004103383024763271\n",
      "Epoch [14531/20000], Training Loss: 0.0036773512526094366, Validation Loss: 0.007157123518539317\n",
      "Epoch [14532/20000], Training Loss: 0.005914959582566682, Validation Loss: 0.004054218490986321\n",
      "Epoch [14533/20000], Training Loss: 0.011257614681588686, Validation Loss: 0.003678768850835893\n",
      "Epoch [14534/20000], Training Loss: 0.01818460984837397, Validation Loss: 0.010119712646022785\n",
      "Epoch [14535/20000], Training Loss: 0.016942439565484944, Validation Loss: 0.007318016210065074\n",
      "Epoch [14536/20000], Training Loss: 0.0050772908216458745, Validation Loss: 0.0043595755669952095\n",
      "Epoch [14537/20000], Training Loss: 0.006875621529096472, Validation Loss: 0.006986560766483989\n",
      "Epoch [14538/20000], Training Loss: 0.005055301995785807, Validation Loss: 0.006752169359066194\n",
      "Epoch [14539/20000], Training Loss: 0.009864234317059786, Validation Loss: 0.008362086822723849\n",
      "Epoch [14540/20000], Training Loss: 0.008659834748998816, Validation Loss: 0.025260599498671685\n",
      "Epoch [14541/20000], Training Loss: 0.031089389490911605, Validation Loss: 0.013254776668873092\n",
      "Epoch [14542/20000], Training Loss: 0.015660205227114993, Validation Loss: 0.003839796096243044\n",
      "Epoch [14543/20000], Training Loss: 0.006506591288988213, Validation Loss: 0.023117019528789178\n",
      "Epoch [14544/20000], Training Loss: 0.015624565172142215, Validation Loss: 0.004422812811064907\n",
      "Epoch [14545/20000], Training Loss: 0.01191651607411716, Validation Loss: 0.00465789980523095\n",
      "Epoch [14546/20000], Training Loss: 0.005779104614962957, Validation Loss: 0.008607644321867287\n",
      "Epoch [14547/20000], Training Loss: 0.007746348253151934, Validation Loss: 0.006014247918794553\n",
      "Epoch [14548/20000], Training Loss: 0.010153185708207144, Validation Loss: 0.004304074101211215\n",
      "Epoch [14549/20000], Training Loss: 0.004815276657381621, Validation Loss: 0.0031146065443191373\n",
      "Epoch [14550/20000], Training Loss: 0.004109171551785299, Validation Loss: 0.003014446691615051\n",
      "Epoch [14551/20000], Training Loss: 0.005392875967247944, Validation Loss: 0.0037698530199800065\n",
      "Epoch [14552/20000], Training Loss: 0.00871776620624587, Validation Loss: 0.0033468346790861423\n",
      "Epoch [14553/20000], Training Loss: 0.007037846267589235, Validation Loss: 0.01838316848235471\n",
      "Epoch [14554/20000], Training Loss: 0.009983353206345262, Validation Loss: 0.007212207386546131\n",
      "Epoch [14555/20000], Training Loss: 0.010623553653203999, Validation Loss: 0.004960473959531829\n",
      "Epoch [14556/20000], Training Loss: 0.008332639564676876, Validation Loss: 0.004753811438183209\n",
      "Epoch [14557/20000], Training Loss: 0.009160745082470254, Validation Loss: 0.017887046964371427\n",
      "Epoch [14558/20000], Training Loss: 0.025235791490039055, Validation Loss: 0.00661495780312424\n",
      "Epoch [14559/20000], Training Loss: 0.007962572138889559, Validation Loss: 0.0074003327186298\n",
      "Epoch [14560/20000], Training Loss: 0.006421234502340667, Validation Loss: 0.009050503373146246\n",
      "Epoch [14561/20000], Training Loss: 0.005635195115116533, Validation Loss: 0.0036044366513809773\n",
      "Epoch [14562/20000], Training Loss: 0.004529343666165785, Validation Loss: 0.004420928457558218\n",
      "Epoch [14563/20000], Training Loss: 0.009564027830492705, Validation Loss: 0.0027740155703210403\n",
      "Epoch [14564/20000], Training Loss: 0.005232914862322754, Validation Loss: 0.016454117638803906\n",
      "Epoch [14565/20000], Training Loss: 0.010744632654158133, Validation Loss: 0.008877497731841382\n",
      "Epoch [14566/20000], Training Loss: 0.022122530928754713, Validation Loss: 0.009672880098511638\n",
      "Epoch [14567/20000], Training Loss: 0.01082926070612952, Validation Loss: 0.010599930266731736\n",
      "Epoch [14568/20000], Training Loss: 0.044395112959058736, Validation Loss: 0.05505711968755448\n",
      "Epoch [14569/20000], Training Loss: 0.035449486159320386, Validation Loss: 0.05219361824648724\n",
      "Epoch [14570/20000], Training Loss: 0.018326655982361575, Validation Loss: 0.016235536107566468\n",
      "Epoch [14571/20000], Training Loss: 0.009739967585379159, Validation Loss: 0.010886996984541394\n",
      "Epoch [14572/20000], Training Loss: 0.006299697186997426, Validation Loss: 0.005374508643864617\n",
      "Epoch [14573/20000], Training Loss: 0.006973683019168675, Validation Loss: 0.005763136515659982\n",
      "Epoch [14574/20000], Training Loss: 0.007191235509798817, Validation Loss: 0.039934876774038584\n",
      "Epoch [14575/20000], Training Loss: 0.006268626818885943, Validation Loss: 0.0384214956845556\n",
      "Epoch [14576/20000], Training Loss: 0.03606155782459349, Validation Loss: 0.04317799423448896\n",
      "Epoch [14577/20000], Training Loss: 0.021346087582060136, Validation Loss: 0.010879103162684365\n",
      "Epoch [14578/20000], Training Loss: 0.023610259385480146, Validation Loss: 0.010310605805426278\n",
      "Epoch [14579/20000], Training Loss: 0.010512582629287084, Validation Loss: 0.006871046884153235\n",
      "Epoch [14580/20000], Training Loss: 0.013701639963463614, Validation Loss: 0.013184460678546865\n",
      "Epoch [14581/20000], Training Loss: 0.009078708972083405, Validation Loss: 0.005685484382669597\n",
      "Epoch [14582/20000], Training Loss: 0.007325857863179408, Validation Loss: 0.005510628949130411\n",
      "Epoch [14583/20000], Training Loss: 0.005429045200019443, Validation Loss: 0.0040507765115573085\n",
      "Epoch [14584/20000], Training Loss: 0.005690419485160548, Validation Loss: 0.003915355435103201\n",
      "Epoch [14585/20000], Training Loss: 0.00881954702864667, Validation Loss: 0.010941259292443257\n",
      "Epoch [14586/20000], Training Loss: 0.008601223353512719, Validation Loss: 0.007481447795277451\n",
      "Epoch [14587/20000], Training Loss: 0.006625537827078786, Validation Loss: 0.0035649103420406225\n",
      "Epoch [14588/20000], Training Loss: 0.007240644489046756, Validation Loss: 0.017453697893610558\n",
      "Epoch [14589/20000], Training Loss: 0.00811830015040219, Validation Loss: 0.0029438727101853174\n",
      "Epoch [14590/20000], Training Loss: 0.006122381911755857, Validation Loss: 0.007306693873875757\n",
      "Epoch [14591/20000], Training Loss: 0.008775018592132255, Validation Loss: 0.004342067554584773\n",
      "Epoch [14592/20000], Training Loss: 0.0070030645968992234, Validation Loss: 0.004577611573184916\n",
      "Epoch [14593/20000], Training Loss: 0.004828815983868496, Validation Loss: 0.00894311955199753\n",
      "Epoch [14594/20000], Training Loss: 0.007403831161874612, Validation Loss: 0.007012200874413773\n",
      "Epoch [14595/20000], Training Loss: 0.006160861254881768, Validation Loss: 0.008373488226450208\n",
      "Epoch [14596/20000], Training Loss: 0.005041304225285005, Validation Loss: 0.011082217230330758\n",
      "Epoch [14597/20000], Training Loss: 0.005607050198591423, Validation Loss: 0.0062379121686670135\n",
      "Epoch [14598/20000], Training Loss: 0.006026054148346053, Validation Loss: 0.0061060299132314885\n",
      "Epoch [14599/20000], Training Loss: 0.007288556221672999, Validation Loss: 0.005326823658653612\n",
      "Epoch [14600/20000], Training Loss: 0.004484554861641803, Validation Loss: 0.003964543375914063\n",
      "Epoch [14601/20000], Training Loss: 0.005580123558632165, Validation Loss: 0.00805037242494622\n",
      "Epoch [14602/20000], Training Loss: 0.011503447260890556, Validation Loss: 0.008046343656523862\n",
      "Epoch [14603/20000], Training Loss: 0.01928637981013383, Validation Loss: 0.005480120249561169\n",
      "Epoch [14604/20000], Training Loss: 0.011118699380728816, Validation Loss: 0.018387191263692926\n",
      "Epoch [14605/20000], Training Loss: 0.011815943896570908, Validation Loss: 0.0034746739300805024\n",
      "Epoch [14606/20000], Training Loss: 0.02186112648632843, Validation Loss: 0.005451286211632299\n",
      "Epoch [14607/20000], Training Loss: 0.024437052969103337, Validation Loss: 0.03001237820301737\n",
      "Epoch [14608/20000], Training Loss: 0.0294108212375355, Validation Loss: 0.011365438944527193\n",
      "Epoch [14609/20000], Training Loss: 0.013129583289940325, Validation Loss: 0.032034310837356576\n",
      "Epoch [14610/20000], Training Loss: 0.028680382861888835, Validation Loss: 0.005938136365112696\n",
      "Epoch [14611/20000], Training Loss: 0.02727318316890757, Validation Loss: 0.08651357037680493\n",
      "Epoch [14612/20000], Training Loss: 0.023589695783032636, Validation Loss: 0.013583589212171806\n",
      "Epoch [14613/20000], Training Loss: 0.02776839318695628, Validation Loss: 0.007557054615872618\n",
      "Epoch [14614/20000], Training Loss: 0.008575793055310246, Validation Loss: 0.008347601177521233\n",
      "Epoch [14615/20000], Training Loss: 0.006473021899312569, Validation Loss: 0.009144794461987172\n",
      "Epoch [14616/20000], Training Loss: 0.005643386801239103, Validation Loss: 0.004868344853216924\n",
      "Epoch [14617/20000], Training Loss: 0.00653556658480576, Validation Loss: 0.004650655000738758\n",
      "Epoch [14618/20000], Training Loss: 0.005516301386316107, Validation Loss: 0.008202990650067828\n",
      "Epoch [14619/20000], Training Loss: 0.006141798367025331, Validation Loss: 0.004624635712918772\n",
      "Epoch [14620/20000], Training Loss: 0.005734762906545906, Validation Loss: 0.0036597375835672598\n",
      "Epoch [14621/20000], Training Loss: 0.00688236327888652, Validation Loss: 0.0037782015158269447\n",
      "Epoch [14622/20000], Training Loss: 0.005283027764692504, Validation Loss: 0.009383974596858049\n",
      "Epoch [14623/20000], Training Loss: 0.004472740968464807, Validation Loss: 0.009535793533512289\n",
      "Epoch [14624/20000], Training Loss: 0.008432963040992851, Validation Loss: 0.003577788327156635\n",
      "Epoch [14625/20000], Training Loss: 0.007228543202342864, Validation Loss: 0.0034115503534029143\n",
      "Epoch [14626/20000], Training Loss: 0.0031095397820796017, Validation Loss: 0.004748753155581689\n",
      "Epoch [14627/20000], Training Loss: 0.006088258216290602, Validation Loss: 0.002795880610525083\n",
      "Epoch [14628/20000], Training Loss: 0.005570876150548949, Validation Loss: 0.01969857726778303\n",
      "Epoch [14629/20000], Training Loss: 0.0298328436882522, Validation Loss: 0.19904048953737533\n",
      "Epoch [14630/20000], Training Loss: 0.07679965776956774, Validation Loss: 0.023395378886616042\n",
      "Epoch [14631/20000], Training Loss: 0.010316215008580392, Validation Loss: 0.014345243773602436\n",
      "Epoch [14632/20000], Training Loss: 0.0052007186708838814, Validation Loss: 0.006287557185714832\n",
      "Epoch [14633/20000], Training Loss: 0.007415678160863796, Validation Loss: 0.004644017785007496\n",
      "Epoch [14634/20000], Training Loss: 0.0052141851400457585, Validation Loss: 0.00851102566066077\n",
      "Epoch [14635/20000], Training Loss: 0.006101237350126861, Validation Loss: 0.011438155323178632\n",
      "Epoch [14636/20000], Training Loss: 0.012264840364748255, Validation Loss: 0.009348225403820314\n",
      "Epoch [14637/20000], Training Loss: 0.00814544904174649, Validation Loss: 0.004284808409189671\n",
      "Epoch [14638/20000], Training Loss: 0.006088786092472479, Validation Loss: 0.003454911462077754\n",
      "Epoch [14639/20000], Training Loss: 0.010134712271142885, Validation Loss: 0.009290440084112919\n",
      "Epoch [14640/20000], Training Loss: 0.011153501171585438, Validation Loss: 0.010298615594860751\n",
      "Epoch [14641/20000], Training Loss: 0.006442602722797476, Validation Loss: 0.003376371953342933\n",
      "Epoch [14642/20000], Training Loss: 0.005274988020703729, Validation Loss: 0.003058677516150965\n",
      "Epoch [14643/20000], Training Loss: 0.010547120592621338, Validation Loss: 0.0030859760895266064\n",
      "Epoch [14644/20000], Training Loss: 0.01956964999200344, Validation Loss: 0.01272692263085722\n",
      "Epoch [14645/20000], Training Loss: 0.022685140881159378, Validation Loss: 0.06217016058253044\n",
      "Epoch [14646/20000], Training Loss: 0.04666713920804406, Validation Loss: 0.015387704006408447\n",
      "Epoch [14647/20000], Training Loss: 0.006393380968282665, Validation Loss: 0.007620273601852594\n",
      "Epoch [14648/20000], Training Loss: 0.0053176030009386265, Validation Loss: 0.004657250151922199\n",
      "Epoch [14649/20000], Training Loss: 0.008282265829620883, Validation Loss: 0.005949560007057438\n",
      "Epoch [14650/20000], Training Loss: 0.007551593165512064, Validation Loss: 0.005047432753895821\n",
      "Epoch [14651/20000], Training Loss: 0.004545901641061992, Validation Loss: 0.004051592480923968\n",
      "Epoch [14652/20000], Training Loss: 0.0062164200665262926, Validation Loss: 0.004548380783359919\n",
      "Epoch [14653/20000], Training Loss: 0.00467693209585767, Validation Loss: 0.003346652829845555\n",
      "Epoch [14654/20000], Training Loss: 0.004417039086027736, Validation Loss: 0.012290785059254818\n",
      "Epoch [14655/20000], Training Loss: 0.008876809563454506, Validation Loss: 0.0036478431674120365\n",
      "Epoch [14656/20000], Training Loss: 0.005249926614591719, Validation Loss: 0.003780616449921987\n",
      "Epoch [14657/20000], Training Loss: 0.00539654443699484, Validation Loss: 0.0036299680724190303\n",
      "Epoch [14658/20000], Training Loss: 0.004527243862477397, Validation Loss: 0.0033700332366574264\n",
      "Epoch [14659/20000], Training Loss: 0.004582141659479281, Validation Loss: 0.003683123251322838\n",
      "Epoch [14660/20000], Training Loss: 0.006810651518337961, Validation Loss: 0.005296564969812932\n",
      "Epoch [14661/20000], Training Loss: 0.00441181241123039, Validation Loss: 0.003976188374784735\n",
      "Epoch [14662/20000], Training Loss: 0.005220108010689728, Validation Loss: 0.004502667163519293\n",
      "Epoch [14663/20000], Training Loss: 0.009235077946089274, Validation Loss: 0.00451169640022085\n",
      "Epoch [14664/20000], Training Loss: 0.010976399951947055, Validation Loss: 0.0031904913908939697\n",
      "Epoch [14665/20000], Training Loss: 0.010946791847735378, Validation Loss: 0.0034047336675330697\n",
      "Epoch [14666/20000], Training Loss: 0.0038617478633347396, Validation Loss: 0.003016854419570336\n",
      "Epoch [14667/20000], Training Loss: 0.0046751451471729, Validation Loss: 0.008092933429453402\n",
      "Epoch [14668/20000], Training Loss: 0.006755049954013056, Validation Loss: 0.008633609053791328\n",
      "Epoch [14669/20000], Training Loss: 0.011604258733443982, Validation Loss: 0.02010666664903685\n",
      "Epoch [14670/20000], Training Loss: 0.011182660145485508, Validation Loss: 0.017344373274746822\n",
      "Epoch [14671/20000], Training Loss: 0.01494300472404575, Validation Loss: 0.01074745606977956\n",
      "Epoch [14672/20000], Training Loss: 0.013730140920545506, Validation Loss: 0.00264237675333285\n",
      "Epoch [14673/20000], Training Loss: 0.017554326953748905, Validation Loss: 0.019012554045773226\n",
      "Epoch [14674/20000], Training Loss: 0.009553966930590312, Validation Loss: 0.0066453092949489345\n",
      "Epoch [14675/20000], Training Loss: 0.009170817847396913, Validation Loss: 0.006288266027321957\n",
      "Epoch [14676/20000], Training Loss: 0.014972151258137143, Validation Loss: 0.0038636667698252075\n",
      "Epoch [14677/20000], Training Loss: 0.008604801805631723, Validation Loss: 0.012794498389166142\n",
      "Epoch [14678/20000], Training Loss: 0.009801583998653638, Validation Loss: 0.014198044648407761\n",
      "Epoch [14679/20000], Training Loss: 0.010772866502000917, Validation Loss: 0.007868277453378956\n",
      "Epoch [14680/20000], Training Loss: 0.008794601015091124, Validation Loss: 0.0035692575483027766\n",
      "Epoch [14681/20000], Training Loss: 0.007205405919015513, Validation Loss: 0.00950692027322475\n",
      "Epoch [14682/20000], Training Loss: 0.006309353218966862, Validation Loss: 0.005032735627877015\n",
      "Epoch [14683/20000], Training Loss: 0.01636700043673045, Validation Loss: 0.017421325951894624\n",
      "Epoch [14684/20000], Training Loss: 0.06003164768013188, Validation Loss: 0.027424452294196402\n",
      "Epoch [14685/20000], Training Loss: 0.04622856791995998, Validation Loss: 0.049806211230280235\n",
      "Epoch [14686/20000], Training Loss: 0.026631443916163074, Validation Loss: 0.006446790151780338\n",
      "Epoch [14687/20000], Training Loss: 0.010613913859872679, Validation Loss: 0.006477848971176822\n",
      "Epoch [14688/20000], Training Loss: 0.006883757043397054, Validation Loss: 0.006222400864999145\n",
      "Epoch [14689/20000], Training Loss: 0.006411705211836046, Validation Loss: 0.005340476230035814\n",
      "Epoch [14690/20000], Training Loss: 0.008318374424041914, Validation Loss: 0.004318800188782136\n",
      "Epoch [14691/20000], Training Loss: 0.00539986053939044, Validation Loss: 0.004830080442372621\n",
      "Epoch [14692/20000], Training Loss: 0.006023103150904977, Validation Loss: 0.003633327204884803\n",
      "Epoch [14693/20000], Training Loss: 0.011715193815429561, Validation Loss: 0.003307566417074018\n",
      "Epoch [14694/20000], Training Loss: 0.019935958900272714, Validation Loss: 0.008435558111614228\n",
      "Epoch [14695/20000], Training Loss: 0.013439466041745618, Validation Loss: 0.035708773881196997\n",
      "Epoch [14696/20000], Training Loss: 0.017518793515461897, Validation Loss: 0.008776361635162695\n",
      "Epoch [14697/20000], Training Loss: 0.0072021076755066004, Validation Loss: 0.009380720520815495\n",
      "Epoch [14698/20000], Training Loss: 0.008888454725300627, Validation Loss: 0.005072589454901585\n",
      "Epoch [14699/20000], Training Loss: 0.007244684857141692, Validation Loss: 0.004178363657729278\n",
      "Epoch [14700/20000], Training Loss: 0.0074957652873958325, Validation Loss: 0.011075091029332336\n",
      "Epoch [14701/20000], Training Loss: 0.020064349887044437, Validation Loss: 0.024942584449490823\n",
      "Epoch [14702/20000], Training Loss: 0.02081531099143571, Validation Loss: 0.011001213961772918\n",
      "Epoch [14703/20000], Training Loss: 0.008128018293064088, Validation Loss: 0.004293828674592858\n",
      "Epoch [14704/20000], Training Loss: 0.005299859881883354, Validation Loss: 0.004570051017854634\n",
      "Epoch [14705/20000], Training Loss: 0.004238000484519944, Validation Loss: 0.004065257232014119\n",
      "Epoch [14706/20000], Training Loss: 0.006905831733352638, Validation Loss: 0.0036972421906804292\n",
      "Epoch [14707/20000], Training Loss: 0.0078090489800420725, Validation Loss: 0.04217510883337218\n",
      "Epoch [14708/20000], Training Loss: 0.015843328319760985, Validation Loss: 0.021245126695459606\n",
      "Epoch [14709/20000], Training Loss: 0.02350876470570386, Validation Loss: 0.04230609097891959\n",
      "Epoch [14710/20000], Training Loss: 0.028863871495039866, Validation Loss: 0.009815002903321459\n",
      "Epoch [14711/20000], Training Loss: 0.01370540781811412, Validation Loss: 0.06484704783984603\n",
      "Epoch [14712/20000], Training Loss: 0.026297098461197623, Validation Loss: 0.0084328677550697\n",
      "Epoch [14713/20000], Training Loss: 0.006922206812305376, Validation Loss: 0.011766716773568728\n",
      "Epoch [14714/20000], Training Loss: 0.013287336523977242, Validation Loss: 0.005556495602750796\n",
      "Epoch [14715/20000], Training Loss: 0.01917962847437593, Validation Loss: 0.028497390911326952\n",
      "Epoch [14716/20000], Training Loss: 0.008304352900138059, Validation Loss: 0.004741379107956943\n",
      "Epoch [14717/20000], Training Loss: 0.008733863842540554, Validation Loss: 0.00984285340486427\n",
      "Epoch [14718/20000], Training Loss: 0.009704543854638814, Validation Loss: 0.003478785996451604\n",
      "Epoch [14719/20000], Training Loss: 0.0051941699286024755, Validation Loss: 0.009144332881187925\n",
      "Epoch [14720/20000], Training Loss: 0.006246016011573374, Validation Loss: 0.008654281224802876\n",
      "Epoch [14721/20000], Training Loss: 0.010813260891674352, Validation Loss: 0.005932836400140624\n",
      "Epoch [14722/20000], Training Loss: 0.00728761527300646, Validation Loss: 0.010667485003677615\n",
      "Epoch [14723/20000], Training Loss: 0.012543979544716422, Validation Loss: 0.01854606197377634\n",
      "Epoch [14724/20000], Training Loss: 0.010346950162784196, Validation Loss: 0.00558633485854695\n",
      "Epoch [14725/20000], Training Loss: 0.005930404552990305, Validation Loss: 0.005702357089765948\n",
      "Epoch [14726/20000], Training Loss: 0.007985921121856096, Validation Loss: 0.005507767586485508\n",
      "Epoch [14727/20000], Training Loss: 0.010360702248622797, Validation Loss: 0.003320014131645621\n",
      "Epoch [14728/20000], Training Loss: 0.005402913886687851, Validation Loss: 0.004292491068387757\n",
      "Epoch [14729/20000], Training Loss: 0.005668263669057134, Validation Loss: 0.0034865712103818885\n",
      "Epoch [14730/20000], Training Loss: 0.00830376790586992, Validation Loss: 0.003230495156775045\n",
      "Epoch [14731/20000], Training Loss: 0.0073458892256894615, Validation Loss: 0.0035881184956200025\n",
      "Epoch [14732/20000], Training Loss: 0.009820959837140566, Validation Loss: 0.011657202114812597\n",
      "Epoch [14733/20000], Training Loss: 0.026979668104364203, Validation Loss: 0.02563376761463587\n",
      "Epoch [14734/20000], Training Loss: 0.01885636331436607, Validation Loss: 0.004610874317766697\n",
      "Epoch [14735/20000], Training Loss: 0.007078475620281617, Validation Loss: 0.009438115814002985\n",
      "Epoch [14736/20000], Training Loss: 0.008737118407485209, Validation Loss: 0.0030059025868272977\n",
      "Epoch [14737/20000], Training Loss: 0.008517387038279724, Validation Loss: 0.016659482781707955\n",
      "Epoch [14738/20000], Training Loss: 0.013994281578301784, Validation Loss: 0.006222906796470592\n",
      "Epoch [14739/20000], Training Loss: 0.0038808440757358248, Validation Loss: 0.008027815553759865\n",
      "Epoch [14740/20000], Training Loss: 0.005257873693023741, Validation Loss: 0.011711534783071045\n",
      "Epoch [14741/20000], Training Loss: 0.006075890064883944, Validation Loss: 0.00683828298338085\n",
      "Epoch [14742/20000], Training Loss: 0.006963788896687869, Validation Loss: 0.002892689680444554\n",
      "Epoch [14743/20000], Training Loss: 0.0054256694503627455, Validation Loss: 0.0027555613465080698\n",
      "Epoch [14744/20000], Training Loss: 0.0043601948351092035, Validation Loss: 0.0027737611556111525\n",
      "Epoch [14745/20000], Training Loss: 0.007598260596166385, Validation Loss: 0.011359412704517973\n",
      "Epoch [14746/20000], Training Loss: 0.010468731070951824, Validation Loss: 0.02266621191009886\n",
      "Epoch [14747/20000], Training Loss: 0.01778035521108125, Validation Loss: 0.006707577461696289\n",
      "Epoch [14748/20000], Training Loss: 0.042759914252591055, Validation Loss: 0.011909358950143858\n",
      "Epoch [14749/20000], Training Loss: 0.023270135283902555, Validation Loss: 0.006331301099550046\n",
      "Epoch [14750/20000], Training Loss: 0.021293151977131077, Validation Loss: 0.01790385372250317\n",
      "Epoch [14751/20000], Training Loss: 0.019918023767266795, Validation Loss: 0.010247135186172531\n",
      "Epoch [14752/20000], Training Loss: 0.028914440998603044, Validation Loss: 0.05478758258484887\n",
      "Epoch [14753/20000], Training Loss: 0.022398693098563984, Validation Loss: 0.0115160531695478\n",
      "Epoch [14754/20000], Training Loss: 0.040965868950089704, Validation Loss: 0.028101605807153516\n",
      "Epoch [14755/20000], Training Loss: 0.018277546956336925, Validation Loss: 0.007152410488872972\n",
      "Epoch [14756/20000], Training Loss: 0.007798556595974203, Validation Loss: 0.006580716924409964\n",
      "Epoch [14757/20000], Training Loss: 0.005929064379805433, Validation Loss: 0.005561723919738272\n",
      "Epoch [14758/20000], Training Loss: 0.005672913102898747, Validation Loss: 0.0067036898717398275\n",
      "Epoch [14759/20000], Training Loss: 0.005733799305744469, Validation Loss: 0.00732667987430042\n",
      "Epoch [14760/20000], Training Loss: 0.0062347844525772545, Validation Loss: 0.005210714195828002\n",
      "Epoch [14761/20000], Training Loss: 0.005533668985922954, Validation Loss: 0.0039607139667623414\n",
      "Epoch [14762/20000], Training Loss: 0.0070376768256770516, Validation Loss: 0.003853312893430224\n",
      "Epoch [14763/20000], Training Loss: 0.0060157386365712485, Validation Loss: 0.011441094459861365\n",
      "Epoch [14764/20000], Training Loss: 0.0073851384617488035, Validation Loss: 0.004014637513803011\n",
      "Epoch [14765/20000], Training Loss: 0.0049468685673283675, Validation Loss: 0.004082197930245002\n",
      "Epoch [14766/20000], Training Loss: 0.007130988910960566, Validation Loss: 0.008282353682070198\n",
      "Epoch [14767/20000], Training Loss: 0.00563594356520168, Validation Loss: 0.0038382071284835157\n",
      "Epoch [14768/20000], Training Loss: 0.004818101733690128, Validation Loss: 0.0031481153453642363\n",
      "Epoch [14769/20000], Training Loss: 0.007735112145642883, Validation Loss: 0.00325445922001271\n",
      "Epoch [14770/20000], Training Loss: 0.003295345921083635, Validation Loss: 0.0034323067978943656\n",
      "Epoch [14771/20000], Training Loss: 0.0037230356924169428, Validation Loss: 0.0042895156993775375\n",
      "Epoch [14772/20000], Training Loss: 0.007002615818886885, Validation Loss: 0.00789301179260528\n",
      "Epoch [14773/20000], Training Loss: 0.00908372422314382, Validation Loss: 0.0044079489368950165\n",
      "Epoch [14774/20000], Training Loss: 0.010137838258482848, Validation Loss: 0.018898609471697064\n",
      "Epoch [14775/20000], Training Loss: 0.014189701602195523, Validation Loss: 0.003523738109239259\n",
      "Epoch [14776/20000], Training Loss: 0.0045565639655771, Validation Loss: 0.004971985064562455\n",
      "Epoch [14777/20000], Training Loss: 0.004065953741831306, Validation Loss: 0.005039483764319032\n",
      "Epoch [14778/20000], Training Loss: 0.008813554870097764, Validation Loss: 0.007194712719530784\n",
      "Epoch [14779/20000], Training Loss: 0.01296232091928167, Validation Loss: 0.004825182685351688\n",
      "Epoch [14780/20000], Training Loss: 0.011730565744073829, Validation Loss: 0.007007033301026208\n",
      "Epoch [14781/20000], Training Loss: 0.010824009505865564, Validation Loss: 0.004926879988639245\n",
      "Epoch [14782/20000], Training Loss: 0.010930453555504625, Validation Loss: 0.005393955521642989\n",
      "Epoch [14783/20000], Training Loss: 0.005556294618665041, Validation Loss: 0.009885902448396726\n",
      "Epoch [14784/20000], Training Loss: 0.023426464976832255, Validation Loss: 0.006300210801637792\n",
      "Epoch [14785/20000], Training Loss: 0.018147020933351347, Validation Loss: 0.0030041208598358287\n",
      "Epoch [14786/20000], Training Loss: 0.004842905884808195, Validation Loss: 0.01284310188556599\n",
      "Epoch [14787/20000], Training Loss: 0.011511944596609933, Validation Loss: 0.004145571347627148\n",
      "Epoch [14788/20000], Training Loss: 0.008542636949901603, Validation Loss: 0.004416306162275825\n",
      "Epoch [14789/20000], Training Loss: 0.011553902977311736, Validation Loss: 0.013142562707246595\n",
      "Epoch [14790/20000], Training Loss: 0.014520610952916155, Validation Loss: 0.007900384647030023\n",
      "Epoch [14791/20000], Training Loss: 0.02596843530357416, Validation Loss: 0.0048739595884873055\n",
      "Epoch [14792/20000], Training Loss: 0.012918846295048882, Validation Loss: 0.005628681793309427\n",
      "Epoch [14793/20000], Training Loss: 0.014814387901554125, Validation Loss: 0.017247893538403874\n",
      "Epoch [14794/20000], Training Loss: 0.0074079496234844555, Validation Loss: 0.0034253250239544286\n",
      "Epoch [14795/20000], Training Loss: 0.008118602933921335, Validation Loss: 0.005008580910310201\n",
      "Epoch [14796/20000], Training Loss: 0.009200454384491812, Validation Loss: 0.0036584634579241992\n",
      "Epoch [14797/20000], Training Loss: 0.007816115166081414, Validation Loss: 0.0030260592848857675\n",
      "Epoch [14798/20000], Training Loss: 0.006053343485681191, Validation Loss: 0.003075312557775029\n",
      "Epoch [14799/20000], Training Loss: 0.005737718365188422, Validation Loss: 0.0027635748013454175\n",
      "Epoch [14800/20000], Training Loss: 0.007150686433305964, Validation Loss: 0.003911154056246805\n",
      "Epoch [14801/20000], Training Loss: 0.010453329151329984, Validation Loss: 0.0057212160854110804\n",
      "Epoch [14802/20000], Training Loss: 0.011678364663891574, Validation Loss: 0.004766877153900039\n",
      "Epoch [14803/20000], Training Loss: 0.016099498318258805, Validation Loss: 0.03164007621152066\n",
      "Epoch [14804/20000], Training Loss: 0.010936585703997739, Validation Loss: 0.007421998055646158\n",
      "Epoch [14805/20000], Training Loss: 0.012103123285023847, Validation Loss: 0.004805456782992775\n",
      "Epoch [14806/20000], Training Loss: 0.02198244261671789, Validation Loss: 0.012140288491232403\n",
      "Epoch [14807/20000], Training Loss: 0.03696805020860795, Validation Loss: 0.10600117385287636\n",
      "Epoch [14808/20000], Training Loss: 0.053833829305533855, Validation Loss: 0.018817444540348203\n",
      "Epoch [14809/20000], Training Loss: 0.032083823873628195, Validation Loss: 0.008441800706415441\n",
      "Epoch [14810/20000], Training Loss: 0.013514069108558553, Validation Loss: 0.007416361011388111\n",
      "Epoch [14811/20000], Training Loss: 0.008201971979620535, Validation Loss: 0.0056768574076500045\n",
      "Epoch [14812/20000], Training Loss: 0.0068646033427545005, Validation Loss: 0.0074427131841210735\n",
      "Epoch [14813/20000], Training Loss: 0.008698047105489033, Validation Loss: 0.004382631162960544\n",
      "Epoch [14814/20000], Training Loss: 0.005371618720736089, Validation Loss: 0.012134043390013316\n",
      "Epoch [14815/20000], Training Loss: 0.013878324158473074, Validation Loss: 0.006184394560721428\n",
      "Epoch [14816/20000], Training Loss: 0.009575222298735753, Validation Loss: 0.00798384417984315\n",
      "Epoch [14817/20000], Training Loss: 0.007299352537562039, Validation Loss: 0.005888198713142691\n",
      "Epoch [14818/20000], Training Loss: 0.007111277336304868, Validation Loss: 0.011058693738173935\n",
      "Epoch [14819/20000], Training Loss: 0.007121565452377711, Validation Loss: 0.004401730928926944\n",
      "Epoch [14820/20000], Training Loss: 0.0094919496644122, Validation Loss: 0.005295557632639701\n",
      "Epoch [14821/20000], Training Loss: 0.014869293561787345, Validation Loss: 0.019024025943597107\n",
      "Epoch [14822/20000], Training Loss: 0.005274178804289217, Validation Loss: 0.0056448371024576515\n",
      "Epoch [14823/20000], Training Loss: 0.004638340290901917, Validation Loss: 0.006652529124199314\n",
      "Epoch [14824/20000], Training Loss: 0.008635220303663768, Validation Loss: 0.0036829946898824873\n",
      "Epoch [14825/20000], Training Loss: 0.009570909898944333, Validation Loss: 0.021683467019881525\n",
      "Epoch [14826/20000], Training Loss: 0.0049213245271987815, Validation Loss: 0.00722374727224308\n",
      "Epoch [14827/20000], Training Loss: 0.007352393272234232, Validation Loss: 0.003063212686711836\n",
      "Epoch [14828/20000], Training Loss: 0.003955085753529731, Validation Loss: 0.0029876313185366954\n",
      "Epoch [14829/20000], Training Loss: 0.003377877372819122, Validation Loss: 0.006828255845019078\n",
      "Epoch [14830/20000], Training Loss: 0.007638438344408931, Validation Loss: 0.02186106517912812\n",
      "Epoch [14831/20000], Training Loss: 0.010604025999287654, Validation Loss: 0.0048761359786441616\n",
      "Epoch [14832/20000], Training Loss: 0.009503175762282419, Validation Loss: 0.009772934533917319\n",
      "Epoch [14833/20000], Training Loss: 0.01889113057812405, Validation Loss: 0.027560707181692123\n",
      "Epoch [14834/20000], Training Loss: 0.014234066949159439, Validation Loss: 0.008134076236905532\n",
      "Epoch [14835/20000], Training Loss: 0.006627669067321611, Validation Loss: 0.010583092156448466\n",
      "Epoch [14836/20000], Training Loss: 0.010962361515599437, Validation Loss: 0.006835988384179569\n",
      "Epoch [14837/20000], Training Loss: 0.010390068013553641, Validation Loss: 0.009190330311148338\n",
      "Epoch [14838/20000], Training Loss: 0.007974195927090477, Validation Loss: 0.002643566311781293\n",
      "Epoch [14839/20000], Training Loss: 0.00634997560596925, Validation Loss: 0.003575445281684012\n",
      "Epoch [14840/20000], Training Loss: 0.005143880235479514, Validation Loss: 0.002928989686531785\n",
      "Epoch [14841/20000], Training Loss: 0.005270619898510631, Validation Loss: 0.009066743579234735\n",
      "Epoch [14842/20000], Training Loss: 0.011619337094348989, Validation Loss: 0.014891035123063145\n",
      "Epoch [14843/20000], Training Loss: 0.012042654562849617, Validation Loss: 0.02415034406287551\n",
      "Epoch [14844/20000], Training Loss: 0.02346256722999637, Validation Loss: 0.02317693137696811\n",
      "Epoch [14845/20000], Training Loss: 0.01471921183734334, Validation Loss: 0.02484173718534041\n",
      "Epoch [14846/20000], Training Loss: 0.058837408617234487, Validation Loss: 0.0210574270030455\n",
      "Epoch [14847/20000], Training Loss: 0.02391774349546592, Validation Loss: 0.017671025563284308\n",
      "Epoch [14848/20000], Training Loss: 0.021223064270868366, Validation Loss: 0.007348634782849623\n",
      "Epoch [14849/20000], Training Loss: 0.008421169481932469, Validation Loss: 0.018739421732037027\n",
      "Epoch [14850/20000], Training Loss: 0.007999108534055008, Validation Loss: 0.00836204561690985\n",
      "Epoch [14851/20000], Training Loss: 0.008014971490151115, Validation Loss: 0.005088355382690273\n",
      "Epoch [14852/20000], Training Loss: 0.006452417662070989, Validation Loss: 0.014699856404651903\n",
      "Epoch [14853/20000], Training Loss: 0.013370157522980921, Validation Loss: 0.007117718080151191\n",
      "Epoch [14854/20000], Training Loss: 0.007530069261942377, Validation Loss: 0.007852608195826671\n",
      "Epoch [14855/20000], Training Loss: 0.005776363054922383, Validation Loss: 0.004498433805305727\n",
      "Epoch [14856/20000], Training Loss: 0.007700813486215858, Validation Loss: 0.0069047564791436655\n",
      "Epoch [14857/20000], Training Loss: 0.009202253923701522, Validation Loss: 0.010877349546630829\n",
      "Epoch [14858/20000], Training Loss: 0.015136485219824993, Validation Loss: 0.005341043755330637\n",
      "Epoch [14859/20000], Training Loss: 0.007900044556923344, Validation Loss: 0.003427461730747642\n",
      "Epoch [14860/20000], Training Loss: 0.008233623301618666, Validation Loss: 0.01519512559553807\n",
      "Epoch [14861/20000], Training Loss: 0.012013862096604757, Validation Loss: 0.014102457656660678\n",
      "Epoch [14862/20000], Training Loss: 0.015092667707774256, Validation Loss: 0.004863933828998412\n",
      "Epoch [14863/20000], Training Loss: 0.005702503225100892, Validation Loss: 0.004099457934816491\n",
      "Epoch [14864/20000], Training Loss: 0.006246532924706116, Validation Loss: 0.0030317479785172247\n",
      "Epoch [14865/20000], Training Loss: 0.008059535678642402, Validation Loss: 0.004891935505156653\n",
      "Epoch [14866/20000], Training Loss: 0.0031946394696465825, Validation Loss: 0.00955669338856957\n",
      "Epoch [14867/20000], Training Loss: 0.011136195015361798, Validation Loss: 0.003599783756781478\n",
      "Epoch [14868/20000], Training Loss: 0.007368971342137749, Validation Loss: 0.0027983426827315077\n",
      "Epoch [14869/20000], Training Loss: 0.006011610578363096, Validation Loss: 0.005123768780186481\n",
      "Epoch [14870/20000], Training Loss: 0.004537234731807465, Validation Loss: 0.005137564892152113\n",
      "Epoch [14871/20000], Training Loss: 0.004949798986802177, Validation Loss: 0.0029701226493484357\n",
      "Epoch [14872/20000], Training Loss: 0.005934589297144807, Validation Loss: 0.00240897001199463\n",
      "Epoch [14873/20000], Training Loss: 0.005319335697484868, Validation Loss: 0.007902979387470152\n",
      "Epoch [14874/20000], Training Loss: 0.007482196279202721, Validation Loss: 0.0030526435871019203\n",
      "Epoch [14875/20000], Training Loss: 0.009309380238911607, Validation Loss: 0.01022348800259871\n",
      "Epoch [14876/20000], Training Loss: 0.004760291836906357, Validation Loss: 0.011673543336252756\n",
      "Epoch [14877/20000], Training Loss: 0.0032935910354093983, Validation Loss: 0.006636423079580059\n",
      "Epoch [14878/20000], Training Loss: 0.008425080643584286, Validation Loss: 0.002728372050237761\n",
      "Epoch [14879/20000], Training Loss: 0.005061206713695096, Validation Loss: 0.006155656924476898\n",
      "Epoch [14880/20000], Training Loss: 0.01220114720781806, Validation Loss: 0.012804068625049531\n",
      "Epoch [14881/20000], Training Loss: 0.014364190418746148, Validation Loss: 0.005575874254432479\n",
      "Epoch [14882/20000], Training Loss: 0.005765008117513, Validation Loss: 0.0024148454092731697\n",
      "Epoch [14883/20000], Training Loss: 0.009036080627725509, Validation Loss: 0.00620334543534134\n",
      "Epoch [14884/20000], Training Loss: 0.009940281863756744, Validation Loss: 0.0034368468364267863\n",
      "Epoch [14885/20000], Training Loss: 0.009102824159656717, Validation Loss: 0.007540117751362305\n",
      "Epoch [14886/20000], Training Loss: 0.03623461282404605, Validation Loss: 0.04269101257835116\n",
      "Epoch [14887/20000], Training Loss: 0.0413004614903392, Validation Loss: 0.031766987804855616\n",
      "Epoch [14888/20000], Training Loss: 0.015248161819597175, Validation Loss: 0.0070493021246043185\n",
      "Epoch [14889/20000], Training Loss: 0.008874447364697906, Validation Loss: 0.012896835113776563\n",
      "Epoch [14890/20000], Training Loss: 0.014074156671995297, Validation Loss: 0.005468013577924384\n",
      "Epoch [14891/20000], Training Loss: 0.010484132296712443, Validation Loss: 0.0045284592415799\n",
      "Epoch [14892/20000], Training Loss: 0.006630760893009275, Validation Loss: 0.004223669072037036\n",
      "Epoch [14893/20000], Training Loss: 0.004857998896831235, Validation Loss: 0.011651181232589696\n",
      "Epoch [14894/20000], Training Loss: 0.006434705877576822, Validation Loss: 0.004957033059440619\n",
      "Epoch [14895/20000], Training Loss: 0.005863243179711779, Validation Loss: 0.003941856826551364\n",
      "Epoch [14896/20000], Training Loss: 0.006353708148318609, Validation Loss: 0.007352271563469066\n",
      "Epoch [14897/20000], Training Loss: 0.006558439463593199, Validation Loss: 0.0037821868418380484\n",
      "Epoch [14898/20000], Training Loss: 0.004948753543112717, Validation Loss: 0.006029284726131274\n",
      "Epoch [14899/20000], Training Loss: 0.009827021266184082, Validation Loss: 0.00726611471563577\n",
      "Epoch [14900/20000], Training Loss: 0.008607372371313562, Validation Loss: 0.022586858432207788\n",
      "Epoch [14901/20000], Training Loss: 0.01034347350027279, Validation Loss: 0.004319988486613294\n",
      "Epoch [14902/20000], Training Loss: 0.005714620839695935, Validation Loss: 0.008420668376256149\n",
      "Epoch [14903/20000], Training Loss: 0.009782313269429974, Validation Loss: 0.017084578462052896\n",
      "Epoch [14904/20000], Training Loss: 0.01482564678216087, Validation Loss: 0.003595210586301586\n",
      "Epoch [14905/20000], Training Loss: 0.006148149022919824, Validation Loss: 0.0035762611068925437\n",
      "Epoch [14906/20000], Training Loss: 0.0050820392547653425, Validation Loss: 0.008485703332906904\n",
      "Epoch [14907/20000], Training Loss: 0.008494182396524204, Validation Loss: 0.00562502544044362\n",
      "Epoch [14908/20000], Training Loss: 0.004489751299063626, Validation Loss: 0.0026281459556299425\n",
      "Epoch [14909/20000], Training Loss: 0.007128233229399784, Validation Loss: 0.01057847183464245\n",
      "Epoch [14910/20000], Training Loss: 0.015086504279419646, Validation Loss: 0.0025229284305173954\n",
      "Epoch [14911/20000], Training Loss: 0.004804449310086056, Validation Loss: 0.0030095372180091967\n",
      "Epoch [14912/20000], Training Loss: 0.009383143182127185, Validation Loss: 0.002923132606531899\n",
      "Epoch [14913/20000], Training Loss: 0.006842553410154194, Validation Loss: 0.032712414559382196\n",
      "Epoch [14914/20000], Training Loss: 0.018163488687211578, Validation Loss: 0.002726578001851213\n",
      "Epoch [14915/20000], Training Loss: 0.012109282477467787, Validation Loss: 0.02153211140208988\n",
      "Epoch [14916/20000], Training Loss: 0.008738429381115307, Validation Loss: 0.0025172213765990597\n",
      "Epoch [14917/20000], Training Loss: 0.013388984177644099, Validation Loss: 0.0048517115377045795\n",
      "Epoch [14918/20000], Training Loss: 0.03385339159805361, Validation Loss: 0.022743490524590015\n",
      "Epoch [14919/20000], Training Loss: 0.018341679841146936, Validation Loss: 0.0034202524195369017\n",
      "Epoch [14920/20000], Training Loss: 0.011803761492566471, Validation Loss: 0.004404468568550003\n",
      "Epoch [14921/20000], Training Loss: 0.009534497566936362, Validation Loss: 0.015157489107183538\n",
      "Epoch [14922/20000], Training Loss: 0.009123308251738698, Validation Loss: 0.003857965458792023\n",
      "Epoch [14923/20000], Training Loss: 0.00851071163612817, Validation Loss: 0.004055212039824666\n",
      "Epoch [14924/20000], Training Loss: 0.004305604657669652, Validation Loss: 0.003783693431244168\n",
      "Epoch [14925/20000], Training Loss: 0.00647427378005107, Validation Loss: 0.008266566788363556\n",
      "Epoch [14926/20000], Training Loss: 0.006038340820980791, Validation Loss: 0.0026570042307168607\n",
      "Epoch [14927/20000], Training Loss: 0.004304748947366274, Validation Loss: 0.007481743456219216\n",
      "Epoch [14928/20000], Training Loss: 0.006902254744740536, Validation Loss: 0.002779086440636545\n",
      "Epoch [14929/20000], Training Loss: 0.01230273554548538, Validation Loss: 0.00490292526209097\n",
      "Epoch [14930/20000], Training Loss: 0.030202279862870846, Validation Loss: 0.006552850019781025\n",
      "Epoch [14931/20000], Training Loss: 0.015380117402985758, Validation Loss: 0.01024733317615811\n",
      "Epoch [14932/20000], Training Loss: 0.011449183822281026, Validation Loss: 0.011951739833292093\n",
      "Epoch [14933/20000], Training Loss: 0.008200507093403888, Validation Loss: 0.004455226882880327\n",
      "Epoch [14934/20000], Training Loss: 0.006549981119210965, Validation Loss: 0.004399747164345135\n",
      "Epoch [14935/20000], Training Loss: 0.007832414390837325, Validation Loss: 0.018606795249784876\n",
      "Epoch [14936/20000], Training Loss: 0.020826703638054563, Validation Loss: 0.015168727841228247\n",
      "Epoch [14937/20000], Training Loss: 0.0188204877633715, Validation Loss: 0.008713525047103756\n",
      "Epoch [14938/20000], Training Loss: 0.015165676451438959, Validation Loss: 0.017103386780766283\n",
      "Epoch [14939/20000], Training Loss: 0.010752663392590225, Validation Loss: 0.006696731542637288\n",
      "Epoch [14940/20000], Training Loss: 0.0062161580970924646, Validation Loss: 0.0037109091733264726\n",
      "Epoch [14941/20000], Training Loss: 0.004789329738352015, Validation Loss: 0.00494678468125022\n",
      "Epoch [14942/20000], Training Loss: 0.0052562608658003485, Validation Loss: 0.004085098904900926\n",
      "Epoch [14943/20000], Training Loss: 0.005281330995135899, Validation Loss: 0.0039056137930891005\n",
      "Epoch [14944/20000], Training Loss: 0.009906533566598747, Validation Loss: 0.006434120128206653\n",
      "Epoch [14945/20000], Training Loss: 0.011732776323957037, Validation Loss: 0.006641045016674761\n",
      "Epoch [14946/20000], Training Loss: 0.012381942857505887, Validation Loss: 0.0171995844068858\n",
      "Epoch [14947/20000], Training Loss: 0.01488701966757487, Validation Loss: 0.006602671967998015\n",
      "Epoch [14948/20000], Training Loss: 0.013724692483914882, Validation Loss: 0.007289146448799506\n",
      "Epoch [14949/20000], Training Loss: 0.0058770069060431395, Validation Loss: 0.004710589242220723\n",
      "Epoch [14950/20000], Training Loss: 0.00909143882362186, Validation Loss: 0.007969560847040416\n",
      "Epoch [14951/20000], Training Loss: 0.031098191816551792, Validation Loss: 0.031521281989659755\n",
      "Epoch [14952/20000], Training Loss: 0.10427775166213646, Validation Loss: 0.06491640556961745\n",
      "Epoch [14953/20000], Training Loss: 0.03489850002551975, Validation Loss: 0.007929923989390122\n",
      "Epoch [14954/20000], Training Loss: 0.014428165967858928, Validation Loss: 0.0137758262829519\n",
      "Epoch [14955/20000], Training Loss: 0.010699850374034472, Validation Loss: 0.009727365340170633\n",
      "Epoch [14956/20000], Training Loss: 0.009956248628441244, Validation Loss: 0.0066794682287374075\n",
      "Epoch [14957/20000], Training Loss: 0.008022610031730437, Validation Loss: 0.005459443604422631\n",
      "Epoch [14958/20000], Training Loss: 0.006330071100299912, Validation Loss: 0.006731669688830623\n",
      "Epoch [14959/20000], Training Loss: 0.006462355066755013, Validation Loss: 0.005508147210564728\n",
      "Epoch [14960/20000], Training Loss: 0.005802918009326926, Validation Loss: 0.004881428175344078\n",
      "Epoch [14961/20000], Training Loss: 0.006204205217987432, Validation Loss: 0.005799151830030626\n",
      "Epoch [14962/20000], Training Loss: 0.0072507023245894486, Validation Loss: 0.004412838369698794\n",
      "Epoch [14963/20000], Training Loss: 0.006515525564152215, Validation Loss: 0.005146873294490563\n",
      "Epoch [14964/20000], Training Loss: 0.005615451276201254, Validation Loss: 0.007640832005332221\n",
      "Epoch [14965/20000], Training Loss: 0.005693408498440736, Validation Loss: 0.0037774098099622017\n",
      "Epoch [14966/20000], Training Loss: 0.007352870080337327, Validation Loss: 0.0036467346782233434\n",
      "Epoch [14967/20000], Training Loss: 0.004855202725398807, Validation Loss: 0.0049927270859646865\n",
      "Epoch [14968/20000], Training Loss: 0.005265565219035905, Validation Loss: 0.014353759588155338\n",
      "Epoch [14969/20000], Training Loss: 0.010421340399521537, Validation Loss: 0.004204170283679809\n",
      "Epoch [14970/20000], Training Loss: 0.008915212633188016, Validation Loss: 0.00395656484007694\n",
      "Epoch [14971/20000], Training Loss: 0.006164914172716506, Validation Loss: 0.0033781382369641377\n",
      "Epoch [14972/20000], Training Loss: 0.016947541805555893, Validation Loss: 0.03788401931524992\n",
      "Epoch [14973/20000], Training Loss: 0.030409591815311745, Validation Loss: 0.05014290341309139\n",
      "Epoch [14974/20000], Training Loss: 0.022062703310179392, Validation Loss: 0.011797514344966098\n",
      "Epoch [14975/20000], Training Loss: 0.015999559425316483, Validation Loss: 0.020421289119050407\n",
      "Epoch [14976/20000], Training Loss: 0.014152555414641808, Validation Loss: 0.0045715937233917914\n",
      "Epoch [14977/20000], Training Loss: 0.006471490614265869, Validation Loss: 0.01114172585698725\n",
      "Epoch [14978/20000], Training Loss: 0.010125973905015176, Validation Loss: 0.00407492200590046\n",
      "Epoch [14979/20000], Training Loss: 0.0070308861364277875, Validation Loss: 0.004257849641037735\n",
      "Epoch [14980/20000], Training Loss: 0.005854702767205059, Validation Loss: 0.0034019800484816898\n",
      "Epoch [14981/20000], Training Loss: 0.0048155904338013245, Validation Loss: 0.003017788990171815\n",
      "Epoch [14982/20000], Training Loss: 0.005228117478379447, Validation Loss: 0.0031896975169857073\n",
      "Epoch [14983/20000], Training Loss: 0.007409044563766136, Validation Loss: 0.003582627000769532\n",
      "Epoch [14984/20000], Training Loss: 0.022436963119876703, Validation Loss: 0.003610381091474681\n",
      "Epoch [14985/20000], Training Loss: 0.011907655920367688, Validation Loss: 0.012724536304761403\n",
      "Epoch [14986/20000], Training Loss: 0.005733896145782172, Validation Loss: 0.0041261733869118545\n",
      "Epoch [14987/20000], Training Loss: 0.006397009583452018, Validation Loss: 0.0037683850931102825\n",
      "Epoch [14988/20000], Training Loss: 0.006739133744434055, Validation Loss: 0.004141433841253158\n",
      "Epoch [14989/20000], Training Loss: 0.0047065642512669524, Validation Loss: 0.004281263844863992\n",
      "Epoch [14990/20000], Training Loss: 0.005674709701582158, Validation Loss: 0.003163048069726417\n",
      "Epoch [14991/20000], Training Loss: 0.005310662274545542, Validation Loss: 0.0034150210467121917\n",
      "Epoch [14992/20000], Training Loss: 0.00453802247076445, Validation Loss: 0.0027377989790409174\n",
      "Epoch [14993/20000], Training Loss: 0.004837833561136254, Validation Loss: 0.0027928361889210584\n",
      "Epoch [14994/20000], Training Loss: 0.004770305767254156, Validation Loss: 0.005550741455511896\n",
      "Epoch [14995/20000], Training Loss: 0.010506152450143904, Validation Loss: 0.007440040158977558\n",
      "Epoch [14996/20000], Training Loss: 0.01471330816821137, Validation Loss: 0.0034089037322091137\n",
      "Epoch [14997/20000], Training Loss: 0.006812442143979881, Validation Loss: 0.006297512269278676\n",
      "Epoch [14998/20000], Training Loss: 0.005717375515294927, Validation Loss: 0.005545069547013416\n",
      "Epoch [14999/20000], Training Loss: 0.006743190508773621, Validation Loss: 0.011781848700984143\n",
      "Epoch [15000/20000], Training Loss: 0.010054438311044578, Validation Loss: 0.0033654594416610956\n",
      "Epoch [15001/20000], Training Loss: 0.008904847247842034, Validation Loss: 0.0037119748014524437\n",
      "Epoch [15002/20000], Training Loss: 0.006994593665908155, Validation Loss: 0.004956244214162504\n",
      "Epoch [15003/20000], Training Loss: 0.006422642793170651, Validation Loss: 0.003818666552340717\n",
      "Epoch [15004/20000], Training Loss: 0.005814376175944095, Validation Loss: 0.003041754994462163\n",
      "Epoch [15005/20000], Training Loss: 0.005351060010850363, Validation Loss: 0.009028975585754554\n",
      "Epoch [15006/20000], Training Loss: 0.008747295132447366, Validation Loss: 0.004387869135519237\n",
      "Epoch [15007/20000], Training Loss: 0.015636988443605202, Validation Loss: 0.004897293529134166\n",
      "Epoch [15008/20000], Training Loss: 0.020029713005767138, Validation Loss: 0.008125770359332105\n",
      "Epoch [15009/20000], Training Loss: 0.005412476939714647, Validation Loss: 0.005655011141170048\n",
      "Epoch [15010/20000], Training Loss: 0.005197097505775413, Validation Loss: 0.006865285046007611\n",
      "Epoch [15011/20000], Training Loss: 0.010760262277991777, Validation Loss: 0.004985115604936411\n",
      "Epoch [15012/20000], Training Loss: 0.011149627461756089, Validation Loss: 0.0040606312333458005\n",
      "Epoch [15013/20000], Training Loss: 0.012547075496643498, Validation Loss: 0.024462635914444166\n",
      "Epoch [15014/20000], Training Loss: 0.012606752510200587, Validation Loss: 0.005140128047243328\n",
      "Epoch [15015/20000], Training Loss: 0.0060036016418182825, Validation Loss: 0.003173794093896097\n",
      "Epoch [15016/20000], Training Loss: 0.006260753568702824, Validation Loss: 0.004446285297653339\n",
      "Epoch [15017/20000], Training Loss: 0.006375844299327582, Validation Loss: 0.00508828854015089\n",
      "Epoch [15018/20000], Training Loss: 0.006107400035546376, Validation Loss: 0.00483303218087805\n",
      "Epoch [15019/20000], Training Loss: 0.00510968609523843, Validation Loss: 0.005245224774422247\n",
      "Epoch [15020/20000], Training Loss: 0.007886392875921697, Validation Loss: 0.00463551237192812\n",
      "Epoch [15021/20000], Training Loss: 0.0033239040193464575, Validation Loss: 0.0027505764626571155\n",
      "Epoch [15022/20000], Training Loss: 0.007448856905934268, Validation Loss: 0.004436555304583375\n",
      "Epoch [15023/20000], Training Loss: 0.011366623649207343, Validation Loss: 0.003914150738157234\n",
      "Epoch [15024/20000], Training Loss: 0.003594896058660067, Validation Loss: 0.013725316950255861\n",
      "Epoch [15025/20000], Training Loss: 0.008055889854274158, Validation Loss: 0.005113180300929328\n",
      "Epoch [15026/20000], Training Loss: 0.0073094577237498015, Validation Loss: 0.006096783859452434\n",
      "Epoch [15027/20000], Training Loss: 0.013259563423291962, Validation Loss: 0.005889821662188004\n",
      "Epoch [15028/20000], Training Loss: 0.005249198468456078, Validation Loss: 0.008918707109324788\n",
      "Epoch [15029/20000], Training Loss: 0.008191565540203425, Validation Loss: 0.004234157155772134\n",
      "Epoch [15030/20000], Training Loss: 0.01634197648880737, Validation Loss: 0.040799110063484745\n",
      "Epoch [15031/20000], Training Loss: 0.022463078723896097, Validation Loss: 0.021232619884919517\n",
      "Epoch [15032/20000], Training Loss: 0.022849330756279023, Validation Loss: 0.004910313441922634\n",
      "Epoch [15033/20000], Training Loss: 0.0071679987633095675, Validation Loss: 0.005359331080253248\n",
      "Epoch [15034/20000], Training Loss: 0.005344574476891596, Validation Loss: 0.00531774114277344\n",
      "Epoch [15035/20000], Training Loss: 0.008696204907859542, Validation Loss: 0.004565502924358055\n",
      "Epoch [15036/20000], Training Loss: 0.006700179814028421, Validation Loss: 0.004556427380493412\n",
      "Epoch [15037/20000], Training Loss: 0.0039248958019015845, Validation Loss: 0.0062158382166217835\n",
      "Epoch [15038/20000], Training Loss: 0.006474210703475235, Validation Loss: 0.006106895856226734\n",
      "Epoch [15039/20000], Training Loss: 0.009489649748762272, Validation Loss: 0.005230550604499072\n",
      "Epoch [15040/20000], Training Loss: 0.007447495730178032, Validation Loss: 0.01117389050987835\n",
      "Epoch [15041/20000], Training Loss: 0.007546541698892335, Validation Loss: 0.012012856546789408\n",
      "Epoch [15042/20000], Training Loss: 0.00631478928179214, Validation Loss: 0.004732488253823013\n",
      "Epoch [15043/20000], Training Loss: 0.00894579403237523, Validation Loss: 0.00466358966268809\n",
      "Epoch [15044/20000], Training Loss: 0.010165337331792606, Validation Loss: 0.004968236102013991\n",
      "Epoch [15045/20000], Training Loss: 0.010297869178300192, Validation Loss: 0.011253067699047464\n",
      "Epoch [15046/20000], Training Loss: 0.014495155686388119, Validation Loss: 0.008865871305468025\n",
      "Epoch [15047/20000], Training Loss: 0.0072743964207447566, Validation Loss: 0.00851548163752556\n",
      "Epoch [15048/20000], Training Loss: 0.006303492054160286, Validation Loss: 0.010569318131144562\n",
      "Epoch [15049/20000], Training Loss: 0.010810136934326562, Validation Loss: 0.0039327157949304555\n",
      "Epoch [15050/20000], Training Loss: 0.008614051145351758, Validation Loss: 0.012645353696190353\n",
      "Epoch [15051/20000], Training Loss: 0.00492846822122114, Validation Loss: 0.00531257970530828\n",
      "Epoch [15052/20000], Training Loss: 0.007376274430130003, Validation Loss: 0.009838125535420012\n",
      "Epoch [15053/20000], Training Loss: 0.011441864542575786, Validation Loss: 0.004301079548957326\n",
      "Epoch [15054/20000], Training Loss: 0.006280079968356793, Validation Loss: 0.0033709196644893416\n",
      "Epoch [15055/20000], Training Loss: 0.005367276261884919, Validation Loss: 0.021115062233978472\n",
      "Epoch [15056/20000], Training Loss: 0.03286640570604504, Validation Loss: 0.007908630654148803\n",
      "Epoch [15057/20000], Training Loss: 0.011821792222950276, Validation Loss: 0.010146882198221923\n",
      "Epoch [15058/20000], Training Loss: 0.03969390070396831, Validation Loss: 0.008293908624834414\n",
      "Epoch [15059/20000], Training Loss: 0.039429927157470956, Validation Loss: 0.03860982720341025\n",
      "Epoch [15060/20000], Training Loss: 0.013532907294575125, Validation Loss: 0.008397640993536858\n",
      "Epoch [15061/20000], Training Loss: 0.00770104379625991, Validation Loss: 0.004933389176429603\n",
      "Epoch [15062/20000], Training Loss: 0.0064778566634881175, Validation Loss: 0.006920004309754242\n",
      "Epoch [15063/20000], Training Loss: 0.006123936953900154, Validation Loss: 0.004846226658911827\n",
      "Epoch [15064/20000], Training Loss: 0.006973944883481765, Validation Loss: 0.003843277974229358\n",
      "Epoch [15065/20000], Training Loss: 0.00965401777648367, Validation Loss: 0.008397148789132282\n",
      "Epoch [15066/20000], Training Loss: 0.007980103097257338, Validation Loss: 0.004054041781403623\n",
      "Epoch [15067/20000], Training Loss: 0.013367679349487713, Validation Loss: 0.015366722823399317\n",
      "Epoch [15068/20000], Training Loss: 0.012070487016379567, Validation Loss: 0.005068228300000207\n",
      "Epoch [15069/20000], Training Loss: 0.004507240111187067, Validation Loss: 0.011976837049315977\n",
      "Epoch [15070/20000], Training Loss: 0.01107455943234267, Validation Loss: 0.00847615790553471\n",
      "Epoch [15071/20000], Training Loss: 0.007539584744336025, Validation Loss: 0.005126758941100375\n",
      "Epoch [15072/20000], Training Loss: 0.004876120910142033, Validation Loss: 0.003167087284652243\n",
      "Epoch [15073/20000], Training Loss: 0.007026848179001328, Validation Loss: 0.007920468292609454\n",
      "Epoch [15074/20000], Training Loss: 0.007545028750947884, Validation Loss: 0.004269790968935665\n",
      "Epoch [15075/20000], Training Loss: 0.018636202362748527, Validation Loss: 0.004223867911100216\n",
      "Epoch [15076/20000], Training Loss: 0.014068143006300358, Validation Loss: 0.004358975745481129\n",
      "Epoch [15077/20000], Training Loss: 0.028946114118152115, Validation Loss: 0.00672666578255451\n",
      "Epoch [15078/20000], Training Loss: 0.015417192706536298, Validation Loss: 0.006400643832116819\n",
      "Epoch [15079/20000], Training Loss: 0.005263631810001763, Validation Loss: 0.0035152430613795615\n",
      "Epoch [15080/20000], Training Loss: 0.004023042867109845, Validation Loss: 0.004168003502038735\n",
      "Epoch [15081/20000], Training Loss: 0.005469703340038125, Validation Loss: 0.0038252789672351246\n",
      "Epoch [15082/20000], Training Loss: 0.005119550126307461, Validation Loss: 0.005797672156715914\n",
      "Epoch [15083/20000], Training Loss: 0.006815903418880355, Validation Loss: 0.0031259619194441107\n",
      "Epoch [15084/20000], Training Loss: 0.004928285423050381, Validation Loss: 0.0045093674001738865\n",
      "Epoch [15085/20000], Training Loss: 0.007285807603953539, Validation Loss: 0.0072439508539044245\n",
      "Epoch [15086/20000], Training Loss: 0.008936261089859596, Validation Loss: 0.014399686828255655\n",
      "Epoch [15087/20000], Training Loss: 0.009494410806967477, Validation Loss: 0.003499144820428909\n",
      "Epoch [15088/20000], Training Loss: 0.00960845492746947, Validation Loss: 0.01931414295280628\n",
      "Epoch [15089/20000], Training Loss: 0.013315410341289993, Validation Loss: 0.005406852545605972\n",
      "Epoch [15090/20000], Training Loss: 0.011109222117478825, Validation Loss: 0.003833080211954384\n",
      "Epoch [15091/20000], Training Loss: 0.013123200903204893, Validation Loss: 0.00849084316621886\n",
      "Epoch [15092/20000], Training Loss: 0.007700439018663019, Validation Loss: 0.02149643003948498\n",
      "Epoch [15093/20000], Training Loss: 0.027617466714283573, Validation Loss: 0.007827733660444609\n",
      "Epoch [15094/20000], Training Loss: 0.03893150144514428, Validation Loss: 0.0232937020602029\n",
      "Epoch [15095/20000], Training Loss: 0.025769916238329773, Validation Loss: 0.007311553295361571\n",
      "Epoch [15096/20000], Training Loss: 0.013832881863761162, Validation Loss: 0.017974789387413433\n",
      "Epoch [15097/20000], Training Loss: 0.011173007456818596, Validation Loss: 0.007225207452708903\n",
      "Epoch [15098/20000], Training Loss: 0.009833366335702262, Validation Loss: 0.004405704307138539\n",
      "Epoch [15099/20000], Training Loss: 0.006686926068921041, Validation Loss: 0.00559053999513383\n",
      "Epoch [15100/20000], Training Loss: 0.005085325799882412, Validation Loss: 0.004539383922871186\n",
      "Epoch [15101/20000], Training Loss: 0.004887397501030526, Validation Loss: 0.004834710883636311\n",
      "Epoch [15102/20000], Training Loss: 0.004608918672991942, Validation Loss: 0.0052022706516194036\n",
      "Epoch [15103/20000], Training Loss: 0.007043526521296631, Validation Loss: 0.006281222608330543\n",
      "Epoch [15104/20000], Training Loss: 0.0066736702013128835, Validation Loss: 0.0032446602272555264\n",
      "Epoch [15105/20000], Training Loss: 0.011616724248077455, Validation Loss: 0.004078566704043797\n",
      "Epoch [15106/20000], Training Loss: 0.00983512495128837, Validation Loss: 0.0073821220017982\n",
      "Epoch [15107/20000], Training Loss: 0.00702803724146049, Validation Loss: 0.004554525717881173\n",
      "Epoch [15108/20000], Training Loss: 0.01239042793167755, Validation Loss: 0.004155733674388416\n",
      "Epoch [15109/20000], Training Loss: 0.016372632901948236, Validation Loss: 0.011213945009246957\n",
      "Epoch [15110/20000], Training Loss: 0.008560205899682063, Validation Loss: 0.006665710996636725\n",
      "Epoch [15111/20000], Training Loss: 0.005872220589351075, Validation Loss: 0.0042351241710696695\n",
      "Epoch [15112/20000], Training Loss: 0.007414411434410795, Validation Loss: 0.004786064216523683\n",
      "Epoch [15113/20000], Training Loss: 0.0053708855411969125, Validation Loss: 0.004861514841445369\n",
      "Epoch [15114/20000], Training Loss: 0.004411099819532994, Validation Loss: 0.0032292337094229047\n",
      "Epoch [15115/20000], Training Loss: 0.0051420538770798885, Validation Loss: 0.003423069039424999\n",
      "Epoch [15116/20000], Training Loss: 0.003686766027927137, Validation Loss: 0.0065018316122019216\n",
      "Epoch [15117/20000], Training Loss: 0.006351629074613031, Validation Loss: 0.003083122463238708\n",
      "Epoch [15118/20000], Training Loss: 0.005102150434270568, Validation Loss: 0.004400476979707429\n",
      "Epoch [15119/20000], Training Loss: 0.006136482931781627, Validation Loss: 0.006555377638789135\n",
      "Epoch [15120/20000], Training Loss: 0.010883453929117033, Validation Loss: 0.004917013068092128\n",
      "Epoch [15121/20000], Training Loss: 0.017335405797244414, Validation Loss: 0.002905993142304299\n",
      "Epoch [15122/20000], Training Loss: 0.022259515745515403, Validation Loss: 0.018152264612061635\n",
      "Epoch [15123/20000], Training Loss: 0.04953404587740806, Validation Loss: 0.010537796794468503\n",
      "Epoch [15124/20000], Training Loss: 0.011980560036525796, Validation Loss: 0.004106909532816593\n",
      "Epoch [15125/20000], Training Loss: 0.011972803111348185, Validation Loss: 0.005890224369106229\n",
      "Epoch [15126/20000], Training Loss: 0.007282553947464164, Validation Loss: 0.004524666140603729\n",
      "Epoch [15127/20000], Training Loss: 0.005372507612004743, Validation Loss: 0.005515969285260118\n",
      "Epoch [15128/20000], Training Loss: 0.00952111801598221, Validation Loss: 0.0042732592795593815\n",
      "Epoch [15129/20000], Training Loss: 0.009658385015979678, Validation Loss: 0.006072783448087024\n",
      "Epoch [15130/20000], Training Loss: 0.007052082733317937, Validation Loss: 0.008830997230623845\n",
      "Epoch [15131/20000], Training Loss: 0.00907018335835476, Validation Loss: 0.011250158373526288\n",
      "Epoch [15132/20000], Training Loss: 0.009602918812431329, Validation Loss: 0.003766426668012944\n",
      "Epoch [15133/20000], Training Loss: 0.004292684208069812, Validation Loss: 0.0033870285543811107\n",
      "Epoch [15134/20000], Training Loss: 0.01095906225964427, Validation Loss: 0.05419709533452988\n",
      "Epoch [15135/20000], Training Loss: 0.018602643224377453, Validation Loss: 0.025349194450124957\n",
      "Epoch [15136/20000], Training Loss: 0.027545909334938706, Validation Loss: 0.04044124254141934\n",
      "Epoch [15137/20000], Training Loss: 0.012296802451081541, Validation Loss: 0.004454580407635181\n",
      "Epoch [15138/20000], Training Loss: 0.005969074319831894, Validation Loss: 0.010869125469748334\n",
      "Epoch [15139/20000], Training Loss: 0.00677020132466818, Validation Loss: 0.006244384538198184\n",
      "Epoch [15140/20000], Training Loss: 0.009081666894157283, Validation Loss: 0.005706695201804642\n",
      "Epoch [15141/20000], Training Loss: 0.010123140652597482, Validation Loss: 0.025415515261091936\n",
      "Epoch [15142/20000], Training Loss: 0.019355091278807128, Validation Loss: 0.005409031992777662\n",
      "Epoch [15143/20000], Training Loss: 0.011527453366267894, Validation Loss: 0.0047573914352302404\n",
      "Epoch [15144/20000], Training Loss: 0.0075766414561907625, Validation Loss: 0.0036432910348074593\n",
      "Epoch [15145/20000], Training Loss: 0.004755538966669296, Validation Loss: 0.003294617656517967\n",
      "Epoch [15146/20000], Training Loss: 0.004610865460043507, Validation Loss: 0.005222976016241261\n",
      "Epoch [15147/20000], Training Loss: 0.008814769948782799, Validation Loss: 0.0076149851144848655\n",
      "Epoch [15148/20000], Training Loss: 0.01247943549852997, Validation Loss: 0.011467049540566368\n",
      "Epoch [15149/20000], Training Loss: 0.01757570103537936, Validation Loss: 0.005684470380114164\n",
      "Epoch [15150/20000], Training Loss: 0.03202522960730546, Validation Loss: 0.031599989959468985\n",
      "Epoch [15151/20000], Training Loss: 0.016934163621044718, Validation Loss: 0.009731157355941227\n",
      "Epoch [15152/20000], Training Loss: 0.018821489943482966, Validation Loss: 0.010877029637673256\n",
      "Epoch [15153/20000], Training Loss: 0.009163188407131071, Validation Loss: 0.0048009457362029806\n",
      "Epoch [15154/20000], Training Loss: 0.008515156418005583, Validation Loss: 0.007274832676386994\n",
      "Epoch [15155/20000], Training Loss: 0.021302996078572636, Validation Loss: 0.025857811527592922\n",
      "Epoch [15156/20000], Training Loss: 0.01341046952916436, Validation Loss: 0.010134126325851062\n",
      "Epoch [15157/20000], Training Loss: 0.010014291638526629, Validation Loss: 0.007006453015557261\n",
      "Epoch [15158/20000], Training Loss: 0.0131283513944384, Validation Loss: 0.0052121588206708635\n",
      "Epoch [15159/20000], Training Loss: 0.012957569128047908, Validation Loss: 0.022714930719562938\n",
      "Epoch [15160/20000], Training Loss: 0.01353156435236867, Validation Loss: 0.005857573236129772\n",
      "Epoch [15161/20000], Training Loss: 0.00820379875118046, Validation Loss: 0.005335964159184771\n",
      "Epoch [15162/20000], Training Loss: 0.0064431902698873144, Validation Loss: 0.004509146966483968\n",
      "Epoch [15163/20000], Training Loss: 0.008134593823342584, Validation Loss: 0.0038753174561785724\n",
      "Epoch [15164/20000], Training Loss: 0.016346081388714344, Validation Loss: 0.009906853955837456\n",
      "Epoch [15165/20000], Training Loss: 0.00754668206770605, Validation Loss: 0.003943799401485619\n",
      "Epoch [15166/20000], Training Loss: 0.004923248113820071, Validation Loss: 0.003981467034657206\n",
      "Epoch [15167/20000], Training Loss: 0.006255982551530386, Validation Loss: 0.0030718643355155235\n",
      "Epoch [15168/20000], Training Loss: 0.003882058483473624, Validation Loss: 0.002853979932779939\n",
      "Epoch [15169/20000], Training Loss: 0.003970975932523808, Validation Loss: 0.005841592792488977\n",
      "Epoch [15170/20000], Training Loss: 0.011238117802089878, Validation Loss: 0.00661423117195561\n",
      "Epoch [15171/20000], Training Loss: 0.012489993517127524, Validation Loss: 0.01222425346541947\n",
      "Epoch [15172/20000], Training Loss: 0.01381416213501195, Validation Loss: 0.007193404978494681\n",
      "Epoch [15173/20000], Training Loss: 0.011308396939836842, Validation Loss: 0.04307650136096137\n",
      "Epoch [15174/20000], Training Loss: 0.022209385928623045, Validation Loss: 0.011037260027900809\n",
      "Epoch [15175/20000], Training Loss: 0.013925164035754278, Validation Loss: 0.014777653717568942\n",
      "Epoch [15176/20000], Training Loss: 0.012139881472519067, Validation Loss: 0.007661853152625587\n",
      "Epoch [15177/20000], Training Loss: 0.00674505658077708, Validation Loss: 0.0073219061433693776\n",
      "Epoch [15178/20000], Training Loss: 0.006282246819734739, Validation Loss: 0.003638252313794007\n",
      "Epoch [15179/20000], Training Loss: 0.010760893038407802, Validation Loss: 0.005188485920978965\n",
      "Epoch [15180/20000], Training Loss: 0.015657769200125977, Validation Loss: 0.006057457119860935\n",
      "Epoch [15181/20000], Training Loss: 0.011067792864715946, Validation Loss: 0.006228922497177761\n",
      "Epoch [15182/20000], Training Loss: 0.004389955005275884, Validation Loss: 0.005289273835493799\n",
      "Epoch [15183/20000], Training Loss: 0.006384187477773854, Validation Loss: 0.004092573555217202\n",
      "Epoch [15184/20000], Training Loss: 0.005486881656647061, Validation Loss: 0.0033587845012709094\n",
      "Epoch [15185/20000], Training Loss: 0.004646107857557321, Validation Loss: 0.005898292542317831\n",
      "Epoch [15186/20000], Training Loss: 0.004421870024608714, Validation Loss: 0.004455662105457289\n",
      "Epoch [15187/20000], Training Loss: 0.004813090280679587, Validation Loss: 0.008802128024399997\n",
      "Epoch [15188/20000], Training Loss: 0.009966456970297648, Validation Loss: 0.004741715599982761\n",
      "Epoch [15189/20000], Training Loss: 0.0054927647215663455, Validation Loss: 0.0023939413751011473\n",
      "Epoch [15190/20000], Training Loss: 0.0032907081029926693, Validation Loss: 0.0022907545694189407\n",
      "Epoch [15191/20000], Training Loss: 0.003867202679300265, Validation Loss: 0.002375332273472005\n",
      "Epoch [15192/20000], Training Loss: 0.006236220771287192, Validation Loss: 0.004996913368814997\n",
      "Epoch [15193/20000], Training Loss: 0.006048630291583582, Validation Loss: 0.010252726545337916\n",
      "Epoch [15194/20000], Training Loss: 0.018234467935534276, Validation Loss: 0.030697387308858643\n",
      "Epoch [15195/20000], Training Loss: 0.024837721827809998, Validation Loss: 0.008327924330599612\n",
      "Epoch [15196/20000], Training Loss: 0.02109984874966488, Validation Loss: 0.010823399597084145\n",
      "Epoch [15197/20000], Training Loss: 0.009592785599254836, Validation Loss: 0.007867912262944244\n",
      "Epoch [15198/20000], Training Loss: 0.007536759483627975, Validation Loss: 0.01167649215408671\n",
      "Epoch [15199/20000], Training Loss: 0.01071875025913219, Validation Loss: 0.00318744912755257\n",
      "Epoch [15200/20000], Training Loss: 0.007134411303013621, Validation Loss: 0.009659940909062113\n",
      "Epoch [15201/20000], Training Loss: 0.006364875164373578, Validation Loss: 0.006609140629239917\n",
      "Epoch [15202/20000], Training Loss: 0.007379071211809267, Validation Loss: 0.0033411812674448795\n",
      "Epoch [15203/20000], Training Loss: 0.003743669491086621, Validation Loss: 0.0034270140877069523\n",
      "Epoch [15204/20000], Training Loss: 0.004145199511965204, Validation Loss: 0.003325643708425438\n",
      "Epoch [15205/20000], Training Loss: 0.008210807997134648, Validation Loss: 0.002535904678630332\n",
      "Epoch [15206/20000], Training Loss: 0.003531668524787384, Validation Loss: 0.013563864199178561\n",
      "Epoch [15207/20000], Training Loss: 0.009485744422168605, Validation Loss: 0.007766154993858183\n",
      "Epoch [15208/20000], Training Loss: 0.009534136999198901, Validation Loss: 0.003166570570463999\n",
      "Epoch [15209/20000], Training Loss: 0.005465000943071833, Validation Loss: 0.004023515220202378\n",
      "Epoch [15210/20000], Training Loss: 0.007338962226185686, Validation Loss: 0.02391646293816458\n",
      "Epoch [15211/20000], Training Loss: 0.014656619867309928, Validation Loss: 0.01771590509123249\n",
      "Epoch [15212/20000], Training Loss: 0.012763779690950676, Validation Loss: 0.005448930847029158\n",
      "Epoch [15213/20000], Training Loss: 0.010416696122514882, Validation Loss: 0.006005564434209695\n",
      "Epoch [15214/20000], Training Loss: 0.005813457413069045, Validation Loss: 0.00749077581401394\n",
      "Epoch [15215/20000], Training Loss: 0.007829116807053131, Validation Loss: 0.01615565562886851\n",
      "Epoch [15216/20000], Training Loss: 0.010065764009985807, Validation Loss: 0.0118780209948974\n",
      "Epoch [15217/20000], Training Loss: 0.01872417017381167, Validation Loss: 0.029449961547340666\n",
      "Epoch [15218/20000], Training Loss: 0.010001413077199166, Validation Loss: 0.017642676697245667\n",
      "Epoch [15219/20000], Training Loss: 0.017323583543543464, Validation Loss: 0.00895005202452662\n",
      "Epoch [15220/20000], Training Loss: 0.016086393655444096, Validation Loss: 0.0031136731052217354\n",
      "Epoch [15221/20000], Training Loss: 0.018842933501218795, Validation Loss: 0.002502231362413112\n",
      "Epoch [15222/20000], Training Loss: 0.01796096417404312, Validation Loss: 0.005847285496344057\n",
      "Epoch [15223/20000], Training Loss: 0.012514346582715266, Validation Loss: 0.00753753151268109\n",
      "Epoch [15224/20000], Training Loss: 0.005021543343510919, Validation Loss: 0.0036426179026358696\n",
      "Epoch [15225/20000], Training Loss: 0.006978596826749188, Validation Loss: 0.003181275849736999\n",
      "Epoch [15226/20000], Training Loss: 0.008152190816222824, Validation Loss: 0.004147591202481944\n",
      "Epoch [15227/20000], Training Loss: 0.003991489221724416, Validation Loss: 0.004931440543198765\n",
      "Epoch [15228/20000], Training Loss: 0.009451386410903848, Validation Loss: 0.005568328939958089\n",
      "Epoch [15229/20000], Training Loss: 0.00841639327804192, Validation Loss: 0.0035926378566287936\n",
      "Epoch [15230/20000], Training Loss: 0.0043314549674927805, Validation Loss: 0.0037700800627445278\n",
      "Epoch [15231/20000], Training Loss: 0.0069597153611929375, Validation Loss: 0.0026477040548986927\n",
      "Epoch [15232/20000], Training Loss: 0.004906091191514861, Validation Loss: 0.0025667463975295457\n",
      "Epoch [15233/20000], Training Loss: 0.005858718382365703, Validation Loss: 0.002236438012041412\n",
      "Epoch [15234/20000], Training Loss: 0.004447111646836025, Validation Loss: 0.002748619942518573\n",
      "Epoch [15235/20000], Training Loss: 0.005871917822332762, Validation Loss: 0.011476908544344562\n",
      "Epoch [15236/20000], Training Loss: 0.008121836947144143, Validation Loss: 0.011727303682684058\n",
      "Epoch [15237/20000], Training Loss: 0.01725074226851575, Validation Loss: 0.006420524209508339\n",
      "Epoch [15238/20000], Training Loss: 0.022520615409770732, Validation Loss: 0.029292783002802025\n",
      "Epoch [15239/20000], Training Loss: 0.014554313425313532, Validation Loss: 0.0032193865247172365\n",
      "Epoch [15240/20000], Training Loss: 0.009655676595230034, Validation Loss: 0.01773122964160784\n",
      "Epoch [15241/20000], Training Loss: 0.008081924323570482, Validation Loss: 0.005027875270979461\n",
      "Epoch [15242/20000], Training Loss: 0.01609777314128483, Validation Loss: 0.042374777548129065\n",
      "Epoch [15243/20000], Training Loss: 0.027264278156435142, Validation Loss: 0.017762794645461457\n",
      "Epoch [15244/20000], Training Loss: 0.036283976377619966, Validation Loss: 0.021578285761929043\n",
      "Epoch [15245/20000], Training Loss: 0.016325763331094225, Validation Loss: 0.009691156995752456\n",
      "Epoch [15246/20000], Training Loss: 0.007700354214258758, Validation Loss: 0.005235905695192082\n",
      "Epoch [15247/20000], Training Loss: 0.01072789333115257, Validation Loss: 0.008474712773351751\n",
      "Epoch [15248/20000], Training Loss: 0.009125456746135439, Validation Loss: 0.004682347618473625\n",
      "Epoch [15249/20000], Training Loss: 0.005493026337370159, Validation Loss: 0.008360386409293596\n",
      "Epoch [15250/20000], Training Loss: 0.0047118207824366565, Validation Loss: 0.01747205321277891\n",
      "Epoch [15251/20000], Training Loss: 0.009818640689315674, Validation Loss: 0.005245659669731746\n",
      "Epoch [15252/20000], Training Loss: 0.006573602187147897, Validation Loss: 0.005321167609203836\n",
      "Epoch [15253/20000], Training Loss: 0.00665151964598668, Validation Loss: 0.005405803308425175\n",
      "Epoch [15254/20000], Training Loss: 0.007503436726567868, Validation Loss: 0.003809771778484818\n",
      "Epoch [15255/20000], Training Loss: 0.004636853993978158, Validation Loss: 0.0032508399551278424\n",
      "Epoch [15256/20000], Training Loss: 0.003796292130061514, Validation Loss: 0.0061641139244449\n",
      "Epoch [15257/20000], Training Loss: 0.00949562630843762, Validation Loss: 0.009218928537198483\n",
      "Epoch [15258/20000], Training Loss: 0.00927934465081697, Validation Loss: 0.006162848024445873\n",
      "Epoch [15259/20000], Training Loss: 0.015135464094100826, Validation Loss: 0.008649366488067958\n",
      "Epoch [15260/20000], Training Loss: 0.027529733877080225, Validation Loss: 0.01657574058377317\n",
      "Epoch [15261/20000], Training Loss: 0.01479250542511831, Validation Loss: 0.02735536651951926\n",
      "Epoch [15262/20000], Training Loss: 0.01751435024250116, Validation Loss: 0.014758741083954061\n",
      "Epoch [15263/20000], Training Loss: 0.0267075306607824, Validation Loss: 0.004962976863952033\n",
      "Epoch [15264/20000], Training Loss: 0.01944387181460375, Validation Loss: 0.011129241736086067\n",
      "Epoch [15265/20000], Training Loss: 0.012382720914120324, Validation Loss: 0.013104627963938704\n",
      "Epoch [15266/20000], Training Loss: 0.011241593464676822, Validation Loss: 0.004624959759383233\n",
      "Epoch [15267/20000], Training Loss: 0.005715710416552611, Validation Loss: 0.004086890973702246\n",
      "Epoch [15268/20000], Training Loss: 0.006277314859939257, Validation Loss: 0.011033762911602064\n",
      "Epoch [15269/20000], Training Loss: 0.011830533291296368, Validation Loss: 0.021469341857092723\n",
      "Epoch [15270/20000], Training Loss: 0.01919926260598004, Validation Loss: 0.0036035509560116896\n",
      "Epoch [15271/20000], Training Loss: 0.041130524523656016, Validation Loss: 0.016139934701279963\n",
      "Epoch [15272/20000], Training Loss: 0.05637706163439101, Validation Loss: 0.012636208747033655\n",
      "Epoch [15273/20000], Training Loss: 0.05066350719425827, Validation Loss: 0.04392560336522625\n",
      "Epoch [15274/20000], Training Loss: 0.054456993945807754, Validation Loss: 0.03686734821116266\n",
      "Epoch [15275/20000], Training Loss: 0.02176908646444125, Validation Loss: 0.009441502775092436\n",
      "Epoch [15276/20000], Training Loss: 0.018400892578467522, Validation Loss: 0.016102213022532754\n",
      "Epoch [15277/20000], Training Loss: 0.012588895607872732, Validation Loss: 0.013902410864895914\n",
      "Epoch [15278/20000], Training Loss: 0.009252517355240084, Validation Loss: 0.005915670748707222\n",
      "Epoch [15279/20000], Training Loss: 0.0071197388294552055, Validation Loss: 0.00461615199658241\n",
      "Epoch [15280/20000], Training Loss: 0.006356108650964286, Validation Loss: 0.004144866518412642\n",
      "Epoch [15281/20000], Training Loss: 0.014145703301193992, Validation Loss: 0.006158619121249233\n",
      "Epoch [15282/20000], Training Loss: 0.011527672413182342, Validation Loss: 0.009842153771647386\n",
      "Epoch [15283/20000], Training Loss: 0.011375457654947725, Validation Loss: 0.0034550068085089994\n",
      "Epoch [15284/20000], Training Loss: 0.014310715844788189, Validation Loss: 0.007834767623430586\n",
      "Epoch [15285/20000], Training Loss: 0.011189989188486444, Validation Loss: 0.010223838903444681\n",
      "Epoch [15286/20000], Training Loss: 0.007302563560577775, Validation Loss: 0.008497183486075041\n",
      "Epoch [15287/20000], Training Loss: 0.015150098645660495, Validation Loss: 0.008598238071012787\n",
      "Epoch [15288/20000], Training Loss: 0.011777599720517173, Validation Loss: 0.009332744295560191\n",
      "Epoch [15289/20000], Training Loss: 0.008130420510237204, Validation Loss: 0.004909222213709897\n",
      "Epoch [15290/20000], Training Loss: 0.007531588282290613, Validation Loss: 0.0050095546539052505\n",
      "Epoch [15291/20000], Training Loss: 0.007994495357187199, Validation Loss: 0.00414860400300654\n",
      "Epoch [15292/20000], Training Loss: 0.004999991306768996, Validation Loss: 0.004169030062276432\n",
      "Epoch [15293/20000], Training Loss: 0.00520891636343939, Validation Loss: 0.006037673451368868\n",
      "Epoch [15294/20000], Training Loss: 0.00708308835497259, Validation Loss: 0.004896746685707379\n",
      "Epoch [15295/20000], Training Loss: 0.005080903895272318, Validation Loss: 0.007416544024001682\n",
      "Epoch [15296/20000], Training Loss: 0.009046580496942624, Validation Loss: 0.0031633930931167747\n",
      "Epoch [15297/20000], Training Loss: 0.008542570095431386, Validation Loss: 0.0063083167937535425\n",
      "Epoch [15298/20000], Training Loss: 0.00682812888162841, Validation Loss: 0.007083601851310057\n",
      "Epoch [15299/20000], Training Loss: 0.005453961710694598, Validation Loss: 0.011133582241557503\n",
      "Epoch [15300/20000], Training Loss: 0.009425534668220539, Validation Loss: 0.004149246196797066\n",
      "Epoch [15301/20000], Training Loss: 0.006776448714928408, Validation Loss: 0.015558095648884782\n",
      "Epoch [15302/20000], Training Loss: 0.01000978599352363, Validation Loss: 0.01051380727251751\n",
      "Epoch [15303/20000], Training Loss: 0.017811615283010594, Validation Loss: 0.003928999092499106\n",
      "Epoch [15304/20000], Training Loss: 0.05811202397204137, Validation Loss: 0.008341056427927112\n",
      "Epoch [15305/20000], Training Loss: 0.02682625888181584, Validation Loss: 0.016769698155777796\n",
      "Epoch [15306/20000], Training Loss: 0.01978739790788759, Validation Loss: 0.011519773630425334\n",
      "Epoch [15307/20000], Training Loss: 0.016673290469563135, Validation Loss: 0.00951898107581352\n",
      "Epoch [15308/20000], Training Loss: 0.010110221370788557, Validation Loss: 0.005161618205625944\n",
      "Epoch [15309/20000], Training Loss: 0.007514717075017481, Validation Loss: 0.005794964999435158\n",
      "Epoch [15310/20000], Training Loss: 0.0068834225198120946, Validation Loss: 0.003965142586642696\n",
      "Epoch [15311/20000], Training Loss: 0.00824779189133551, Validation Loss: 0.006174630297207497\n",
      "Epoch [15312/20000], Training Loss: 0.007570420907411192, Validation Loss: 0.014373378029891448\n",
      "Epoch [15313/20000], Training Loss: 0.0073318148123299965, Validation Loss: 0.004415818774730702\n",
      "Epoch [15314/20000], Training Loss: 0.006390209170474138, Validation Loss: 0.004522038049685819\n",
      "Epoch [15315/20000], Training Loss: 0.005153540073869018, Validation Loss: 0.0039010027675663965\n",
      "Epoch [15316/20000], Training Loss: 0.004671720750000219, Validation Loss: 0.006460131128408264\n",
      "Epoch [15317/20000], Training Loss: 0.012658499005348338, Validation Loss: 0.004254025008390265\n",
      "Epoch [15318/20000], Training Loss: 0.018014026398044995, Validation Loss: 0.0433355165379388\n",
      "Epoch [15319/20000], Training Loss: 0.017018744158641703, Validation Loss: 0.0056948694087356816\n",
      "Epoch [15320/20000], Training Loss: 0.007775834213362032, Validation Loss: 0.005382496658586645\n",
      "Epoch [15321/20000], Training Loss: 0.005816597181613188, Validation Loss: 0.0039234296424415244\n",
      "Epoch [15322/20000], Training Loss: 0.005695062344143155, Validation Loss: 0.007524111487731895\n",
      "Epoch [15323/20000], Training Loss: 0.006181310020110686, Validation Loss: 0.003475766355046473\n",
      "Epoch [15324/20000], Training Loss: 0.003767917932626525, Validation Loss: 0.0027261163838454574\n",
      "Epoch [15325/20000], Training Loss: 0.0045070425840094686, Validation Loss: 0.0026724728295588474\n",
      "Epoch [15326/20000], Training Loss: 0.004668934973649032, Validation Loss: 0.00658820522949805\n",
      "Epoch [15327/20000], Training Loss: 0.009748350770681263, Validation Loss: 0.005873980067150419\n",
      "Epoch [15328/20000], Training Loss: 0.018731881970390014, Validation Loss: 0.015165685276900117\n",
      "Epoch [15329/20000], Training Loss: 0.02299688901302943, Validation Loss: 0.0059256230015812105\n",
      "Epoch [15330/20000], Training Loss: 0.01686333926461105, Validation Loss: 0.004756398516681605\n",
      "Epoch [15331/20000], Training Loss: 0.016706985172017345, Validation Loss: 0.005755665649656144\n",
      "Epoch [15332/20000], Training Loss: 0.016041708533586125, Validation Loss: 0.006297071393094354\n",
      "Epoch [15333/20000], Training Loss: 0.008928370205727074, Validation Loss: 0.01997756665306432\n",
      "Epoch [15334/20000], Training Loss: 0.009495366896901811, Validation Loss: 0.004756716628777207\n",
      "Epoch [15335/20000], Training Loss: 0.00618216855504932, Validation Loss: 0.004628357168120901\n",
      "Epoch [15336/20000], Training Loss: 0.004749014915432781, Validation Loss: 0.003955892071853885\n",
      "Epoch [15337/20000], Training Loss: 0.008627435138415811, Validation Loss: 0.0033692477849359276\n",
      "Epoch [15338/20000], Training Loss: 0.020780846122054948, Validation Loss: 0.03857553643839719\n",
      "Epoch [15339/20000], Training Loss: 0.009927810595010177, Validation Loss: 0.004299321181419344\n",
      "Epoch [15340/20000], Training Loss: 0.011217356325687433, Validation Loss: 0.004825914095955875\n",
      "Epoch [15341/20000], Training Loss: 0.008053604820036395, Validation Loss: 0.004244368845194096\n",
      "Epoch [15342/20000], Training Loss: 0.007530949017174342, Validation Loss: 0.004732476920915956\n",
      "Epoch [15343/20000], Training Loss: 0.004176384494972548, Validation Loss: 0.009791189239228035\n",
      "Epoch [15344/20000], Training Loss: 0.007713909729708186, Validation Loss: 0.00486402397381929\n",
      "Epoch [15345/20000], Training Loss: 0.008947137352801551, Validation Loss: 0.0030413113698338884\n",
      "Epoch [15346/20000], Training Loss: 0.006072100739471255, Validation Loss: 0.017954015039971898\n",
      "Epoch [15347/20000], Training Loss: 0.004772200587727379, Validation Loss: 0.0077912876370852535\n",
      "Epoch [15348/20000], Training Loss: 0.0044710378424528486, Validation Loss: 0.01685114045228277\n",
      "Epoch [15349/20000], Training Loss: 0.01105478983975315, Validation Loss: 0.005566075511373139\n",
      "Epoch [15350/20000], Training Loss: 0.012646488674883065, Validation Loss: 0.004954712432157956\n",
      "Epoch [15351/20000], Training Loss: 0.007511678792070597, Validation Loss: 0.011258910676198356\n",
      "Epoch [15352/20000], Training Loss: 0.012625256691967885, Validation Loss: 0.010879262217457207\n",
      "Epoch [15353/20000], Training Loss: 0.00863946826679499, Validation Loss: 0.006042183157836056\n",
      "Epoch [15354/20000], Training Loss: 0.003159669937433916, Validation Loss: 0.007309237827681559\n",
      "Epoch [15355/20000], Training Loss: 0.005664041583908589, Validation Loss: 0.0030798149248412337\n",
      "Epoch [15356/20000], Training Loss: 0.004425941910044783, Validation Loss: 0.003200534905795908\n",
      "Epoch [15357/20000], Training Loss: 0.005236867453017372, Validation Loss: 0.008992242266641273\n",
      "Epoch [15358/20000], Training Loss: 0.01899298097474197, Validation Loss: 0.00700314365667566\n",
      "Epoch [15359/20000], Training Loss: 0.013849014895510794, Validation Loss: 0.003919930497056037\n",
      "Epoch [15360/20000], Training Loss: 0.007260750417896945, Validation Loss: 0.004192362356544122\n",
      "Epoch [15361/20000], Training Loss: 0.004746141201134638, Validation Loss: 0.00799815903883432\n",
      "Epoch [15362/20000], Training Loss: 0.005678533833426107, Validation Loss: 0.0026978734948476663\n",
      "Epoch [15363/20000], Training Loss: 0.006109814717742016, Validation Loss: 0.002795945146240962\n",
      "Epoch [15364/20000], Training Loss: 0.004332090617806118, Validation Loss: 0.005453021095073251\n",
      "Epoch [15365/20000], Training Loss: 0.011208205247904906, Validation Loss: 0.010003668455673116\n",
      "Epoch [15366/20000], Training Loss: 0.004326329849295802, Validation Loss: 0.004293103992710159\n",
      "Epoch [15367/20000], Training Loss: 0.004816581931663677, Validation Loss: 0.0050708876390563725\n",
      "Epoch [15368/20000], Training Loss: 0.0048642748900290045, Validation Loss: 0.002386379123039017\n",
      "Epoch [15369/20000], Training Loss: 0.004244735528897893, Validation Loss: 0.0018670832923163097\n",
      "Epoch [15370/20000], Training Loss: 0.01965614451910369, Validation Loss: 0.01852364026542221\n",
      "Epoch [15371/20000], Training Loss: 0.014626046140652502, Validation Loss: 0.016801601541893824\n",
      "Epoch [15372/20000], Training Loss: 0.01426136851244207, Validation Loss: 0.010930623859167538\n",
      "Epoch [15373/20000], Training Loss: 0.011302569624344219, Validation Loss: 0.009078433103287003\n",
      "Epoch [15374/20000], Training Loss: 0.02213972198140774, Validation Loss: 0.050145311014995024\n",
      "Epoch [15375/20000], Training Loss: 0.048679298319452106, Validation Loss: 0.03352030526314463\n",
      "Epoch [15376/20000], Training Loss: 0.0374580221884701, Validation Loss: 0.011422294125840107\n",
      "Epoch [15377/20000], Training Loss: 0.13129133840058266, Validation Loss: 0.03704217262457299\n",
      "Epoch [15378/20000], Training Loss: 0.053874548458095105, Validation Loss: 0.015194439142952333\n",
      "Epoch [15379/20000], Training Loss: 0.020612971590708185, Validation Loss: 0.005885652000380506\n",
      "Epoch [15380/20000], Training Loss: 0.017541120340216106, Validation Loss: 0.006512819402684889\n",
      "Epoch [15381/20000], Training Loss: 0.05531617480820777, Validation Loss: 0.04841238002826483\n",
      "Epoch [15382/20000], Training Loss: 0.022935719162757908, Validation Loss: 0.008049553922780237\n",
      "Epoch [15383/20000], Training Loss: 0.009008127859228157, Validation Loss: 0.03051418331294728\n",
      "Epoch [15384/20000], Training Loss: 0.023797749584186283, Validation Loss: 0.015185593095211693\n",
      "Epoch [15385/20000], Training Loss: 0.010420558104150197, Validation Loss: 0.011490017207605849\n",
      "Epoch [15386/20000], Training Loss: 0.005280363774093466, Validation Loss: 0.0026325733467404333\n",
      "Epoch [15387/20000], Training Loss: 0.008918480300703127, Validation Loss: 0.009590275751959\n",
      "Epoch [15388/20000], Training Loss: 0.044365671580505604, Validation Loss: 0.0061318563031317825\n",
      "Epoch [15389/20000], Training Loss: 0.0848678342632151, Validation Loss: 0.07067796657495265\n",
      "Epoch [15390/20000], Training Loss: 0.04608574828931263, Validation Loss: 0.02469761267483201\n",
      "Epoch [15391/20000], Training Loss: 0.022155646567365954, Validation Loss: 0.018992975401330017\n",
      "Epoch [15392/20000], Training Loss: 0.0173978412224512, Validation Loss: 0.01285462548113671\n",
      "Epoch [15393/20000], Training Loss: 0.0139173001682918, Validation Loss: 0.014565390227029898\n",
      "Epoch [15394/20000], Training Loss: 0.014006076746487192, Validation Loss: 0.011108022116656815\n",
      "Epoch [15395/20000], Training Loss: 0.009914475854852103, Validation Loss: 0.008744504277793956\n",
      "Epoch [15396/20000], Training Loss: 0.007456295861629769, Validation Loss: 0.00657338672941218\n",
      "Epoch [15397/20000], Training Loss: 0.007353455360446658, Validation Loss: 0.0063920973833384255\n",
      "Epoch [15398/20000], Training Loss: 0.007841596429768418, Validation Loss: 0.006847770477179438\n",
      "Epoch [15399/20000], Training Loss: 0.007150738333751049, Validation Loss: 0.00641506053736001\n",
      "Epoch [15400/20000], Training Loss: 0.00789928185570586, Validation Loss: 0.005895451085052628\n",
      "Epoch [15401/20000], Training Loss: 0.005881759087580056, Validation Loss: 0.00471104632431109\n",
      "Epoch [15402/20000], Training Loss: 0.0050191340567055575, Validation Loss: 0.00467695277347957\n",
      "Epoch [15403/20000], Training Loss: 0.005400818839137044, Validation Loss: 0.004748970538685171\n",
      "Epoch [15404/20000], Training Loss: 0.004663229396101087, Validation Loss: 0.0040031476605041205\n",
      "Epoch [15405/20000], Training Loss: 0.004762749731155631, Validation Loss: 0.0038209103670072053\n",
      "Epoch [15406/20000], Training Loss: 0.004581533122940787, Validation Loss: 0.011350787565530953\n",
      "Epoch [15407/20000], Training Loss: 0.0077914165366174916, Validation Loss: 0.0034204591046968096\n",
      "Epoch [15408/20000], Training Loss: 0.006701673674147709, Validation Loss: 0.003165897586878178\n",
      "Epoch [15409/20000], Training Loss: 0.006378075249033698, Validation Loss: 0.010240289766873698\n",
      "Epoch [15410/20000], Training Loss: 0.03723300875130268, Validation Loss: 0.015837702127490565\n",
      "Epoch [15411/20000], Training Loss: 0.02138319667038299, Validation Loss: 0.006861982710509957\n",
      "Epoch [15412/20000], Training Loss: 0.007824973174137995, Validation Loss: 0.004583602741636891\n",
      "Epoch [15413/20000], Training Loss: 0.005501073375451629, Validation Loss: 0.005319929454969952\n",
      "Epoch [15414/20000], Training Loss: 0.005755826614664069, Validation Loss: 0.006415729715500211\n",
      "Epoch [15415/20000], Training Loss: 0.008025150205607392, Validation Loss: 0.003629333813112875\n",
      "Epoch [15416/20000], Training Loss: 0.021823047002336744, Validation Loss: 0.030901368061872096\n",
      "Epoch [15417/20000], Training Loss: 0.010325915665557009, Validation Loss: 0.004267237187844687\n",
      "Epoch [15418/20000], Training Loss: 0.00938534060904723, Validation Loss: 0.0037261576262080454\n",
      "Epoch [15419/20000], Training Loss: 0.005670277369582826, Validation Loss: 0.0060954542599398155\n",
      "Epoch [15420/20000], Training Loss: 0.004158863283269706, Validation Loss: 0.0036458750857134093\n",
      "Epoch [15421/20000], Training Loss: 0.005283285932299415, Validation Loss: 0.004029477428054358\n",
      "Epoch [15422/20000], Training Loss: 0.005044279590167987, Validation Loss: 0.0030425420001602027\n",
      "Epoch [15423/20000], Training Loss: 0.0052919981891526345, Validation Loss: 0.0031846561761871123\n",
      "Epoch [15424/20000], Training Loss: 0.006171586600560529, Validation Loss: 0.005865494221084384\n",
      "Epoch [15425/20000], Training Loss: 0.0066556448566968485, Validation Loss: 0.003981464260020725\n",
      "Epoch [15426/20000], Training Loss: 0.005811203519572571, Validation Loss: 0.00574782637814053\n",
      "Epoch [15427/20000], Training Loss: 0.012202631320438482, Validation Loss: 0.008668140091948575\n",
      "Epoch [15428/20000], Training Loss: 0.005779953944251507, Validation Loss: 0.010125569819279574\n",
      "Epoch [15429/20000], Training Loss: 0.02073215482076713, Validation Loss: 0.0035263105835799786\n",
      "Epoch [15430/20000], Training Loss: 0.032863841874391904, Validation Loss: 0.0053897309929392834\n",
      "Epoch [15431/20000], Training Loss: 0.027313692410643853, Validation Loss: 0.004925783315562577\n",
      "Epoch [15432/20000], Training Loss: 0.0098470352282415, Validation Loss: 0.00944162741862036\n",
      "Epoch [15433/20000], Training Loss: 0.005490573453114068, Validation Loss: 0.004325652482512916\n",
      "Epoch [15434/20000], Training Loss: 0.004609665272125442, Validation Loss: 0.008399228290597551\n",
      "Epoch [15435/20000], Training Loss: 0.00912231050374755, Validation Loss: 0.00914528726663022\n",
      "Epoch [15436/20000], Training Loss: 0.008024915030546254, Validation Loss: 0.003431130542294016\n",
      "Epoch [15437/20000], Training Loss: 0.007102755166540321, Validation Loss: 0.005371524111162833\n",
      "Epoch [15438/20000], Training Loss: 0.007113411888505132, Validation Loss: 0.002795146120892663\n",
      "Epoch [15439/20000], Training Loss: 0.004041586258348876, Validation Loss: 0.007207402136830814\n",
      "Epoch [15440/20000], Training Loss: 0.004646831700353816, Validation Loss: 0.005174693703726315\n",
      "Epoch [15441/20000], Training Loss: 0.005199158651521429, Validation Loss: 0.005676161317425813\n",
      "Epoch [15442/20000], Training Loss: 0.0067555873517579, Validation Loss: 0.0051350026386582925\n",
      "Epoch [15443/20000], Training Loss: 0.0080337458239228, Validation Loss: 0.018682687238442623\n",
      "Epoch [15444/20000], Training Loss: 0.012253857395210486, Validation Loss: 0.004709089921265682\n",
      "Epoch [15445/20000], Training Loss: 0.004776985434221258, Validation Loss: 0.008372881083831527\n",
      "Epoch [15446/20000], Training Loss: 0.004460423561145684, Validation Loss: 0.00835187843560232\n",
      "Epoch [15447/20000], Training Loss: 0.007494116280976674, Validation Loss: 0.0027199650447496788\n",
      "Epoch [15448/20000], Training Loss: 0.006614557808538686, Validation Loss: 0.04336530298624958\n",
      "Epoch [15449/20000], Training Loss: 0.0305057193675436, Validation Loss: 0.00968414695069291\n",
      "Epoch [15450/20000], Training Loss: 0.046084410817586886, Validation Loss: 0.047435023688848735\n",
      "Epoch [15451/20000], Training Loss: 0.02627805251625302, Validation Loss: 0.011898733942293635\n",
      "Epoch [15452/20000], Training Loss: 0.013973075913132302, Validation Loss: 0.007251165190913257\n",
      "Epoch [15453/20000], Training Loss: 0.009907555596977804, Validation Loss: 0.007213947278882772\n",
      "Epoch [15454/20000], Training Loss: 0.011029905615698356, Validation Loss: 0.015545667778886647\n",
      "Epoch [15455/20000], Training Loss: 0.009779007340382253, Validation Loss: 0.004905624807193476\n",
      "Epoch [15456/20000], Training Loss: 0.004548104010089966, Validation Loss: 0.004149462064514101\n",
      "Epoch [15457/20000], Training Loss: 0.006512941538988214, Validation Loss: 0.005757418193594178\n",
      "Epoch [15458/20000], Training Loss: 0.013020647431923342, Validation Loss: 0.014710557404690659\n",
      "Epoch [15459/20000], Training Loss: 0.006514183248197826, Validation Loss: 0.005420211960178278\n",
      "Epoch [15460/20000], Training Loss: 0.005763702136131802, Validation Loss: 0.0038580173953204977\n",
      "Epoch [15461/20000], Training Loss: 0.005782596947808217, Validation Loss: 0.011797498017901578\n",
      "Epoch [15462/20000], Training Loss: 0.014192251561325975, Validation Loss: 0.01091473975970311\n",
      "Epoch [15463/20000], Training Loss: 0.012823623862849283, Validation Loss: 0.005238021993885614\n",
      "Epoch [15464/20000], Training Loss: 0.007581498344994283, Validation Loss: 0.005641654278002609\n",
      "Epoch [15465/20000], Training Loss: 0.004571352827562285, Validation Loss: 0.003968686050139273\n",
      "Epoch [15466/20000], Training Loss: 0.00475175580966639, Validation Loss: 0.00294225290754976\n",
      "Epoch [15467/20000], Training Loss: 0.004181811886285557, Validation Loss: 0.004770248262566922\n",
      "Epoch [15468/20000], Training Loss: 0.003921839080118973, Validation Loss: 0.004808306700106917\n",
      "Epoch [15469/20000], Training Loss: 0.006626929190263452, Validation Loss: 0.0031420857788810558\n",
      "Epoch [15470/20000], Training Loss: 0.013149999018163985, Validation Loss: 0.003115237723827237\n",
      "Epoch [15471/20000], Training Loss: 0.036689246717344304, Validation Loss: 0.025386909208644632\n",
      "Epoch [15472/20000], Training Loss: 0.026162867553336713, Validation Loss: 0.11369061682905497\n",
      "Epoch [15473/20000], Training Loss: 0.07032257890594858, Validation Loss: 0.023695145726914113\n",
      "Epoch [15474/20000], Training Loss: 0.044405219926764924, Validation Loss: 0.06325157869105169\n",
      "Epoch [15475/20000], Training Loss: 0.023824102900172357, Validation Loss: 0.007155537533500527\n",
      "Epoch [15476/20000], Training Loss: 0.013258538250478782, Validation Loss: 0.007207575214879723\n",
      "Epoch [15477/20000], Training Loss: 0.00827960654195132, Validation Loss: 0.010311632628812057\n",
      "Epoch [15478/20000], Training Loss: 0.00810985783547429, Validation Loss: 0.006261776096315381\n",
      "Epoch [15479/20000], Training Loss: 0.008786824284706134, Validation Loss: 0.0065811196641902825\n",
      "Epoch [15480/20000], Training Loss: 0.006345615630769836, Validation Loss: 0.005459268989069089\n",
      "Epoch [15481/20000], Training Loss: 0.005645693920582419, Validation Loss: 0.004224164155687049\n",
      "Epoch [15482/20000], Training Loss: 0.0070994478524620975, Validation Loss: 0.013672973491949847\n",
      "Epoch [15483/20000], Training Loss: 0.013574668252009101, Validation Loss: 0.014508153592541666\n",
      "Epoch [15484/20000], Training Loss: 0.006289597600958327, Validation Loss: 0.011537137229985157\n",
      "Epoch [15485/20000], Training Loss: 0.013306525961005329, Validation Loss: 0.005848990658386859\n",
      "Epoch [15486/20000], Training Loss: 0.0070006191703474285, Validation Loss: 0.004147239309306526\n",
      "Epoch [15487/20000], Training Loss: 0.007210500737918275, Validation Loss: 0.00453546707341701\n",
      "Epoch [15488/20000], Training Loss: 0.006962751348120426, Validation Loss: 0.0041746823475656515\n",
      "Epoch [15489/20000], Training Loss: 0.004650048163057363, Validation Loss: 0.00784843649002084\n",
      "Epoch [15490/20000], Training Loss: 0.005762974845700748, Validation Loss: 0.0037749130331497277\n",
      "Epoch [15491/20000], Training Loss: 0.005686438655954719, Validation Loss: 0.00314187391936116\n",
      "Epoch [15492/20000], Training Loss: 0.006419711945844548, Validation Loss: 0.0032661437719312403\n",
      "Epoch [15493/20000], Training Loss: 0.007410837832139805, Validation Loss: 0.011098483736133298\n",
      "Epoch [15494/20000], Training Loss: 0.006491946565802209, Validation Loss: 0.010974999717406675\n",
      "Epoch [15495/20000], Training Loss: 0.008349180649799694, Validation Loss: 0.005973472304974946\n",
      "Epoch [15496/20000], Training Loss: 0.005221394649457319, Validation Loss: 0.012768589892860272\n",
      "Epoch [15497/20000], Training Loss: 0.007103345355777232, Validation Loss: 0.005683853309343638\n",
      "Epoch [15498/20000], Training Loss: 0.004094831501445422, Validation Loss: 0.005111112518842934\n",
      "Epoch [15499/20000], Training Loss: 0.00630245432382383, Validation Loss: 0.004523582715367093\n",
      "Epoch [15500/20000], Training Loss: 0.006486630966849459, Validation Loss: 0.004510600629602465\n",
      "Epoch [15501/20000], Training Loss: 0.029566448617385634, Validation Loss: 0.01459262019646498\n",
      "Epoch [15502/20000], Training Loss: 0.01619037232947968, Validation Loss: 0.07074035009897475\n",
      "Epoch [15503/20000], Training Loss: 0.013411371033985884, Validation Loss: 0.022689242448414654\n",
      "Epoch [15504/20000], Training Loss: 0.019605249485331506, Validation Loss: 0.027748029146964955\n",
      "Epoch [15505/20000], Training Loss: 0.02137869906125291, Validation Loss: 0.0215671911526144\n",
      "Epoch [15506/20000], Training Loss: 0.01303702610299037, Validation Loss: 0.005350997849733728\n",
      "Epoch [15507/20000], Training Loss: 0.01338130513197809, Validation Loss: 0.03126792343599457\n",
      "Epoch [15508/20000], Training Loss: 0.014912939597187298, Validation Loss: 0.005670502369915604\n",
      "Epoch [15509/20000], Training Loss: 0.017847483978455005, Validation Loss: 0.007952928834274864\n",
      "Epoch [15510/20000], Training Loss: 0.032938349925314823, Validation Loss: 0.014681888009131885\n",
      "Epoch [15511/20000], Training Loss: 0.03790623669816081, Validation Loss: 0.01951845696456595\n",
      "Epoch [15512/20000], Training Loss: 0.012613944672856243, Validation Loss: 0.008232518448985347\n",
      "Epoch [15513/20000], Training Loss: 0.00673458435007238, Validation Loss: 0.005953110910679372\n",
      "Epoch [15514/20000], Training Loss: 0.00662707813403277, Validation Loss: 0.005259806721905339\n",
      "Epoch [15515/20000], Training Loss: 0.006154702499251081, Validation Loss: 0.005406644699809655\n",
      "Epoch [15516/20000], Training Loss: 0.006269559117746277, Validation Loss: 0.005512741355514906\n",
      "Epoch [15517/20000], Training Loss: 0.0062852072478921505, Validation Loss: 0.0042757868379063825\n",
      "Epoch [15518/20000], Training Loss: 0.006635265462266814, Validation Loss: 0.004704308198774194\n",
      "Epoch [15519/20000], Training Loss: 0.005571089082098167, Validation Loss: 0.005393489212451641\n",
      "Epoch [15520/20000], Training Loss: 0.00445184113842905, Validation Loss: 0.004196208978311792\n",
      "Epoch [15521/20000], Training Loss: 0.005246519414023558, Validation Loss: 0.00842289390554568\n",
      "Epoch [15522/20000], Training Loss: 0.007003709186132515, Validation Loss: 0.005021317394325463\n",
      "Epoch [15523/20000], Training Loss: 0.009435486780213458, Validation Loss: 0.0053312523977524185\n",
      "Epoch [15524/20000], Training Loss: 0.005420906847600592, Validation Loss: 0.006618427781193402\n",
      "Epoch [15525/20000], Training Loss: 0.006778228753189198, Validation Loss: 0.011802282783071136\n",
      "Epoch [15526/20000], Training Loss: 0.006629568096416604, Validation Loss: 0.004720911733565052\n",
      "Epoch [15527/20000], Training Loss: 0.008824868528920757, Validation Loss: 0.0062185872921915065\n",
      "Epoch [15528/20000], Training Loss: 0.005371602109465, Validation Loss: 0.003961923742565432\n",
      "Epoch [15529/20000], Training Loss: 0.004133214884599771, Validation Loss: 0.006446309392329394\n",
      "Epoch [15530/20000], Training Loss: 0.008784459799893998, Validation Loss: 0.0031495214075043915\n",
      "Epoch [15531/20000], Training Loss: 0.009783653061731457, Validation Loss: 0.014087232986071285\n",
      "Epoch [15532/20000], Training Loss: 0.014371878569623473, Validation Loss: 0.00329851947554159\n",
      "Epoch [15533/20000], Training Loss: 0.0054656420480958855, Validation Loss: 0.006796306714292969\n",
      "Epoch [15534/20000], Training Loss: 0.00982749711796974, Validation Loss: 0.005271344314037053\n",
      "Epoch [15535/20000], Training Loss: 0.015943102243389667, Validation Loss: 0.005431367469416922\n",
      "Epoch [15536/20000], Training Loss: 0.018958160862634292, Validation Loss: 0.01109126402305297\n",
      "Epoch [15537/20000], Training Loss: 0.013736043890405978, Validation Loss: 0.00521898426019824\n",
      "Epoch [15538/20000], Training Loss: 0.007305714384918955, Validation Loss: 0.0036649407959251585\n",
      "Epoch [15539/20000], Training Loss: 0.006159317400553326, Validation Loss: 0.002986466354181725\n",
      "Epoch [15540/20000], Training Loss: 0.007205800707782016, Validation Loss: 0.013519031394805947\n",
      "Epoch [15541/20000], Training Loss: 0.02072683012601179, Validation Loss: 0.06682859893356051\n",
      "Epoch [15542/20000], Training Loss: 0.026886590315760777, Validation Loss: 0.012883977299293442\n",
      "Epoch [15543/20000], Training Loss: 0.020336015264717844, Validation Loss: 0.005813926425147865\n",
      "Epoch [15544/20000], Training Loss: 0.022357151106397005, Validation Loss: 0.02711166171372627\n",
      "Epoch [15545/20000], Training Loss: 0.024293085917763944, Validation Loss: 0.035589296165564326\n",
      "Epoch [15546/20000], Training Loss: 0.024031092425242866, Validation Loss: 0.012374115866315094\n",
      "Epoch [15547/20000], Training Loss: 0.015130631143360265, Validation Loss: 0.015924602746963504\n",
      "Epoch [15548/20000], Training Loss: 0.007435087846325976, Validation Loss: 0.003825359497150365\n",
      "Epoch [15549/20000], Training Loss: 0.0064592897355656275, Validation Loss: 0.0036207252924886596\n",
      "Epoch [15550/20000], Training Loss: 0.005487537755210984, Validation Loss: 0.006577132270349852\n",
      "Epoch [15551/20000], Training Loss: 0.005794474697073123, Validation Loss: 0.010792176832540332\n",
      "Epoch [15552/20000], Training Loss: 0.00769469328224659, Validation Loss: 0.0065281013295296186\n",
      "Epoch [15553/20000], Training Loss: 0.010952606516590484, Validation Loss: 0.003668373859674493\n",
      "Epoch [15554/20000], Training Loss: 0.008280809573729389, Validation Loss: 0.003942766869729085\n",
      "Epoch [15555/20000], Training Loss: 0.004983286143604866, Validation Loss: 0.0035118905274765166\n",
      "Epoch [15556/20000], Training Loss: 0.0054807680925088266, Validation Loss: 0.004705971603894695\n",
      "Epoch [15557/20000], Training Loss: 0.00821520558591666, Validation Loss: 0.003657289319266926\n",
      "Epoch [15558/20000], Training Loss: 0.0037555452269901124, Validation Loss: 0.003387128064731269\n",
      "Epoch [15559/20000], Training Loss: 0.007049861048082156, Validation Loss: 0.004220757393564645\n",
      "Epoch [15560/20000], Training Loss: 0.015874597941157326, Validation Loss: 0.0041943260035606856\n",
      "Epoch [15561/20000], Training Loss: 0.007194295292720199, Validation Loss: 0.007418816643103318\n",
      "Epoch [15562/20000], Training Loss: 0.004182858941411334, Validation Loss: 0.005552420352920996\n",
      "Epoch [15563/20000], Training Loss: 0.007169763249943831, Validation Loss: 0.002796106739969436\n",
      "Epoch [15564/20000], Training Loss: 0.014403014975999082, Validation Loss: 0.010845055271472408\n",
      "Epoch [15565/20000], Training Loss: 0.014765899750532949, Validation Loss: 0.00477516822452521\n",
      "Epoch [15566/20000], Training Loss: 0.00781878846886944, Validation Loss: 0.004486271155276429\n",
      "Epoch [15567/20000], Training Loss: 0.007211727905282065, Validation Loss: 0.0028208250833033424\n",
      "Epoch [15568/20000], Training Loss: 0.006699481935876455, Validation Loss: 0.011697407360527125\n",
      "Epoch [15569/20000], Training Loss: 0.008182888226461469, Validation Loss: 0.004484824975620484\n",
      "Epoch [15570/20000], Training Loss: 0.008930041716666892, Validation Loss: 0.005774442863255174\n",
      "Epoch [15571/20000], Training Loss: 0.005761198491589832, Validation Loss: 0.0039774576982507925\n",
      "Epoch [15572/20000], Training Loss: 0.009081718729591641, Validation Loss: 0.010999110261244444\n",
      "Epoch [15573/20000], Training Loss: 0.009194393112238686, Validation Loss: 0.006932827307776153\n",
      "Epoch [15574/20000], Training Loss: 0.00553775385872411, Validation Loss: 0.004604888680792928\n",
      "Epoch [15575/20000], Training Loss: 0.00586830497403363, Validation Loss: 0.002760946086540575\n",
      "Epoch [15576/20000], Training Loss: 0.004358829667905541, Validation Loss: 0.007782251108437798\n",
      "Epoch [15577/20000], Training Loss: 0.012681138781512604, Validation Loss: 0.0045163973367647325\n",
      "Epoch [15578/20000], Training Loss: 0.015835891416437726, Validation Loss: 0.004543089810926751\n",
      "Epoch [15579/20000], Training Loss: 0.008362414214518919, Validation Loss: 0.0034725101265587156\n",
      "Epoch [15580/20000], Training Loss: 0.0067101098716843156, Validation Loss: 0.006158734272633573\n",
      "Epoch [15581/20000], Training Loss: 0.006453335915596524, Validation Loss: 0.0034155522672741864\n",
      "Epoch [15582/20000], Training Loss: 0.004563363018340689, Validation Loss: 0.013593636720660987\n",
      "Epoch [15583/20000], Training Loss: 0.009864960347970606, Validation Loss: 0.008170434140733882\n",
      "Epoch [15584/20000], Training Loss: 0.016470724692096286, Validation Loss: 0.009879444060583387\n",
      "Epoch [15585/20000], Training Loss: 0.01636371751997753, Validation Loss: 0.007566763229030636\n",
      "Epoch [15586/20000], Training Loss: 0.009967532959032204, Validation Loss: 0.011066458387566468\n",
      "Epoch [15587/20000], Training Loss: 0.01443615391450034, Validation Loss: 0.011354013612227782\n",
      "Epoch [15588/20000], Training Loss: 0.013032922523312404, Validation Loss: 0.009923636480899794\n",
      "Epoch [15589/20000], Training Loss: 0.012509366538974323, Validation Loss: 0.005068271227566069\n",
      "Epoch [15590/20000], Training Loss: 0.01220102115530608, Validation Loss: 0.003560808254641766\n",
      "Epoch [15591/20000], Training Loss: 0.006094537683696087, Validation Loss: 0.0031774712594801737\n",
      "Epoch [15592/20000], Training Loss: 0.004507365585076124, Validation Loss: 0.005891349382831588\n",
      "Epoch [15593/20000], Training Loss: 0.0059305113931519115, Validation Loss: 0.003256927925548571\n",
      "Epoch [15594/20000], Training Loss: 0.010334301619358095, Validation Loss: 0.0034925169145159714\n",
      "Epoch [15595/20000], Training Loss: 0.00993018635927001, Validation Loss: 0.0027822847104352626\n",
      "Epoch [15596/20000], Training Loss: 0.006158881527142057, Validation Loss: 0.0027979089812688444\n",
      "Epoch [15597/20000], Training Loss: 0.005661705018740447, Validation Loss: 0.008827428998691695\n",
      "Epoch [15598/20000], Training Loss: 0.010536108218470222, Validation Loss: 0.0067066461742846615\n",
      "Epoch [15599/20000], Training Loss: 0.010082531018254246, Validation Loss: 0.00684004122686284\n",
      "Epoch [15600/20000], Training Loss: 0.012008218112701538, Validation Loss: 0.011313697057110923\n",
      "Epoch [15601/20000], Training Loss: 0.013628645523567684, Validation Loss: 0.029266272272382467\n",
      "Epoch [15602/20000], Training Loss: 0.018650816930468345, Validation Loss: 0.012686505315027066\n",
      "Epoch [15603/20000], Training Loss: 0.017590123093603843, Validation Loss: 0.004344375604497619\n",
      "Epoch [15604/20000], Training Loss: 0.017374956231021445, Validation Loss: 0.007507233015660731\n",
      "Epoch [15605/20000], Training Loss: 0.010604104937686185, Validation Loss: 0.0033141443771979797\n",
      "Epoch [15606/20000], Training Loss: 0.005520341105563732, Validation Loss: 0.003417465956384139\n",
      "Epoch [15607/20000], Training Loss: 0.003975385399306626, Validation Loss: 0.0037991360295563997\n",
      "Epoch [15608/20000], Training Loss: 0.00439231277934076, Validation Loss: 0.0034480378131543587\n",
      "Epoch [15609/20000], Training Loss: 0.008592667229614952, Validation Loss: 0.016347709777099744\n",
      "Epoch [15610/20000], Training Loss: 0.0065043276200802734, Validation Loss: 0.0044227681698560325\n",
      "Epoch [15611/20000], Training Loss: 0.015368458696424827, Validation Loss: 0.005202846145519959\n",
      "Epoch [15612/20000], Training Loss: 0.01408131903736107, Validation Loss: 0.022713827428300908\n",
      "Epoch [15613/20000], Training Loss: 0.015662643061854657, Validation Loss: 0.030962484339466658\n",
      "Epoch [15614/20000], Training Loss: 0.02936785379889833, Validation Loss: 0.025038654257273203\n",
      "Epoch [15615/20000], Training Loss: 0.04414599374701668, Validation Loss: 0.01417534612119198\n",
      "Epoch [15616/20000], Training Loss: 0.01226451673885874, Validation Loss: 0.0154834230031286\n",
      "Epoch [15617/20000], Training Loss: 0.008260443986468349, Validation Loss: 0.007199144356463817\n",
      "Epoch [15618/20000], Training Loss: 0.008594195017524595, Validation Loss: 0.00455735563965833\n",
      "Epoch [15619/20000], Training Loss: 0.012965525976531873, Validation Loss: 0.009423058746116502\n",
      "Epoch [15620/20000], Training Loss: 0.005797945832975009, Validation Loss: 0.004380762078134182\n",
      "Epoch [15621/20000], Training Loss: 0.005014118681564079, Validation Loss: 0.005481169958199773\n",
      "Epoch [15622/20000], Training Loss: 0.006872881507401222, Validation Loss: 0.0038095182888880654\n",
      "Epoch [15623/20000], Training Loss: 0.006529206714601189, Validation Loss: 0.003231022556844686\n",
      "Epoch [15624/20000], Training Loss: 0.006114038526512948, Validation Loss: 0.003982036740386063\n",
      "Epoch [15625/20000], Training Loss: 0.006565766037965659, Validation Loss: 0.0033053542803316285\n",
      "Epoch [15626/20000], Training Loss: 0.009759616268503126, Validation Loss: 0.003990706704956081\n",
      "Epoch [15627/20000], Training Loss: 0.007784554387269184, Validation Loss: 0.022582275792956352\n",
      "Epoch [15628/20000], Training Loss: 0.011065567906129559, Validation Loss: 0.006118295279782432\n",
      "Epoch [15629/20000], Training Loss: 0.007643330825625786, Validation Loss: 0.0028588294018326134\n",
      "Epoch [15630/20000], Training Loss: 0.0072833715192765726, Validation Loss: 0.007838606474771997\n",
      "Epoch [15631/20000], Training Loss: 0.006350681786508565, Validation Loss: 0.007971357554197311\n",
      "Epoch [15632/20000], Training Loss: 0.008029019112395222, Validation Loss: 0.003839996855406753\n",
      "Epoch [15633/20000], Training Loss: 0.00493672501657524, Validation Loss: 0.005725295427380104\n",
      "Epoch [15634/20000], Training Loss: 0.016460173301831155, Validation Loss: 0.002919786376878619\n",
      "Epoch [15635/20000], Training Loss: 0.01749653520528227, Validation Loss: 0.013185458523886544\n",
      "Epoch [15636/20000], Training Loss: 0.03252874999140788, Validation Loss: 0.0454022969518389\n",
      "Epoch [15637/20000], Training Loss: 0.020216732460539788, Validation Loss: 0.058969646692276\n",
      "Epoch [15638/20000], Training Loss: 0.04234865207581606, Validation Loss: 0.018706474250779235\n",
      "Epoch [15639/20000], Training Loss: 0.011055393331584387, Validation Loss: 0.022813047771905865\n",
      "Epoch [15640/20000], Training Loss: 0.012609391198826156, Validation Loss: 0.008951374279757627\n",
      "Epoch [15641/20000], Training Loss: 0.010993420784611121, Validation Loss: 0.007279007736542553\n",
      "Epoch [15642/20000], Training Loss: 0.005167056170258937, Validation Loss: 0.004696741840234608\n",
      "Epoch [15643/20000], Training Loss: 0.005988233269558155, Validation Loss: 0.0043458191690644654\n",
      "Epoch [15644/20000], Training Loss: 0.0044961223278992945, Validation Loss: 0.0061582685710683495\n",
      "Epoch [15645/20000], Training Loss: 0.0050804998198665475, Validation Loss: 0.0121922719982572\n",
      "Epoch [15646/20000], Training Loss: 0.0066002387455747725, Validation Loss: 0.003777646216008179\n",
      "Epoch [15647/20000], Training Loss: 0.006807196112017014, Validation Loss: 0.00801412191460571\n",
      "Epoch [15648/20000], Training Loss: 0.008908632780990697, Validation Loss: 0.006375513644989894\n",
      "Epoch [15649/20000], Training Loss: 0.011449217781773768, Validation Loss: 0.011460837365770462\n",
      "Epoch [15650/20000], Training Loss: 0.004540197960782929, Validation Loss: 0.005021821684286871\n",
      "Epoch [15651/20000], Training Loss: 0.0035191765657925445, Validation Loss: 0.00523776712923661\n",
      "Epoch [15652/20000], Training Loss: 0.007292979949852452, Validation Loss: 0.012738574151204623\n",
      "Epoch [15653/20000], Training Loss: 0.0073217060604032925, Validation Loss: 0.007311043357109526\n",
      "Epoch [15654/20000], Training Loss: 0.007199133937579713, Validation Loss: 0.0051824123095722185\n",
      "Epoch [15655/20000], Training Loss: 0.006867888325359672, Validation Loss: 0.003524318610746694\n",
      "Epoch [15656/20000], Training Loss: 0.005413466422576262, Validation Loss: 0.004433096546767347\n",
      "Epoch [15657/20000], Training Loss: 0.005923650397432668, Validation Loss: 0.0036060978740154936\n",
      "Epoch [15658/20000], Training Loss: 0.008696389767075223, Validation Loss: 0.0030915737404311827\n",
      "Epoch [15659/20000], Training Loss: 0.022384764942606643, Validation Loss: 0.00367070077607577\n",
      "Epoch [15660/20000], Training Loss: 0.025263028604188418, Validation Loss: 0.012288176812808058\n",
      "Epoch [15661/20000], Training Loss: 0.02471312910240288, Validation Loss: 0.04570419367928226\n",
      "Epoch [15662/20000], Training Loss: 0.02537297014649604, Validation Loss: 0.04241940375053283\n",
      "Epoch [15663/20000], Training Loss: 0.028338033276018644, Validation Loss: 0.054624224458426136\n",
      "Epoch [15664/20000], Training Loss: 0.029300544011805738, Validation Loss: 0.011777199084950294\n",
      "Epoch [15665/20000], Training Loss: 0.01774964956400384, Validation Loss: 0.0065929918421017775\n",
      "Epoch [15666/20000], Training Loss: 0.007076953966004241, Validation Loss: 0.011960453313544608\n",
      "Epoch [15667/20000], Training Loss: 0.010451933709971075, Validation Loss: 0.010400824091862302\n",
      "Epoch [15668/20000], Training Loss: 0.011666071422431352, Validation Loss: 0.008948869380213458\n",
      "Epoch [15669/20000], Training Loss: 0.009900523224912052, Validation Loss: 0.013307118826170543\n",
      "Epoch [15670/20000], Training Loss: 0.01096760024769797, Validation Loss: 0.0046945829013306396\n",
      "Epoch [15671/20000], Training Loss: 0.013332619752353432, Validation Loss: 0.017391762588574573\n",
      "Epoch [15672/20000], Training Loss: 0.008432942422976859, Validation Loss: 0.003691417449967308\n",
      "Epoch [15673/20000], Training Loss: 0.008502620174632674, Validation Loss: 0.007230784988743315\n",
      "Epoch [15674/20000], Training Loss: 0.011228082402210151, Validation Loss: 0.006695241374671923\n",
      "Epoch [15675/20000], Training Loss: 0.006676197901210149, Validation Loss: 0.00452162369596668\n",
      "Epoch [15676/20000], Training Loss: 0.0045214744625679615, Validation Loss: 0.005847079444472618\n",
      "Epoch [15677/20000], Training Loss: 0.009657738531326945, Validation Loss: 0.0038626517955110884\n",
      "Epoch [15678/20000], Training Loss: 0.0067404654621624005, Validation Loss: 0.0043959964978950794\n",
      "Epoch [15679/20000], Training Loss: 0.005243417715454208, Validation Loss: 0.004187414858133057\n",
      "Epoch [15680/20000], Training Loss: 0.005322401047773643, Validation Loss: 0.0044742818372530534\n",
      "Epoch [15681/20000], Training Loss: 0.006524863869149807, Validation Loss: 0.02413896502612443\n",
      "Epoch [15682/20000], Training Loss: 0.018770552281889, Validation Loss: 0.025232682815014246\n",
      "Epoch [15683/20000], Training Loss: 0.02033189035308323, Validation Loss: 0.0064662686936886515\n",
      "Epoch [15684/20000], Training Loss: 0.013581899648475624, Validation Loss: 0.019148334255175672\n",
      "Epoch [15685/20000], Training Loss: 0.009710884049127344, Validation Loss: 0.006682357928273827\n",
      "Epoch [15686/20000], Training Loss: 0.007838223773433128, Validation Loss: 0.010425225589861904\n",
      "Epoch [15687/20000], Training Loss: 0.009419542728989785, Validation Loss: 0.004940657191550457\n",
      "Epoch [15688/20000], Training Loss: 0.007777625279101942, Validation Loss: 0.0069167592248667075\n",
      "Epoch [15689/20000], Training Loss: 0.016386890191373103, Validation Loss: 0.00463826354241194\n",
      "Epoch [15690/20000], Training Loss: 0.011698473005667762, Validation Loss: 0.026807517877649935\n",
      "Epoch [15691/20000], Training Loss: 0.01689895725160438, Validation Loss: 0.005279910996169487\n",
      "Epoch [15692/20000], Training Loss: 0.01719467159793047, Validation Loss: 0.004362365704710872\n",
      "Epoch [15693/20000], Training Loss: 0.008699332612845214, Validation Loss: 0.0061563100076804335\n",
      "Epoch [15694/20000], Training Loss: 0.00616358797049088, Validation Loss: 0.006292596182420189\n",
      "Epoch [15695/20000], Training Loss: 0.013387249707843043, Validation Loss: 0.011645722286382514\n",
      "Epoch [15696/20000], Training Loss: 0.01564075834737975, Validation Loss: 0.004412279989135673\n",
      "Epoch [15697/20000], Training Loss: 0.005909989708535639, Validation Loss: 0.0035937415919745164\n",
      "Epoch [15698/20000], Training Loss: 0.003616007444049631, Validation Loss: 0.003088339768136653\n",
      "Epoch [15699/20000], Training Loss: 0.006790762012445027, Validation Loss: 0.0032213246137514162\n",
      "Epoch [15700/20000], Training Loss: 0.0070135813813457, Validation Loss: 0.00352015595842557\n",
      "Epoch [15701/20000], Training Loss: 0.007817740735065724, Validation Loss: 0.011734499605706463\n",
      "Epoch [15702/20000], Training Loss: 0.01064543222400971, Validation Loss: 0.004717797635754957\n",
      "Epoch [15703/20000], Training Loss: 0.0132374277828993, Validation Loss: 0.0032400048565851386\n",
      "Epoch [15704/20000], Training Loss: 0.006040394787435487, Validation Loss: 0.004841714295473817\n",
      "Epoch [15705/20000], Training Loss: 0.008643492230995824, Validation Loss: 0.0172516150531735\n",
      "Epoch [15706/20000], Training Loss: 0.01670998546796909, Validation Loss: 0.022981230701908894\n",
      "Epoch [15707/20000], Training Loss: 0.012960083449018254, Validation Loss: 0.005745332402558542\n",
      "Epoch [15708/20000], Training Loss: 0.007824299919385729, Validation Loss: 0.004245100369907107\n",
      "Epoch [15709/20000], Training Loss: 0.004839437437892359, Validation Loss: 0.006475817404606232\n",
      "Epoch [15710/20000], Training Loss: 0.0053741500019636335, Validation Loss: 0.003953097179158403\n",
      "Epoch [15711/20000], Training Loss: 0.0070078604736149176, Validation Loss: 0.004786634428780546\n",
      "Epoch [15712/20000], Training Loss: 0.004101388150177497, Validation Loss: 0.007124807326144004\n",
      "Epoch [15713/20000], Training Loss: 0.006285185134921838, Validation Loss: 0.002748494290366515\n",
      "Epoch [15714/20000], Training Loss: 0.003929669386447391, Validation Loss: 0.005466358262023923\n",
      "Epoch [15715/20000], Training Loss: 0.0056389300515158015, Validation Loss: 0.002712723290144936\n",
      "Epoch [15716/20000], Training Loss: 0.006014801807655853, Validation Loss: 0.006610137818224108\n",
      "Epoch [15717/20000], Training Loss: 0.0034619474716107235, Validation Loss: 0.01910416515871093\n",
      "Epoch [15718/20000], Training Loss: 0.017541138640288927, Validation Loss: 0.014714679148590787\n",
      "Epoch [15719/20000], Training Loss: 0.015572398314085538, Validation Loss: 0.0046036923556300735\n",
      "Epoch [15720/20000], Training Loss: 0.0027469147358455564, Validation Loss: 0.004926294643678679\n",
      "Epoch [15721/20000], Training Loss: 0.00583606478695791, Validation Loss: 0.0028222997519224335\n",
      "Epoch [15722/20000], Training Loss: 0.006152922608764909, Validation Loss: 0.002830714104821449\n",
      "Epoch [15723/20000], Training Loss: 0.0056828140950528905, Validation Loss: 0.00849309357741722\n",
      "Epoch [15724/20000], Training Loss: 0.01046215944058661, Validation Loss: 0.010451904635308972\n",
      "Epoch [15725/20000], Training Loss: 0.009304967701609712, Validation Loss: 0.0034495209012769734\n",
      "Epoch [15726/20000], Training Loss: 0.006718318715554362, Validation Loss: 0.00499823349127837\n",
      "Epoch [15727/20000], Training Loss: 0.0062540245718472375, Validation Loss: 0.003016227725796889\n",
      "Epoch [15728/20000], Training Loss: 0.009931453926713272, Validation Loss: 0.005150606895704647\n",
      "Epoch [15729/20000], Training Loss: 0.004736762860764949, Validation Loss: 0.004237354348737605\n",
      "Epoch [15730/20000], Training Loss: 0.004891212365432044, Validation Loss: 0.004979585125482799\n",
      "Epoch [15731/20000], Training Loss: 0.011903955556330661, Validation Loss: 0.004630529682325811\n",
      "Epoch [15732/20000], Training Loss: 0.006588118669502104, Validation Loss: 0.0029237999124551084\n",
      "Epoch [15733/20000], Training Loss: 0.005568871347821057, Validation Loss: 0.006668346869245318\n",
      "Epoch [15734/20000], Training Loss: 0.006404082694643876, Validation Loss: 0.0040766295978491894\n",
      "Epoch [15735/20000], Training Loss: 0.0060071684750612675, Validation Loss: 0.020584657719853437\n",
      "Epoch [15736/20000], Training Loss: 0.01895373873165746, Validation Loss: 0.013761854071544479\n",
      "Epoch [15737/20000], Training Loss: 0.016867128944643435, Validation Loss: 0.02379117567336055\n",
      "Epoch [15738/20000], Training Loss: 0.010986196047139987, Validation Loss: 0.0023539199445435444\n",
      "Epoch [15739/20000], Training Loss: 0.004271968251227268, Validation Loss: 0.002866869436754119\n",
      "Epoch [15740/20000], Training Loss: 0.004596060279548096, Validation Loss: 0.017981351486277792\n",
      "Epoch [15741/20000], Training Loss: 0.010963166326226721, Validation Loss: 0.010544712156328304\n",
      "Epoch [15742/20000], Training Loss: 0.008422153706630655, Validation Loss: 0.0033733578048853463\n",
      "Epoch [15743/20000], Training Loss: 0.004818592564073957, Validation Loss: 0.006518720031043098\n",
      "Epoch [15744/20000], Training Loss: 0.00984191586758243, Validation Loss: 0.004798617829011298\n",
      "Epoch [15745/20000], Training Loss: 0.011838303683297064, Validation Loss: 0.005617795264551465\n",
      "Epoch [15746/20000], Training Loss: 0.019738621855480636, Validation Loss: 0.003192130011512465\n",
      "Epoch [15747/20000], Training Loss: 0.02103991937383449, Validation Loss: 0.021785900129803634\n",
      "Epoch [15748/20000], Training Loss: 0.021011812213276113, Validation Loss: 0.010087428548013339\n",
      "Epoch [15749/20000], Training Loss: 0.015796168490071847, Validation Loss: 0.007348337432635077\n",
      "Epoch [15750/20000], Training Loss: 0.01117059051882409, Validation Loss: 0.015276035565745718\n",
      "Epoch [15751/20000], Training Loss: 0.012467386448812639, Validation Loss: 0.009258348146610322\n",
      "Epoch [15752/20000], Training Loss: 0.015807844750400233, Validation Loss: 0.005641561837177331\n",
      "Epoch [15753/20000], Training Loss: 0.00831173052977517, Validation Loss: 0.008458229611398499\n",
      "Epoch [15754/20000], Training Loss: 0.014264940452189226, Validation Loss: 0.011805881080910073\n",
      "Epoch [15755/20000], Training Loss: 0.015947808388903337, Validation Loss: 0.01682137724570726\n",
      "Epoch [15756/20000], Training Loss: 0.015255529286309868, Validation Loss: 0.008623888184656012\n",
      "Epoch [15757/20000], Training Loss: 0.011977330151044694, Validation Loss: 0.006211737190812171\n",
      "Epoch [15758/20000], Training Loss: 0.0052215127579984255, Validation Loss: 0.0035100944390116873\n",
      "Epoch [15759/20000], Training Loss: 0.004335472475629233, Validation Loss: 0.003240403787334018\n",
      "Epoch [15760/20000], Training Loss: 0.004343107849698364, Validation Loss: 0.003071338966021488\n",
      "Epoch [15761/20000], Training Loss: 0.007578774430190346, Validation Loss: 0.005056821431054459\n",
      "Epoch [15762/20000], Training Loss: 0.005063214397523552, Validation Loss: 0.005267999206393168\n",
      "Epoch [15763/20000], Training Loss: 0.008219218739083902, Validation Loss: 0.005669769889713697\n",
      "Epoch [15764/20000], Training Loss: 0.00621399785450194, Validation Loss: 0.011365765572686199\n",
      "Epoch [15765/20000], Training Loss: 0.007947190041055105, Validation Loss: 0.006731287126258272\n",
      "Epoch [15766/20000], Training Loss: 0.0071518831802360894, Validation Loss: 0.002852602542765172\n",
      "Epoch [15767/20000], Training Loss: 0.005296201000214972, Validation Loss: 0.0032720503919373106\n",
      "Epoch [15768/20000], Training Loss: 0.009389987605183185, Validation Loss: 0.004359674113874329\n",
      "Epoch [15769/20000], Training Loss: 0.0065272111654946586, Validation Loss: 0.003982885125078868\n",
      "Epoch [15770/20000], Training Loss: 0.007061386951542252, Validation Loss: 0.004719215046942996\n",
      "Epoch [15771/20000], Training Loss: 0.004065890401112223, Validation Loss: 0.004889657455939123\n",
      "Epoch [15772/20000], Training Loss: 0.00691795778818362, Validation Loss: 0.0038309124070069955\n",
      "Epoch [15773/20000], Training Loss: 0.008414638010435738, Validation Loss: 0.007756081176928053\n",
      "Epoch [15774/20000], Training Loss: 0.005677820611578811, Validation Loss: 0.006348074103617104\n",
      "Epoch [15775/20000], Training Loss: 0.004571293738471078, Validation Loss: 0.005325496928598567\n",
      "Epoch [15776/20000], Training Loss: 0.0037345607473590853, Validation Loss: 0.0030851932453070624\n",
      "Epoch [15777/20000], Training Loss: 0.004584721765240829, Validation Loss: 0.00794742033551348\n",
      "Epoch [15778/20000], Training Loss: 0.005383864230160336, Validation Loss: 0.0028450651248094366\n",
      "Epoch [15779/20000], Training Loss: 0.009338480279568882, Validation Loss: 0.0036176372637837873\n",
      "Epoch [15780/20000], Training Loss: 0.015286470867327548, Validation Loss: 0.00243098525863036\n",
      "Epoch [15781/20000], Training Loss: 0.019370605478928025, Validation Loss: 0.049429846519087946\n",
      "Epoch [15782/20000], Training Loss: 0.03590886039678155, Validation Loss: 0.07552563839479783\n",
      "Epoch [15783/20000], Training Loss: 0.05107518187807208, Validation Loss: 0.011314009265002369\n",
      "Epoch [15784/20000], Training Loss: 0.017338252543205663, Validation Loss: 0.01741641377904151\n",
      "Epoch [15785/20000], Training Loss: 0.026210475402487127, Validation Loss: 0.014938962288660124\n",
      "Epoch [15786/20000], Training Loss: 0.019688458676682785, Validation Loss: 0.01157861668687105\n",
      "Epoch [15787/20000], Training Loss: 0.014666294936822461, Validation Loss: 0.022511578564413305\n",
      "Epoch [15788/20000], Training Loss: 0.028564954705611205, Validation Loss: 0.013721355904283026\n",
      "Epoch [15789/20000], Training Loss: 0.021081233509383828, Validation Loss: 0.005977360644530043\n",
      "Epoch [15790/20000], Training Loss: 0.0204170193007615, Validation Loss: 0.010065572373475464\n",
      "Epoch [15791/20000], Training Loss: 0.00747007058401193, Validation Loss: 0.005616212880009527\n",
      "Epoch [15792/20000], Training Loss: 0.00878573719819542, Validation Loss: 0.015463345285236477\n",
      "Epoch [15793/20000], Training Loss: 0.015099140364717252, Validation Loss: 0.005410679415295557\n",
      "Epoch [15794/20000], Training Loss: 0.010140572870438649, Validation Loss: 0.014941224049835\n",
      "Epoch [15795/20000], Training Loss: 0.014273228484463678, Validation Loss: 0.004517630302107786\n",
      "Epoch [15796/20000], Training Loss: 0.007363777017287377, Validation Loss: 0.008953520632977082\n",
      "Epoch [15797/20000], Training Loss: 0.0065485731881511, Validation Loss: 0.0054724066020046536\n",
      "Epoch [15798/20000], Training Loss: 0.006381482210209859, Validation Loss: 0.003940498106528269\n",
      "Epoch [15799/20000], Training Loss: 0.004919824511619352, Validation Loss: 0.003383601809678386\n",
      "Epoch [15800/20000], Training Loss: 0.004977720116065549, Validation Loss: 0.004200694425395469\n",
      "Epoch [15801/20000], Training Loss: 0.004307299983338453, Validation Loss: 0.003013321066536686\n",
      "Epoch [15802/20000], Training Loss: 0.005328741505114262, Validation Loss: 0.0026164720442526856\n",
      "Epoch [15803/20000], Training Loss: 0.003918045856802824, Validation Loss: 0.010617360871912907\n",
      "Epoch [15804/20000], Training Loss: 0.016799096155279716, Validation Loss: 0.007030166355655554\n",
      "Epoch [15805/20000], Training Loss: 0.020341536197520327, Validation Loss: 0.004543152179632922\n",
      "Epoch [15806/20000], Training Loss: 0.013459057948369133, Validation Loss: 0.006767602493669399\n",
      "Epoch [15807/20000], Training Loss: 0.00869208711031076, Validation Loss: 0.012312794216419272\n",
      "Epoch [15808/20000], Training Loss: 0.01859278686606558, Validation Loss: 0.0043559787920263\n",
      "Epoch [15809/20000], Training Loss: 0.011952417112947191, Validation Loss: 0.005139295238989944\n",
      "Epoch [15810/20000], Training Loss: 0.007055070563670337, Validation Loss: 0.009365361670526389\n",
      "Epoch [15811/20000], Training Loss: 0.00596452684765479, Validation Loss: 0.008145757519025446\n",
      "Epoch [15812/20000], Training Loss: 0.006058346010831883, Validation Loss: 0.005247856371280729\n",
      "Epoch [15813/20000], Training Loss: 0.006466842202436445, Validation Loss: 0.002589254265421305\n",
      "Epoch [15814/20000], Training Loss: 0.010592102946247905, Validation Loss: 0.008734298913706442\n",
      "Epoch [15815/20000], Training Loss: 0.010889852099353448, Validation Loss: 0.007959402989529605\n",
      "Epoch [15816/20000], Training Loss: 0.006947718490014917, Validation Loss: 0.005097982346017612\n",
      "Epoch [15817/20000], Training Loss: 0.006192879122344104, Validation Loss: 0.0030087875046196206\n",
      "Epoch [15818/20000], Training Loss: 0.0071467615905151305, Validation Loss: 0.005442376321281872\n",
      "Epoch [15819/20000], Training Loss: 0.005885907127906519, Validation Loss: 0.010080852545946135\n",
      "Epoch [15820/20000], Training Loss: 0.008971389236519047, Validation Loss: 0.007947855935027384\n",
      "Epoch [15821/20000], Training Loss: 0.012881007309520751, Validation Loss: 0.008465445467455087\n",
      "Epoch [15822/20000], Training Loss: 0.01406750364445283, Validation Loss: 0.0024037371122704243\n",
      "Epoch [15823/20000], Training Loss: 0.008134034217489119, Validation Loss: 0.009026935324520341\n",
      "Epoch [15824/20000], Training Loss: 0.014973499321578336, Validation Loss: 0.013700623576206843\n",
      "Epoch [15825/20000], Training Loss: 0.018987087708199266, Validation Loss: 0.02080697599120302\n",
      "Epoch [15826/20000], Training Loss: 0.009725092446907573, Validation Loss: 0.0026660034844983977\n",
      "Epoch [15827/20000], Training Loss: 0.005716182021257866, Validation Loss: 0.009099505043493297\n",
      "Epoch [15828/20000], Training Loss: 0.007748875279609165, Validation Loss: 0.00812328675718683\n",
      "Epoch [15829/20000], Training Loss: 0.01155263300461229, Validation Loss: 0.004819876003089466\n",
      "Epoch [15830/20000], Training Loss: 0.03271274194724226, Validation Loss: 0.012421480068805326\n",
      "Epoch [15831/20000], Training Loss: 0.019075281019987806, Validation Loss: 0.024968177080214534\n",
      "Epoch [15832/20000], Training Loss: 0.015958850319813273, Validation Loss: 0.005687699093766295\n",
      "Epoch [15833/20000], Training Loss: 0.009975262001327272, Validation Loss: 0.004253700926337193\n",
      "Epoch [15834/20000], Training Loss: 0.004529428381439564, Validation Loss: 0.005424491333675662\n",
      "Epoch [15835/20000], Training Loss: 0.006430999901307847, Validation Loss: 0.0037221046034931404\n",
      "Epoch [15836/20000], Training Loss: 0.006281194701192102, Validation Loss: 0.0042080871481045634\n",
      "Epoch [15837/20000], Training Loss: 0.004510358878470859, Validation Loss: 0.014007658831110135\n",
      "Epoch [15838/20000], Training Loss: 0.009968963108674091, Validation Loss: 0.007352639605335052\n",
      "Epoch [15839/20000], Training Loss: 0.011796633869900168, Validation Loss: 0.004545855219866206\n",
      "Epoch [15840/20000], Training Loss: 0.019521069769065695, Validation Loss: 0.02237905105305669\n",
      "Epoch [15841/20000], Training Loss: 0.01371303343746279, Validation Loss: 0.008396466464125205\n",
      "Epoch [15842/20000], Training Loss: 0.006909828769754053, Validation Loss: 0.004108796149517361\n",
      "Epoch [15843/20000], Training Loss: 0.007397204394302597, Validation Loss: 0.005302386141820569\n",
      "Epoch [15844/20000], Training Loss: 0.014327466510621889, Validation Loss: 0.01907154704941271\n",
      "Epoch [15845/20000], Training Loss: 0.013155738798169685, Validation Loss: 0.010932755523704304\n",
      "Epoch [15846/20000], Training Loss: 0.005965246998779808, Validation Loss: 0.004072824442606101\n",
      "Epoch [15847/20000], Training Loss: 0.003987869122024027, Validation Loss: 0.005680212695639944\n",
      "Epoch [15848/20000], Training Loss: 0.0073474819270943825, Validation Loss: 0.002842267601060782\n",
      "Epoch [15849/20000], Training Loss: 0.007267417851821929, Validation Loss: 0.0029781750411367325\n",
      "Epoch [15850/20000], Training Loss: 0.003592577364283248, Validation Loss: 0.004967583681067449\n",
      "Epoch [15851/20000], Training Loss: 0.006576574440259719, Validation Loss: 0.0035443256636729487\n",
      "Epoch [15852/20000], Training Loss: 0.00468815039546046, Validation Loss: 0.007307115906395926\n",
      "Epoch [15853/20000], Training Loss: 0.013017526405999054, Validation Loss: 0.0030431326833234734\n",
      "Epoch [15854/20000], Training Loss: 0.007349793357726802, Validation Loss: 0.00249492853450084\n",
      "Epoch [15855/20000], Training Loss: 0.004509033728167212, Validation Loss: 0.0032741007354409094\n",
      "Epoch [15856/20000], Training Loss: 0.0036657990210875774, Validation Loss: 0.004810179387865583\n",
      "Epoch [15857/20000], Training Loss: 0.004295972105800112, Validation Loss: 0.003386921719498109\n",
      "Epoch [15858/20000], Training Loss: 0.004163990762858053, Validation Loss: 0.004653694989703932\n",
      "Epoch [15859/20000], Training Loss: 0.004776890751359579, Validation Loss: 0.0034711111146392922\n",
      "Epoch [15860/20000], Training Loss: 0.004712398058992611, Validation Loss: 0.00762078998533257\n",
      "Epoch [15861/20000], Training Loss: 0.012425455620229644, Validation Loss: 0.030589964063178092\n",
      "Epoch [15862/20000], Training Loss: 0.030003248581367332, Validation Loss: 0.017725816828065857\n",
      "Epoch [15863/20000], Training Loss: 0.01925493934374702, Validation Loss: 0.009578757773332265\n",
      "Epoch [15864/20000], Training Loss: 0.011635879748999807, Validation Loss: 0.007308285601851594\n",
      "Epoch [15865/20000], Training Loss: 0.01160236739971359, Validation Loss: 0.01093728124503254\n",
      "Epoch [15866/20000], Training Loss: 0.012839562272217466, Validation Loss: 0.010956493605790903\n",
      "Epoch [15867/20000], Training Loss: 0.007057603785402274, Validation Loss: 0.005960119112486252\n",
      "Epoch [15868/20000], Training Loss: 0.009294989949674053, Validation Loss: 0.020284217757567342\n",
      "Epoch [15869/20000], Training Loss: 0.015507605380548739, Validation Loss: 0.007446620647632114\n",
      "Epoch [15870/20000], Training Loss: 0.010384288537482331, Validation Loss: 0.0035216105642120526\n",
      "Epoch [15871/20000], Training Loss: 0.007884665939595184, Validation Loss: 0.017615159973518462\n",
      "Epoch [15872/20000], Training Loss: 0.016835991328800252, Validation Loss: 0.04267440204109464\n",
      "Epoch [15873/20000], Training Loss: 0.03351962492784618, Validation Loss: 0.044707289763859355\n",
      "Epoch [15874/20000], Training Loss: 0.020178555579669983, Validation Loss: 0.03201755201215357\n",
      "Epoch [15875/20000], Training Loss: 0.02694463966430963, Validation Loss: 0.01687814917694855\n",
      "Epoch [15876/20000], Training Loss: 0.01726907440960141, Validation Loss: 0.005728938030837786\n",
      "Epoch [15877/20000], Training Loss: 0.013228385018010158, Validation Loss: 0.015014929896487377\n",
      "Epoch [15878/20000], Training Loss: 0.008426839817340286, Validation Loss: 0.005814836934985935\n",
      "Epoch [15879/20000], Training Loss: 0.006604866411574347, Validation Loss: 0.003785568443656676\n",
      "Epoch [15880/20000], Training Loss: 0.005039032386516088, Validation Loss: 0.003844731401988393\n",
      "Epoch [15881/20000], Training Loss: 0.004638770258420014, Validation Loss: 0.007729629617447894\n",
      "Epoch [15882/20000], Training Loss: 0.010804518106462118, Validation Loss: 0.013854907359937784\n",
      "Epoch [15883/20000], Training Loss: 0.008010882221112427, Validation Loss: 0.0067916096020488636\n",
      "Epoch [15884/20000], Training Loss: 0.004202934649323912, Validation Loss: 0.004948184063825011\n",
      "Epoch [15885/20000], Training Loss: 0.006386672875379301, Validation Loss: 0.0034401209253940935\n",
      "Epoch [15886/20000], Training Loss: 0.0032815828827941524, Validation Loss: 0.0036960819050237204\n",
      "Epoch [15887/20000], Training Loss: 0.0042354407175610375, Validation Loss: 0.007413403590834581\n",
      "Epoch [15888/20000], Training Loss: 0.009198781323253311, Validation Loss: 0.004791054215447835\n",
      "Epoch [15889/20000], Training Loss: 0.010631759721685998, Validation Loss: 0.01409951557538394\n",
      "Epoch [15890/20000], Training Loss: 0.01224480514915933, Validation Loss: 0.008369813141109003\n",
      "Epoch [15891/20000], Training Loss: 0.01616463263053447, Validation Loss: 0.00951961973437768\n",
      "Epoch [15892/20000], Training Loss: 0.009034565824549645, Validation Loss: 0.007363087697316394\n",
      "Epoch [15893/20000], Training Loss: 0.00592496241110244, Validation Loss: 0.004545791584533276\n",
      "Epoch [15894/20000], Training Loss: 0.005835786650745182, Validation Loss: 0.0039592901190778195\n",
      "Epoch [15895/20000], Training Loss: 0.009238294809100418, Validation Loss: 0.014962547059577653\n",
      "Epoch [15896/20000], Training Loss: 0.01090535867634961, Validation Loss: 0.00475659575210674\n",
      "Epoch [15897/20000], Training Loss: 0.007270132663377028, Validation Loss: 0.003004643476915856\n",
      "Epoch [15898/20000], Training Loss: 0.017491159201329407, Validation Loss: 0.0036766355225351663\n",
      "Epoch [15899/20000], Training Loss: 0.020629613175905042, Validation Loss: 0.01593392491499149\n",
      "Epoch [15900/20000], Training Loss: 0.012953628662022052, Validation Loss: 0.008229027337331623\n",
      "Epoch [15901/20000], Training Loss: 0.01452943459922348, Validation Loss: 0.0670173732297761\n",
      "Epoch [15902/20000], Training Loss: 0.0266208769023381, Validation Loss: 0.01038337087290944\n",
      "Epoch [15903/20000], Training Loss: 0.007360577293314107, Validation Loss: 0.0045670430862289535\n",
      "Epoch [15904/20000], Training Loss: 0.006793062153779569, Validation Loss: 0.003374571070545634\n",
      "Epoch [15905/20000], Training Loss: 0.006275852834473231, Validation Loss: 0.003200824516569017\n",
      "Epoch [15906/20000], Training Loss: 0.005379405986916806, Validation Loss: 0.005429099614976616\n",
      "Epoch [15907/20000], Training Loss: 0.007642068182966406, Validation Loss: 0.005094897782341005\n",
      "Epoch [15908/20000], Training Loss: 0.010946206837505404, Validation Loss: 0.005260519584013115\n",
      "Epoch [15909/20000], Training Loss: 0.013812896742787675, Validation Loss: 0.011379774815265722\n",
      "Epoch [15910/20000], Training Loss: 0.011212647616568054, Validation Loss: 0.003804390328133495\n",
      "Epoch [15911/20000], Training Loss: 0.013222678138747921, Validation Loss: 0.04517823777028498\n",
      "Epoch [15912/20000], Training Loss: 0.028574386213741882, Validation Loss: 0.08339593878814153\n",
      "Epoch [15913/20000], Training Loss: 0.07995637466332742, Validation Loss: 0.015441937825406577\n",
      "Epoch [15914/20000], Training Loss: 0.03382970148231834, Validation Loss: 0.035219696868547805\n",
      "Epoch [15915/20000], Training Loss: 0.01986079787158295, Validation Loss: 0.008776577496766842\n",
      "Epoch [15916/20000], Training Loss: 0.011310022610164847, Validation Loss: 0.008343030054411267\n",
      "Epoch [15917/20000], Training Loss: 0.00826063023331309, Validation Loss: 0.007559340565913786\n",
      "Epoch [15918/20000], Training Loss: 0.00845489309826267, Validation Loss: 0.005670006276830009\n",
      "Epoch [15919/20000], Training Loss: 0.006129849963761964, Validation Loss: 0.005169374984361192\n",
      "Epoch [15920/20000], Training Loss: 0.006706529928903494, Validation Loss: 0.009376103865179767\n",
      "Epoch [15921/20000], Training Loss: 0.008979144073756678, Validation Loss: 0.0081099842588575\n",
      "Epoch [15922/20000], Training Loss: 0.005827489309532601, Validation Loss: 0.0043781169186317215\n",
      "Epoch [15923/20000], Training Loss: 0.00476475886327015, Validation Loss: 0.005598575482639659\n",
      "Epoch [15924/20000], Training Loss: 0.005416881068608096, Validation Loss: 0.003979330570529825\n",
      "Epoch [15925/20000], Training Loss: 0.004919480645858089, Validation Loss: 0.003908880000370167\n",
      "Epoch [15926/20000], Training Loss: 0.0040431470614359045, Validation Loss: 0.003229320583547032\n",
      "Epoch [15927/20000], Training Loss: 0.007278415563632734, Validation Loss: 0.003176698672009965\n",
      "Epoch [15928/20000], Training Loss: 0.0036246333620510995, Validation Loss: 0.005286112710516357\n",
      "Epoch [15929/20000], Training Loss: 0.010651753392266983, Validation Loss: 0.003507917783077273\n",
      "Epoch [15930/20000], Training Loss: 0.014205143244388247, Validation Loss: 0.015133310606658356\n",
      "Epoch [15931/20000], Training Loss: 0.012992526923231449, Validation Loss: 0.008574777515606502\n",
      "Epoch [15932/20000], Training Loss: 0.008340929264834682, Validation Loss: 0.0030125355075298027\n",
      "Epoch [15933/20000], Training Loss: 0.008269874266781179, Validation Loss: 0.04895390144416271\n",
      "Epoch [15934/20000], Training Loss: 0.009589564448106103, Validation Loss: 0.005850345974561091\n",
      "Epoch [15935/20000], Training Loss: 0.007128901895027541, Validation Loss: 0.004764043539772866\n",
      "Epoch [15936/20000], Training Loss: 0.00621434878121363, Validation Loss: 0.017615704398070343\n",
      "Epoch [15937/20000], Training Loss: 0.010175957482065965, Validation Loss: 0.0027744219350876224\n",
      "Epoch [15938/20000], Training Loss: 0.004298545486692872, Validation Loss: 0.002763471571272971\n",
      "Epoch [15939/20000], Training Loss: 0.0048785715334815904, Validation Loss: 0.006326399386001357\n",
      "Epoch [15940/20000], Training Loss: 0.004268010708204072, Validation Loss: 0.003869182638183939\n",
      "Epoch [15941/20000], Training Loss: 0.006285439335832572, Validation Loss: 0.010338780759151891\n",
      "Epoch [15942/20000], Training Loss: 0.0113434565881302, Validation Loss: 0.022227566689253464\n",
      "Epoch [15943/20000], Training Loss: 0.01460770165512518, Validation Loss: 0.010324921592006082\n",
      "Epoch [15944/20000], Training Loss: 0.0056511450778283946, Validation Loss: 0.003166012801078263\n",
      "Epoch [15945/20000], Training Loss: 0.006689754969556816, Validation Loss: 0.013011014355080808\n",
      "Epoch [15946/20000], Training Loss: 0.01257688004989177, Validation Loss: 0.008015097900560184\n",
      "Epoch [15947/20000], Training Loss: 0.010041126516755736, Validation Loss: 0.0075416051173421905\n",
      "Epoch [15948/20000], Training Loss: 0.012257510335725133, Validation Loss: 0.009228221539944386\n",
      "Epoch [15949/20000], Training Loss: 0.014051412467389517, Validation Loss: 0.00203808771128123\n",
      "Epoch [15950/20000], Training Loss: 0.006414803636679218, Validation Loss: 0.004635455585831944\n",
      "Epoch [15951/20000], Training Loss: 0.004921606852024395, Validation Loss: 0.009935322891221534\n",
      "Epoch [15952/20000], Training Loss: 0.004552155503136289, Validation Loss: 0.021196186010326707\n",
      "Epoch [15953/20000], Training Loss: 0.01746023504321264, Validation Loss: 0.040760115321193426\n",
      "Epoch [15954/20000], Training Loss: 0.06649984685827803, Validation Loss: 0.024355197591441093\n",
      "Epoch [15955/20000], Training Loss: 0.01930175838359511, Validation Loss: 0.004858709887481716\n",
      "Epoch [15956/20000], Training Loss: 0.007913173895628591, Validation Loss: 0.0033542283509032495\n",
      "Epoch [15957/20000], Training Loss: 0.004210209704134675, Validation Loss: 0.007442661960451547\n",
      "Epoch [15958/20000], Training Loss: 0.010569933498897757, Validation Loss: 0.002889811071997071\n",
      "Epoch [15959/20000], Training Loss: 0.01019368418841233, Validation Loss: 0.006750096837990713\n",
      "Epoch [15960/20000], Training Loss: 0.005002538021861775, Validation Loss: 0.00333137144091235\n",
      "Epoch [15961/20000], Training Loss: 0.004368691212481021, Validation Loss: 0.005187560983055205\n",
      "Epoch [15962/20000], Training Loss: 0.004677714567281198, Validation Loss: 0.042068135525499165\n",
      "Epoch [15963/20000], Training Loss: 0.012364296472761842, Validation Loss: 0.004322083278558173\n",
      "Epoch [15964/20000], Training Loss: 0.007031711300701967, Validation Loss: 0.01496929808386826\n",
      "Epoch [15965/20000], Training Loss: 0.016218059545280994, Validation Loss: 0.01644560481820823\n",
      "Epoch [15966/20000], Training Loss: 0.012892880834572549, Validation Loss: 0.006889540051133736\n",
      "Epoch [15967/20000], Training Loss: 0.008784754058329522, Validation Loss: 0.0071029949847785535\n",
      "Epoch [15968/20000], Training Loss: 0.012263475162658974, Validation Loss: 0.004057089062038293\n",
      "Epoch [15969/20000], Training Loss: 0.0055155606491358155, Validation Loss: 0.004567239169037451\n",
      "Epoch [15970/20000], Training Loss: 0.006542295105256406, Validation Loss: 0.006407546444971746\n",
      "Epoch [15971/20000], Training Loss: 0.013928850257538474, Validation Loss: 0.06336240470409393\n",
      "Epoch [15972/20000], Training Loss: 0.03830923269457084, Validation Loss: 0.004912608286735417\n",
      "Epoch [15973/20000], Training Loss: 0.008691442474758202, Validation Loss: 0.004751301049044559\n",
      "Epoch [15974/20000], Training Loss: 0.006854428373376972, Validation Loss: 0.006007714729372373\n",
      "Epoch [15975/20000], Training Loss: 0.015793118038605565, Validation Loss: 0.00463662750647466\n",
      "Epoch [15976/20000], Training Loss: 0.0072456965213599945, Validation Loss: 0.007067414050539573\n",
      "Epoch [15977/20000], Training Loss: 0.005572812689956257, Validation Loss: 0.0034588257130597606\n",
      "Epoch [15978/20000], Training Loss: 0.006630929752386042, Validation Loss: 0.012667674757157752\n",
      "Epoch [15979/20000], Training Loss: 0.016873022167016773, Validation Loss: 0.017721550539136106\n",
      "Epoch [15980/20000], Training Loss: 0.0077704738042874465, Validation Loss: 0.004203578757457768\n",
      "Epoch [15981/20000], Training Loss: 0.006431063506170176, Validation Loss: 0.0056273344784524915\n",
      "Epoch [15982/20000], Training Loss: 0.004564095677259112, Validation Loss: 0.008943590335548464\n",
      "Epoch [15983/20000], Training Loss: 0.004475792451557936, Validation Loss: 0.006340099500083137\n",
      "Epoch [15984/20000], Training Loss: 0.007574737084464037, Validation Loss: 0.003948984070911745\n",
      "Epoch [15985/20000], Training Loss: 0.005226932607391583, Validation Loss: 0.00859682042417887\n",
      "Epoch [15986/20000], Training Loss: 0.006662603617631713, Validation Loss: 0.0027091824467945854\n",
      "Epoch [15987/20000], Training Loss: 0.004918244251582239, Validation Loss: 0.003612832039126244\n",
      "Epoch [15988/20000], Training Loss: 0.006501649905268485, Validation Loss: 0.0030520006360663337\n",
      "Epoch [15989/20000], Training Loss: 0.004682083297146684, Validation Loss: 0.002396550055618337\n",
      "Epoch [15990/20000], Training Loss: 0.0046485113354200235, Validation Loss: 0.0021244334455036073\n",
      "Epoch [15991/20000], Training Loss: 0.004346184602711999, Validation Loss: 0.00257054256382586\n",
      "Epoch [15992/20000], Training Loss: 0.00421689106432106, Validation Loss: 0.009024097717234544\n",
      "Epoch [15993/20000], Training Loss: 0.012047523313542894, Validation Loss: 0.0040025363746982925\n",
      "Epoch [15994/20000], Training Loss: 0.03672875847288586, Validation Loss: 0.03049841918462854\n",
      "Epoch [15995/20000], Training Loss: 0.03308695555564815, Validation Loss: 0.0031482059639556604\n",
      "Epoch [15996/20000], Training Loss: 0.011234670216034763, Validation Loss: 0.010429806142534637\n",
      "Epoch [15997/20000], Training Loss: 0.011281446643157065, Validation Loss: 0.007480685733529439\n",
      "Epoch [15998/20000], Training Loss: 0.0063751038416220195, Validation Loss: 0.00607540831294427\n",
      "Epoch [15999/20000], Training Loss: 0.007859214574896864, Validation Loss: 0.004108322366554213\n",
      "Epoch [16000/20000], Training Loss: 0.006063742613020752, Validation Loss: 0.005848538666618892\n",
      "Epoch [16001/20000], Training Loss: 0.007476362232150028, Validation Loss: 0.014618519096919564\n",
      "Epoch [16002/20000], Training Loss: 0.01130687873808256, Validation Loss: 0.004991555004524892\n",
      "Epoch [16003/20000], Training Loss: 0.01481794309295635, Validation Loss: 0.016805050096569935\n",
      "Epoch [16004/20000], Training Loss: 0.027810846794766673, Validation Loss: 0.04622989573648998\n",
      "Epoch [16005/20000], Training Loss: 0.033350656199867705, Validation Loss: 0.026333242790142677\n",
      "Epoch [16006/20000], Training Loss: 0.02246215695465383, Validation Loss: 0.006905684915758619\n",
      "Epoch [16007/20000], Training Loss: 0.012382582267101887, Validation Loss: 0.009696153664111315\n",
      "Epoch [16008/20000], Training Loss: 0.013250738407285618, Validation Loss: 0.006650716938424928\n",
      "Epoch [16009/20000], Training Loss: 0.008535912245861255, Validation Loss: 0.004084029763162356\n",
      "Epoch [16010/20000], Training Loss: 0.008784337514173655, Validation Loss: 0.007373790257491978\n",
      "Epoch [16011/20000], Training Loss: 0.006585683991423659, Validation Loss: 0.00555209680808511\n",
      "Epoch [16012/20000], Training Loss: 0.006024612512971673, Validation Loss: 0.0028702210668458683\n",
      "Epoch [16013/20000], Training Loss: 0.004093109767511903, Validation Loss: 0.004848610643751708\n",
      "Epoch [16014/20000], Training Loss: 0.0057182411869039895, Validation Loss: 0.002434543889663386\n",
      "Epoch [16015/20000], Training Loss: 0.008350914362901156, Validation Loss: 0.0031263031897654303\n",
      "Epoch [16016/20000], Training Loss: 0.005609199650020206, Validation Loss: 0.005614693210127641\n",
      "Epoch [16017/20000], Training Loss: 0.0050013662226514755, Validation Loss: 0.01041941531005751\n",
      "Epoch [16018/20000], Training Loss: 0.009999724827659, Validation Loss: 0.0037159998822323763\n",
      "Epoch [16019/20000], Training Loss: 0.00616070285832393, Validation Loss: 0.006090048367728481\n",
      "Epoch [16020/20000], Training Loss: 0.00828952488622495, Validation Loss: 0.0028721146562509553\n",
      "Epoch [16021/20000], Training Loss: 0.0063137685920245145, Validation Loss: 0.01270128041505879\n",
      "Epoch [16022/20000], Training Loss: 0.007855832706159813, Validation Loss: 0.006711750426733586\n",
      "Epoch [16023/20000], Training Loss: 0.008548942782324178, Validation Loss: 0.0022922314438429564\n",
      "Epoch [16024/20000], Training Loss: 0.004615662982230008, Validation Loss: 0.003918980080572153\n",
      "Epoch [16025/20000], Training Loss: 0.012080196721529188, Validation Loss: 0.008303803940572249\n",
      "Epoch [16026/20000], Training Loss: 0.006242872725644182, Validation Loss: 0.003871529412332312\n",
      "Epoch [16027/20000], Training Loss: 0.004819917770094305, Validation Loss: 0.004726600916437562\n",
      "Epoch [16028/20000], Training Loss: 0.005621759898661237, Validation Loss: 0.0020909663848592075\n",
      "Epoch [16029/20000], Training Loss: 0.006301489096326155, Validation Loss: 0.0040139504174430424\n",
      "Epoch [16030/20000], Training Loss: 0.006191615361915735, Validation Loss: 0.004973555713009188\n",
      "Epoch [16031/20000], Training Loss: 0.02000475784434944, Validation Loss: 0.037131649042878835\n",
      "Epoch [16032/20000], Training Loss: 0.009532376321788303, Validation Loss: 0.00400612834629961\n",
      "Epoch [16033/20000], Training Loss: 0.010416722451606932, Validation Loss: 0.013539809800152288\n",
      "Epoch [16034/20000], Training Loss: 0.009927828779263428, Validation Loss: 0.009650970635597817\n",
      "Epoch [16035/20000], Training Loss: 0.009776537566462398, Validation Loss: 0.005969624311244928\n",
      "Epoch [16036/20000], Training Loss: 0.02364492082389396, Validation Loss: 0.008692495391863394\n",
      "Epoch [16037/20000], Training Loss: 0.022609718444609177, Validation Loss: 0.024203690833285062\n",
      "Epoch [16038/20000], Training Loss: 0.010659716763951848, Validation Loss: 0.004270244723622099\n",
      "Epoch [16039/20000], Training Loss: 0.004926553972576845, Validation Loss: 0.003650962876033535\n",
      "Epoch [16040/20000], Training Loss: 0.004861877432891301, Validation Loss: 0.006766139263616579\n",
      "Epoch [16041/20000], Training Loss: 0.005367260521390043, Validation Loss: 0.00619910039629953\n",
      "Epoch [16042/20000], Training Loss: 0.008296380879723333, Validation Loss: 0.0056059392768005875\n",
      "Epoch [16043/20000], Training Loss: 0.007405905028073383, Validation Loss: 0.006665550039305046\n",
      "Epoch [16044/20000], Training Loss: 0.007604908211630702, Validation Loss: 0.004363965473042202\n",
      "Epoch [16045/20000], Training Loss: 0.0035048750154861147, Validation Loss: 0.0032886170591401712\n",
      "Epoch [16046/20000], Training Loss: 0.005676882429855011, Validation Loss: 0.0034699109050934742\n",
      "Epoch [16047/20000], Training Loss: 0.006045160141573953, Validation Loss: 0.0026633576994921785\n",
      "Epoch [16048/20000], Training Loss: 0.003373628703083601, Validation Loss: 0.003368230519967781\n",
      "Epoch [16049/20000], Training Loss: 0.004310804493538204, Validation Loss: 0.012060401132519669\n",
      "Epoch [16050/20000], Training Loss: 0.03416613748868258, Validation Loss: 0.006324757707533393\n",
      "Epoch [16051/20000], Training Loss: 0.0767105542347833, Validation Loss: 0.04323396193239607\n",
      "Epoch [16052/20000], Training Loss: 0.0664512421647357, Validation Loss: 0.044082529842855024\n",
      "Epoch [16053/20000], Training Loss: 0.02267217290188585, Validation Loss: 0.013889801062240232\n",
      "Epoch [16054/20000], Training Loss: 0.014916478306986392, Validation Loss: 0.010395828268753706\n",
      "Epoch [16055/20000], Training Loss: 0.008521954761818051, Validation Loss: 0.009083013627046301\n",
      "Epoch [16056/20000], Training Loss: 0.008873069394667255, Validation Loss: 0.00639938831804037\n",
      "Epoch [16057/20000], Training Loss: 0.006681812382469486, Validation Loss: 0.0058468514793515525\n",
      "Epoch [16058/20000], Training Loss: 0.00575170893820801, Validation Loss: 0.006550569065087886\n",
      "Epoch [16059/20000], Training Loss: 0.006465493172527204, Validation Loss: 0.00467694451920713\n",
      "Epoch [16060/20000], Training Loss: 0.005828865230016943, Validation Loss: 0.009174591228916793\n",
      "Epoch [16061/20000], Training Loss: 0.015131020174781693, Validation Loss: 0.01038451157630403\n",
      "Epoch [16062/20000], Training Loss: 0.017505383337265812, Validation Loss: 0.020249174375619277\n",
      "Epoch [16063/20000], Training Loss: 0.015213882857486689, Validation Loss: 0.0051721482527291585\n",
      "Epoch [16064/20000], Training Loss: 0.01054054597726203, Validation Loss: 0.011891106032732881\n",
      "Epoch [16065/20000], Training Loss: 0.008767784822599165, Validation Loss: 0.004885757655563043\n",
      "Epoch [16066/20000], Training Loss: 0.004610312570548558, Validation Loss: 0.004355323965161957\n",
      "Epoch [16067/20000], Training Loss: 0.006111060379355747, Validation Loss: 0.005664093291410881\n",
      "Epoch [16068/20000], Training Loss: 0.00855296670956055, Validation Loss: 0.00513752514964468\n",
      "Epoch [16069/20000], Training Loss: 0.01862812105017448, Validation Loss: 0.0042647410132719155\n",
      "Epoch [16070/20000], Training Loss: 0.008353686739740494, Validation Loss: 0.004341874499483617\n",
      "Epoch [16071/20000], Training Loss: 0.006427277052093164, Validation Loss: 0.005144747989147293\n",
      "Epoch [16072/20000], Training Loss: 0.0077845685092532745, Validation Loss: 0.003288882641225724\n",
      "Epoch [16073/20000], Training Loss: 0.011975958035951148, Validation Loss: 0.017656515751566224\n",
      "Epoch [16074/20000], Training Loss: 0.006616304834194254, Validation Loss: 0.0036710887716812288\n",
      "Epoch [16075/20000], Training Loss: 0.007311609211943245, Validation Loss: 0.008555990208944106\n",
      "Epoch [16076/20000], Training Loss: 0.00645226713511095, Validation Loss: 0.007930515733148376\n",
      "Epoch [16077/20000], Training Loss: 0.004155569084105082, Validation Loss: 0.0041056224976900755\n",
      "Epoch [16078/20000], Training Loss: 0.006074096192475865, Validation Loss: 0.008939191832074101\n",
      "Epoch [16079/20000], Training Loss: 0.005736981157107428, Validation Loss: 0.015423883162676637\n",
      "Epoch [16080/20000], Training Loss: 0.011483019220176314, Validation Loss: 0.010161342964108503\n",
      "Epoch [16081/20000], Training Loss: 0.008744049031780119, Validation Loss: 0.012993853805320699\n",
      "Epoch [16082/20000], Training Loss: 0.007921428147320901, Validation Loss: 0.003887073690759971\n",
      "Epoch [16083/20000], Training Loss: 0.005487916526492752, Validation Loss: 0.0031999843114666293\n",
      "Epoch [16084/20000], Training Loss: 0.009866642544563677, Validation Loss: 0.00364126206135063\n",
      "Epoch [16085/20000], Training Loss: 0.007871354521284957, Validation Loss: 0.0034504736041950463\n",
      "Epoch [16086/20000], Training Loss: 0.003579514184951092, Validation Loss: 0.002922819574383323\n",
      "Epoch [16087/20000], Training Loss: 0.0038678216938126753, Validation Loss: 0.0036155333010751355\n",
      "Epoch [16088/20000], Training Loss: 0.00723628792911768, Validation Loss: 0.00599613416540836\n",
      "Epoch [16089/20000], Training Loss: 0.017036986453084473, Validation Loss: 0.003224692852070362\n",
      "Epoch [16090/20000], Training Loss: 0.005790044503393281, Validation Loss: 0.0095621400645801\n",
      "Epoch [16091/20000], Training Loss: 0.009225207246581617, Validation Loss: 0.003115827450593029\n",
      "Epoch [16092/20000], Training Loss: 0.007301769958368303, Validation Loss: 0.00975257517004456\n",
      "Epoch [16093/20000], Training Loss: 0.014991502233897336, Validation Loss: 0.010883398696283391\n",
      "Epoch [16094/20000], Training Loss: 0.01758657972888094, Validation Loss: 0.0037775930416371157\n",
      "Epoch [16095/20000], Training Loss: 0.011838092803373002, Validation Loss: 0.016408010652022704\n",
      "Epoch [16096/20000], Training Loss: 0.011555077403110252, Validation Loss: 0.009306988550136586\n",
      "Epoch [16097/20000], Training Loss: 0.008206098918890348, Validation Loss: 0.005340078244663411\n",
      "Epoch [16098/20000], Training Loss: 0.006687706731359607, Validation Loss: 0.003828068061658791\n",
      "Epoch [16099/20000], Training Loss: 0.006833183620723763, Validation Loss: 0.01073932694540807\n",
      "Epoch [16100/20000], Training Loss: 0.00982570691381365, Validation Loss: 0.003600899106073387\n",
      "Epoch [16101/20000], Training Loss: 0.004298827866607878, Validation Loss: 0.0030969320365290542\n",
      "Epoch [16102/20000], Training Loss: 0.007356541755143553, Validation Loss: 0.002401821567769754\n",
      "Epoch [16103/20000], Training Loss: 0.007212316966095906, Validation Loss: 0.0035913919087745072\n",
      "Epoch [16104/20000], Training Loss: 0.007713380970277025, Validation Loss: 0.003748357314121011\n",
      "Epoch [16105/20000], Training Loss: 0.011522211369471133, Validation Loss: 0.006362716696233304\n",
      "Epoch [16106/20000], Training Loss: 0.02158040251742932, Validation Loss: 0.008120076465155019\n",
      "Epoch [16107/20000], Training Loss: 0.009799668108046587, Validation Loss: 0.002963880292598325\n",
      "Epoch [16108/20000], Training Loss: 0.02382348527109051, Validation Loss: 0.017198189601259233\n",
      "Epoch [16109/20000], Training Loss: 0.03174889923528618, Validation Loss: 0.011090935068074898\n",
      "Epoch [16110/20000], Training Loss: 0.007391641968362299, Validation Loss: 0.005084815179437945\n",
      "Epoch [16111/20000], Training Loss: 0.009075325296310308, Validation Loss: 0.007307830542850383\n",
      "Epoch [16112/20000], Training Loss: 0.007274112150038751, Validation Loss: 0.007360090546780741\n",
      "Epoch [16113/20000], Training Loss: 0.007495375296067712, Validation Loss: 0.0030485166402586727\n",
      "Epoch [16114/20000], Training Loss: 0.005311020009685308, Validation Loss: 0.0044947032636999396\n",
      "Epoch [16115/20000], Training Loss: 0.007399737373426822, Validation Loss: 0.002796812965454722\n",
      "Epoch [16116/20000], Training Loss: 0.008245184217230417, Validation Loss: 0.005641138708302914\n",
      "Epoch [16117/20000], Training Loss: 0.007782603049365987, Validation Loss: 0.007537588514326542\n",
      "Epoch [16118/20000], Training Loss: 0.008982157045725867, Validation Loss: 0.004265094494477191\n",
      "Epoch [16119/20000], Training Loss: 0.00735503519120227, Validation Loss: 0.003252512426524642\n",
      "Epoch [16120/20000], Training Loss: 0.011409181603531644, Validation Loss: 0.008217103844977623\n",
      "Epoch [16121/20000], Training Loss: 0.006384861501698781, Validation Loss: 0.006105656593204246\n",
      "Epoch [16122/20000], Training Loss: 0.01170434780864785, Validation Loss: 0.004740980262965877\n",
      "Epoch [16123/20000], Training Loss: 0.012327718392563318, Validation Loss: 0.003007851779784687\n",
      "Epoch [16124/20000], Training Loss: 0.019282618294320337, Validation Loss: 0.014415419883724618\n",
      "Epoch [16125/20000], Training Loss: 0.024458672337849357, Validation Loss: 0.007085594669106889\n",
      "Epoch [16126/20000], Training Loss: 0.010376560296143518, Validation Loss: 0.004074597641277933\n",
      "Epoch [16127/20000], Training Loss: 0.008900731561360382, Validation Loss: 0.005124692668587727\n",
      "Epoch [16128/20000], Training Loss: 0.006206152511107835, Validation Loss: 0.010364821718462777\n",
      "Epoch [16129/20000], Training Loss: 0.005060882291997716, Validation Loss: 0.007822817194063905\n",
      "Epoch [16130/20000], Training Loss: 0.01091244397373105, Validation Loss: 0.004021666667658305\n",
      "Epoch [16131/20000], Training Loss: 0.018292556761480228, Validation Loss: 0.03584234416511894\n",
      "Epoch [16132/20000], Training Loss: 0.02989754154727312, Validation Loss: 0.06126301203457827\n",
      "Epoch [16133/20000], Training Loss: 0.03441024165035093, Validation Loss: 0.009252443003268647\n",
      "Epoch [16134/20000], Training Loss: 0.01718294358579442, Validation Loss: 0.007604201150960372\n",
      "Epoch [16135/20000], Training Loss: 0.009270701886342587, Validation Loss: 0.003930628836745395\n",
      "Epoch [16136/20000], Training Loss: 0.005094679265048236, Validation Loss: 0.008562700631743945\n",
      "Epoch [16137/20000], Training Loss: 0.00529890172245879, Validation Loss: 0.014463234736012964\n",
      "Epoch [16138/20000], Training Loss: 0.01146804792110093, Validation Loss: 0.006155848361621403\n",
      "Epoch [16139/20000], Training Loss: 0.006251255673955062, Validation Loss: 0.009485099419415116\n",
      "Epoch [16140/20000], Training Loss: 0.010609230832155194, Validation Loss: 0.004165508560926615\n",
      "Epoch [16141/20000], Training Loss: 0.007789401380380566, Validation Loss: 0.005839599213313055\n",
      "Epoch [16142/20000], Training Loss: 0.005220501530727363, Validation Loss: 0.009143697076224136\n",
      "Epoch [16143/20000], Training Loss: 0.010227127400867175, Validation Loss: 0.0052994602786315325\n",
      "Epoch [16144/20000], Training Loss: 0.0189769869279449, Validation Loss: 0.011063413746082265\n",
      "Epoch [16145/20000], Training Loss: 0.021773183618539145, Validation Loss: 0.017247357419234634\n",
      "Epoch [16146/20000], Training Loss: 0.02466620464942285, Validation Loss: 0.01043659494925667\n",
      "Epoch [16147/20000], Training Loss: 0.020732654199361735, Validation Loss: 0.039935806618554386\n",
      "Epoch [16148/20000], Training Loss: 0.04313770097256305, Validation Loss: 0.012252045128941388\n",
      "Epoch [16149/20000], Training Loss: 0.026440691223667403, Validation Loss: 0.009260648615054379\n",
      "Epoch [16150/20000], Training Loss: 0.008828571373929403, Validation Loss: 0.006414151091299962\n",
      "Epoch [16151/20000], Training Loss: 0.007847343850049324, Validation Loss: 0.005965560939914992\n",
      "Epoch [16152/20000], Training Loss: 0.005852751870406792, Validation Loss: 0.00794173345482412\n",
      "Epoch [16153/20000], Training Loss: 0.005852642267460136, Validation Loss: 0.004698725188956944\n",
      "Epoch [16154/20000], Training Loss: 0.006496773070206733, Validation Loss: 0.004030977083134856\n",
      "Epoch [16155/20000], Training Loss: 0.004083559453387612, Validation Loss: 0.004633597202592503\n",
      "Epoch [16156/20000], Training Loss: 0.003801381780898997, Validation Loss: 0.0033640730196355306\n",
      "Epoch [16157/20000], Training Loss: 0.004798393981348324, Validation Loss: 0.003671882739346139\n",
      "Epoch [16158/20000], Training Loss: 0.004404331179102883, Validation Loss: 0.004433961854169459\n",
      "Epoch [16159/20000], Training Loss: 0.004937603678886912, Validation Loss: 0.007830568544834802\n",
      "Epoch [16160/20000], Training Loss: 0.006684495587218667, Validation Loss: 0.00453802329932402\n",
      "Epoch [16161/20000], Training Loss: 0.004871887062888293, Validation Loss: 0.0034451075845822743\n",
      "Epoch [16162/20000], Training Loss: 0.00432045088850178, Validation Loss: 0.0028588559890018977\n",
      "Epoch [16163/20000], Training Loss: 0.004828631297056875, Validation Loss: 0.002741639368476204\n",
      "Epoch [16164/20000], Training Loss: 0.0031647263813100934, Validation Loss: 0.003032324474628264\n",
      "Epoch [16165/20000], Training Loss: 0.003927803473613624, Validation Loss: 0.0037356755941410874\n",
      "Epoch [16166/20000], Training Loss: 0.003934406238840893, Validation Loss: 0.009079482549540604\n",
      "Epoch [16167/20000], Training Loss: 0.006973031033599649, Validation Loss: 0.0030645777488852283\n",
      "Epoch [16168/20000], Training Loss: 0.010129647664143704, Validation Loss: 0.002199937409936784\n",
      "Epoch [16169/20000], Training Loss: 0.0065947164579743655, Validation Loss: 0.008545491912826305\n",
      "Epoch [16170/20000], Training Loss: 0.004500224376637821, Validation Loss: 0.006212772321329599\n",
      "Epoch [16171/20000], Training Loss: 0.005799586578827335, Validation Loss: 0.002571811203616952\n",
      "Epoch [16172/20000], Training Loss: 0.0035506105476607835, Validation Loss: 0.002739892656895702\n",
      "Epoch [16173/20000], Training Loss: 0.005435978455352597, Validation Loss: 0.002214989332541352\n",
      "Epoch [16174/20000], Training Loss: 0.004279279640676188, Validation Loss: 0.0021157752014409376\n",
      "Epoch [16175/20000], Training Loss: 0.0029000658053389217, Validation Loss: 0.002064511674002948\n",
      "Epoch [16176/20000], Training Loss: 0.004524913317124758, Validation Loss: 0.015568823287540284\n",
      "Epoch [16177/20000], Training Loss: 0.011451634079484003, Validation Loss: 0.004892091135866111\n",
      "Epoch [16178/20000], Training Loss: 0.013989529805197012, Validation Loss: 0.008199201966640797\n",
      "Epoch [16179/20000], Training Loss: 0.027607574644467343, Validation Loss: 0.0935370686947538\n",
      "Epoch [16180/20000], Training Loss: 0.0587903723997962, Validation Loss: 0.05208830888178844\n",
      "Epoch [16181/20000], Training Loss: 0.04975542624015361, Validation Loss: 0.005809031368633961\n",
      "Epoch [16182/20000], Training Loss: 0.03289889871874558, Validation Loss: 0.020715553978724904\n",
      "Epoch [16183/20000], Training Loss: 0.024544590377315347, Validation Loss: 0.012392463347491096\n",
      "Epoch [16184/20000], Training Loss: 0.0157800963011141, Validation Loss: 0.02335644907374704\n",
      "Epoch [16185/20000], Training Loss: 0.011787723292530115, Validation Loss: 0.006973574837039516\n",
      "Epoch [16186/20000], Training Loss: 0.006737341077366311, Validation Loss: 0.005872489453898717\n",
      "Epoch [16187/20000], Training Loss: 0.00721272274859075, Validation Loss: 0.007637803638187636\n",
      "Epoch [16188/20000], Training Loss: 0.00638851073537288, Validation Loss: 0.0042659593172434795\n",
      "Epoch [16189/20000], Training Loss: 0.00541145168348781, Validation Loss: 0.003923144397552798\n",
      "Epoch [16190/20000], Training Loss: 0.005980271483297527, Validation Loss: 0.005328622079600466\n",
      "Epoch [16191/20000], Training Loss: 0.005883856312461181, Validation Loss: 0.004086382537118587\n",
      "Epoch [16192/20000], Training Loss: 0.011221725764958787, Validation Loss: 0.007555374135530916\n",
      "Epoch [16193/20000], Training Loss: 0.008667480362679012, Validation Loss: 0.0055765559115518015\n",
      "Epoch [16194/20000], Training Loss: 0.007932821533781212, Validation Loss: 0.01063371484614655\n",
      "Epoch [16195/20000], Training Loss: 0.010771843188974475, Validation Loss: 0.004282946565155044\n",
      "Epoch [16196/20000], Training Loss: 0.00669728694629157, Validation Loss: 0.011631263470332926\n",
      "Epoch [16197/20000], Training Loss: 0.007446099086175995, Validation Loss: 0.00399948691382893\n",
      "Epoch [16198/20000], Training Loss: 0.0047697262859559076, Validation Loss: 0.004910589298196909\n",
      "Epoch [16199/20000], Training Loss: 0.007784210245258042, Validation Loss: 0.005628211773665618\n",
      "Epoch [16200/20000], Training Loss: 0.01561680131331806, Validation Loss: 0.0036516827032236116\n",
      "Epoch [16201/20000], Training Loss: 0.013196154903978043, Validation Loss: 0.01051377463006377\n",
      "Epoch [16202/20000], Training Loss: 0.008262631303036219, Validation Loss: 0.0032203885938681003\n",
      "Epoch [16203/20000], Training Loss: 0.004493109210410954, Validation Loss: 0.00339599900324369\n",
      "Epoch [16204/20000], Training Loss: 0.005544731434513649, Validation Loss: 0.006888121177228511\n",
      "Epoch [16205/20000], Training Loss: 0.005389933632354119, Validation Loss: 0.006478458427103284\n",
      "Epoch [16206/20000], Training Loss: 0.00618958890637649, Validation Loss: 0.004488869647400203\n",
      "Epoch [16207/20000], Training Loss: 0.006233322257425503, Validation Loss: 0.0036301304898062525\n",
      "Epoch [16208/20000], Training Loss: 0.006022784147977031, Validation Loss: 0.003906242499437797\n",
      "Epoch [16209/20000], Training Loss: 0.007298399872736029, Validation Loss: 0.0027258431718268283\n",
      "Epoch [16210/20000], Training Loss: 0.009636411531477762, Validation Loss: 0.004654849321001718\n",
      "Epoch [16211/20000], Training Loss: 0.006970167407416739, Validation Loss: 0.01011022506285717\n",
      "Epoch [16212/20000], Training Loss: 0.007131317160591217, Validation Loss: 0.00695494174910197\n",
      "Epoch [16213/20000], Training Loss: 0.007706714746226291, Validation Loss: 0.004244879007740922\n",
      "Epoch [16214/20000], Training Loss: 0.005260014684089194, Validation Loss: 0.012210463905442583\n",
      "Epoch [16215/20000], Training Loss: 0.007171913619718647, Validation Loss: 0.004654685414019636\n",
      "Epoch [16216/20000], Training Loss: 0.013172214808167024, Validation Loss: 0.012565836077637218\n",
      "Epoch [16217/20000], Training Loss: 0.0302585898025427, Validation Loss: 0.002713387611598656\n",
      "Epoch [16218/20000], Training Loss: 0.016218798848318068, Validation Loss: 0.005297726116354802\n",
      "Epoch [16219/20000], Training Loss: 0.02186392221899171, Validation Loss: 0.007967753940759468\n",
      "Epoch [16220/20000], Training Loss: 0.010242066587254937, Validation Loss: 0.005840495879307842\n",
      "Epoch [16221/20000], Training Loss: 0.016912696700144027, Validation Loss: 0.01870338645364564\n",
      "Epoch [16222/20000], Training Loss: 0.009252823416967917, Validation Loss: 0.009204309899448062\n",
      "Epoch [16223/20000], Training Loss: 0.0070986295428170365, Validation Loss: 0.00591289702179308\n",
      "Epoch [16224/20000], Training Loss: 0.00598899315394062, Validation Loss: 0.0034961231196157477\n",
      "Epoch [16225/20000], Training Loss: 0.0051188596657344276, Validation Loss: 0.0031909709183354706\n",
      "Epoch [16226/20000], Training Loss: 0.004780583258252591, Validation Loss: 0.003875272785908696\n",
      "Epoch [16227/20000], Training Loss: 0.0044057926257810975, Validation Loss: 0.008020534634332372\n",
      "Epoch [16228/20000], Training Loss: 0.006106432870150716, Validation Loss: 0.007591178415089376\n",
      "Epoch [16229/20000], Training Loss: 0.005348764031623432, Validation Loss: 0.006251022885859235\n",
      "Epoch [16230/20000], Training Loss: 0.00859868605246967, Validation Loss: 0.0031757166363429895\n",
      "Epoch [16231/20000], Training Loss: 0.006321791956971927, Validation Loss: 0.005905459413394202\n",
      "Epoch [16232/20000], Training Loss: 0.0073241576044194645, Validation Loss: 0.0028756664637705764\n",
      "Epoch [16233/20000], Training Loss: 0.005249249234826233, Validation Loss: 0.005069669240916548\n",
      "Epoch [16234/20000], Training Loss: 0.01234129716509155, Validation Loss: 0.02000637072641252\n",
      "Epoch [16235/20000], Training Loss: 0.007696003445778921, Validation Loss: 0.002171266461028308\n",
      "Epoch [16236/20000], Training Loss: 0.007288137322112627, Validation Loss: 0.0021575525390724287\n",
      "Epoch [16237/20000], Training Loss: 0.008329493259744985, Validation Loss: 0.0033702153120128537\n",
      "Epoch [16238/20000], Training Loss: 0.006028204457834363, Validation Loss: 0.0029762908539832417\n",
      "Epoch [16239/20000], Training Loss: 0.011096295483682184, Validation Loss: 0.0062279855621190415\n",
      "Epoch [16240/20000], Training Loss: 0.010390308066104938, Validation Loss: 0.00728469880400289\n",
      "Epoch [16241/20000], Training Loss: 0.008118842323006399, Validation Loss: 0.004573513264875075\n",
      "Epoch [16242/20000], Training Loss: 0.0038856965911691077, Validation Loss: 0.002758293693387616\n",
      "Epoch [16243/20000], Training Loss: 0.007134762728778047, Validation Loss: 0.01597457252708941\n",
      "Epoch [16244/20000], Training Loss: 0.00642804940746698, Validation Loss: 0.005762771897086326\n",
      "Epoch [16245/20000], Training Loss: 0.005263580994713786, Validation Loss: 0.004900071075812341\n",
      "Epoch [16246/20000], Training Loss: 0.006876006028116015, Validation Loss: 0.004789340809931085\n",
      "Epoch [16247/20000], Training Loss: 0.004624411746460412, Validation Loss: 0.006458431877265168\n",
      "Epoch [16248/20000], Training Loss: 0.012457678563285819, Validation Loss: 0.003401523338462807\n",
      "Epoch [16249/20000], Training Loss: 0.011888607856235467, Validation Loss: 0.0042078970986038555\n",
      "Epoch [16250/20000], Training Loss: 0.006577943075431644, Validation Loss: 0.003386840136975524\n",
      "Epoch [16251/20000], Training Loss: 0.0043907832103806345, Validation Loss: 0.004760193581187114\n",
      "Epoch [16252/20000], Training Loss: 0.011460579426316795, Validation Loss: 0.008830991041504461\n",
      "Epoch [16253/20000], Training Loss: 0.004825771209393029, Validation Loss: 0.004786213803882551\n",
      "Epoch [16254/20000], Training Loss: 0.0039491907603925415, Validation Loss: 0.02832710510725584\n",
      "Epoch [16255/20000], Training Loss: 0.016612090594922586, Validation Loss: 0.014543721600464974\n",
      "Epoch [16256/20000], Training Loss: 0.011261420876378127, Validation Loss: 0.01149893192057659\n",
      "Epoch [16257/20000], Training Loss: 0.017946664978613365, Validation Loss: 0.009038282395134673\n",
      "Epoch [16258/20000], Training Loss: 0.01414093317336145, Validation Loss: 0.006552676743822872\n",
      "Epoch [16259/20000], Training Loss: 0.018995468163991194, Validation Loss: 0.014010899367609216\n",
      "Epoch [16260/20000], Training Loss: 0.014184899270601039, Validation Loss: 0.011898635387687151\n",
      "Epoch [16261/20000], Training Loss: 0.026244712947768027, Validation Loss: 0.01234928279050766\n",
      "Epoch [16262/20000], Training Loss: 0.01066829881537095, Validation Loss: 0.018306393708679036\n",
      "Epoch [16263/20000], Training Loss: 0.02158706757472828, Validation Loss: 0.012410935047750532\n",
      "Epoch [16264/20000], Training Loss: 0.014220332903538033, Validation Loss: 0.012215611733973968\n",
      "Epoch [16265/20000], Training Loss: 0.010497130811147923, Validation Loss: 0.01852696841316675\n",
      "Epoch [16266/20000], Training Loss: 0.010418128003871223, Validation Loss: 0.008060963703883317\n",
      "Epoch [16267/20000], Training Loss: 0.009624670807690043, Validation Loss: 0.005027610540968246\n",
      "Epoch [16268/20000], Training Loss: 0.005219643509397949, Validation Loss: 0.0029555083351401174\n",
      "Epoch [16269/20000], Training Loss: 0.0042281650519933045, Validation Loss: 0.005617867597107663\n",
      "Epoch [16270/20000], Training Loss: 0.007271611099505597, Validation Loss: 0.0036733565835152604\n",
      "Epoch [16271/20000], Training Loss: 0.0072186053792912775, Validation Loss: 0.006260590794292572\n",
      "Epoch [16272/20000], Training Loss: 0.008430197937832824, Validation Loss: 0.008089711424557496\n",
      "Epoch [16273/20000], Training Loss: 0.006819565076629682, Validation Loss: 0.004170848013964295\n",
      "Epoch [16274/20000], Training Loss: 0.0049257252747858206, Validation Loss: 0.00465060384420189\n",
      "Epoch [16275/20000], Training Loss: 0.00784471217983602, Validation Loss: 0.00336427708468019\n",
      "Epoch [16276/20000], Training Loss: 0.013879246754575953, Validation Loss: 0.006657169511546712\n",
      "Epoch [16277/20000], Training Loss: 0.01199356576190829, Validation Loss: 0.01938181991239882\n",
      "Epoch [16278/20000], Training Loss: 0.011105759212828812, Validation Loss: 0.0027194544059858028\n",
      "Epoch [16279/20000], Training Loss: 0.011684157226097471, Validation Loss: 0.0031488601693300416\n",
      "Epoch [16280/20000], Training Loss: 0.04052620264181834, Validation Loss: 0.007593040836473973\n",
      "Epoch [16281/20000], Training Loss: 0.06494904389658782, Validation Loss: 0.011700797405995087\n",
      "Epoch [16282/20000], Training Loss: 0.04074157102877507, Validation Loss: 0.006895282230678796\n",
      "Epoch [16283/20000], Training Loss: 0.015366147013472593, Validation Loss: 0.007939116416017506\n",
      "Epoch [16284/20000], Training Loss: 0.011114351185304778, Validation Loss: 0.006205781182740923\n",
      "Epoch [16285/20000], Training Loss: 0.009474518168190116, Validation Loss: 0.007239614734690544\n",
      "Epoch [16286/20000], Training Loss: 0.009020267653145961, Validation Loss: 0.004478994757391774\n",
      "Epoch [16287/20000], Training Loss: 0.007535532292552359, Validation Loss: 0.008712802323348765\n",
      "Epoch [16288/20000], Training Loss: 0.008394122664217971, Validation Loss: 0.019513710195436067\n",
      "Epoch [16289/20000], Training Loss: 0.010335057957230933, Validation Loss: 0.0033143109345863714\n",
      "Epoch [16290/20000], Training Loss: 0.0064942901330401325, Validation Loss: 0.007947315765380023\n",
      "Epoch [16291/20000], Training Loss: 0.010677252863907987, Validation Loss: 0.0039043858972718226\n",
      "Epoch [16292/20000], Training Loss: 0.005952562731775325, Validation Loss: 0.009213694084142585\n",
      "Epoch [16293/20000], Training Loss: 0.008380457266607013, Validation Loss: 0.007328185861532737\n",
      "Epoch [16294/20000], Training Loss: 0.010555462086423566, Validation Loss: 0.003396690950775597\n",
      "Epoch [16295/20000], Training Loss: 0.012486635150188314, Validation Loss: 0.0064747549417266525\n",
      "Epoch [16296/20000], Training Loss: 0.0072850820071477885, Validation Loss: 0.005194317620980168\n",
      "Epoch [16297/20000], Training Loss: 0.006000715405597086, Validation Loss: 0.018186717585194856\n",
      "Epoch [16298/20000], Training Loss: 0.017546624143765906, Validation Loss: 0.011090419070538917\n",
      "Epoch [16299/20000], Training Loss: 0.007811134678117274, Validation Loss: 0.004730765511794743\n",
      "Epoch [16300/20000], Training Loss: 0.010689030168578029, Validation Loss: 0.006356854477847459\n",
      "Epoch [16301/20000], Training Loss: 0.011079757700956958, Validation Loss: 0.004691262168443748\n",
      "Epoch [16302/20000], Training Loss: 0.009210377438908577, Validation Loss: 0.004007712334181893\n",
      "Epoch [16303/20000], Training Loss: 0.005520821695037219, Validation Loss: 0.0038933312682404774\n",
      "Epoch [16304/20000], Training Loss: 0.003928657528116933, Validation Loss: 0.0041084644005471615\n",
      "Epoch [16305/20000], Training Loss: 0.0056762047991339515, Validation Loss: 0.002599899931526904\n",
      "Epoch [16306/20000], Training Loss: 0.0042926280418344375, Validation Loss: 0.0033971361480951145\n",
      "Epoch [16307/20000], Training Loss: 0.004508609163181632, Validation Loss: 0.004279630030489605\n",
      "Epoch [16308/20000], Training Loss: 0.007431067862073958, Validation Loss: 0.006194540714724488\n",
      "Epoch [16309/20000], Training Loss: 0.009462167291401005, Validation Loss: 0.006574555579975693\n",
      "Epoch [16310/20000], Training Loss: 0.009701362935759659, Validation Loss: 0.007718363079858038\n",
      "Epoch [16311/20000], Training Loss: 0.012792896547970096, Validation Loss: 0.03708242252469995\n",
      "Epoch [16312/20000], Training Loss: 0.029224677136751, Validation Loss: 0.007431759223757288\n",
      "Epoch [16313/20000], Training Loss: 0.012695359806197562, Validation Loss: 0.003443648809999494\n",
      "Epoch [16314/20000], Training Loss: 0.007457337047432995, Validation Loss: 0.007774731317113037\n",
      "Epoch [16315/20000], Training Loss: 0.008871397751915668, Validation Loss: 0.0062046598022753585\n",
      "Epoch [16316/20000], Training Loss: 0.006655767759606793, Validation Loss: 0.0031209206546642626\n",
      "Epoch [16317/20000], Training Loss: 0.007591322170952708, Validation Loss: 0.004430126958652636\n",
      "Epoch [16318/20000], Training Loss: 0.006605866514291847, Validation Loss: 0.0027814800406369094\n",
      "Epoch [16319/20000], Training Loss: 0.006589594236824529, Validation Loss: 0.016383216449334485\n",
      "Epoch [16320/20000], Training Loss: 0.021341462619147933, Validation Loss: 0.014766131173658437\n",
      "Epoch [16321/20000], Training Loss: 0.036075516738362366, Validation Loss: 0.018498946557201634\n",
      "Epoch [16322/20000], Training Loss: 0.015682199883485737, Validation Loss: 0.045643775364884016\n",
      "Epoch [16323/20000], Training Loss: 0.028673309878545945, Validation Loss: 0.00795748311333812\n",
      "Epoch [16324/20000], Training Loss: 0.012441312740390589, Validation Loss: 0.009051266490863392\n",
      "Epoch [16325/20000], Training Loss: 0.010022662374207616, Validation Loss: 0.008964283003738924\n",
      "Epoch [16326/20000], Training Loss: 0.01147988141539307, Validation Loss: 0.014257091853973438\n",
      "Epoch [16327/20000], Training Loss: 0.007157641467399246, Validation Loss: 0.016335282343404953\n",
      "Epoch [16328/20000], Training Loss: 0.008141776466800366, Validation Loss: 0.0037167184125174962\n",
      "Epoch [16329/20000], Training Loss: 0.004357831070332655, Validation Loss: 0.003624650695915729\n",
      "Epoch [16330/20000], Training Loss: 0.006630060244268472, Validation Loss: 0.022845714237397778\n",
      "Epoch [16331/20000], Training Loss: 0.010290617386220089, Validation Loss: 0.01127995548241805\n",
      "Epoch [16332/20000], Training Loss: 0.006805361050997557, Validation Loss: 0.01864308065672286\n",
      "Epoch [16333/20000], Training Loss: 0.010905841516692558, Validation Loss: 0.007744492999466145\n",
      "Epoch [16334/20000], Training Loss: 0.0072817414022990435, Validation Loss: 0.010556549351414495\n",
      "Epoch [16335/20000], Training Loss: 0.014081988999480797, Validation Loss: 0.007722760494159177\n",
      "Epoch [16336/20000], Training Loss: 0.009231531435424196, Validation Loss: 0.00509655718592837\n",
      "Epoch [16337/20000], Training Loss: 0.005219034140671803, Validation Loss: 0.0037370269221647895\n",
      "Epoch [16338/20000], Training Loss: 0.007944387253181568, Validation Loss: 0.003957681682842511\n",
      "Epoch [16339/20000], Training Loss: 0.0079723738433261, Validation Loss: 0.003581819319283649\n",
      "Epoch [16340/20000], Training Loss: 0.014145470211328106, Validation Loss: 0.01577428411902864\n",
      "Epoch [16341/20000], Training Loss: 0.012063801134770204, Validation Loss: 0.012047474909780118\n",
      "Epoch [16342/20000], Training Loss: 0.011073616725168125, Validation Loss: 0.00620444367583214\n",
      "Epoch [16343/20000], Training Loss: 0.007177023720162522, Validation Loss: 0.0072177562013637075\n",
      "Epoch [16344/20000], Training Loss: 0.004438207532075467, Validation Loss: 0.010291130638181909\n",
      "Epoch [16345/20000], Training Loss: 0.01710311071760121, Validation Loss: 0.00540693292006316\n",
      "Epoch [16346/20000], Training Loss: 0.013182016222604684, Validation Loss: 0.017155771143744475\n",
      "Epoch [16347/20000], Training Loss: 0.012358004622780885, Validation Loss: 0.004471146516972827\n",
      "Epoch [16348/20000], Training Loss: 0.00843091318113563, Validation Loss: 0.0026676700370964162\n",
      "Epoch [16349/20000], Training Loss: 0.005445050059019455, Validation Loss: 0.003524449405253855\n",
      "Epoch [16350/20000], Training Loss: 0.004688103406546621, Validation Loss: 0.005062747705279337\n",
      "Epoch [16351/20000], Training Loss: 0.006949182288735756, Validation Loss: 0.006123185127210361\n",
      "Epoch [16352/20000], Training Loss: 0.007933103044706513, Validation Loss: 0.00702853062635508\n",
      "Epoch [16353/20000], Training Loss: 0.012829468902574652, Validation Loss: 0.0074922728069380705\n",
      "Epoch [16354/20000], Training Loss: 0.010742921511434491, Validation Loss: 0.018023101472872987\n",
      "Epoch [16355/20000], Training Loss: 0.02809088113073293, Validation Loss: 0.01787369586211948\n",
      "Epoch [16356/20000], Training Loss: 0.012551565450849012, Validation Loss: 0.010234818122605864\n",
      "Epoch [16357/20000], Training Loss: 0.02408135760924779, Validation Loss: 0.012507931038131954\n",
      "Epoch [16358/20000], Training Loss: 0.028708785238060437, Validation Loss: 0.046905389858431255\n",
      "Epoch [16359/20000], Training Loss: 0.014971266645131567, Validation Loss: 0.0073149603991709\n",
      "Epoch [16360/20000], Training Loss: 0.007078767219094776, Validation Loss: 0.006358209887626539\n",
      "Epoch [16361/20000], Training Loss: 0.0070488176601689444, Validation Loss: 0.0048384414148142495\n",
      "Epoch [16362/20000], Training Loss: 0.004369402558950242, Validation Loss: 0.0046483035227343605\n",
      "Epoch [16363/20000], Training Loss: 0.0051206677196984985, Validation Loss: 0.0033021466954161432\n",
      "Epoch [16364/20000], Training Loss: 0.005819412402031178, Validation Loss: 0.008666090301687131\n",
      "Epoch [16365/20000], Training Loss: 0.011128618897990756, Validation Loss: 0.004649267814816734\n",
      "Epoch [16366/20000], Training Loss: 0.004841525251062454, Validation Loss: 0.0032979522812378753\n",
      "Epoch [16367/20000], Training Loss: 0.004310206230002223, Validation Loss: 0.0028053419394495904\n",
      "Epoch [16368/20000], Training Loss: 0.0060056379531618275, Validation Loss: 0.004260053493088175\n",
      "Epoch [16369/20000], Training Loss: 0.009161282689352188, Validation Loss: 0.018389830806480063\n",
      "Epoch [16370/20000], Training Loss: 0.008623722462743899, Validation Loss: 0.004727526561534238\n",
      "Epoch [16371/20000], Training Loss: 0.006609213045781611, Validation Loss: 0.007770888487120832\n",
      "Epoch [16372/20000], Training Loss: 0.014793848111626826, Validation Loss: 0.008946648852478185\n",
      "Epoch [16373/20000], Training Loss: 0.016484550649952974, Validation Loss: 0.005135199432471284\n",
      "Epoch [16374/20000], Training Loss: 0.006123147543405594, Validation Loss: 0.003814693494900569\n",
      "Epoch [16375/20000], Training Loss: 0.007127582670112328, Validation Loss: 0.005360624194046133\n",
      "Epoch [16376/20000], Training Loss: 0.007828812838332462, Validation Loss: 0.004055552048688048\n",
      "Epoch [16377/20000], Training Loss: 0.004254210545994397, Validation Loss: 0.0031474624263521734\n",
      "Epoch [16378/20000], Training Loss: 0.005211173918464088, Validation Loss: 0.0062911289847258035\n",
      "Epoch [16379/20000], Training Loss: 0.012086241133926836, Validation Loss: 0.009937070918301316\n",
      "Epoch [16380/20000], Training Loss: 0.009547834889027789, Validation Loss: 0.006390984662197979\n",
      "Epoch [16381/20000], Training Loss: 0.008944211781324287, Validation Loss: 0.005747786087194332\n",
      "Epoch [16382/20000], Training Loss: 0.007627803442509113, Validation Loss: 0.005808638575225749\n",
      "Epoch [16383/20000], Training Loss: 0.004060036838512003, Validation Loss: 0.007605654006945255\n",
      "Epoch [16384/20000], Training Loss: 0.0039917160008501795, Validation Loss: 0.006876795169253975\n",
      "Epoch [16385/20000], Training Loss: 0.008752597379498184, Validation Loss: 0.0027391492259636963\n",
      "Epoch [16386/20000], Training Loss: 0.019591240776207997, Validation Loss: 0.0039065186165316845\n",
      "Epoch [16387/20000], Training Loss: 0.018788470096296805, Validation Loss: 0.012484261117486331\n",
      "Epoch [16388/20000], Training Loss: 0.05845395383533157, Validation Loss: 0.0061437234744174795\n",
      "Epoch [16389/20000], Training Loss: 0.07988334073784895, Validation Loss: 0.02479133176334512\n",
      "Epoch [16390/20000], Training Loss: 0.035385875255867304, Validation Loss: 0.04038328309858001\n",
      "Epoch [16391/20000], Training Loss: 0.02180601657996054, Validation Loss: 0.024726779142708568\n",
      "Epoch [16392/20000], Training Loss: 0.01971623448272502, Validation Loss: 0.01150222425771322\n",
      "Epoch [16393/20000], Training Loss: 0.014138879325141065, Validation Loss: 0.020557673690202005\n",
      "Epoch [16394/20000], Training Loss: 0.018044895941524634, Validation Loss: 0.0072481981603689394\n",
      "Epoch [16395/20000], Training Loss: 0.008970727166992478, Validation Loss: 0.006298800703928821\n",
      "Epoch [16396/20000], Training Loss: 0.0064058423532904795, Validation Loss: 0.005842787380836815\n",
      "Epoch [16397/20000], Training Loss: 0.00608384062278284, Validation Loss: 0.004115594473693156\n",
      "Epoch [16398/20000], Training Loss: 0.008822667919698038, Validation Loss: 0.0034759192505303205\n",
      "Epoch [16399/20000], Training Loss: 0.016391264487797992, Validation Loss: 0.009536805718510217\n",
      "Epoch [16400/20000], Training Loss: 0.009281704092115563, Validation Loss: 0.0032144048912964456\n",
      "Epoch [16401/20000], Training Loss: 0.007111103053570592, Validation Loss: 0.003793771672396267\n",
      "Epoch [16402/20000], Training Loss: 0.010712248339716877, Validation Loss: 0.003552882265755183\n",
      "Epoch [16403/20000], Training Loss: 0.008347368370907913, Validation Loss: 0.005022276762434481\n",
      "Epoch [16404/20000], Training Loss: 0.006205544584580431, Validation Loss: 0.0031559832711280933\n",
      "Epoch [16405/20000], Training Loss: 0.004636168586460242, Validation Loss: 0.003939815961320013\n",
      "Epoch [16406/20000], Training Loss: 0.004477278036834572, Validation Loss: 0.006043644234300213\n",
      "Epoch [16407/20000], Training Loss: 0.0047676554841018515, Validation Loss: 0.008582184611705195\n",
      "Epoch [16408/20000], Training Loss: 0.0067297892445432285, Validation Loss: 0.004278636749517847\n",
      "Epoch [16409/20000], Training Loss: 0.008008560627266499, Validation Loss: 0.0053682459959085395\n",
      "Epoch [16410/20000], Training Loss: 0.0053966621812183545, Validation Loss: 0.007979457273380388\n",
      "Epoch [16411/20000], Training Loss: 0.0059202063325106535, Validation Loss: 0.005372783041337925\n",
      "Epoch [16412/20000], Training Loss: 0.005864588986615412, Validation Loss: 0.008645142869341054\n",
      "Epoch [16413/20000], Training Loss: 0.0032098900696187877, Validation Loss: 0.011510381193444021\n",
      "Epoch [16414/20000], Training Loss: 0.006353189045122625, Validation Loss: 0.004711068360420535\n",
      "Epoch [16415/20000], Training Loss: 0.003979395175909823, Validation Loss: 0.0020151190480390824\n",
      "Epoch [16416/20000], Training Loss: 0.005034332857966157, Validation Loss: 0.0028790675181460707\n",
      "Epoch [16417/20000], Training Loss: 0.0036588296000315624, Validation Loss: 0.0048630677762192065\n",
      "Epoch [16418/20000], Training Loss: 0.006579216970879186, Validation Loss: 0.0032799187576220135\n",
      "Epoch [16419/20000], Training Loss: 0.0075292725767732395, Validation Loss: 0.0059398812917887\n",
      "Epoch [16420/20000], Training Loss: 0.01044629613160072, Validation Loss: 0.002191846832090505\n",
      "Epoch [16421/20000], Training Loss: 0.009206280309237107, Validation Loss: 0.016986514845573515\n",
      "Epoch [16422/20000], Training Loss: 0.014711285430686465, Validation Loss: 0.019307784603122168\n",
      "Epoch [16423/20000], Training Loss: 0.012829578423536856, Validation Loss: 0.002403742921184282\n",
      "Epoch [16424/20000], Training Loss: 0.07615792141950806, Validation Loss: 0.03069026778082064\n",
      "Epoch [16425/20000], Training Loss: 0.0383693565111441, Validation Loss: 0.14125772974479592\n",
      "Epoch [16426/20000], Training Loss: 0.08234267222828098, Validation Loss: 0.00874401781428445\n",
      "Epoch [16427/20000], Training Loss: 0.02574719235001664, Validation Loss: 0.012373858061066454\n",
      "Epoch [16428/20000], Training Loss: 0.012038057736520256, Validation Loss: 0.01477249955475557\n",
      "Epoch [16429/20000], Training Loss: 0.012441316498942407, Validation Loss: 0.00972121752613678\n",
      "Epoch [16430/20000], Training Loss: 0.008994655217975378, Validation Loss: 0.008016274630725613\n",
      "Epoch [16431/20000], Training Loss: 0.0082180884034772, Validation Loss: 0.008394828183306962\n",
      "Epoch [16432/20000], Training Loss: 0.00723874060037945, Validation Loss: 0.005929054201370069\n",
      "Epoch [16433/20000], Training Loss: 0.005890694783634639, Validation Loss: 0.004835380983934101\n",
      "Epoch [16434/20000], Training Loss: 0.005927137217603169, Validation Loss: 0.006306425391269751\n",
      "Epoch [16435/20000], Training Loss: 0.008659318282817756, Validation Loss: 0.007462457272988284\n",
      "Epoch [16436/20000], Training Loss: 0.009354147827252746, Validation Loss: 0.010500013861149324\n",
      "Epoch [16437/20000], Training Loss: 0.00847460717237222, Validation Loss: 0.008074007887856918\n",
      "Epoch [16438/20000], Training Loss: 0.006266705621133691, Validation Loss: 0.005328675294638937\n",
      "Epoch [16439/20000], Training Loss: 0.00785961280156958, Validation Loss: 0.004707807939086349\n",
      "Epoch [16440/20000], Training Loss: 0.007191871967474331, Validation Loss: 0.016996627540700724\n",
      "Epoch [16441/20000], Training Loss: 0.006870376178475064, Validation Loss: 0.01324057332229651\n",
      "Epoch [16442/20000], Training Loss: 0.01089239705210535, Validation Loss: 0.015859579676876255\n",
      "Epoch [16443/20000], Training Loss: 0.00874504962044804, Validation Loss: 0.01157955333692033\n",
      "Epoch [16444/20000], Training Loss: 0.007524796636321948, Validation Loss: 0.015620537925575004\n",
      "Epoch [16445/20000], Training Loss: 0.011711792016285472, Validation Loss: 0.003893081089298026\n",
      "Epoch [16446/20000], Training Loss: 0.004560609706488654, Validation Loss: 0.019635372714689892\n",
      "Epoch [16447/20000], Training Loss: 0.023560860658367995, Validation Loss: 0.02065456947940092\n",
      "Epoch [16448/20000], Training Loss: 0.08085351470539795, Validation Loss: 0.040305408082368946\n",
      "Epoch [16449/20000], Training Loss: 0.06055597072814375, Validation Loss: 0.07313491590165151\n",
      "Epoch [16450/20000], Training Loss: 0.0604798424506693, Validation Loss: 0.06675109980324968\n",
      "Epoch [16451/20000], Training Loss: 0.035102042860151936, Validation Loss: 0.016763755332379202\n",
      "Epoch [16452/20000], Training Loss: 0.013483888419744159, Validation Loss: 0.008956926748949323\n",
      "Epoch [16453/20000], Training Loss: 0.007869122167383986, Validation Loss: 0.006998166720742509\n",
      "Epoch [16454/20000], Training Loss: 0.007117950108035335, Validation Loss: 0.005766733954845092\n",
      "Epoch [16455/20000], Training Loss: 0.005428689290835921, Validation Loss: 0.005186637611554293\n",
      "Epoch [16456/20000], Training Loss: 0.0052351414675026065, Validation Loss: 0.005674030043857263\n",
      "Epoch [16457/20000], Training Loss: 0.007041962035665554, Validation Loss: 0.005735214590350678\n",
      "Epoch [16458/20000], Training Loss: 0.005496294823907582, Validation Loss: 0.0032438075005854572\n",
      "Epoch [16459/20000], Training Loss: 0.005656528964339357, Validation Loss: 0.004412200472780776\n",
      "Epoch [16460/20000], Training Loss: 0.00671502818503151, Validation Loss: 0.0029244192618084037\n",
      "Epoch [16461/20000], Training Loss: 0.017568569304525487, Validation Loss: 0.003682361073272641\n",
      "Epoch [16462/20000], Training Loss: 0.03149618198222015, Validation Loss: 0.012841740528325778\n",
      "Epoch [16463/20000], Training Loss: 0.011656581482384354, Validation Loss: 0.006516565356930316\n",
      "Epoch [16464/20000], Training Loss: 0.008142899770194032, Validation Loss: 0.008815043562305749\n",
      "Epoch [16465/20000], Training Loss: 0.008902571604786707, Validation Loss: 0.010608735761079782\n",
      "Epoch [16466/20000], Training Loss: 0.009613140897792099, Validation Loss: 0.007578056481263117\n",
      "Epoch [16467/20000], Training Loss: 0.01984480007636843, Validation Loss: 0.013863598685881178\n",
      "Epoch [16468/20000], Training Loss: 0.00847828480618773, Validation Loss: 0.004927906736968778\n",
      "Epoch [16469/20000], Training Loss: 0.005823450977914035, Validation Loss: 0.0041163361464562976\n",
      "Epoch [16470/20000], Training Loss: 0.007374248095272508, Validation Loss: 0.002817490346927798\n",
      "Epoch [16471/20000], Training Loss: 0.005380419594335503, Validation Loss: 0.010687529785140924\n",
      "Epoch [16472/20000], Training Loss: 0.013847057681622183, Validation Loss: 0.009237305330068054\n",
      "Epoch [16473/20000], Training Loss: 0.008417708131414006, Validation Loss: 0.003197967879199126\n",
      "Epoch [16474/20000], Training Loss: 0.009829005094486223, Validation Loss: 0.002780070097324077\n",
      "Epoch [16475/20000], Training Loss: 0.006978468665952927, Validation Loss: 0.005768456065587608\n",
      "Epoch [16476/20000], Training Loss: 0.008131398490929444, Validation Loss: 0.005280813614937025\n",
      "Epoch [16477/20000], Training Loss: 0.00611203022033026, Validation Loss: 0.004210624514624379\n",
      "Epoch [16478/20000], Training Loss: 0.0031536525673751775, Validation Loss: 0.0030201467538810384\n",
      "Epoch [16479/20000], Training Loss: 0.010853669112423501, Validation Loss: 0.01482324487220207\n",
      "Epoch [16480/20000], Training Loss: 0.03482642580846524, Validation Loss: 0.013652076928475967\n",
      "Epoch [16481/20000], Training Loss: 0.017605677800020203, Validation Loss: 0.007488876985480474\n",
      "Epoch [16482/20000], Training Loss: 0.010636289587377437, Validation Loss: 0.007613083405531752\n",
      "Epoch [16483/20000], Training Loss: 0.008498921316848802, Validation Loss: 0.004822411368813562\n",
      "Epoch [16484/20000], Training Loss: 0.006118411067810874, Validation Loss: 0.0026519092927474374\n",
      "Epoch [16485/20000], Training Loss: 0.003790898362889753, Validation Loss: 0.0028165480222566786\n",
      "Epoch [16486/20000], Training Loss: 0.004367714130012246, Validation Loss: 0.0028740096706678336\n",
      "Epoch [16487/20000], Training Loss: 0.005943928257299851, Validation Loss: 0.0031924896054980373\n",
      "Epoch [16488/20000], Training Loss: 0.0050596571329931195, Validation Loss: 0.0028994742596179434\n",
      "Epoch [16489/20000], Training Loss: 0.004067725329430719, Validation Loss: 0.008889423816913785\n",
      "Epoch [16490/20000], Training Loss: 0.006241066715249742, Validation Loss: 0.0031679931844946297\n",
      "Epoch [16491/20000], Training Loss: 0.0068304442867364356, Validation Loss: 0.003494302318119123\n",
      "Epoch [16492/20000], Training Loss: 0.01404455616258409, Validation Loss: 0.00730812518005384\n",
      "Epoch [16493/20000], Training Loss: 0.005066363147177201, Validation Loss: 0.0034692644689597557\n",
      "Epoch [16494/20000], Training Loss: 0.004354663917183643, Validation Loss: 0.0031759291243491005\n",
      "Epoch [16495/20000], Training Loss: 0.004709767704688212, Validation Loss: 0.0030775665278256004\n",
      "Epoch [16496/20000], Training Loss: 0.009025217372774412, Validation Loss: 0.0034605215180181386\n",
      "Epoch [16497/20000], Training Loss: 0.006626481393465967, Validation Loss: 0.002392314834247194\n",
      "Epoch [16498/20000], Training Loss: 0.003950667319908722, Validation Loss: 0.0029137119598627065\n",
      "Epoch [16499/20000], Training Loss: 0.009895535917556637, Validation Loss: 0.09200161465342457\n",
      "Epoch [16500/20000], Training Loss: 0.03761010046517705, Validation Loss: 0.019642258038645975\n",
      "Epoch [16501/20000], Training Loss: 0.03886820917458473, Validation Loss: 0.018958818502143666\n",
      "Epoch [16502/20000], Training Loss: 0.017097538071019307, Validation Loss: 0.017277025484613868\n",
      "Epoch [16503/20000], Training Loss: 0.014700838019572464, Validation Loss: 0.006418930123743315\n",
      "Epoch [16504/20000], Training Loss: 0.007810722762120089, Validation Loss: 0.004949126683852357\n",
      "Epoch [16505/20000], Training Loss: 0.004802643506471733, Validation Loss: 0.004371463786501043\n",
      "Epoch [16506/20000], Training Loss: 0.005070629833166355, Validation Loss: 0.004231785444752794\n",
      "Epoch [16507/20000], Training Loss: 0.004123763094737244, Validation Loss: 0.0037594794926627223\n",
      "Epoch [16508/20000], Training Loss: 0.004483639911216285, Validation Loss: 0.003343640331356923\n",
      "Epoch [16509/20000], Training Loss: 0.004056917463978087, Validation Loss: 0.003006421510865514\n",
      "Epoch [16510/20000], Training Loss: 0.005263548680580906, Validation Loss: 0.0026597093479340727\n",
      "Epoch [16511/20000], Training Loss: 0.007834984847639654, Validation Loss: 0.0027374767144578032\n",
      "Epoch [16512/20000], Training Loss: 0.007235981291419843, Validation Loss: 0.011719644867968003\n",
      "Epoch [16513/20000], Training Loss: 0.00489057643270436, Validation Loss: 0.0037447526091304007\n",
      "Epoch [16514/20000], Training Loss: 0.005211799899240889, Validation Loss: 0.014328970470809768\n",
      "Epoch [16515/20000], Training Loss: 0.017079748480422756, Validation Loss: 0.005138824032189808\n",
      "Epoch [16516/20000], Training Loss: 0.013620106510643382, Validation Loss: 0.008117514828758403\n",
      "Epoch [16517/20000], Training Loss: 0.015680538117261937, Validation Loss: 0.019379083078343486\n",
      "Epoch [16518/20000], Training Loss: 0.009930898714044036, Validation Loss: 0.004598692022804178\n",
      "Epoch [16519/20000], Training Loss: 0.007788008153771183, Validation Loss: 0.00760587169307166\n",
      "Epoch [16520/20000], Training Loss: 0.008396960749191098, Validation Loss: 0.0026572158389054834\n",
      "Epoch [16521/20000], Training Loss: 0.007604355752326748, Validation Loss: 0.033258394471369586\n",
      "Epoch [16522/20000], Training Loss: 0.012609699074216354, Validation Loss: 0.003227890294429666\n",
      "Epoch [16523/20000], Training Loss: 0.005481426598895008, Validation Loss: 0.005914446659252829\n",
      "Epoch [16524/20000], Training Loss: 0.005533676272412842, Validation Loss: 0.0029517335719577075\n",
      "Epoch [16525/20000], Training Loss: 0.005969846135745424, Validation Loss: 0.009134850864061914\n",
      "Epoch [16526/20000], Training Loss: 0.011812608988423432, Validation Loss: 0.010261875269603274\n",
      "Epoch [16527/20000], Training Loss: 0.0071377840135288095, Validation Loss: 0.005795106029665736\n",
      "Epoch [16528/20000], Training Loss: 0.012972075693791598, Validation Loss: 0.0113303499695538\n",
      "Epoch [16529/20000], Training Loss: 0.021865467299254045, Validation Loss: 0.004963504552748269\n",
      "Epoch [16530/20000], Training Loss: 0.00836996222554782, Validation Loss: 0.002937665643421683\n",
      "Epoch [16531/20000], Training Loss: 0.006816210869666455, Validation Loss: 0.00451261272517708\n",
      "Epoch [16532/20000], Training Loss: 0.006913657303812215, Validation Loss: 0.003287868876921277\n",
      "Epoch [16533/20000], Training Loss: 0.0037453780792670193, Validation Loss: 0.006316694710600487\n",
      "Epoch [16534/20000], Training Loss: 0.019492126118815838, Validation Loss: 0.011226121357363658\n",
      "Epoch [16535/20000], Training Loss: 0.009818566273159897, Validation Loss: 0.0034937184163936763\n",
      "Epoch [16536/20000], Training Loss: 0.007024522575583043, Validation Loss: 0.0029005481293840696\n",
      "Epoch [16537/20000], Training Loss: 0.003962461199989775, Validation Loss: 0.0061607396103268925\n",
      "Epoch [16538/20000], Training Loss: 0.005584094424453464, Validation Loss: 0.0027927198510175727\n",
      "Epoch [16539/20000], Training Loss: 0.005826388264332698, Validation Loss: 0.0033380461187602706\n",
      "Epoch [16540/20000], Training Loss: 0.009468703087934825, Validation Loss: 0.0031833264068384143\n",
      "Epoch [16541/20000], Training Loss: 0.012046441949288627, Validation Loss: 0.004222570935410151\n",
      "Epoch [16542/20000], Training Loss: 0.0061377140038010635, Validation Loss: 0.0067431395877231085\n",
      "Epoch [16543/20000], Training Loss: 0.006076030639113534, Validation Loss: 0.005969622089578154\n",
      "Epoch [16544/20000], Training Loss: 0.004565853641127303, Validation Loss: 0.0021790013124516894\n",
      "Epoch [16545/20000], Training Loss: 0.009776223884338313, Validation Loss: 0.001983033801104761\n",
      "Epoch [16546/20000], Training Loss: 0.003206997068575999, Validation Loss: 0.00649242565437004\n",
      "Epoch [16547/20000], Training Loss: 0.008797103797329595, Validation Loss: 0.03174711310699812\n",
      "Epoch [16548/20000], Training Loss: 0.024497370249394277, Validation Loss: 0.0526894257436652\n",
      "Epoch [16549/20000], Training Loss: 0.024139815259127926, Validation Loss: 0.005597203233280977\n",
      "Epoch [16550/20000], Training Loss: 0.006174029392630993, Validation Loss: 0.006128454497779135\n",
      "Epoch [16551/20000], Training Loss: 0.006687526234600227, Validation Loss: 0.00370755760394607\n",
      "Epoch [16552/20000], Training Loss: 0.01162441506286476, Validation Loss: 0.003221153882414602\n",
      "Epoch [16553/20000], Training Loss: 0.007755993031813497, Validation Loss: 0.0050233822014855425\n",
      "Epoch [16554/20000], Training Loss: 0.007509398220073698, Validation Loss: 0.00678124778379896\n",
      "Epoch [16555/20000], Training Loss: 0.004847345304434774, Validation Loss: 0.00885964530923703\n",
      "Epoch [16556/20000], Training Loss: 0.010081188485076251, Validation Loss: 0.0026626892251201987\n",
      "Epoch [16557/20000], Training Loss: 0.009384169165839142, Validation Loss: 0.010759415588438515\n",
      "Epoch [16558/20000], Training Loss: 0.006040722142123351, Validation Loss: 0.00568001062077265\n",
      "Epoch [16559/20000], Training Loss: 0.012768848617627424, Validation Loss: 0.0031497233023927034\n",
      "Epoch [16560/20000], Training Loss: 0.010758735847438012, Validation Loss: 0.006650026876766206\n",
      "Epoch [16561/20000], Training Loss: 0.009735233258522515, Validation Loss: 0.006590578007729521\n",
      "Epoch [16562/20000], Training Loss: 0.020030653693148843, Validation Loss: 0.016237154070820128\n",
      "Epoch [16563/20000], Training Loss: 0.01689586169231916, Validation Loss: 0.02405449508556304\n",
      "Epoch [16564/20000], Training Loss: 0.027580809194010465, Validation Loss: 0.013335374583095055\n",
      "Epoch [16565/20000], Training Loss: 0.013483630065041195, Validation Loss: 0.005856822012252191\n",
      "Epoch [16566/20000], Training Loss: 0.013854043279674702, Validation Loss: 0.004654446012444532\n",
      "Epoch [16567/20000], Training Loss: 0.010380808981218641, Validation Loss: 0.0038477184480538757\n",
      "Epoch [16568/20000], Training Loss: 0.006996755494024042, Validation Loss: 0.0034839511496220004\n",
      "Epoch [16569/20000], Training Loss: 0.00604207775073259, Validation Loss: 0.007546426595332197\n",
      "Epoch [16570/20000], Training Loss: 0.006366901809087722, Validation Loss: 0.008091486439456183\n",
      "Epoch [16571/20000], Training Loss: 0.007684036054083013, Validation Loss: 0.0035213431787361124\n",
      "Epoch [16572/20000], Training Loss: 0.005654513128060249, Validation Loss: 0.006928108067118791\n",
      "Epoch [16573/20000], Training Loss: 0.0058793687217075785, Validation Loss: 0.0035685412960495683\n",
      "Epoch [16574/20000], Training Loss: 0.0036457961388285704, Validation Loss: 0.004620083474699367\n",
      "Epoch [16575/20000], Training Loss: 0.010800034349293648, Validation Loss: 0.005022052409481095\n",
      "Epoch [16576/20000], Training Loss: 0.03598754295152113, Validation Loss: 0.05869500977652416\n",
      "Epoch [16577/20000], Training Loss: 0.0289980972916978, Validation Loss: 0.005874398701724705\n",
      "Epoch [16578/20000], Training Loss: 0.011565216850223286, Validation Loss: 0.0059271062303863096\n",
      "Epoch [16579/20000], Training Loss: 0.005918576839446489, Validation Loss: 0.004371811313447438\n",
      "Epoch [16580/20000], Training Loss: 0.00481292673378318, Validation Loss: 0.0040271996867893905\n",
      "Epoch [16581/20000], Training Loss: 0.006409498573475503, Validation Loss: 0.0036413946175702532\n",
      "Epoch [16582/20000], Training Loss: 0.0057261975889559835, Validation Loss: 0.0040637012913612314\n",
      "Epoch [16583/20000], Training Loss: 0.00458370457012539, Validation Loss: 0.004025659890054261\n",
      "Epoch [16584/20000], Training Loss: 0.004152744050121068, Validation Loss: 0.00353572392929631\n",
      "Epoch [16585/20000], Training Loss: 0.0056964548545823035, Validation Loss: 0.01267136101211822\n",
      "Epoch [16586/20000], Training Loss: 0.005527657164580242, Validation Loss: 0.0046453066918370435\n",
      "Epoch [16587/20000], Training Loss: 0.0053773379918961185, Validation Loss: 0.003969776943074012\n",
      "Epoch [16588/20000], Training Loss: 0.007250019933832975, Validation Loss: 0.003302345750903831\n",
      "Epoch [16589/20000], Training Loss: 0.008754122890033094, Validation Loss: 0.011739554149764079\n",
      "Epoch [16590/20000], Training Loss: 0.009341823242721148, Validation Loss: 0.008287148151014565\n",
      "Epoch [16591/20000], Training Loss: 0.010787753919690399, Validation Loss: 0.005759465441640019\n",
      "Epoch [16592/20000], Training Loss: 0.006377099367715086, Validation Loss: 0.008194506613488528\n",
      "Epoch [16593/20000], Training Loss: 0.006374726578672251, Validation Loss: 0.012787753183926857\n",
      "Epoch [16594/20000], Training Loss: 0.005304341812006896, Validation Loss: 0.007186540701831315\n",
      "Epoch [16595/20000], Training Loss: 0.008947641252198808, Validation Loss: 0.011701926647307974\n",
      "Epoch [16596/20000], Training Loss: 0.028341806202661246, Validation Loss: 0.02778581955678047\n",
      "Epoch [16597/20000], Training Loss: 0.017812909238273278, Validation Loss: 0.01299475855609802\n",
      "Epoch [16598/20000], Training Loss: 0.009102146529975081, Validation Loss: 0.004779680584791912\n",
      "Epoch [16599/20000], Training Loss: 0.0071960111054067966, Validation Loss: 0.004145442973955111\n",
      "Epoch [16600/20000], Training Loss: 0.004123273697846993, Validation Loss: 0.0033017609138856767\n",
      "Epoch [16601/20000], Training Loss: 0.005351869606654093, Validation Loss: 0.0035117903524755955\n",
      "Epoch [16602/20000], Training Loss: 0.004732739248928348, Validation Loss: 0.004786686285267799\n",
      "Epoch [16603/20000], Training Loss: 0.004404333572373227, Validation Loss: 0.0027650521880222064\n",
      "Epoch [16604/20000], Training Loss: 0.007396229709489229, Validation Loss: 0.005864380112325701\n",
      "Epoch [16605/20000], Training Loss: 0.022831281445125536, Validation Loss: 0.022334655412935587\n",
      "Epoch [16606/20000], Training Loss: 0.009485402926137405, Validation Loss: 0.0715095294373376\n",
      "Epoch [16607/20000], Training Loss: 0.038621814538990815, Validation Loss: 0.015348942684275954\n",
      "Epoch [16608/20000], Training Loss: 0.021447984465339687, Validation Loss: 0.005131986482400795\n",
      "Epoch [16609/20000], Training Loss: 0.008308795944283734, Validation Loss: 0.005466188376024506\n",
      "Epoch [16610/20000], Training Loss: 0.006310898006112049, Validation Loss: 0.0039057504293806924\n",
      "Epoch [16611/20000], Training Loss: 0.004329670234125972, Validation Loss: 0.004911454194892592\n",
      "Epoch [16612/20000], Training Loss: 0.005065779488566997, Validation Loss: 0.005963137639244736\n",
      "Epoch [16613/20000], Training Loss: 0.0051526193773107866, Validation Loss: 0.009719189123383593\n",
      "Epoch [16614/20000], Training Loss: 0.0060184400899743196, Validation Loss: 0.004257629924853482\n",
      "Epoch [16615/20000], Training Loss: 0.007048115716315806, Validation Loss: 0.002962347984898317\n",
      "Epoch [16616/20000], Training Loss: 0.012791818109690212, Validation Loss: 0.0030096391316557863\n",
      "Epoch [16617/20000], Training Loss: 0.012936657751976912, Validation Loss: 0.005465483997679649\n",
      "Epoch [16618/20000], Training Loss: 0.007330600957045265, Validation Loss: 0.004202932855293611\n",
      "Epoch [16619/20000], Training Loss: 0.008733018273362956, Validation Loss: 0.009512734200274943\n",
      "Epoch [16620/20000], Training Loss: 0.008134275775384074, Validation Loss: 0.027172081172466278\n",
      "Epoch [16621/20000], Training Loss: 0.015852472077126225, Validation Loss: 0.009780556878208058\n",
      "Epoch [16622/20000], Training Loss: 0.008405072352616116, Validation Loss: 0.009153527003135284\n",
      "Epoch [16623/20000], Training Loss: 0.011220875283470377, Validation Loss: 0.0030481986801285535\n",
      "Epoch [16624/20000], Training Loss: 0.018603792128227985, Validation Loss: 0.01169663029057639\n",
      "Epoch [16625/20000], Training Loss: 0.006684504151053261, Validation Loss: 0.005948270671069622\n",
      "Epoch [16626/20000], Training Loss: 0.0096320076970317, Validation Loss: 0.004605310636439494\n",
      "Epoch [16627/20000], Training Loss: 0.004321157102822326, Validation Loss: 0.0034325464429068716\n",
      "Epoch [16628/20000], Training Loss: 0.004463459293349713, Validation Loss: 0.004201644765479224\n",
      "Epoch [16629/20000], Training Loss: 0.029246997010237204, Validation Loss: 0.018765704972404974\n",
      "Epoch [16630/20000], Training Loss: 0.019823450557688766, Validation Loss: 0.015587329778879524\n",
      "Epoch [16631/20000], Training Loss: 0.009818423471837636, Validation Loss: 0.007266622902147678\n",
      "Epoch [16632/20000], Training Loss: 0.007131336348276562, Validation Loss: 0.010802811137363255\n",
      "Epoch [16633/20000], Training Loss: 0.007063950062607357, Validation Loss: 0.003674895473368776\n",
      "Epoch [16634/20000], Training Loss: 0.0074789868831836815, Validation Loss: 0.0033743575688650446\n",
      "Epoch [16635/20000], Training Loss: 0.007676169928994828, Validation Loss: 0.009499297962292985\n",
      "Epoch [16636/20000], Training Loss: 0.019620455240913413, Validation Loss: 0.004901808577913462\n",
      "Epoch [16637/20000], Training Loss: 0.004757097589650324, Validation Loss: 0.022698249202221632\n",
      "Epoch [16638/20000], Training Loss: 0.00956827596590821, Validation Loss: 0.005658174419482417\n",
      "Epoch [16639/20000], Training Loss: 0.007011592465070342, Validation Loss: 0.0033431107266499132\n",
      "Epoch [16640/20000], Training Loss: 0.0035471220346932697, Validation Loss: 0.005577867152196364\n",
      "Epoch [16641/20000], Training Loss: 0.0062250714088025105, Validation Loss: 0.004715761905264188\n",
      "Epoch [16642/20000], Training Loss: 0.0059671650906888574, Validation Loss: 0.0047704341370261106\n",
      "Epoch [16643/20000], Training Loss: 0.0047184475032346586, Validation Loss: 0.006049863129344303\n",
      "Epoch [16644/20000], Training Loss: 0.01044143710104046, Validation Loss: 0.003933482407309913\n",
      "Epoch [16645/20000], Training Loss: 0.005101466605213188, Validation Loss: 0.00276849160108607\n",
      "Epoch [16646/20000], Training Loss: 0.007700056222737268, Validation Loss: 0.00503580194035813\n",
      "Epoch [16647/20000], Training Loss: 0.007327185182865443, Validation Loss: 0.003184859024851296\n",
      "Epoch [16648/20000], Training Loss: 0.014089647271508252, Validation Loss: 0.0028069550323287984\n",
      "Epoch [16649/20000], Training Loss: 0.02296870998855281, Validation Loss: 0.0075412944787929415\n",
      "Epoch [16650/20000], Training Loss: 0.006549630302678062, Validation Loss: 0.011150902070637366\n",
      "Epoch [16651/20000], Training Loss: 0.005647254423820414, Validation Loss: 0.00401002969856229\n",
      "Epoch [16652/20000], Training Loss: 0.008594058497448844, Validation Loss: 0.0057030140108541415\n",
      "Epoch [16653/20000], Training Loss: 0.0043527181987558705, Validation Loss: 0.0044820883445513505\n",
      "Epoch [16654/20000], Training Loss: 0.005166882465021315, Validation Loss: 0.008572085594227058\n",
      "Epoch [16655/20000], Training Loss: 0.01103791000059573, Validation Loss: 0.06413769296237413\n",
      "Epoch [16656/20000], Training Loss: 0.06876629286223793, Validation Loss: 0.03178029214789836\n",
      "Epoch [16657/20000], Training Loss: 0.034504770600636094, Validation Loss: 0.016971420378037498\n",
      "Epoch [16658/20000], Training Loss: 0.009731507321703248, Validation Loss: 0.007490426823226569\n",
      "Epoch [16659/20000], Training Loss: 0.0076026408351026475, Validation Loss: 0.006441931920961557\n",
      "Epoch [16660/20000], Training Loss: 0.005577021731629169, Validation Loss: 0.00606450598215177\n",
      "Epoch [16661/20000], Training Loss: 0.006360637945294797, Validation Loss: 0.004424561901820263\n",
      "Epoch [16662/20000], Training Loss: 0.006584638476072412, Validation Loss: 0.004026269167427797\n",
      "Epoch [16663/20000], Training Loss: 0.004586319178477944, Validation Loss: 0.005308451483418012\n",
      "Epoch [16664/20000], Training Loss: 0.004231071148491797, Validation Loss: 0.005494762758481946\n",
      "Epoch [16665/20000], Training Loss: 0.004518331720711493, Validation Loss: 0.02381794367517887\n",
      "Epoch [16666/20000], Training Loss: 0.013462457326728594, Validation Loss: 0.004990190878507812\n",
      "Epoch [16667/20000], Training Loss: 0.009200441899468257, Validation Loss: 0.005685472250450273\n",
      "Epoch [16668/20000], Training Loss: 0.007494482226840253, Validation Loss: 0.007578680493907259\n",
      "Epoch [16669/20000], Training Loss: 0.007180300312126876, Validation Loss: 0.007384787459590277\n",
      "Epoch [16670/20000], Training Loss: 0.008741417030770597, Validation Loss: 0.00491329904175635\n",
      "Epoch [16671/20000], Training Loss: 0.004211385696960081, Validation Loss: 0.003291041378201877\n",
      "Epoch [16672/20000], Training Loss: 0.004538645428380862, Validation Loss: 0.0030780753450654025\n",
      "Epoch [16673/20000], Training Loss: 0.0045835238209162655, Validation Loss: 0.0035514970180816658\n",
      "Epoch [16674/20000], Training Loss: 0.0042780362522795, Validation Loss: 0.002749693758673986\n",
      "Epoch [16675/20000], Training Loss: 0.004531571184218462, Validation Loss: 0.006958575387135793\n",
      "Epoch [16676/20000], Training Loss: 0.01066788848428197, Validation Loss: 0.012407876285067601\n",
      "Epoch [16677/20000], Training Loss: 0.01854745221914657, Validation Loss: 0.020037502051579975\n",
      "Epoch [16678/20000], Training Loss: 0.010920199060430085, Validation Loss: 0.004029747732925415\n",
      "Epoch [16679/20000], Training Loss: 0.003931424972376719, Validation Loss: 0.005620992626341238\n",
      "Epoch [16680/20000], Training Loss: 0.005320812162348635, Validation Loss: 0.0026449925178226863\n",
      "Epoch [16681/20000], Training Loss: 0.007918086915326836, Validation Loss: 0.0023035269100485844\n",
      "Epoch [16682/20000], Training Loss: 0.007612722345844044, Validation Loss: 0.0035913966858446755\n",
      "Epoch [16683/20000], Training Loss: 0.006807385568598485, Validation Loss: 0.0069711946084104835\n",
      "Epoch [16684/20000], Training Loss: 0.005827378588427174, Validation Loss: 0.002756901562730313\n",
      "Epoch [16685/20000], Training Loss: 0.006516155618212187, Validation Loss: 0.00353806955048619\n",
      "Epoch [16686/20000], Training Loss: 0.003832034874254272, Validation Loss: 0.0029692640036062456\n",
      "Epoch [16687/20000], Training Loss: 0.008658956703064697, Validation Loss: 0.0023227450251065923\n",
      "Epoch [16688/20000], Training Loss: 0.018861195638887045, Validation Loss: 0.004877285435738034\n",
      "Epoch [16689/20000], Training Loss: 0.027691683444572845, Validation Loss: 0.005210820156144123\n",
      "Epoch [16690/20000], Training Loss: 0.019710474940308944, Validation Loss: 0.007446509077392908\n",
      "Epoch [16691/20000], Training Loss: 0.005333025666654326, Validation Loss: 0.012135507006789099\n",
      "Epoch [16692/20000], Training Loss: 0.009707305073139392, Validation Loss: 0.011876540524778626\n",
      "Epoch [16693/20000], Training Loss: 0.012211836228353994, Validation Loss: 0.010394510978680873\n",
      "Epoch [16694/20000], Training Loss: 0.009200613564190283, Validation Loss: 0.016402329223930274\n",
      "Epoch [16695/20000], Training Loss: 0.01851693442105378, Validation Loss: 0.006776454400525227\n",
      "Epoch [16696/20000], Training Loss: 0.008313397993333638, Validation Loss: 0.010174824324038448\n",
      "Epoch [16697/20000], Training Loss: 0.007349268720645341, Validation Loss: 0.003073435999080435\n",
      "Epoch [16698/20000], Training Loss: 0.006875983465371454, Validation Loss: 0.009419380021948908\n",
      "Epoch [16699/20000], Training Loss: 0.006008641010339488, Validation Loss: 0.0039695636388970895\n",
      "Epoch [16700/20000], Training Loss: 0.004786131881181583, Validation Loss: 0.0027394858809460004\n",
      "Epoch [16701/20000], Training Loss: 0.00532501491579751, Validation Loss: 0.0031461805777142082\n",
      "Epoch [16702/20000], Training Loss: 0.0056374187099988505, Validation Loss: 0.002414692903536337\n",
      "Epoch [16703/20000], Training Loss: 0.006716072309895286, Validation Loss: 0.019941934517452384\n",
      "Epoch [16704/20000], Training Loss: 0.03249372888240032, Validation Loss: 0.08648577545370374\n",
      "Epoch [16705/20000], Training Loss: 0.05002043701408963, Validation Loss: 0.016680620626546\n",
      "Epoch [16706/20000], Training Loss: 0.02038079001067672, Validation Loss: 0.009463352342797953\n",
      "Epoch [16707/20000], Training Loss: 0.014567995826707505, Validation Loss: 0.00559256058444849\n",
      "Epoch [16708/20000], Training Loss: 0.009865871506398045, Validation Loss: 0.006506383393700942\n",
      "Epoch [16709/20000], Training Loss: 0.00606148515154408, Validation Loss: 0.0038258348669890957\n",
      "Epoch [16710/20000], Training Loss: 0.0049837677984864315, Validation Loss: 0.0034844317400646624\n",
      "Epoch [16711/20000], Training Loss: 0.005660088316809768, Validation Loss: 0.010813457891861682\n",
      "Epoch [16712/20000], Training Loss: 0.007605988599867227, Validation Loss: 0.0065563329181778885\n",
      "Epoch [16713/20000], Training Loss: 0.010764154983917251, Validation Loss: 0.0036019991733726086\n",
      "Epoch [16714/20000], Training Loss: 0.004775415025505936, Validation Loss: 0.011916317710936675\n",
      "Epoch [16715/20000], Training Loss: 0.009109444053527633, Validation Loss: 0.006008063322586362\n",
      "Epoch [16716/20000], Training Loss: 0.007358108892471397, Validation Loss: 0.004338841522098846\n",
      "Epoch [16717/20000], Training Loss: 0.008942480126799117, Validation Loss: 0.00603643050302751\n",
      "Epoch [16718/20000], Training Loss: 0.00793490164895567, Validation Loss: 0.0029803226024435353\n",
      "Epoch [16719/20000], Training Loss: 0.0064016194633690505, Validation Loss: 0.010482378703226743\n",
      "Epoch [16720/20000], Training Loss: 0.006284973625692406, Validation Loss: 0.005738709275301497\n",
      "Epoch [16721/20000], Training Loss: 0.005877839870663593, Validation Loss: 0.004709995936439425\n",
      "Epoch [16722/20000], Training Loss: 0.008884508529653041, Validation Loss: 0.022149824670384535\n",
      "Epoch [16723/20000], Training Loss: 0.010550648074761122, Validation Loss: 0.009376691654876647\n",
      "Epoch [16724/20000], Training Loss: 0.012038178785035467, Validation Loss: 0.004588390511970124\n",
      "Epoch [16725/20000], Training Loss: 0.007642444023596389, Validation Loss: 0.004494217769878414\n",
      "Epoch [16726/20000], Training Loss: 0.007421257518022425, Validation Loss: 0.003506248498393522\n",
      "Epoch [16727/20000], Training Loss: 0.011355297674786666, Validation Loss: 0.010129907833841989\n",
      "Epoch [16728/20000], Training Loss: 0.0409621093215508, Validation Loss: 0.03992158280951636\n",
      "Epoch [16729/20000], Training Loss: 0.0364376617050896, Validation Loss: 0.01818218576502772\n",
      "Epoch [16730/20000], Training Loss: 0.046899644889240984, Validation Loss: 0.029285761138552857\n",
      "Epoch [16731/20000], Training Loss: 0.014235599361459858, Validation Loss: 0.005413768033260372\n",
      "Epoch [16732/20000], Training Loss: 0.008773018980199205, Validation Loss: 0.005276929813297622\n",
      "Epoch [16733/20000], Training Loss: 0.005413575465874081, Validation Loss: 0.006408054034142907\n",
      "Epoch [16734/20000], Training Loss: 0.005441797626198357, Validation Loss: 0.004310470860527208\n",
      "Epoch [16735/20000], Training Loss: 0.0059738398007799075, Validation Loss: 0.004516056199382287\n",
      "Epoch [16736/20000], Training Loss: 0.00648420732866433, Validation Loss: 0.004694651791374278\n",
      "Epoch [16737/20000], Training Loss: 0.005018732585345528, Validation Loss: 0.006027888773759926\n",
      "Epoch [16738/20000], Training Loss: 0.0063519860059143085, Validation Loss: 0.005588967499891494\n",
      "Epoch [16739/20000], Training Loss: 0.005456748226216794, Validation Loss: 0.003971389746627807\n",
      "Epoch [16740/20000], Training Loss: 0.004274191104091837, Validation Loss: 0.0038779700817827794\n",
      "Epoch [16741/20000], Training Loss: 0.0063555587819012415, Validation Loss: 0.01070104788030741\n",
      "Epoch [16742/20000], Training Loss: 0.008527366597653392, Validation Loss: 0.004093007251497178\n",
      "Epoch [16743/20000], Training Loss: 0.004264300863807152, Validation Loss: 0.004857579966921298\n",
      "Epoch [16744/20000], Training Loss: 0.00628042457148175, Validation Loss: 0.013023047701738375\n",
      "Epoch [16745/20000], Training Loss: 0.015174263114853861, Validation Loss: 0.016548454661253636\n",
      "Epoch [16746/20000], Training Loss: 0.032375077746629746, Validation Loss: 0.049854162548269694\n",
      "Epoch [16747/20000], Training Loss: 0.029205374100716393, Validation Loss: 0.013527433932590701\n",
      "Epoch [16748/20000], Training Loss: 0.030821712901732617, Validation Loss: 0.013422679934072741\n",
      "Epoch [16749/20000], Training Loss: 0.007103647018457845, Validation Loss: 0.012156628738598869\n",
      "Epoch [16750/20000], Training Loss: 0.010362137036281638, Validation Loss: 0.01164278148692402\n",
      "Epoch [16751/20000], Training Loss: 0.008832934734527953, Validation Loss: 0.005627001213091327\n",
      "Epoch [16752/20000], Training Loss: 0.004914347207107182, Validation Loss: 0.003716733744202624\n",
      "Epoch [16753/20000], Training Loss: 0.0046839886461437375, Validation Loss: 0.0036306749335121174\n",
      "Epoch [16754/20000], Training Loss: 0.006090070118911431, Validation Loss: 0.00458825431639046\n",
      "Epoch [16755/20000], Training Loss: 0.007801327382594536, Validation Loss: 0.005773307306022879\n",
      "Epoch [16756/20000], Training Loss: 0.005332640588416585, Validation Loss: 0.007383289689723175\n",
      "Epoch [16757/20000], Training Loss: 0.008133633923924728, Validation Loss: 0.0047658759354146495\n",
      "Epoch [16758/20000], Training Loss: 0.007278980701812543, Validation Loss: 0.006445529275956257\n",
      "Epoch [16759/20000], Training Loss: 0.004806722600603409, Validation Loss: 0.003461659637764062\n",
      "Epoch [16760/20000], Training Loss: 0.008418338049658556, Validation Loss: 0.007655293819068122\n",
      "Epoch [16761/20000], Training Loss: 0.01155356767860535, Validation Loss: 0.00475230749460709\n",
      "Epoch [16762/20000], Training Loss: 0.006765992588563157, Validation Loss: 0.0030430837702092157\n",
      "Epoch [16763/20000], Training Loss: 0.015495074555344348, Validation Loss: 0.0032081370533936543\n",
      "Epoch [16764/20000], Training Loss: 0.04018868103490344, Validation Loss: 0.04013255254007423\n",
      "Epoch [16765/20000], Training Loss: 0.03330506239061443, Validation Loss: 0.010168308061630731\n",
      "Epoch [16766/20000], Training Loss: 0.02353410716750659, Validation Loss: 0.005602870676999113\n",
      "Epoch [16767/20000], Training Loss: 0.006731975108518132, Validation Loss: 0.011246849924494515\n",
      "Epoch [16768/20000], Training Loss: 0.007954168628706761, Validation Loss: 0.009460735776363955\n",
      "Epoch [16769/20000], Training Loss: 0.005852462913026102, Validation Loss: 0.00434794086637729\n",
      "Epoch [16770/20000], Training Loss: 0.0065292224337879035, Validation Loss: 0.006997531156100324\n",
      "Epoch [16771/20000], Training Loss: 0.005113816707827417, Validation Loss: 0.004116283087554962\n",
      "Epoch [16772/20000], Training Loss: 0.003991416653877066, Validation Loss: 0.003584535311291453\n",
      "Epoch [16773/20000], Training Loss: 0.0044787684996013665, Validation Loss: 0.0037760788317789157\n",
      "Epoch [16774/20000], Training Loss: 0.007194264288825382, Validation Loss: 0.0051915710288325135\n",
      "Epoch [16775/20000], Training Loss: 0.006029081125494226, Validation Loss: 0.004990097393571444\n",
      "Epoch [16776/20000], Training Loss: 0.007401775996965755, Validation Loss: 0.0039616636557095065\n",
      "Epoch [16777/20000], Training Loss: 0.005112385801891962, Validation Loss: 0.003558144473881697\n",
      "Epoch [16778/20000], Training Loss: 0.005587354541473489, Validation Loss: 0.00497007890930489\n",
      "Epoch [16779/20000], Training Loss: 0.006427531554696283, Validation Loss: 0.005911462336890898\n",
      "Epoch [16780/20000], Training Loss: 0.007842711420640367, Validation Loss: 0.0037948473527063937\n",
      "Epoch [16781/20000], Training Loss: 0.01649892428914817, Validation Loss: 0.007005819652592403\n",
      "Epoch [16782/20000], Training Loss: 0.009375661342606431, Validation Loss: 0.003751358179891489\n",
      "Epoch [16783/20000], Training Loss: 0.0047011254521618995, Validation Loss: 0.0032058653894251016\n",
      "Epoch [16784/20000], Training Loss: 0.006967721002313608, Validation Loss: 0.003950260572245108\n",
      "Epoch [16785/20000], Training Loss: 0.007236774908838736, Validation Loss: 0.0034504110786132613\n",
      "Epoch [16786/20000], Training Loss: 0.004550537976943555, Validation Loss: 0.00520808465944631\n",
      "Epoch [16787/20000], Training Loss: 0.0054273801800133826, Validation Loss: 0.006769297046296093\n",
      "Epoch [16788/20000], Training Loss: 0.009420084949202387, Validation Loss: 0.0025512170840623787\n",
      "Epoch [16789/20000], Training Loss: 0.009173157945562187, Validation Loss: 0.004396764998859288\n",
      "Epoch [16790/20000], Training Loss: 0.007373479278092938, Validation Loss: 0.003494456727126695\n",
      "Epoch [16791/20000], Training Loss: 0.006151362569813301, Validation Loss: 0.0067569193080641654\n",
      "Epoch [16792/20000], Training Loss: 0.015542073756997914, Validation Loss: 0.0033879136060876685\n",
      "Epoch [16793/20000], Training Loss: 0.02490787908469169, Validation Loss: 0.0032734205190000205\n",
      "Epoch [16794/20000], Training Loss: 0.006834762649045193, Validation Loss: 0.0038707224269646395\n",
      "Epoch [16795/20000], Training Loss: 0.007173074955582186, Validation Loss: 0.005119185640519878\n",
      "Epoch [16796/20000], Training Loss: 0.014372408251355344, Validation Loss: 0.022421144608350523\n",
      "Epoch [16797/20000], Training Loss: 0.056010463680122156, Validation Loss: 0.012354200302460023\n",
      "Epoch [16798/20000], Training Loss: 0.043337424007144625, Validation Loss: 0.0091518798426843\n",
      "Epoch [16799/20000], Training Loss: 0.022239798567690223, Validation Loss: 0.012572015392119542\n",
      "Epoch [16800/20000], Training Loss: 0.011438381719407127, Validation Loss: 0.02528564648699873\n",
      "Epoch [16801/20000], Training Loss: 0.011230933261686005, Validation Loss: 0.005502891460799171\n",
      "Epoch [16802/20000], Training Loss: 0.007386648318060907, Validation Loss: 0.011289588790597398\n",
      "Epoch [16803/20000], Training Loss: 0.007741129545487573, Validation Loss: 0.007168539897489319\n",
      "Epoch [16804/20000], Training Loss: 0.006291397962726819, Validation Loss: 0.0036305685548764393\n",
      "Epoch [16805/20000], Training Loss: 0.007757292640495247, Validation Loss: 0.00462531785843469\n",
      "Epoch [16806/20000], Training Loss: 0.006405043004113915, Validation Loss: 0.00392191600385721\n",
      "Epoch [16807/20000], Training Loss: 0.008834571387394265, Validation Loss: 0.005876251430116497\n",
      "Epoch [16808/20000], Training Loss: 0.008412159231219058, Validation Loss: 0.006881846485165559\n",
      "Epoch [16809/20000], Training Loss: 0.008853977459304068, Validation Loss: 0.004652794662295686\n",
      "Epoch [16810/20000], Training Loss: 0.00827437476047115, Validation Loss: 0.007064501894205932\n",
      "Epoch [16811/20000], Training Loss: 0.006445293080365185, Validation Loss: 0.0036651056775675833\n",
      "Epoch [16812/20000], Training Loss: 0.005926979564329875, Validation Loss: 0.008973267879209093\n",
      "Epoch [16813/20000], Training Loss: 0.02055990048601026, Validation Loss: 0.01387501587956753\n",
      "Epoch [16814/20000], Training Loss: 0.009651209220464807, Validation Loss: 0.006793315053107651\n",
      "Epoch [16815/20000], Training Loss: 0.006772626716805722, Validation Loss: 0.003257333555130312\n",
      "Epoch [16816/20000], Training Loss: 0.005561603541406969, Validation Loss: 0.002947087688020084\n",
      "Epoch [16817/20000], Training Loss: 0.007438419599046548, Validation Loss: 0.0027253622158939705\n",
      "Epoch [16818/20000], Training Loss: 0.01378316711426513, Validation Loss: 0.011556335796321985\n",
      "Epoch [16819/20000], Training Loss: 0.009992910079225632, Validation Loss: 0.00741118043856827\n",
      "Epoch [16820/20000], Training Loss: 0.011141733706088936, Validation Loss: 0.006995405640119705\n",
      "Epoch [16821/20000], Training Loss: 0.019450636568633075, Validation Loss: 0.030204035311872497\n",
      "Epoch [16822/20000], Training Loss: 0.013813192497764248, Validation Loss: 0.012735702097559235\n",
      "Epoch [16823/20000], Training Loss: 0.013303224309181263, Validation Loss: 0.019786133217484396\n",
      "Epoch [16824/20000], Training Loss: 0.013041913204818099, Validation Loss: 0.009223710281661721\n",
      "Epoch [16825/20000], Training Loss: 0.01134513685870583, Validation Loss: 0.012090527179322765\n",
      "Epoch [16826/20000], Training Loss: 0.014657147122696708, Validation Loss: 0.0069182239594575935\n",
      "Epoch [16827/20000], Training Loss: 0.008220500406293598, Validation Loss: 0.0026864978045446314\n",
      "Epoch [16828/20000], Training Loss: 0.008524765417600324, Validation Loss: 0.003697907832598649\n",
      "Epoch [16829/20000], Training Loss: 0.008912058079399035, Validation Loss: 0.004909660581195193\n",
      "Epoch [16830/20000], Training Loss: 0.004675544909592385, Validation Loss: 0.0024382503071324535\n",
      "Epoch [16831/20000], Training Loss: 0.004650478223993559, Validation Loss: 0.0027928479722437566\n",
      "Epoch [16832/20000], Training Loss: 0.004615741875048636, Validation Loss: 0.008146673631732224\n",
      "Epoch [16833/20000], Training Loss: 0.007235923537726714, Validation Loss: 0.008613171477505155\n",
      "Epoch [16834/20000], Training Loss: 0.007268100347200094, Validation Loss: 0.00833607790621198\n",
      "Epoch [16835/20000], Training Loss: 0.013360294601755283, Validation Loss: 0.0039857982229623955\n",
      "Epoch [16836/20000], Training Loss: 0.01980732636002358, Validation Loss: 0.003378264751550627\n",
      "Epoch [16837/20000], Training Loss: 0.012467512833869219, Validation Loss: 0.00380606197254098\n",
      "Epoch [16838/20000], Training Loss: 0.007361620539865855, Validation Loss: 0.0023127783698758514\n",
      "Epoch [16839/20000], Training Loss: 0.006065345882754107, Validation Loss: 0.005824203849432877\n",
      "Epoch [16840/20000], Training Loss: 0.003980196464648803, Validation Loss: 0.0021400212783849576\n",
      "Epoch [16841/20000], Training Loss: 0.0031846533856878523, Validation Loss: 0.0019958794656125584\n",
      "Epoch [16842/20000], Training Loss: 0.007528491937721681, Validation Loss: 0.002936467415300927\n",
      "Epoch [16843/20000], Training Loss: 0.007122795186920224, Validation Loss: 0.002794032139450207\n",
      "Epoch [16844/20000], Training Loss: 0.006281433257754543, Validation Loss: 0.004857095760196931\n",
      "Epoch [16845/20000], Training Loss: 0.005793906137634102, Validation Loss: 0.0026532768844853244\n",
      "Epoch [16846/20000], Training Loss: 0.004849065224594337, Validation Loss: 0.0021019878749411547\n",
      "Epoch [16847/20000], Training Loss: 0.005323428417406311, Validation Loss: 0.0021530414983337615\n",
      "Epoch [16848/20000], Training Loss: 0.005781465799373109, Validation Loss: 0.0044976231348000184\n",
      "Epoch [16849/20000], Training Loss: 0.018872717583138052, Validation Loss: 0.007867928363981027\n",
      "Epoch [16850/20000], Training Loss: 0.029397468984825537, Validation Loss: 0.00705105112288708\n",
      "Epoch [16851/20000], Training Loss: 0.027130782221740497, Validation Loss: 0.01115845879906731\n",
      "Epoch [16852/20000], Training Loss: 0.007878696962766949, Validation Loss: 0.014136419840497183\n",
      "Epoch [16853/20000], Training Loss: 0.009565497594719221, Validation Loss: 0.0038075631110448933\n",
      "Epoch [16854/20000], Training Loss: 0.009018692005286408, Validation Loss: 0.004303852928991603\n",
      "Epoch [16855/20000], Training Loss: 0.005235906620198095, Validation Loss: 0.006312051848064064\n",
      "Epoch [16856/20000], Training Loss: 0.009999738559210007, Validation Loss: 0.006401728017280307\n",
      "Epoch [16857/20000], Training Loss: 0.006895885272699941, Validation Loss: 0.01444175254733469\n",
      "Epoch [16858/20000], Training Loss: 0.008978808668741425, Validation Loss: 0.0031268373063407955\n",
      "Epoch [16859/20000], Training Loss: 0.00622681116406706, Validation Loss: 0.009268950351172731\n",
      "Epoch [16860/20000], Training Loss: 0.011860860178005948, Validation Loss: 0.0021884429578895898\n",
      "Epoch [16861/20000], Training Loss: 0.004885059280370895, Validation Loss: 0.005521016168732621\n",
      "Epoch [16862/20000], Training Loss: 0.0065621492540230975, Validation Loss: 0.003779738548473688\n",
      "Epoch [16863/20000], Training Loss: 0.005391471322842075, Validation Loss: 0.0025159409679450357\n",
      "Epoch [16864/20000], Training Loss: 0.003402668004046713, Validation Loss: 0.003671327013322972\n",
      "Epoch [16865/20000], Training Loss: 0.007345365713782874, Validation Loss: 0.002676076996477507\n",
      "Epoch [16866/20000], Training Loss: 0.009100230908578462, Validation Loss: 0.020113478528888402\n",
      "Epoch [16867/20000], Training Loss: 0.016736894115248497, Validation Loss: 0.01803074232809974\n",
      "Epoch [16868/20000], Training Loss: 0.020350923114165198, Validation Loss: 0.022038160047234863\n",
      "Epoch [16869/20000], Training Loss: 0.021116662928501943, Validation Loss: 0.0031746778504364265\n",
      "Epoch [16870/20000], Training Loss: 0.007795370680209349, Validation Loss: 0.003632954436866953\n",
      "Epoch [16871/20000], Training Loss: 0.004884203721303493, Validation Loss: 0.004451425951858466\n",
      "Epoch [16872/20000], Training Loss: 0.007790535587056573, Validation Loss: 0.002772273989519038\n",
      "Epoch [16873/20000], Training Loss: 0.004703015194926203, Validation Loss: 0.013875815514163595\n",
      "Epoch [16874/20000], Training Loss: 0.02632427875167715, Validation Loss: 0.007250776673540064\n",
      "Epoch [16875/20000], Training Loss: 0.026494664230995113, Validation Loss: 0.03083467759721056\n",
      "Epoch [16876/20000], Training Loss: 0.00927186861073486, Validation Loss: 0.00448041446262388\n",
      "Epoch [16877/20000], Training Loss: 0.014572446476709697, Validation Loss: 0.01595895098852245\n",
      "Epoch [16878/20000], Training Loss: 0.008372854788571462, Validation Loss: 0.005101495869658785\n",
      "Epoch [16879/20000], Training Loss: 0.00974470233735961, Validation Loss: 0.005139909392728522\n",
      "Epoch [16880/20000], Training Loss: 0.011517152802948008, Validation Loss: 0.0227041786543862\n",
      "Epoch [16881/20000], Training Loss: 0.013291576794082565, Validation Loss: 0.006588312026348068\n",
      "Epoch [16882/20000], Training Loss: 0.008747308446150523, Validation Loss: 0.020238173255425602\n",
      "Epoch [16883/20000], Training Loss: 0.024169026156804257, Validation Loss: 0.009306779893825683\n",
      "Epoch [16884/20000], Training Loss: 0.019500913948128584, Validation Loss: 0.03961075919469132\n",
      "Epoch [16885/20000], Training Loss: 0.019453911212622188, Validation Loss: 0.008972354367123054\n",
      "Epoch [16886/20000], Training Loss: 0.005848461982428229, Validation Loss: 0.006675484201953756\n",
      "Epoch [16887/20000], Training Loss: 0.00627505003753218, Validation Loss: 0.005557763292943362\n",
      "Epoch [16888/20000], Training Loss: 0.0061154854099316125, Validation Loss: 0.004455648041783888\n",
      "Epoch [16889/20000], Training Loss: 0.006561199111664402, Validation Loss: 0.0038428457504185837\n",
      "Epoch [16890/20000], Training Loss: 0.00571156894251804, Validation Loss: 0.005461271248277492\n",
      "Epoch [16891/20000], Training Loss: 0.006462989484394451, Validation Loss: 0.003415628668493907\n",
      "Epoch [16892/20000], Training Loss: 0.0074986853842347045, Validation Loss: 0.004352702262378065\n",
      "Epoch [16893/20000], Training Loss: 0.010470347351136817, Validation Loss: 0.004207509071721104\n",
      "Epoch [16894/20000], Training Loss: 0.011919285591700048, Validation Loss: 0.006846483315436152\n",
      "Epoch [16895/20000], Training Loss: 0.011152801019696719, Validation Loss: 0.013094168056610327\n",
      "Epoch [16896/20000], Training Loss: 0.006392269940988626, Validation Loss: 0.0032010126395302257\n",
      "Epoch [16897/20000], Training Loss: 0.0047084153702599, Validation Loss: 0.006775391197079118\n",
      "Epoch [16898/20000], Training Loss: 0.012783781213329348, Validation Loss: 0.010557905904059533\n",
      "Epoch [16899/20000], Training Loss: 0.007208801429572175, Validation Loss: 0.013699267050244332\n",
      "Epoch [16900/20000], Training Loss: 0.009065186518065664, Validation Loss: 0.004391091817996795\n",
      "Epoch [16901/20000], Training Loss: 0.008458496569671427, Validation Loss: 0.01902154416531435\n",
      "Epoch [16902/20000], Training Loss: 0.013262574986064075, Validation Loss: 0.0027335866886529303\n",
      "Epoch [16903/20000], Training Loss: 0.006266599691506209, Validation Loss: 0.014358445403307657\n",
      "Epoch [16904/20000], Training Loss: 0.008285086359267422, Validation Loss: 0.0025936748208640176\n",
      "Epoch [16905/20000], Training Loss: 0.01268735780208122, Validation Loss: 0.0033142558943356653\n",
      "Epoch [16906/20000], Training Loss: 0.014341717912299958, Validation Loss: 0.002798242070775408\n",
      "Epoch [16907/20000], Training Loss: 0.016127651329173074, Validation Loss: 0.002805576115347565\n",
      "Epoch [16908/20000], Training Loss: 0.009584736926805297, Validation Loss: 0.0029491083860427075\n",
      "Epoch [16909/20000], Training Loss: 0.003664421762940557, Validation Loss: 0.003487405425071342\n",
      "Epoch [16910/20000], Training Loss: 0.0038304662874517298, Validation Loss: 0.0028193263793586504\n",
      "Epoch [16911/20000], Training Loss: 0.004606356349540874, Validation Loss: 0.0026883395674803167\n",
      "Epoch [16912/20000], Training Loss: 0.0042288112711738836, Validation Loss: 0.006666883169323992\n",
      "Epoch [16913/20000], Training Loss: 0.005693549077567488, Validation Loss: 0.0035648306764000737\n",
      "Epoch [16914/20000], Training Loss: 0.005808252939459635, Validation Loss: 0.003896657435561224\n",
      "Epoch [16915/20000], Training Loss: 0.008563290192542539, Validation Loss: 0.013716837076263541\n",
      "Epoch [16916/20000], Training Loss: 0.007505630244850181, Validation Loss: 0.0026775466497192773\n",
      "Epoch [16917/20000], Training Loss: 0.01362674918657701, Validation Loss: 0.005703708175745267\n",
      "Epoch [16918/20000], Training Loss: 0.016775803695764018, Validation Loss: 0.0037481943726537076\n",
      "Epoch [16919/20000], Training Loss: 0.016235705841349306, Validation Loss: 0.007236181126249479\n",
      "Epoch [16920/20000], Training Loss: 0.032289264676884546, Validation Loss: 0.006261033674271871\n",
      "Epoch [16921/20000], Training Loss: 0.012665840713972492, Validation Loss: 0.004976598241528661\n",
      "Epoch [16922/20000], Training Loss: 0.014738326617053385, Validation Loss: 0.06533820288522092\n",
      "Epoch [16923/20000], Training Loss: 0.019475900461397293, Validation Loss: 0.007956832068150328\n",
      "Epoch [16924/20000], Training Loss: 0.010805258884959455, Validation Loss: 0.006341658072634411\n",
      "Epoch [16925/20000], Training Loss: 0.0072380208086022845, Validation Loss: 0.004426608409078524\n",
      "Epoch [16926/20000], Training Loss: 0.006266942519557363, Validation Loss: 0.020054522369593313\n",
      "Epoch [16927/20000], Training Loss: 0.006996494361633917, Validation Loss: 0.0036005685972457157\n",
      "Epoch [16928/20000], Training Loss: 0.0037974065323526573, Validation Loss: 0.0046049158992462124\n",
      "Epoch [16929/20000], Training Loss: 0.0038621321800746955, Validation Loss: 0.005589591527572819\n",
      "Epoch [16930/20000], Training Loss: 0.006495862569343964, Validation Loss: 0.0033577700433917407\n",
      "Epoch [16931/20000], Training Loss: 0.0036815246743831915, Validation Loss: 0.005490693298749406\n",
      "Epoch [16932/20000], Training Loss: 0.004070106763850033, Validation Loss: 0.0029501996762051974\n",
      "Epoch [16933/20000], Training Loss: 0.005762951759736877, Validation Loss: 0.0030733160318985953\n",
      "Epoch [16934/20000], Training Loss: 0.0037135558824437404, Validation Loss: 0.003471880208788493\n",
      "Epoch [16935/20000], Training Loss: 0.005873834706603377, Validation Loss: 0.0033522210494780374\n",
      "Epoch [16936/20000], Training Loss: 0.00438456643106682, Validation Loss: 0.004057804049081939\n",
      "Epoch [16937/20000], Training Loss: 0.005850822754707353, Validation Loss: 0.002319800071613936\n",
      "Epoch [16938/20000], Training Loss: 0.008081432284078411, Validation Loss: 0.0027029713933801602\n",
      "Epoch [16939/20000], Training Loss: 0.0055121866742484016, Validation Loss: 0.025023721158506956\n",
      "Epoch [16940/20000], Training Loss: 0.013316654387641133, Validation Loss: 0.006931227506346409\n",
      "Epoch [16941/20000], Training Loss: 0.014483511433354579, Validation Loss: 0.005595052865185072\n",
      "Epoch [16942/20000], Training Loss: 0.004560415587288195, Validation Loss: 0.0033849301786712765\n",
      "Epoch [16943/20000], Training Loss: 0.008287827663090346, Validation Loss: 0.002731617471824726\n",
      "Epoch [16944/20000], Training Loss: 0.01899410717929381, Validation Loss: 0.006341129809227043\n",
      "Epoch [16945/20000], Training Loss: 0.007299115778420985, Validation Loss: 0.04423800110816973\n",
      "Epoch [16946/20000], Training Loss: 0.0431862556374004, Validation Loss: 0.025983808828251985\n",
      "Epoch [16947/20000], Training Loss: 0.02514987970659734, Validation Loss: 0.004078568824575817\n",
      "Epoch [16948/20000], Training Loss: 0.019657377885388478, Validation Loss: 0.014906326791675608\n",
      "Epoch [16949/20000], Training Loss: 0.022580037492194345, Validation Loss: 0.011532158447932619\n",
      "Epoch [16950/20000], Training Loss: 0.028079917665828753, Validation Loss: 0.016693730697139045\n",
      "Epoch [16951/20000], Training Loss: 0.017930650659504214, Validation Loss: 0.011550072668455869\n",
      "Epoch [16952/20000], Training Loss: 0.011164203791330303, Validation Loss: 0.008927387737967247\n",
      "Epoch [16953/20000], Training Loss: 0.006147873763568766, Validation Loss: 0.006301651128793091\n",
      "Epoch [16954/20000], Training Loss: 0.008036635157460685, Validation Loss: 0.004950899495656001\n",
      "Epoch [16955/20000], Training Loss: 0.012956689784394777, Validation Loss: 0.012043515220473413\n",
      "Epoch [16956/20000], Training Loss: 0.009460513019283618, Validation Loss: 0.006237016410475806\n",
      "Epoch [16957/20000], Training Loss: 0.008661653680714412, Validation Loss: 0.00403648787667724\n",
      "Epoch [16958/20000], Training Loss: 0.004899208144966646, Validation Loss: 0.003979398431781078\n",
      "Epoch [16959/20000], Training Loss: 0.0056397793149309495, Validation Loss: 0.0033966983883464102\n",
      "Epoch [16960/20000], Training Loss: 0.010482808383780398, Validation Loss: 0.010281154890977352\n",
      "Epoch [16961/20000], Training Loss: 0.007990963838113072, Validation Loss: 0.010316887674865225\n",
      "Epoch [16962/20000], Training Loss: 0.01695306494353385, Validation Loss: 0.012141786950617675\n",
      "Epoch [16963/20000], Training Loss: 0.013023121958602652, Validation Loss: 0.004810431833293803\n",
      "Epoch [16964/20000], Training Loss: 0.008432203454763762, Validation Loss: 0.003463283453996015\n",
      "Epoch [16965/20000], Training Loss: 0.00473610334354037, Validation Loss: 0.005681646795044014\n",
      "Epoch [16966/20000], Training Loss: 0.0050183662803777096, Validation Loss: 0.0036132487798730445\n",
      "Epoch [16967/20000], Training Loss: 0.006723647069654655, Validation Loss: 0.0028384056494528765\n",
      "Epoch [16968/20000], Training Loss: 0.006032345335240409, Validation Loss: 0.007568807875545526\n",
      "Epoch [16969/20000], Training Loss: 0.004623791130110736, Validation Loss: 0.00865371445625637\n",
      "Epoch [16970/20000], Training Loss: 0.007889975692835702, Validation Loss: 0.0028463833144551287\n",
      "Epoch [16971/20000], Training Loss: 0.004925026060391247, Validation Loss: 0.008190787863147374\n",
      "Epoch [16972/20000], Training Loss: 0.006068093704016064, Validation Loss: 0.0032916926379024125\n",
      "Epoch [16973/20000], Training Loss: 0.006616135184750809, Validation Loss: 0.002533065674575547\n",
      "Epoch [16974/20000], Training Loss: 0.006137710441440244, Validation Loss: 0.002694996226145869\n",
      "Epoch [16975/20000], Training Loss: 0.004626978012571011, Validation Loss: 0.007148611492435742\n",
      "Epoch [16976/20000], Training Loss: 0.005602467480847346, Validation Loss: 0.0038955878963033035\n",
      "Epoch [16977/20000], Training Loss: 0.005715866818198785, Validation Loss: 0.011154753039954406\n",
      "Epoch [16978/20000], Training Loss: 0.012151274888310581, Validation Loss: 0.008136525442669804\n",
      "Epoch [16979/20000], Training Loss: 0.0076027417282083275, Validation Loss: 0.008253092546127404\n",
      "Epoch [16980/20000], Training Loss: 0.011304731680346387, Validation Loss: 0.0020749894047298006\n",
      "Epoch [16981/20000], Training Loss: 0.007605982213135576, Validation Loss: 0.02600266637310598\n",
      "Epoch [16982/20000], Training Loss: 0.02174248455313383, Validation Loss: 0.008352217740762415\n",
      "Epoch [16983/20000], Training Loss: 0.007077709074240245, Validation Loss: 0.003322321061552019\n",
      "Epoch [16984/20000], Training Loss: 0.015184199132428538, Validation Loss: 0.010773371638997253\n",
      "Epoch [16985/20000], Training Loss: 0.012532678648962505, Validation Loss: 0.01199760800227725\n",
      "Epoch [16986/20000], Training Loss: 0.006611913047631138, Validation Loss: 0.013436008139998583\n",
      "Epoch [16987/20000], Training Loss: 0.00879725357559177, Validation Loss: 0.017839872145227174\n",
      "Epoch [16988/20000], Training Loss: 0.006509974718158317, Validation Loss: 0.005263321549139935\n",
      "Epoch [16989/20000], Training Loss: 0.006618797435034399, Validation Loss: 0.003991949343462713\n",
      "Epoch [16990/20000], Training Loss: 0.005296862131217495, Validation Loss: 0.0062812467646448395\n",
      "Epoch [16991/20000], Training Loss: 0.005161625466696153, Validation Loss: 0.002326325289739576\n",
      "Epoch [16992/20000], Training Loss: 0.004476546855118256, Validation Loss: 0.0034875032735135797\n",
      "Epoch [16993/20000], Training Loss: 0.009012173882443417, Validation Loss: 0.00231910338508052\n",
      "Epoch [16994/20000], Training Loss: 0.004071669970081919, Validation Loss: 0.006682119997663114\n",
      "Epoch [16995/20000], Training Loss: 0.008431075144211977, Validation Loss: 0.0073266780247150465\n",
      "Epoch [16996/20000], Training Loss: 0.011903916995444368, Validation Loss: 0.007332349548233553\n",
      "Epoch [16997/20000], Training Loss: 0.006745960301356847, Validation Loss: 0.005431203096943264\n",
      "Epoch [16998/20000], Training Loss: 0.009911809262121096, Validation Loss: 0.006224036982028372\n",
      "Epoch [16999/20000], Training Loss: 0.008326591956574703, Validation Loss: 0.02050267825169254\n",
      "Epoch [17000/20000], Training Loss: 0.015603462982523655, Validation Loss: 0.004422523384281347\n",
      "Epoch [17001/20000], Training Loss: 0.005373104041999406, Validation Loss: 0.007806340774653872\n",
      "Epoch [17002/20000], Training Loss: 0.008334458815430221, Validation Loss: 0.006232687006453581\n",
      "Epoch [17003/20000], Training Loss: 0.005739542927455789, Validation Loss: 0.005259039346146381\n",
      "Epoch [17004/20000], Training Loss: 0.007119975574564056, Validation Loss: 0.017140411585888922\n",
      "Epoch [17005/20000], Training Loss: 0.016516788338776678, Validation Loss: 0.0051540406773499725\n",
      "Epoch [17006/20000], Training Loss: 0.03438863794092738, Validation Loss: 0.014066295665023595\n",
      "Epoch [17007/20000], Training Loss: 0.042841753718676046, Validation Loss: 0.024615589941504368\n",
      "Epoch [17008/20000], Training Loss: 0.05872821269440465, Validation Loss: 0.010220932376666627\n",
      "Epoch [17009/20000], Training Loss: 0.014013572338236762, Validation Loss: 0.007527198923201338\n",
      "Epoch [17010/20000], Training Loss: 0.011099244077091239, Validation Loss: 0.006471725450069243\n",
      "Epoch [17011/20000], Training Loss: 0.007627614813307966, Validation Loss: 0.004731457468925068\n",
      "Epoch [17012/20000], Training Loss: 0.006939548520936764, Validation Loss: 0.005359773097292678\n",
      "Epoch [17013/20000], Training Loss: 0.005611542209018288, Validation Loss: 0.005093264055679876\n",
      "Epoch [17014/20000], Training Loss: 0.007554290733034057, Validation Loss: 0.01839512799691884\n",
      "Epoch [17015/20000], Training Loss: 0.009071310388338003, Validation Loss: 0.006675870145792422\n",
      "Epoch [17016/20000], Training Loss: 0.006837446572067815, Validation Loss: 0.0035193523524006715\n",
      "Epoch [17017/20000], Training Loss: 0.004082478063147781, Validation Loss: 0.004747212988762938\n",
      "Epoch [17018/20000], Training Loss: 0.005478706647409126, Validation Loss: 0.00380710607012702\n",
      "Epoch [17019/20000], Training Loss: 0.007924153922691144, Validation Loss: 0.0038091391504216127\n",
      "Epoch [17020/20000], Training Loss: 0.005211390213974353, Validation Loss: 0.0033751814563812233\n",
      "Epoch [17021/20000], Training Loss: 0.006140901640849604, Validation Loss: 0.005554694374904047\n",
      "Epoch [17022/20000], Training Loss: 0.007993847324542003, Validation Loss: 0.0030693780983678414\n",
      "Epoch [17023/20000], Training Loss: 0.00894090434433435, Validation Loss: 0.003927281707813535\n",
      "Epoch [17024/20000], Training Loss: 0.004570209757990337, Validation Loss: 0.0031292016579363186\n",
      "Epoch [17025/20000], Training Loss: 0.0033981686865445226, Validation Loss: 0.003653103222774932\n",
      "Epoch [17026/20000], Training Loss: 0.00806607564819777, Validation Loss: 0.002923030910233239\n",
      "Epoch [17027/20000], Training Loss: 0.004459495282000197, Validation Loss: 0.003416166442833316\n",
      "Epoch [17028/20000], Training Loss: 0.0052907969636310425, Validation Loss: 0.0036457220023456494\n",
      "Epoch [17029/20000], Training Loss: 0.005987961924152582, Validation Loss: 0.0028157595278014264\n",
      "Epoch [17030/20000], Training Loss: 0.0038786223490855525, Validation Loss: 0.0033893628252030794\n",
      "Epoch [17031/20000], Training Loss: 0.005342737164547933, Validation Loss: 0.003791843477382139\n",
      "Epoch [17032/20000], Training Loss: 0.0052191484968976565, Validation Loss: 0.003587174240070569\n",
      "Epoch [17033/20000], Training Loss: 0.005819817163813111, Validation Loss: 0.003644788313773501\n",
      "Epoch [17034/20000], Training Loss: 0.011962226458893253, Validation Loss: 0.025260538395222097\n",
      "Epoch [17035/20000], Training Loss: 0.009167030869450952, Validation Loss: 0.008301510357534585\n",
      "Epoch [17036/20000], Training Loss: 0.01278328690802612, Validation Loss: 0.0024478713871078283\n",
      "Epoch [17037/20000], Training Loss: 0.00527084974679123, Validation Loss: 0.00311403441001953\n",
      "Epoch [17038/20000], Training Loss: 0.006936140786690105, Validation Loss: 0.015181245282420807\n",
      "Epoch [17039/20000], Training Loss: 0.007341168125939086, Validation Loss: 0.004709625826450293\n",
      "Epoch [17040/20000], Training Loss: 0.007501465250762911, Validation Loss: 0.005485625141107319\n",
      "Epoch [17041/20000], Training Loss: 0.00657878376971764, Validation Loss: 0.0034869559138139194\n",
      "Epoch [17042/20000], Training Loss: 0.011306050020461303, Validation Loss: 0.005968682102415311\n",
      "Epoch [17043/20000], Training Loss: 0.016537824627838354, Validation Loss: 0.04830997650112445\n",
      "Epoch [17044/20000], Training Loss: 0.012893280068445685, Validation Loss: 0.007737096326588398\n",
      "Epoch [17045/20000], Training Loss: 0.015499917848501354, Validation Loss: 0.006728924719680965\n",
      "Epoch [17046/20000], Training Loss: 0.007435332312655062, Validation Loss: 0.011304324062292688\n",
      "Epoch [17047/20000], Training Loss: 0.020911168500008768, Validation Loss: 0.012462326477327175\n",
      "Epoch [17048/20000], Training Loss: 0.004525608920955294, Validation Loss: 0.004691233874657895\n",
      "Epoch [17049/20000], Training Loss: 0.003549671675760432, Validation Loss: 0.0036559873643776347\n",
      "Epoch [17050/20000], Training Loss: 0.00581944305154788, Validation Loss: 0.009879065091565411\n",
      "Epoch [17051/20000], Training Loss: 0.026219636713254398, Validation Loss: 0.018722516259848426\n",
      "Epoch [17052/20000], Training Loss: 0.031216021729570848, Validation Loss: 0.012256232590068259\n",
      "Epoch [17053/20000], Training Loss: 0.06291472136841289, Validation Loss: 0.018722230330325645\n",
      "Epoch [17054/20000], Training Loss: 0.029489372260546327, Validation Loss: 0.1018976950645489\n",
      "Epoch [17055/20000], Training Loss: 0.045662004978761876, Validation Loss: 0.06319020412151335\n",
      "Epoch [17056/20000], Training Loss: 0.017910425990651544, Validation Loss: 0.006050231862900546\n",
      "Epoch [17057/20000], Training Loss: 0.007447241531605998, Validation Loss: 0.007224339334487121\n",
      "Epoch [17058/20000], Training Loss: 0.008108621340397804, Validation Loss: 0.006206600714059667\n",
      "Epoch [17059/20000], Training Loss: 0.006644651792677385, Validation Loss: 0.006991956945164698\n",
      "Epoch [17060/20000], Training Loss: 0.005905953115351232, Validation Loss: 0.004659742161298449\n",
      "Epoch [17061/20000], Training Loss: 0.0048571790435484475, Validation Loss: 0.004837363682416448\n",
      "Epoch [17062/20000], Training Loss: 0.004977830683092829, Validation Loss: 0.0041423892078569224\n",
      "Epoch [17063/20000], Training Loss: 0.004677311885253792, Validation Loss: 0.006611231102653599\n",
      "Epoch [17064/20000], Training Loss: 0.0054218159431392065, Validation Loss: 0.0046825256272151295\n",
      "Epoch [17065/20000], Training Loss: 0.015133151136978995, Validation Loss: 0.010110299720729441\n",
      "Epoch [17066/20000], Training Loss: 0.006963112631118358, Validation Loss: 0.006449176933875985\n",
      "Epoch [17067/20000], Training Loss: 0.006054036412998747, Validation Loss: 0.008527974771034076\n",
      "Epoch [17068/20000], Training Loss: 0.009645122916221485, Validation Loss: 0.008221890857592078\n",
      "Epoch [17069/20000], Training Loss: 0.016158118980716348, Validation Loss: 0.015510220655284814\n",
      "Epoch [17070/20000], Training Loss: 0.0120714518956707, Validation Loss: 0.008265870004704214\n",
      "Epoch [17071/20000], Training Loss: 0.008454394306422077, Validation Loss: 0.003313560634365688\n",
      "Epoch [17072/20000], Training Loss: 0.003758383317615101, Validation Loss: 0.0036898188250761776\n",
      "Epoch [17073/20000], Training Loss: 0.005337491835234687, Validation Loss: 0.012113357496109992\n",
      "Epoch [17074/20000], Training Loss: 0.007304851863799351, Validation Loss: 0.003679843547152276\n",
      "Epoch [17075/20000], Training Loss: 0.011235172848630879, Validation Loss: 0.003517231568229917\n",
      "Epoch [17076/20000], Training Loss: 0.0031640895425536337, Validation Loss: 0.004496166353725721\n",
      "Epoch [17077/20000], Training Loss: 0.004134833360564828, Validation Loss: 0.003149601868879407\n",
      "Epoch [17078/20000], Training Loss: 0.007170238033203142, Validation Loss: 0.002604543611261104\n",
      "Epoch [17079/20000], Training Loss: 0.01715248479324925, Validation Loss: 0.017058054080146285\n",
      "Epoch [17080/20000], Training Loss: 0.007171462003368235, Validation Loss: 0.00601860674441923\n",
      "Epoch [17081/20000], Training Loss: 0.008519499909847841, Validation Loss: 0.003809089640171237\n",
      "Epoch [17082/20000], Training Loss: 0.008389649780708299, Validation Loss: 0.009046474453271003\n",
      "Epoch [17083/20000], Training Loss: 0.005948391741575116, Validation Loss: 0.009513906342687917\n",
      "Epoch [17084/20000], Training Loss: 0.004603406537041467, Validation Loss: 0.004640647424845871\n",
      "Epoch [17085/20000], Training Loss: 0.002489520298175713, Validation Loss: 0.0066862916618271265\n",
      "Epoch [17086/20000], Training Loss: 0.0035625814284035834, Validation Loss: 0.011854736561580077\n",
      "Epoch [17087/20000], Training Loss: 0.006097018634526259, Validation Loss: 0.003446682647247494\n",
      "Epoch [17088/20000], Training Loss: 0.005982528293056281, Validation Loss: 0.003023392128194207\n",
      "Epoch [17089/20000], Training Loss: 0.007430016750212027, Validation Loss: 0.003301939259759176\n",
      "Epoch [17090/20000], Training Loss: 0.010519199255700837, Validation Loss: 0.0022147174165401446\n",
      "Epoch [17091/20000], Training Loss: 0.019685195745426296, Validation Loss: 0.025562260033828898\n",
      "Epoch [17092/20000], Training Loss: 0.019383778418289564, Validation Loss: 0.008746941627561813\n",
      "Epoch [17093/20000], Training Loss: 0.005091333337839542, Validation Loss: 0.00254861060115135\n",
      "Epoch [17094/20000], Training Loss: 0.0034575026168438073, Validation Loss: 0.004829080434008892\n",
      "Epoch [17095/20000], Training Loss: 0.0036929061957705145, Validation Loss: 0.0045909900182112195\n",
      "Epoch [17096/20000], Training Loss: 0.00956728173670334, Validation Loss: 0.005663785721232606\n",
      "Epoch [17097/20000], Training Loss: 0.008496804055799398, Validation Loss: 0.01599804940633507\n",
      "Epoch [17098/20000], Training Loss: 0.010556353412019754, Validation Loss: 0.008182954872973713\n",
      "Epoch [17099/20000], Training Loss: 0.00536674136266616, Validation Loss: 0.010295203953454573\n",
      "Epoch [17100/20000], Training Loss: 0.010873854677308867, Validation Loss: 0.00243768950345169\n",
      "Epoch [17101/20000], Training Loss: 0.00864465059380797, Validation Loss: 0.010781353805220412\n",
      "Epoch [17102/20000], Training Loss: 0.006899360954970949, Validation Loss: 0.0028723596726657158\n",
      "Epoch [17103/20000], Training Loss: 0.0071934972947929054, Validation Loss: 0.002366937425634446\n",
      "Epoch [17104/20000], Training Loss: 0.0042221139945987874, Validation Loss: 0.002486547884754791\n",
      "Epoch [17105/20000], Training Loss: 0.0038657476255009116, Validation Loss: 0.0035604518415590725\n",
      "Epoch [17106/20000], Training Loss: 0.006976542911754028, Validation Loss: 0.0022162335398108824\n",
      "Epoch [17107/20000], Training Loss: 0.005839243434560818, Validation Loss: 0.002008334766896182\n",
      "Epoch [17108/20000], Training Loss: 0.003953806166747041, Validation Loss: 0.006342691993048837\n",
      "Epoch [17109/20000], Training Loss: 0.004833488038250218, Validation Loss: 0.0017936656542999632\n",
      "Epoch [17110/20000], Training Loss: 0.004207530779240187, Validation Loss: 0.0027659056690799155\n",
      "Epoch [17111/20000], Training Loss: 0.004206262783554848, Validation Loss: 0.004108417285985004\n",
      "Epoch [17112/20000], Training Loss: 0.005227159591592161, Validation Loss: 0.002514111931341339\n",
      "Epoch [17113/20000], Training Loss: 0.011392764578756345, Validation Loss: 0.0029089966178137733\n",
      "Epoch [17114/20000], Training Loss: 0.02131068044764106, Validation Loss: 0.02482372309005346\n",
      "Epoch [17115/20000], Training Loss: 0.0352877453613993, Validation Loss: 0.009911582178876088\n",
      "Epoch [17116/20000], Training Loss: 0.0533040515952702, Validation Loss: 0.007276033684539536\n",
      "Epoch [17117/20000], Training Loss: 0.05275139262500618, Validation Loss: 0.03922158575733192\n",
      "Epoch [17118/20000], Training Loss: 0.031600195997660716, Validation Loss: 0.01414620307342115\n",
      "Epoch [17119/20000], Training Loss: 0.03108995677238064, Validation Loss: 0.007924760231097545\n",
      "Epoch [17120/20000], Training Loss: 0.011888077628100291, Validation Loss: 0.00904487890416994\n",
      "Epoch [17121/20000], Training Loss: 0.008790540741756558, Validation Loss: 0.006180086685258094\n",
      "Epoch [17122/20000], Training Loss: 0.006853895220306835, Validation Loss: 0.005690961077399541\n",
      "Epoch [17123/20000], Training Loss: 0.0063779180969244665, Validation Loss: 0.006129596757726595\n",
      "Epoch [17124/20000], Training Loss: 0.006372810561775363, Validation Loss: 0.004057990835271141\n",
      "Epoch [17125/20000], Training Loss: 0.005034515849859288, Validation Loss: 0.002962626045193214\n",
      "Epoch [17126/20000], Training Loss: 0.00457367596772965, Validation Loss: 0.012077126851805606\n",
      "Epoch [17127/20000], Training Loss: 0.006111388913788167, Validation Loss: 0.0023503802576943906\n",
      "Epoch [17128/20000], Training Loss: 0.011644742237487142, Validation Loss: 0.016311731719045954\n",
      "Epoch [17129/20000], Training Loss: 0.02007643007841661, Validation Loss: 0.009200601369652108\n",
      "Epoch [17130/20000], Training Loss: 0.01115098948918915, Validation Loss: 0.007967119440815492\n",
      "Epoch [17131/20000], Training Loss: 0.008132396250273035, Validation Loss: 0.005209338502985312\n",
      "Epoch [17132/20000], Training Loss: 0.011371870460737097, Validation Loss: 0.008647367047111791\n",
      "Epoch [17133/20000], Training Loss: 0.010450097058700132, Validation Loss: 0.005798695992053483\n",
      "Epoch [17134/20000], Training Loss: 0.01450097419602311, Validation Loss: 0.002930265074407349\n",
      "Epoch [17135/20000], Training Loss: 0.006330352305602511, Validation Loss: 0.012007084316952696\n",
      "Epoch [17136/20000], Training Loss: 0.0056179496576861864, Validation Loss: 0.0037631083024016426\n",
      "Epoch [17137/20000], Training Loss: 0.006735618246758739, Validation Loss: 0.004606012195789473\n",
      "Epoch [17138/20000], Training Loss: 0.037103395866649534, Validation Loss: 0.009640628269382358\n",
      "Epoch [17139/20000], Training Loss: 0.03841394977981898, Validation Loss: 0.020914093952991892\n",
      "Epoch [17140/20000], Training Loss: 0.01933279178047087, Validation Loss: 0.004977118613287043\n",
      "Epoch [17141/20000], Training Loss: 0.013975142134378464, Validation Loss: 0.005242833125951815\n",
      "Epoch [17142/20000], Training Loss: 0.005900928347338257, Validation Loss: 0.0055781587059970305\n",
      "Epoch [17143/20000], Training Loss: 0.0062108927273324555, Validation Loss: 0.005803720077938714\n",
      "Epoch [17144/20000], Training Loss: 0.0056196654742442275, Validation Loss: 0.004883989366589958\n",
      "Epoch [17145/20000], Training Loss: 0.006315172267412501, Validation Loss: 0.003096317994384939\n",
      "Epoch [17146/20000], Training Loss: 0.017901832666601485, Validation Loss: 0.012767559183739266\n",
      "Epoch [17147/20000], Training Loss: 0.01477384722082726, Validation Loss: 0.008758727921009637\n",
      "Epoch [17148/20000], Training Loss: 0.008695747160735274, Validation Loss: 0.004250225656887453\n",
      "Epoch [17149/20000], Training Loss: 0.006155645667247102, Validation Loss: 0.005849628784541697\n",
      "Epoch [17150/20000], Training Loss: 0.010906398224636047, Validation Loss: 0.004307012789657172\n",
      "Epoch [17151/20000], Training Loss: 0.009513714994422376, Validation Loss: 0.024858835552420928\n",
      "Epoch [17152/20000], Training Loss: 0.01416597403918526, Validation Loss: 0.0030475462917252116\n",
      "Epoch [17153/20000], Training Loss: 0.004105661812770579, Validation Loss: 0.003387806066273567\n",
      "Epoch [17154/20000], Training Loss: 0.004128196879970046, Validation Loss: 0.012645688240596985\n",
      "Epoch [17155/20000], Training Loss: 0.006695123605043461, Validation Loss: 0.003415857394507401\n",
      "Epoch [17156/20000], Training Loss: 0.006953653648711874, Validation Loss: 0.005514629717278967\n",
      "Epoch [17157/20000], Training Loss: 0.009134586576172816, Validation Loss: 0.005893776385391902\n",
      "Epoch [17158/20000], Training Loss: 0.007380998418739182, Validation Loss: 0.0023143198679248045\n",
      "Epoch [17159/20000], Training Loss: 0.012867325481498224, Validation Loss: 0.007724111180059524\n",
      "Epoch [17160/20000], Training Loss: 0.011576165033537629, Validation Loss: 0.015653598089039874\n",
      "Epoch [17161/20000], Training Loss: 0.013640732661770016, Validation Loss: 0.003553316453614944\n",
      "Epoch [17162/20000], Training Loss: 0.011075857575698527, Validation Loss: 0.030378574952529953\n",
      "Epoch [17163/20000], Training Loss: 0.016442209574701598, Validation Loss: 0.006789971037128291\n",
      "Epoch [17164/20000], Training Loss: 0.01030816618627536, Validation Loss: 0.004322553570161218\n",
      "Epoch [17165/20000], Training Loss: 0.021099273514534746, Validation Loss: 0.016852813374687362\n",
      "Epoch [17166/20000], Training Loss: 0.008053771702439658, Validation Loss: 0.006755090884427923\n",
      "Epoch [17167/20000], Training Loss: 0.004006160421080754, Validation Loss: 0.003945010566641015\n",
      "Epoch [17168/20000], Training Loss: 0.005487258912970512, Validation Loss: 0.004495630065670346\n",
      "Epoch [17169/20000], Training Loss: 0.003722275965219457, Validation Loss: 0.006267074975068318\n",
      "Epoch [17170/20000], Training Loss: 0.004999747863621451, Validation Loss: 0.0028647346319823036\n",
      "Epoch [17171/20000], Training Loss: 0.0051643306663079, Validation Loss: 0.0027457708066975783\n",
      "Epoch [17172/20000], Training Loss: 0.011143237511922572, Validation Loss: 0.007319731980333343\n",
      "Epoch [17173/20000], Training Loss: 0.01036664110298651, Validation Loss: 0.0044429752359990615\n",
      "Epoch [17174/20000], Training Loss: 0.00605492886146359, Validation Loss: 0.007776858352188455\n",
      "Epoch [17175/20000], Training Loss: 0.0047331657858324305, Validation Loss: 0.005167254611486831\n",
      "Epoch [17176/20000], Training Loss: 0.007165208299706397, Validation Loss: 0.010391248057515184\n",
      "Epoch [17177/20000], Training Loss: 0.004430085004189875, Validation Loss: 0.003840303303554752\n",
      "Epoch [17178/20000], Training Loss: 0.009293082230215077, Validation Loss: 0.015263054159415137\n",
      "Epoch [17179/20000], Training Loss: 0.010307558321593595, Validation Loss: 0.003114149636234988\n",
      "Epoch [17180/20000], Training Loss: 0.012170540576854754, Validation Loss: 0.004322112197741986\n",
      "Epoch [17181/20000], Training Loss: 0.006781578581597257, Validation Loss: 0.006118642665774572\n",
      "Epoch [17182/20000], Training Loss: 0.009907433654810964, Validation Loss: 0.007078128741436984\n",
      "Epoch [17183/20000], Training Loss: 0.006323266677749676, Validation Loss: 0.0032154452511703457\n",
      "Epoch [17184/20000], Training Loss: 0.010714603371265443, Validation Loss: 0.01178670874137611\n",
      "Epoch [17185/20000], Training Loss: 0.010889843838542999, Validation Loss: 0.003493402343077077\n",
      "Epoch [17186/20000], Training Loss: 0.006557844643664014, Validation Loss: 0.0043935306202307345\n",
      "Epoch [17187/20000], Training Loss: 0.019819398088397326, Validation Loss: 0.0067099306259391645\n",
      "Epoch [17188/20000], Training Loss: 0.008488515863843662, Validation Loss: 0.008859298199597422\n",
      "Epoch [17189/20000], Training Loss: 0.009775049409654457, Validation Loss: 0.00335949092296055\n",
      "Epoch [17190/20000], Training Loss: 0.006672534713288769, Validation Loss: 0.003945090888931547\n",
      "Epoch [17191/20000], Training Loss: 0.005033664470764572, Validation Loss: 0.0026389980963290277\n",
      "Epoch [17192/20000], Training Loss: 0.0045867728320964874, Validation Loss: 0.01888794398741392\n",
      "Epoch [17193/20000], Training Loss: 0.011553284371651833, Validation Loss: 0.01795254275202759\n",
      "Epoch [17194/20000], Training Loss: 0.009193230064348816, Validation Loss: 0.004164526152767768\n",
      "Epoch [17195/20000], Training Loss: 0.006047586729567099, Validation Loss: 0.002602370129443849\n",
      "Epoch [17196/20000], Training Loss: 0.006334441946299714, Validation Loss: 0.006472746832790126\n",
      "Epoch [17197/20000], Training Loss: 0.007581132779835441, Validation Loss: 0.0033986445301659807\n",
      "Epoch [17198/20000], Training Loss: 0.005487706349404066, Validation Loss: 0.004329424635057276\n",
      "Epoch [17199/20000], Training Loss: 0.00603011798609389, Validation Loss: 0.0028322290154150697\n",
      "Epoch [17200/20000], Training Loss: 0.003709942173112982, Validation Loss: 0.002393483231204474\n",
      "Epoch [17201/20000], Training Loss: 0.004041806177026176, Validation Loss: 0.004324043435811323\n",
      "Epoch [17202/20000], Training Loss: 0.019680515572669526, Validation Loss: 0.026297886456762374\n",
      "Epoch [17203/20000], Training Loss: 0.012313811617267285, Validation Loss: 0.013687098931045605\n",
      "Epoch [17204/20000], Training Loss: 0.016279688246494776, Validation Loss: 0.025708717960355124\n",
      "Epoch [17205/20000], Training Loss: 0.030652472231736283, Validation Loss: 0.006946849495444962\n",
      "Epoch [17206/20000], Training Loss: 0.008348818302952818, Validation Loss: 0.004969502738284908\n",
      "Epoch [17207/20000], Training Loss: 0.005882384770042596, Validation Loss: 0.003630312214623082\n",
      "Epoch [17208/20000], Training Loss: 0.004169080870009826, Validation Loss: 0.0041676281047887415\n",
      "Epoch [17209/20000], Training Loss: 0.00699167639101818, Validation Loss: 0.0026924280841903936\n",
      "Epoch [17210/20000], Training Loss: 0.005021204136677885, Validation Loss: 0.0035103782865537614\n",
      "Epoch [17211/20000], Training Loss: 0.00556444750951154, Validation Loss: 0.0036482134995066773\n",
      "Epoch [17212/20000], Training Loss: 0.005726752083449226, Validation Loss: 0.021184004044959365\n",
      "Epoch [17213/20000], Training Loss: 0.014499539846708234, Validation Loss: 0.015258866314687631\n",
      "Epoch [17214/20000], Training Loss: 0.007920413629368081, Validation Loss: 0.006147085822111196\n",
      "Epoch [17215/20000], Training Loss: 0.010916108721825626, Validation Loss: 0.006381111253467094\n",
      "Epoch [17216/20000], Training Loss: 0.006415538906627002, Validation Loss: 0.008296966552738472\n",
      "Epoch [17217/20000], Training Loss: 0.011216835168722485, Validation Loss: 0.005029804273731754\n",
      "Epoch [17218/20000], Training Loss: 0.009705673562296267, Validation Loss: 0.003548364811737148\n",
      "Epoch [17219/20000], Training Loss: 0.005796831960911472, Validation Loss: 0.003906484654805634\n",
      "Epoch [17220/20000], Training Loss: 0.005200208046257363, Validation Loss: 0.0039058032591847386\n",
      "Epoch [17221/20000], Training Loss: 0.006356943412226558, Validation Loss: 0.0029582096935598656\n",
      "Epoch [17222/20000], Training Loss: 0.0069884190935616585, Validation Loss: 0.002751114616267155\n",
      "Epoch [17223/20000], Training Loss: 0.012975326329719141, Validation Loss: 0.008017005299087538\n",
      "Epoch [17224/20000], Training Loss: 0.019445385152981283, Validation Loss: 0.014994840536800754\n",
      "Epoch [17225/20000], Training Loss: 0.010421151558278195, Validation Loss: 0.008078367386766169\n",
      "Epoch [17226/20000], Training Loss: 0.010791377196645564, Validation Loss: 0.009497050994286759\n",
      "Epoch [17227/20000], Training Loss: 0.009971394450466116, Validation Loss: 0.012753026672304\n",
      "Epoch [17228/20000], Training Loss: 0.007347224870889997, Validation Loss: 0.003048235675622481\n",
      "Epoch [17229/20000], Training Loss: 0.006621259301027749, Validation Loss: 0.024509372721825434\n",
      "Epoch [17230/20000], Training Loss: 0.007722502622657755, Validation Loss: 0.023696918746346846\n",
      "Epoch [17231/20000], Training Loss: 0.009350779799238554, Validation Loss: 0.011044456756902965\n",
      "Epoch [17232/20000], Training Loss: 0.006063354970787519, Validation Loss: 0.0025662821476120368\n",
      "Epoch [17233/20000], Training Loss: 0.0059358535976831005, Validation Loss: 0.006391948461131035\n",
      "Epoch [17234/20000], Training Loss: 0.004387866242073609, Validation Loss: 0.009958375777553014\n",
      "Epoch [17235/20000], Training Loss: 0.0044331150342519065, Validation Loss: 0.0022634738596347403\n",
      "Epoch [17236/20000], Training Loss: 0.0047172543630045515, Validation Loss: 0.00431289369453834\n",
      "Epoch [17237/20000], Training Loss: 0.008786981242467715, Validation Loss: 0.00770688439453518\n",
      "Epoch [17238/20000], Training Loss: 0.002339895072249679, Validation Loss: 0.0036768455203011197\n",
      "Epoch [17239/20000], Training Loss: 0.003569264272365607, Validation Loss: 0.002197559921845102\n",
      "Epoch [17240/20000], Training Loss: 0.004136994725351022, Validation Loss: 0.004410617685594621\n",
      "Epoch [17241/20000], Training Loss: 0.003900462433388644, Validation Loss: 0.01150491901063333\n",
      "Epoch [17242/20000], Training Loss: 0.007851321246042582, Validation Loss: 0.004729951723577812\n",
      "Epoch [17243/20000], Training Loss: 0.006415752437533229, Validation Loss: 0.003292968719533107\n",
      "Epoch [17244/20000], Training Loss: 0.00773970484189184, Validation Loss: 0.014159884969038852\n",
      "Epoch [17245/20000], Training Loss: 0.006535386216195808, Validation Loss: 0.003793938385003449\n",
      "Epoch [17246/20000], Training Loss: 0.008609332247812875, Validation Loss: 0.0045858045898555955\n",
      "Epoch [17247/20000], Training Loss: 0.006344103159790393, Validation Loss: 0.006759060178524123\n",
      "Epoch [17248/20000], Training Loss: 0.008665791651115537, Validation Loss: 0.031802878848145644\n",
      "Epoch [17249/20000], Training Loss: 0.015755790192218098, Validation Loss: 0.048420709158693044\n",
      "Epoch [17250/20000], Training Loss: 0.020284458261877653, Validation Loss: 0.017350460269621445\n",
      "Epoch [17251/20000], Training Loss: 0.025934462842997164, Validation Loss: 0.023193116166761958\n",
      "Epoch [17252/20000], Training Loss: 0.016746851214390648, Validation Loss: 0.006120834375037511\n",
      "Epoch [17253/20000], Training Loss: 0.009813085246735551, Validation Loss: 0.00448907302297264\n",
      "Epoch [17254/20000], Training Loss: 0.004731186979760034, Validation Loss: 0.004442919317158646\n",
      "Epoch [17255/20000], Training Loss: 0.005321162062633048, Validation Loss: 0.003678025356472912\n",
      "Epoch [17256/20000], Training Loss: 0.007853160597733222, Validation Loss: 0.006392105881663447\n",
      "Epoch [17257/20000], Training Loss: 0.0075937694325278115, Validation Loss: 0.02013721367960995\n",
      "Epoch [17258/20000], Training Loss: 0.017101001251181254, Validation Loss: 0.00939217424352831\n",
      "Epoch [17259/20000], Training Loss: 0.021166733615245903, Validation Loss: 0.04830574989318848\n",
      "Epoch [17260/20000], Training Loss: 0.018062234339400414, Validation Loss: 0.027120706758328856\n",
      "Epoch [17261/20000], Training Loss: 0.02117904262766907, Validation Loss: 0.02120873810989524\n",
      "Epoch [17262/20000], Training Loss: 0.031648394990043016, Validation Loss: 0.017623933625061147\n",
      "Epoch [17263/20000], Training Loss: 0.026271749723101884, Validation Loss: 0.014613795526299458\n",
      "Epoch [17264/20000], Training Loss: 0.012105009639526543, Validation Loss: 0.011233606109667475\n",
      "Epoch [17265/20000], Training Loss: 0.006781101746517899, Validation Loss: 0.004750229991805456\n",
      "Epoch [17266/20000], Training Loss: 0.005376685194538108, Validation Loss: 0.004315163451536238\n",
      "Epoch [17267/20000], Training Loss: 0.0035652933958252625, Validation Loss: 0.0034958857084560874\n",
      "Epoch [17268/20000], Training Loss: 0.00506078042963054, Validation Loss: 0.003937873425558686\n",
      "Epoch [17269/20000], Training Loss: 0.00456727817020562, Validation Loss: 0.003223278166075031\n",
      "Epoch [17270/20000], Training Loss: 0.006798215058682088, Validation Loss: 0.0031017435233414875\n",
      "Epoch [17271/20000], Training Loss: 0.004403230060623693, Validation Loss: 0.004022815966755774\n",
      "Epoch [17272/20000], Training Loss: 0.006188335246114158, Validation Loss: 0.003271548562685338\n",
      "Epoch [17273/20000], Training Loss: 0.005298662989454377, Validation Loss: 0.002482050783407606\n",
      "Epoch [17274/20000], Training Loss: 0.008393742479191653, Validation Loss: 0.006622191117183921\n",
      "Epoch [17275/20000], Training Loss: 0.008522855545119714, Validation Loss: 0.006038790177890623\n",
      "Epoch [17276/20000], Training Loss: 0.007156452222781289, Validation Loss: 0.004589836346033095\n",
      "Epoch [17277/20000], Training Loss: 0.008486160105348972, Validation Loss: 0.004662932695224773\n",
      "Epoch [17278/20000], Training Loss: 0.03030821281996968, Validation Loss: 0.017879311287958086\n",
      "Epoch [17279/20000], Training Loss: 0.023834321946196724, Validation Loss: 0.004343663535996102\n",
      "Epoch [17280/20000], Training Loss: 0.005476785108320266, Validation Loss: 0.006017154982041215\n",
      "Epoch [17281/20000], Training Loss: 0.010986936804589018, Validation Loss: 0.003747860180203994\n",
      "Epoch [17282/20000], Training Loss: 0.00679002526054059, Validation Loss: 0.0040968943793135105\n",
      "Epoch [17283/20000], Training Loss: 0.004640309738793543, Validation Loss: 0.0038210484585365773\n",
      "Epoch [17284/20000], Training Loss: 0.005798311840278204, Validation Loss: 0.0066940053698833855\n",
      "Epoch [17285/20000], Training Loss: 0.007677153640543111, Validation Loss: 0.006776033585427095\n",
      "Epoch [17286/20000], Training Loss: 0.0175827259642704, Validation Loss: 0.004044706689279888\n",
      "Epoch [17287/20000], Training Loss: 0.010427517394418828, Validation Loss: 0.011991169187240304\n",
      "Epoch [17288/20000], Training Loss: 0.008922726977159203, Validation Loss: 0.004128684522726759\n",
      "Epoch [17289/20000], Training Loss: 0.01131471351878385, Validation Loss: 0.01585008789386069\n",
      "Epoch [17290/20000], Training Loss: 0.009841504499230982, Validation Loss: 0.0036686134160012667\n",
      "Epoch [17291/20000], Training Loss: 0.006047350537431028, Validation Loss: 0.00359492607699314\n",
      "Epoch [17292/20000], Training Loss: 0.004442004165477036, Validation Loss: 0.0036266269081173842\n",
      "Epoch [17293/20000], Training Loss: 0.0043392603433208676, Validation Loss: 0.0027241807055297727\n",
      "Epoch [17294/20000], Training Loss: 0.005200862007248881, Validation Loss: 0.002867499898734547\n",
      "Epoch [17295/20000], Training Loss: 0.00861412877098441, Validation Loss: 0.005236117068672664\n",
      "Epoch [17296/20000], Training Loss: 0.008058605235938947, Validation Loss: 0.008180052457776454\n",
      "Epoch [17297/20000], Training Loss: 0.006770207739983951, Validation Loss: 0.0072654062615976\n",
      "Epoch [17298/20000], Training Loss: 0.005629226382942372, Validation Loss: 0.012917126486358567\n",
      "Epoch [17299/20000], Training Loss: 0.0077654771469367135, Validation Loss: 0.005975267251155206\n",
      "Epoch [17300/20000], Training Loss: 0.005112625556648709, Validation Loss: 0.002620398540914593\n",
      "Epoch [17301/20000], Training Loss: 0.012389087532379366, Validation Loss: 0.04608891159296036\n",
      "Epoch [17302/20000], Training Loss: 0.03423262747881901, Validation Loss: 0.01917839181675975\n",
      "Epoch [17303/20000], Training Loss: 0.01510598922842681, Validation Loss: 0.006768695777107021\n",
      "Epoch [17304/20000], Training Loss: 0.011366081843269578, Validation Loss: 0.010004455474187888\n",
      "Epoch [17305/20000], Training Loss: 0.006086710518242658, Validation Loss: 0.004282536498141936\n",
      "Epoch [17306/20000], Training Loss: 0.005806241973914439, Validation Loss: 0.00402612054306977\n",
      "Epoch [17307/20000], Training Loss: 0.016956832887704616, Validation Loss: 0.0034413761851733887\n",
      "Epoch [17308/20000], Training Loss: 0.013429908746599852, Validation Loss: 0.008292665960221221\n",
      "Epoch [17309/20000], Training Loss: 0.007631585103808902, Validation Loss: 0.01073528593740363\n",
      "Epoch [17310/20000], Training Loss: 0.009539068156690129, Validation Loss: 0.004361349435664559\n",
      "Epoch [17311/20000], Training Loss: 0.0060568160645613845, Validation Loss: 0.006536542643776296\n",
      "Epoch [17312/20000], Training Loss: 0.017100486130012933, Validation Loss: 0.009109716155372942\n",
      "Epoch [17313/20000], Training Loss: 0.014041832673163819, Validation Loss: 0.007594616602825026\n",
      "Epoch [17314/20000], Training Loss: 0.014422849814374266, Validation Loss: 0.0037376801988153424\n",
      "Epoch [17315/20000], Training Loss: 0.020674129875260405, Validation Loss: 0.016624736466774644\n",
      "Epoch [17316/20000], Training Loss: 0.012421718984894272, Validation Loss: 0.006372235148022549\n",
      "Epoch [17317/20000], Training Loss: 0.007801988095577274, Validation Loss: 0.01330299761275657\n",
      "Epoch [17318/20000], Training Loss: 0.006809140454736605, Validation Loss: 0.003969399669293645\n",
      "Epoch [17319/20000], Training Loss: 0.008352391134914277, Validation Loss: 0.016960011872340277\n",
      "Epoch [17320/20000], Training Loss: 0.008980692767571392, Validation Loss: 0.0036218818887091076\n",
      "Epoch [17321/20000], Training Loss: 0.005080838206071446, Validation Loss: 0.004331164957854271\n",
      "Epoch [17322/20000], Training Loss: 0.008300133518657406, Validation Loss: 0.013752800811622128\n",
      "Epoch [17323/20000], Training Loss: 0.009777820642089605, Validation Loss: 0.007286331468354287\n",
      "Epoch [17324/20000], Training Loss: 0.008496600248950667, Validation Loss: 0.026345586364836242\n",
      "Epoch [17325/20000], Training Loss: 0.015908170602675194, Validation Loss: 0.01343940040299328\n",
      "Epoch [17326/20000], Training Loss: 0.020842100051839094, Validation Loss: 0.013276647194290742\n",
      "Epoch [17327/20000], Training Loss: 0.020140258885086432, Validation Loss: 0.008421741735316703\n",
      "Epoch [17328/20000], Training Loss: 0.018592201573872962, Validation Loss: 0.01294173088624818\n",
      "Epoch [17329/20000], Training Loss: 0.01919297312425832, Validation Loss: 0.007079380360238982\n",
      "Epoch [17330/20000], Training Loss: 0.009470149203317695, Validation Loss: 0.014319144854618191\n",
      "Epoch [17331/20000], Training Loss: 0.01096196730837359, Validation Loss: 0.02987548283277403\n",
      "Epoch [17332/20000], Training Loss: 0.04187943433128437, Validation Loss: 0.013906226902619731\n",
      "Epoch [17333/20000], Training Loss: 0.013961041903322828, Validation Loss: 0.0060001544508043025\n",
      "Epoch [17334/20000], Training Loss: 0.007540570746641606, Validation Loss: 0.006121491100941482\n",
      "Epoch [17335/20000], Training Loss: 0.004668218682387045, Validation Loss: 0.004095916905971109\n",
      "Epoch [17336/20000], Training Loss: 0.005924827390117571, Validation Loss: 0.004504071580443271\n",
      "Epoch [17337/20000], Training Loss: 0.006331788388446772, Validation Loss: 0.007630689203269849\n",
      "Epoch [17338/20000], Training Loss: 0.006461998651372726, Validation Loss: 0.007442346064505857\n",
      "Epoch [17339/20000], Training Loss: 0.010332569914420933, Validation Loss: 0.005837419024307984\n",
      "Epoch [17340/20000], Training Loss: 0.005796258375734656, Validation Loss: 0.0031885861815533927\n",
      "Epoch [17341/20000], Training Loss: 0.007401070449434753, Validation Loss: 0.004653101359821221\n",
      "Epoch [17342/20000], Training Loss: 0.005041145778510067, Validation Loss: 0.008812594094506495\n",
      "Epoch [17343/20000], Training Loss: 0.005032652812847768, Validation Loss: 0.00574927893584487\n",
      "Epoch [17344/20000], Training Loss: 0.005423556814223828, Validation Loss: 0.00801515952016692\n",
      "Epoch [17345/20000], Training Loss: 0.009939493833891382, Validation Loss: 0.05222101616007941\n",
      "Epoch [17346/20000], Training Loss: 0.038125965413363314, Validation Loss: 0.0867221440587725\n",
      "Epoch [17347/20000], Training Loss: 0.07915765066198739, Validation Loss: 0.024058211395251085\n",
      "Epoch [17348/20000], Training Loss: 0.029646569873826984, Validation Loss: 0.024033696931188127\n",
      "Epoch [17349/20000], Training Loss: 0.016920439764362527, Validation Loss: 0.010060523612214151\n",
      "Epoch [17350/20000], Training Loss: 0.014292618715054621, Validation Loss: 0.006459114261758904\n",
      "Epoch [17351/20000], Training Loss: 0.009257040922031072, Validation Loss: 0.007312857284936981\n",
      "Epoch [17352/20000], Training Loss: 0.006011986024012523, Validation Loss: 0.004986973588598532\n",
      "Epoch [17353/20000], Training Loss: 0.00664965586760705, Validation Loss: 0.004597520995385589\n",
      "Epoch [17354/20000], Training Loss: 0.010393849582344825, Validation Loss: 0.008861608625466033\n",
      "Epoch [17355/20000], Training Loss: 0.008147736946037654, Validation Loss: 0.004714195324600616\n",
      "Epoch [17356/20000], Training Loss: 0.005175668848096393, Validation Loss: 0.004222308807854347\n",
      "Epoch [17357/20000], Training Loss: 0.012007478175551764, Validation Loss: 0.006371625975394376\n",
      "Epoch [17358/20000], Training Loss: 0.013732462866755668, Validation Loss: 0.007751502928794971\n",
      "Epoch [17359/20000], Training Loss: 0.006311792489473841, Validation Loss: 0.005643142255763313\n",
      "Epoch [17360/20000], Training Loss: 0.004732073422928806, Validation Loss: 0.004129086157981416\n",
      "Epoch [17361/20000], Training Loss: 0.004967108113744091, Validation Loss: 0.003472205339514391\n",
      "Epoch [17362/20000], Training Loss: 0.004117124965497558, Validation Loss: 0.005192345546374648\n",
      "Epoch [17363/20000], Training Loss: 0.007694090706146588, Validation Loss: 0.005038789880746297\n",
      "Epoch [17364/20000], Training Loss: 0.004948956229002631, Validation Loss: 0.0037513340565770536\n",
      "Epoch [17365/20000], Training Loss: 0.005964300249096206, Validation Loss: 0.002796533155698341\n",
      "Epoch [17366/20000], Training Loss: 0.004896945701537854, Validation Loss: 0.004872863015324433\n",
      "Epoch [17367/20000], Training Loss: 0.007283988758510012, Validation Loss: 0.019178777134857685\n",
      "Epoch [17368/20000], Training Loss: 0.020296889324007288, Validation Loss: 0.004800155979958936\n",
      "Epoch [17369/20000], Training Loss: 0.017968339327905727, Validation Loss: 0.01700885008501544\n",
      "Epoch [17370/20000], Training Loss: 0.016534215425053844, Validation Loss: 0.007947695105140129\n",
      "Epoch [17371/20000], Training Loss: 0.016241436414670067, Validation Loss: 0.003640675324534445\n",
      "Epoch [17372/20000], Training Loss: 0.00661430379945419, Validation Loss: 0.005129940546786267\n",
      "Epoch [17373/20000], Training Loss: 0.004325351511527385, Validation Loss: 0.0038452012446216744\n",
      "Epoch [17374/20000], Training Loss: 0.004357322371756059, Validation Loss: 0.008521284242848799\n",
      "Epoch [17375/20000], Training Loss: 0.006327202846607959, Validation Loss: 0.003673232432768086\n",
      "Epoch [17376/20000], Training Loss: 0.006584654360527306, Validation Loss: 0.0027970230602850804\n",
      "Epoch [17377/20000], Training Loss: 0.008010740793127167, Validation Loss: 0.0029187693420571563\n",
      "Epoch [17378/20000], Training Loss: 0.01223331867991614, Validation Loss: 0.014792374734367944\n",
      "Epoch [17379/20000], Training Loss: 0.0082772660493252, Validation Loss: 0.007136166494874837\n",
      "Epoch [17380/20000], Training Loss: 0.009822499068312547, Validation Loss: 0.004881001859292633\n",
      "Epoch [17381/20000], Training Loss: 0.008461605067818059, Validation Loss: 0.006118141556864235\n",
      "Epoch [17382/20000], Training Loss: 0.009750409000064662, Validation Loss: 0.005844238180959331\n",
      "Epoch [17383/20000], Training Loss: 0.004687762827545937, Validation Loss: 0.005547203685162165\n",
      "Epoch [17384/20000], Training Loss: 0.01477751916640305, Validation Loss: 0.0028322603289357745\n",
      "Epoch [17385/20000], Training Loss: 0.02622604844412178, Validation Loss: 0.005498145176683725\n",
      "Epoch [17386/20000], Training Loss: 0.040088999671362605, Validation Loss: 0.07882027753761836\n",
      "Epoch [17387/20000], Training Loss: 0.06101528088921831, Validation Loss: 0.0420087607843714\n",
      "Epoch [17388/20000], Training Loss: 0.03712261426283346, Validation Loss: 0.007678607745256878\n",
      "Epoch [17389/20000], Training Loss: 0.016284254395681534, Validation Loss: 0.009919079738243195\n",
      "Epoch [17390/20000], Training Loss: 0.010573645762633532, Validation Loss: 0.009100059008494445\n",
      "Epoch [17391/20000], Training Loss: 0.008069769566645846, Validation Loss: 0.008724715827586391\n",
      "Epoch [17392/20000], Training Loss: 0.00919537025453922, Validation Loss: 0.00532616764921366\n",
      "Epoch [17393/20000], Training Loss: 0.009673975067146654, Validation Loss: 0.012078517249262535\n",
      "Epoch [17394/20000], Training Loss: 0.020966336559337963, Validation Loss: 0.004889865606434452\n",
      "Epoch [17395/20000], Training Loss: 0.028149224967429682, Validation Loss: 0.014126027002931306\n",
      "Epoch [17396/20000], Training Loss: 0.006587333742312954, Validation Loss: 0.004474849319765725\n",
      "Epoch [17397/20000], Training Loss: 0.00551730008857427, Validation Loss: 0.0041425169626242665\n",
      "Epoch [17398/20000], Training Loss: 0.005031835040946524, Validation Loss: 0.003952462972225773\n",
      "Epoch [17399/20000], Training Loss: 0.004863916023168713, Validation Loss: 0.003953345656294397\n",
      "Epoch [17400/20000], Training Loss: 0.005930975545197725, Validation Loss: 0.004589938825297543\n",
      "Epoch [17401/20000], Training Loss: 0.004343666206945532, Validation Loss: 0.003554019212576698\n",
      "Epoch [17402/20000], Training Loss: 0.003727559953916731, Validation Loss: 0.004435104950134981\n",
      "Epoch [17403/20000], Training Loss: 0.005705808265442361, Validation Loss: 0.007739045391129349\n",
      "Epoch [17404/20000], Training Loss: 0.01129136731989482, Validation Loss: 0.0038966996199121307\n",
      "Epoch [17405/20000], Training Loss: 0.008862202542202016, Validation Loss: 0.01632183285892097\n",
      "Epoch [17406/20000], Training Loss: 0.008570543930025971, Validation Loss: 0.004208244466584648\n",
      "Epoch [17407/20000], Training Loss: 0.0066711015284194475, Validation Loss: 0.0052557401307815966\n",
      "Epoch [17408/20000], Training Loss: 0.013841600613626983, Validation Loss: 0.011345438260052968\n",
      "Epoch [17409/20000], Training Loss: 0.007392493833322078, Validation Loss: 0.012622546271554081\n",
      "Epoch [17410/20000], Training Loss: 0.014376442723525023, Validation Loss: 0.004294991060923085\n",
      "Epoch [17411/20000], Training Loss: 0.005073436194964308, Validation Loss: 0.0037646157269855584\n",
      "Epoch [17412/20000], Training Loss: 0.0047425712831942034, Validation Loss: 0.002829229877104441\n",
      "Epoch [17413/20000], Training Loss: 0.006187527463355634, Validation Loss: 0.002626920452724422\n",
      "Epoch [17414/20000], Training Loss: 0.006449460477077602, Validation Loss: 0.00650375940611119\n",
      "Epoch [17415/20000], Training Loss: 0.006257470742379415, Validation Loss: 0.0056981011792759575\n",
      "Epoch [17416/20000], Training Loss: 0.008240977335455162, Validation Loss: 0.0036059708455868427\n",
      "Epoch [17417/20000], Training Loss: 0.024254490578771635, Validation Loss: 0.011851144954562203\n",
      "Epoch [17418/20000], Training Loss: 0.007891499736355658, Validation Loss: 0.00734174574193155\n",
      "Epoch [17419/20000], Training Loss: 0.008648356522465, Validation Loss: 0.012274706470114851\n",
      "Epoch [17420/20000], Training Loss: 0.010350790589914791, Validation Loss: 0.005384904613111829\n",
      "Epoch [17421/20000], Training Loss: 0.005647301572285609, Validation Loss: 0.003770168486942229\n",
      "Epoch [17422/20000], Training Loss: 0.006069195606479687, Validation Loss: 0.009806781741125311\n",
      "Epoch [17423/20000], Training Loss: 0.01388843550168011, Validation Loss: 0.008767446602811495\n",
      "Epoch [17424/20000], Training Loss: 0.015367342653397438, Validation Loss: 0.004661506688914158\n",
      "Epoch [17425/20000], Training Loss: 0.007960135603622933, Validation Loss: 0.003145731831855494\n",
      "Epoch [17426/20000], Training Loss: 0.008030232865057283, Validation Loss: 0.002703499980418869\n",
      "Epoch [17427/20000], Training Loss: 0.00595969370988314, Validation Loss: 0.005636268694486256\n",
      "Epoch [17428/20000], Training Loss: 0.00537770502497941, Validation Loss: 0.002793011430616624\n",
      "Epoch [17429/20000], Training Loss: 0.004565161357667031, Validation Loss: 0.002891340675730208\n",
      "Epoch [17430/20000], Training Loss: 0.005182271652821717, Validation Loss: 0.007206813033138188\n",
      "Epoch [17431/20000], Training Loss: 0.010343814178538326, Validation Loss: 0.010192138542022047\n",
      "Epoch [17432/20000], Training Loss: 0.013956594999850072, Validation Loss: 0.004805340556413081\n",
      "Epoch [17433/20000], Training Loss: 0.012038040099598999, Validation Loss: 0.011486531870473231\n",
      "Epoch [17434/20000], Training Loss: 0.007677326495987862, Validation Loss: 0.009439955390236423\n",
      "Epoch [17435/20000], Training Loss: 0.00800414813547994, Validation Loss: 0.003438218164210982\n",
      "Epoch [17436/20000], Training Loss: 0.01090290188689583, Validation Loss: 0.01373402827552428\n",
      "Epoch [17437/20000], Training Loss: 0.010194859494471789, Validation Loss: 0.020425320735999515\n",
      "Epoch [17438/20000], Training Loss: 0.018696838929567354, Validation Loss: 0.005344155044963633\n",
      "Epoch [17439/20000], Training Loss: 0.009506736329058185, Validation Loss: 0.005740980484659186\n",
      "Epoch [17440/20000], Training Loss: 0.008187789636784666, Validation Loss: 0.0033446858602807763\n",
      "Epoch [17441/20000], Training Loss: 0.004745811595057603, Validation Loss: 0.004027992443542112\n",
      "Epoch [17442/20000], Training Loss: 0.003975902859695323, Validation Loss: 0.010820380545088323\n",
      "Epoch [17443/20000], Training Loss: 0.0050749011258761, Validation Loss: 0.0026395367209254006\n",
      "Epoch [17444/20000], Training Loss: 0.014152582429752718, Validation Loss: 0.0026664974949616904\n",
      "Epoch [17445/20000], Training Loss: 0.00832768909198681, Validation Loss: 0.0045112463911731755\n",
      "Epoch [17446/20000], Training Loss: 0.004034852938340399, Validation Loss: 0.004000798273470263\n",
      "Epoch [17447/20000], Training Loss: 0.00275035747212574, Validation Loss: 0.00658274734658859\n",
      "Epoch [17448/20000], Training Loss: 0.01038576371398189, Validation Loss: 0.005418426423732743\n",
      "Epoch [17449/20000], Training Loss: 0.021436592437177233, Validation Loss: 0.005222217362774363\n",
      "Epoch [17450/20000], Training Loss: 0.010294964090072816, Validation Loss: 0.002310627165168007\n",
      "Epoch [17451/20000], Training Loss: 0.008859485819162469, Validation Loss: 0.007091642987299059\n",
      "Epoch [17452/20000], Training Loss: 0.007549946081488608, Validation Loss: 0.02670613889183318\n",
      "Epoch [17453/20000], Training Loss: 0.011905090718024829, Validation Loss: 0.0171762847208551\n",
      "Epoch [17454/20000], Training Loss: 0.015989909678215293, Validation Loss: 0.002838971895823969\n",
      "Epoch [17455/20000], Training Loss: 0.012840523066448182, Validation Loss: 0.023716805757161348\n",
      "Epoch [17456/20000], Training Loss: 0.013883822674480533, Validation Loss: 0.0048700011892485835\n",
      "Epoch [17457/20000], Training Loss: 0.022997167733722432, Validation Loss: 0.012086081185949964\n",
      "Epoch [17458/20000], Training Loss: 0.01389925576924205, Validation Loss: 0.013432535542441266\n",
      "Epoch [17459/20000], Training Loss: 0.005077937816427688, Validation Loss: 0.00667854215694752\n",
      "Epoch [17460/20000], Training Loss: 0.009997698115642249, Validation Loss: 0.004305365271053794\n",
      "Epoch [17461/20000], Training Loss: 0.006217370078827246, Validation Loss: 0.010191455549959627\n",
      "Epoch [17462/20000], Training Loss: 0.007128760575504854, Validation Loss: 0.0033758002017499245\n",
      "Epoch [17463/20000], Training Loss: 0.006217251530740343, Validation Loss: 0.0024870883084176705\n",
      "Epoch [17464/20000], Training Loss: 0.00420161385015153, Validation Loss: 0.006164910238502365\n",
      "Epoch [17465/20000], Training Loss: 0.00644257706829064, Validation Loss: 0.004829331709925394\n",
      "Epoch [17466/20000], Training Loss: 0.006618555883635834, Validation Loss: 0.0024120940875985465\n",
      "Epoch [17467/20000], Training Loss: 0.004351981628393072, Validation Loss: 0.011356829665601254\n",
      "Epoch [17468/20000], Training Loss: 0.007117082232330826, Validation Loss: 0.0027369981732008027\n",
      "Epoch [17469/20000], Training Loss: 0.004294113676876472, Validation Loss: 0.005018086177887452\n",
      "Epoch [17470/20000], Training Loss: 0.013302588886290323, Validation Loss: 0.008948390704712705\n",
      "Epoch [17471/20000], Training Loss: 0.011875659825558873, Validation Loss: 0.004276577616111661\n",
      "Epoch [17472/20000], Training Loss: 0.00980167682532088, Validation Loss: 0.007112301087805333\n",
      "Epoch [17473/20000], Training Loss: 0.007154207884533597, Validation Loss: 0.068193211682488\n",
      "Epoch [17474/20000], Training Loss: 0.01707848536482613, Validation Loss: 0.0036145697148308362\n",
      "Epoch [17475/20000], Training Loss: 0.01055609100743433, Validation Loss: 0.006194052154164052\n",
      "Epoch [17476/20000], Training Loss: 0.008514079247626276, Validation Loss: 0.01158301691923823\n",
      "Epoch [17477/20000], Training Loss: 0.003857250133868157, Validation Loss: 0.0033728944197901367\n",
      "Epoch [17478/20000], Training Loss: 0.0057376282463371675, Validation Loss: 0.0022285526586460214\n",
      "Epoch [17479/20000], Training Loss: 0.0037795641513897393, Validation Loss: 0.002160376852923477\n",
      "Epoch [17480/20000], Training Loss: 0.005546006916631667, Validation Loss: 0.0032155175951617708\n",
      "Epoch [17481/20000], Training Loss: 0.00809346754561245, Validation Loss: 0.0043917045778861545\n",
      "Epoch [17482/20000], Training Loss: 0.007336813927141504, Validation Loss: 0.0032062034276539525\n",
      "Epoch [17483/20000], Training Loss: 0.005360597291395867, Validation Loss: 0.004063810820562984\n",
      "Epoch [17484/20000], Training Loss: 0.0033943284917898253, Validation Loss: 0.0029806127485125854\n",
      "Epoch [17485/20000], Training Loss: 0.003254893646434149, Validation Loss: 0.005472233834942552\n",
      "Epoch [17486/20000], Training Loss: 0.0038009358099121266, Validation Loss: 0.002684439108445128\n",
      "Epoch [17487/20000], Training Loss: 0.007683228409275346, Validation Loss: 0.004192713791642697\n",
      "Epoch [17488/20000], Training Loss: 0.008408575825991907, Validation Loss: 0.0137532998342067\n",
      "Epoch [17489/20000], Training Loss: 0.013494639349249025, Validation Loss: 0.012677458871621639\n",
      "Epoch [17490/20000], Training Loss: 0.013220006728975673, Validation Loss: 0.010872496595288692\n",
      "Epoch [17491/20000], Training Loss: 0.025093433018420392, Validation Loss: 0.015377163416859161\n",
      "Epoch [17492/20000], Training Loss: 0.01939264224152534, Validation Loss: 0.003704010134762837\n",
      "Epoch [17493/20000], Training Loss: 0.007672099474543107, Validation Loss: 0.008478280589250582\n",
      "Epoch [17494/20000], Training Loss: 0.007818796350225707, Validation Loss: 0.00349972039897338\n",
      "Epoch [17495/20000], Training Loss: 0.006739575596010842, Validation Loss: 0.0040696077654076125\n",
      "Epoch [17496/20000], Training Loss: 0.004212243569782004, Validation Loss: 0.00292249830506892\n",
      "Epoch [17497/20000], Training Loss: 0.004299576945706056, Validation Loss: 0.005723110200571181\n",
      "Epoch [17498/20000], Training Loss: 0.004806020269045673, Validation Loss: 0.0024119289342466057\n",
      "Epoch [17499/20000], Training Loss: 0.004244271278642893, Validation Loss: 0.0024678266906591937\n",
      "Epoch [17500/20000], Training Loss: 0.007251426724538744, Validation Loss: 0.010226587332519037\n",
      "Epoch [17501/20000], Training Loss: 0.010598342703555577, Validation Loss: 0.004712222817753144\n",
      "Epoch [17502/20000], Training Loss: 0.006578095313476037, Validation Loss: 0.0044912108080881795\n",
      "Epoch [17503/20000], Training Loss: 0.011762699505457672, Validation Loss: 0.003227501691859918\n",
      "Epoch [17504/20000], Training Loss: 0.02271374316465621, Validation Loss: 0.03895150437685047\n",
      "Epoch [17505/20000], Training Loss: 0.041538896565595805, Validation Loss: 0.03388936472005976\n",
      "Epoch [17506/20000], Training Loss: 0.016328806251944376, Validation Loss: 0.009266570077629214\n",
      "Epoch [17507/20000], Training Loss: 0.006648162863699879, Validation Loss: 0.00721354177221666\n",
      "Epoch [17508/20000], Training Loss: 0.005124772359132683, Validation Loss: 0.004035941824054569\n",
      "Epoch [17509/20000], Training Loss: 0.004463709358075972, Validation Loss: 0.003422968688395332\n",
      "Epoch [17510/20000], Training Loss: 0.004048024565625903, Validation Loss: 0.007784655317665849\n",
      "Epoch [17511/20000], Training Loss: 0.00786562905789052, Validation Loss: 0.004738994541445538\n",
      "Epoch [17512/20000], Training Loss: 0.008703706407686695, Validation Loss: 0.007940842727194097\n",
      "Epoch [17513/20000], Training Loss: 0.012790276940255094, Validation Loss: 0.01132649553328616\n",
      "Epoch [17514/20000], Training Loss: 0.00677324745005795, Validation Loss: 0.004057455542004382\n",
      "Epoch [17515/20000], Training Loss: 0.005677934329599209, Validation Loss: 0.005164429678449275\n",
      "Epoch [17516/20000], Training Loss: 0.012671420569988965, Validation Loss: 0.004370977636430419\n",
      "Epoch [17517/20000], Training Loss: 0.007734015096502844, Validation Loss: 0.05004486267223131\n",
      "Epoch [17518/20000], Training Loss: 0.018421590668107717, Validation Loss: 0.003348462001153725\n",
      "Epoch [17519/20000], Training Loss: 0.02005977554654237, Validation Loss: 0.0062023781662987375\n",
      "Epoch [17520/20000], Training Loss: 0.02709153924544288, Validation Loss: 0.01948361225160105\n",
      "Epoch [17521/20000], Training Loss: 0.018135003726551906, Validation Loss: 0.05113785607474191\n",
      "Epoch [17522/20000], Training Loss: 0.02153771500057441, Validation Loss: 0.00904569854693767\n",
      "Epoch [17523/20000], Training Loss: 0.014132299690183052, Validation Loss: 0.006794777723768007\n",
      "Epoch [17524/20000], Training Loss: 0.014872847249664898, Validation Loss: 0.058550765007177166\n",
      "Epoch [17525/20000], Training Loss: 0.022839692533515126, Validation Loss: 0.008322702457306994\n",
      "Epoch [17526/20000], Training Loss: 0.007214320301760121, Validation Loss: 0.00813273646470686\n",
      "Epoch [17527/20000], Training Loss: 0.006783918427702572, Validation Loss: 0.006361995091929979\n",
      "Epoch [17528/20000], Training Loss: 0.015044298630008208, Validation Loss: 0.05232764141900199\n",
      "Epoch [17529/20000], Training Loss: 0.032110445647518304, Validation Loss: 0.0068117558425622796\n",
      "Epoch [17530/20000], Training Loss: 0.010571556359796628, Validation Loss: 0.0069707692345322165\n",
      "Epoch [17531/20000], Training Loss: 0.009571066562784836, Validation Loss: 0.0041323125406163095\n",
      "Epoch [17532/20000], Training Loss: 0.007257812407000789, Validation Loss: 0.007216326343561482\n",
      "Epoch [17533/20000], Training Loss: 0.00722008289968861, Validation Loss: 0.005742555883357544\n",
      "Epoch [17534/20000], Training Loss: 0.011020967290141839, Validation Loss: 0.008143738311317162\n",
      "Epoch [17535/20000], Training Loss: 0.01348163266551085, Validation Loss: 0.004855109145884323\n",
      "Epoch [17536/20000], Training Loss: 0.009793444432684087, Validation Loss: 0.003911023859761597\n",
      "Epoch [17537/20000], Training Loss: 0.014929711169311284, Validation Loss: 0.014422715478759379\n",
      "Epoch [17538/20000], Training Loss: 0.012131787774186316, Validation Loss: 0.009135706470387913\n",
      "Epoch [17539/20000], Training Loss: 0.01769771029025183, Validation Loss: 0.018806056518639837\n",
      "Epoch [17540/20000], Training Loss: 0.00870141511586553, Validation Loss: 0.0058777186620454875\n",
      "Epoch [17541/20000], Training Loss: 0.006364936654530798, Validation Loss: 0.0041042729364159794\n",
      "Epoch [17542/20000], Training Loss: 0.004597256500606558, Validation Loss: 0.005224389423217355\n",
      "Epoch [17543/20000], Training Loss: 0.004842914024525921, Validation Loss: 0.0040585071717993875\n",
      "Epoch [17544/20000], Training Loss: 0.0037993256384114865, Validation Loss: 0.003713106270884785\n",
      "Epoch [17545/20000], Training Loss: 0.006136090776635683, Validation Loss: 0.00951529779870596\n",
      "Epoch [17546/20000], Training Loss: 0.01297936241982305, Validation Loss: 0.006077845643242321\n",
      "Epoch [17547/20000], Training Loss: 0.007778798566765285, Validation Loss: 0.0051410659174768655\n",
      "Epoch [17548/20000], Training Loss: 0.01188869155053648, Validation Loss: 0.008421252366757932\n",
      "Epoch [17549/20000], Training Loss: 0.014703942001818047, Validation Loss: 0.003311625681948034\n",
      "Epoch [17550/20000], Training Loss: 0.010089744689009552, Validation Loss: 0.0029525281362574253\n",
      "Epoch [17551/20000], Training Loss: 0.01301887753340582, Validation Loss: 0.005586287245803801\n",
      "Epoch [17552/20000], Training Loss: 0.0062498999717977965, Validation Loss: 0.0033570975564832105\n",
      "Epoch [17553/20000], Training Loss: 0.004481258979857168, Validation Loss: 0.02392870666725295\n",
      "Epoch [17554/20000], Training Loss: 0.007983225239773415, Validation Loss: 0.006117009924700434\n",
      "Epoch [17555/20000], Training Loss: 0.011142274919880688, Validation Loss: 0.005307569536463761\n",
      "Epoch [17556/20000], Training Loss: 0.007502604161345516, Validation Loss: 0.00914141535759387\n",
      "Epoch [17557/20000], Training Loss: 0.008458676328051038, Validation Loss: 0.010036708460705023\n",
      "Epoch [17558/20000], Training Loss: 0.014285658423822107, Validation Loss: 0.005285726467164627\n",
      "Epoch [17559/20000], Training Loss: 0.008480936861555424, Validation Loss: 0.008485901028120767\n",
      "Epoch [17560/20000], Training Loss: 0.012823684039684162, Validation Loss: 0.005300577361152367\n",
      "Epoch [17561/20000], Training Loss: 0.025959128148055503, Validation Loss: 0.008159628108882992\n",
      "Epoch [17562/20000], Training Loss: 0.046595142895967, Validation Loss: 0.02050244169575855\n",
      "Epoch [17563/20000], Training Loss: 0.03795838252907353, Validation Loss: 0.05189185163804463\n",
      "Epoch [17564/20000], Training Loss: 0.0326238070681159, Validation Loss: 0.013500726722538405\n",
      "Epoch [17565/20000], Training Loss: 0.01596322454445596, Validation Loss: 0.02099399422458451\n",
      "Epoch [17566/20000], Training Loss: 0.016631795129277243, Validation Loss: 0.010539866784322149\n",
      "Epoch [17567/20000], Training Loss: 0.006616618234797248, Validation Loss: 0.005142686358053604\n",
      "Epoch [17568/20000], Training Loss: 0.005715490485664855, Validation Loss: 0.004211056784078138\n",
      "Epoch [17569/20000], Training Loss: 0.0046684823776428986, Validation Loss: 0.005647157385423764\n",
      "Epoch [17570/20000], Training Loss: 0.005672733832138225, Validation Loss: 0.004291472433693314\n",
      "Epoch [17571/20000], Training Loss: 0.005467931986329079, Validation Loss: 0.0033570726269465108\n",
      "Epoch [17572/20000], Training Loss: 0.0054669445588453, Validation Loss: 0.004127891800777661\n",
      "Epoch [17573/20000], Training Loss: 0.004834590695151876, Validation Loss: 0.0034953882984331617\n",
      "Epoch [17574/20000], Training Loss: 0.005898841203556263, Validation Loss: 0.006592012124678371\n",
      "Epoch [17575/20000], Training Loss: 0.016404581908789493, Validation Loss: 0.009184632184250212\n",
      "Epoch [17576/20000], Training Loss: 0.007158532871211979, Validation Loss: 0.004050442849161799\n",
      "Epoch [17577/20000], Training Loss: 0.008393903350484575, Validation Loss: 0.012319194988653944\n",
      "Epoch [17578/20000], Training Loss: 0.0063792857171733885, Validation Loss: 0.003465602174577725\n",
      "Epoch [17579/20000], Training Loss: 0.006408958286816154, Validation Loss: 0.0029027787802382433\n",
      "Epoch [17580/20000], Training Loss: 0.00613903390330961, Validation Loss: 0.008045955694147915\n",
      "Epoch [17581/20000], Training Loss: 0.017436060957217187, Validation Loss: 0.014873117208480837\n",
      "Epoch [17582/20000], Training Loss: 0.01145602879945987, Validation Loss: 0.00876303363059251\n",
      "Epoch [17583/20000], Training Loss: 0.013340472853867271, Validation Loss: 0.02527658428464617\n",
      "Epoch [17584/20000], Training Loss: 0.016738068555631407, Validation Loss: 0.007018416587795556\n",
      "Epoch [17585/20000], Training Loss: 0.007385270805960837, Validation Loss: 0.0070932487689744205\n",
      "Epoch [17586/20000], Training Loss: 0.004637420996394732, Validation Loss: 0.00345984507091663\n",
      "Epoch [17587/20000], Training Loss: 0.004360971024912682, Validation Loss: 0.002476180604268058\n",
      "Epoch [17588/20000], Training Loss: 0.00903059889346228, Validation Loss: 0.0028116814127774163\n",
      "Epoch [17589/20000], Training Loss: 0.011054642251110636, Validation Loss: 0.00874396968309509\n",
      "Epoch [17590/20000], Training Loss: 0.005799352831604868, Validation Loss: 0.009353694905299925\n",
      "Epoch [17591/20000], Training Loss: 0.007317130978240779, Validation Loss: 0.0037145162433661013\n",
      "Epoch [17592/20000], Training Loss: 0.007558387652222533, Validation Loss: 0.0023845467284228697\n",
      "Epoch [17593/20000], Training Loss: 0.004712727264891977, Validation Loss: 0.005012713778261654\n",
      "Epoch [17594/20000], Training Loss: 0.00932496374402295, Validation Loss: 0.002842704865474422\n",
      "Epoch [17595/20000], Training Loss: 0.0049666337680329365, Validation Loss: 0.005748041506324629\n",
      "Epoch [17596/20000], Training Loss: 0.005541824408542847, Validation Loss: 0.004951967707837975\n",
      "Epoch [17597/20000], Training Loss: 0.008033290224986953, Validation Loss: 0.003057600206914465\n",
      "Epoch [17598/20000], Training Loss: 0.0073689033355809185, Validation Loss: 0.03885726098503385\n",
      "Epoch [17599/20000], Training Loss: 0.013205149277512516, Validation Loss: 0.0067216031519430385\n",
      "Epoch [17600/20000], Training Loss: 0.008495115038905559, Validation Loss: 0.008046414573186628\n",
      "Epoch [17601/20000], Training Loss: 0.01451306686379082, Validation Loss: 0.013855590726639262\n",
      "Epoch [17602/20000], Training Loss: 0.010178170214520119, Validation Loss: 0.033303351806742806\n",
      "Epoch [17603/20000], Training Loss: 0.011175510431972466, Validation Loss: 0.010293532961181231\n",
      "Epoch [17604/20000], Training Loss: 0.015284912443998369, Validation Loss: 0.004516575551991164\n",
      "Epoch [17605/20000], Training Loss: 0.01754802014485384, Validation Loss: 0.027261596819357654\n",
      "Epoch [17606/20000], Training Loss: 0.02282662131827757, Validation Loss: 0.013748730992435121\n",
      "Epoch [17607/20000], Training Loss: 0.02166448856795406, Validation Loss: 0.005452230838795938\n",
      "Epoch [17608/20000], Training Loss: 0.00808307812050251, Validation Loss: 0.007391179378117287\n",
      "Epoch [17609/20000], Training Loss: 0.005170544563952717, Validation Loss: 0.006067047760423215\n",
      "Epoch [17610/20000], Training Loss: 0.008135271932198, Validation Loss: 0.003403453755220253\n",
      "Epoch [17611/20000], Training Loss: 0.014539875529278237, Validation Loss: 0.007299306297296357\n",
      "Epoch [17612/20000], Training Loss: 0.022384736952051502, Validation Loss: 0.01952008230817464\n",
      "Epoch [17613/20000], Training Loss: 0.015498056595138874, Validation Loss: 0.015685535070385278\n",
      "Epoch [17614/20000], Training Loss: 0.01989559352244084, Validation Loss: 0.015767065592740437\n",
      "Epoch [17615/20000], Training Loss: 0.01750332547609495, Validation Loss: 0.010681417388176294\n",
      "Epoch [17616/20000], Training Loss: 0.014746771464290629, Validation Loss: 0.011543026460070391\n",
      "Epoch [17617/20000], Training Loss: 0.009785802091105975, Validation Loss: 0.010809199751488394\n",
      "Epoch [17618/20000], Training Loss: 0.006457644724702861, Validation Loss: 0.00335974822335158\n",
      "Epoch [17619/20000], Training Loss: 0.006643942170610119, Validation Loss: 0.007589061789705642\n",
      "Epoch [17620/20000], Training Loss: 0.0063751437514188835, Validation Loss: 0.008574682820057232\n",
      "Epoch [17621/20000], Training Loss: 0.004551469954354356, Validation Loss: 0.004887763151494854\n",
      "Epoch [17622/20000], Training Loss: 0.007510913777098592, Validation Loss: 0.004147812608027351\n",
      "Epoch [17623/20000], Training Loss: 0.006135767592890521, Validation Loss: 0.003320711813414213\n",
      "Epoch [17624/20000], Training Loss: 0.005326882154414696, Validation Loss: 0.0032237938971264213\n",
      "Epoch [17625/20000], Training Loss: 0.004709400785421687, Validation Loss: 0.0076263443167726135\n",
      "Epoch [17626/20000], Training Loss: 0.007149771177669858, Validation Loss: 0.009239165314776806\n",
      "Epoch [17627/20000], Training Loss: 0.021683923859070222, Validation Loss: 0.02669758527284439\n",
      "Epoch [17628/20000], Training Loss: 0.013471242871738858, Validation Loss: 0.02234222779848746\n",
      "Epoch [17629/20000], Training Loss: 0.01175951573272219, Validation Loss: 0.04805525924478348\n",
      "Epoch [17630/20000], Training Loss: 0.016489865712043996, Validation Loss: 0.008334701157386227\n",
      "Epoch [17631/20000], Training Loss: 0.011552013857973569, Validation Loss: 0.005835347430066029\n",
      "Epoch [17632/20000], Training Loss: 0.006996841988958684, Validation Loss: 0.006723292620990367\n",
      "Epoch [17633/20000], Training Loss: 0.0045797877685669975, Validation Loss: 0.004058086313785567\n",
      "Epoch [17634/20000], Training Loss: 0.00632421856568856, Validation Loss: 0.004242243627772821\n",
      "Epoch [17635/20000], Training Loss: 0.006197697376982043, Validation Loss: 0.014006386111891813\n",
      "Epoch [17636/20000], Training Loss: 0.003893722540177169, Validation Loss: 0.008504706522587271\n",
      "Epoch [17637/20000], Training Loss: 0.0065181289412846255, Validation Loss: 0.002964047199563366\n",
      "Epoch [17638/20000], Training Loss: 0.006861474384273996, Validation Loss: 0.01218542330233116\n",
      "Epoch [17639/20000], Training Loss: 0.00563017966175851, Validation Loss: 0.009429942336427657\n",
      "Epoch [17640/20000], Training Loss: 0.009396119957594198, Validation Loss: 0.009314410313631295\n",
      "Epoch [17641/20000], Training Loss: 0.00438912663785881, Validation Loss: 0.0031631537886137252\n",
      "Epoch [17642/20000], Training Loss: 0.003920110838667564, Validation Loss: 0.011966440028378789\n",
      "Epoch [17643/20000], Training Loss: 0.0062006282413286885, Validation Loss: 0.003370319394250416\n",
      "Epoch [17644/20000], Training Loss: 0.0022742738934198314, Validation Loss: 0.016466166291918553\n",
      "Epoch [17645/20000], Training Loss: 0.007597886583624813, Validation Loss: 0.004640198567703245\n",
      "Epoch [17646/20000], Training Loss: 0.007348314772700958, Validation Loss: 0.0032167742330277732\n",
      "Epoch [17647/20000], Training Loss: 0.010753501297585899, Validation Loss: 0.008707079206871459\n",
      "Epoch [17648/20000], Training Loss: 0.005195843306345134, Validation Loss: 0.0037422100853056134\n",
      "Epoch [17649/20000], Training Loss: 0.006529476663282756, Validation Loss: 0.011621587710188971\n",
      "Epoch [17650/20000], Training Loss: 0.007250300531658078, Validation Loss: 0.009130615423387072\n",
      "Epoch [17651/20000], Training Loss: 0.004545698661136807, Validation Loss: 0.008522428904793145\n",
      "Epoch [17652/20000], Training Loss: 0.009267349361185293, Validation Loss: 0.005826884215431606\n",
      "Epoch [17653/20000], Training Loss: 0.008742655171123001, Validation Loss: 0.004259240935280915\n",
      "Epoch [17654/20000], Training Loss: 0.03647596497213357, Validation Loss: 0.043367051197506154\n",
      "Epoch [17655/20000], Training Loss: 0.03740079650222989, Validation Loss: 0.02106640597204851\n",
      "Epoch [17656/20000], Training Loss: 0.030468491072367345, Validation Loss: 0.0288561205573907\n",
      "Epoch [17657/20000], Training Loss: 0.031928568743751384, Validation Loss: 0.044461768123259304\n",
      "Epoch [17658/20000], Training Loss: 0.02070262243172952, Validation Loss: 0.005556328139001769\n",
      "Epoch [17659/20000], Training Loss: 0.007467913655675927, Validation Loss: 0.007443925984050891\n",
      "Epoch [17660/20000], Training Loss: 0.006080619017926178, Validation Loss: 0.004910257534447737\n",
      "Epoch [17661/20000], Training Loss: 0.0059536910931845865, Validation Loss: 0.004317856968977399\n",
      "Epoch [17662/20000], Training Loss: 0.0064661281836119345, Validation Loss: 0.007597840758216583\n",
      "Epoch [17663/20000], Training Loss: 0.007917043127235956, Validation Loss: 0.004495864980670815\n",
      "Epoch [17664/20000], Training Loss: 0.0107012509033666, Validation Loss: 0.0035250296056688057\n",
      "Epoch [17665/20000], Training Loss: 0.01624254590777647, Validation Loss: 0.012554293679800464\n",
      "Epoch [17666/20000], Training Loss: 0.011963951842127634, Validation Loss: 0.011180105773374357\n",
      "Epoch [17667/20000], Training Loss: 0.010089880233863369, Validation Loss: 0.01036663453888512\n",
      "Epoch [17668/20000], Training Loss: 0.007203652614927185, Validation Loss: 0.01761029953855012\n",
      "Epoch [17669/20000], Training Loss: 0.012803150559193455, Validation Loss: 0.01206130555848566\n",
      "Epoch [17670/20000], Training Loss: 0.012180966797456807, Validation Loss: 0.05188980683474905\n",
      "Epoch [17671/20000], Training Loss: 0.040144635001654806, Validation Loss: 0.02660611827143709\n",
      "Epoch [17672/20000], Training Loss: 0.028965493394935038, Validation Loss: 0.015883711748598688\n",
      "Epoch [17673/20000], Training Loss: 0.014382869983299835, Validation Loss: 0.01584170543844115\n",
      "Epoch [17674/20000], Training Loss: 0.009447942143079249, Validation Loss: 0.04880191802684359\n",
      "Epoch [17675/20000], Training Loss: 0.02239194800079401, Validation Loss: 0.0052260330254146736\n",
      "Epoch [17676/20000], Training Loss: 0.014889980953220012, Validation Loss: 0.005665194442293853\n",
      "Epoch [17677/20000], Training Loss: 0.012591247866761737, Validation Loss: 0.011377920890436448\n",
      "Epoch [17678/20000], Training Loss: 0.010216555942731378, Validation Loss: 0.006759147576858534\n",
      "Epoch [17679/20000], Training Loss: 0.004535344443866052, Validation Loss: 0.00335011476749766\n",
      "Epoch [17680/20000], Training Loss: 0.0058904405616755995, Validation Loss: 0.004703303281009446\n",
      "Epoch [17681/20000], Training Loss: 0.004615435756696635, Validation Loss: 0.002673433569112506\n",
      "Epoch [17682/20000], Training Loss: 0.0043264385850696795, Validation Loss: 0.007771787098038528\n",
      "Epoch [17683/20000], Training Loss: 0.01061948277232919, Validation Loss: 0.005413216721960014\n",
      "Epoch [17684/20000], Training Loss: 0.004337139069840694, Validation Loss: 0.00854471472463406\n",
      "Epoch [17685/20000], Training Loss: 0.00627386456839117, Validation Loss: 0.003124912777734729\n",
      "Epoch [17686/20000], Training Loss: 0.005855804495045699, Validation Loss: 0.0058394915350429445\n",
      "Epoch [17687/20000], Training Loss: 0.005896345283287831, Validation Loss: 0.018842137310766054\n",
      "Epoch [17688/20000], Training Loss: 0.011015873419588778, Validation Loss: 0.006147309504415206\n",
      "Epoch [17689/20000], Training Loss: 0.00862013403920108, Validation Loss: 0.006648457181561008\n",
      "Epoch [17690/20000], Training Loss: 0.005168836381374474, Validation Loss: 0.0025570468821121445\n",
      "Epoch [17691/20000], Training Loss: 0.005218479098922606, Validation Loss: 0.0021575420789251404\n",
      "Epoch [17692/20000], Training Loss: 0.0039052948988488163, Validation Loss: 0.004719081923172703\n",
      "Epoch [17693/20000], Training Loss: 0.00487207501438596, Validation Loss: 0.004355195669382235\n",
      "Epoch [17694/20000], Training Loss: 0.011212047638504632, Validation Loss: 0.012929544244749585\n",
      "Epoch [17695/20000], Training Loss: 0.013843111116459892, Validation Loss: 0.0066908902929299074\n",
      "Epoch [17696/20000], Training Loss: 0.00839801800260176, Validation Loss: 0.006608345604129896\n",
      "Epoch [17697/20000], Training Loss: 0.006061117883941084, Validation Loss: 0.003141793282227618\n",
      "Epoch [17698/20000], Training Loss: 0.014185744384089307, Validation Loss: 0.015761539511475218\n",
      "Epoch [17699/20000], Training Loss: 0.009442388282824002, Validation Loss: 0.005447297113717957\n",
      "Epoch [17700/20000], Training Loss: 0.004937641808282933, Validation Loss: 0.0027520038268592983\n",
      "Epoch [17701/20000], Training Loss: 0.005298649798143222, Validation Loss: 0.00363826495972067\n",
      "Epoch [17702/20000], Training Loss: 0.005736871192441738, Validation Loss: 0.0032895560017916887\n",
      "Epoch [17703/20000], Training Loss: 0.006080420562467127, Validation Loss: 0.010393061804670367\n",
      "Epoch [17704/20000], Training Loss: 0.006713717710226774, Validation Loss: 0.004215337034079677\n",
      "Epoch [17705/20000], Training Loss: 0.009615208735600131, Validation Loss: 0.0027761882369138186\n",
      "Epoch [17706/20000], Training Loss: 0.004354993007447254, Validation Loss: 0.004110748920636199\n",
      "Epoch [17707/20000], Training Loss: 0.009684175423379722, Validation Loss: 0.027066153177662398\n",
      "Epoch [17708/20000], Training Loss: 0.026959294872735233, Validation Loss: 0.003967865669612333\n",
      "Epoch [17709/20000], Training Loss: 0.017859990421844225, Validation Loss: 0.004835274248282419\n",
      "Epoch [17710/20000], Training Loss: 0.02943847358359822, Validation Loss: 0.011659889161984032\n",
      "Epoch [17711/20000], Training Loss: 0.03928708528733945, Validation Loss: 0.06085790632745817\n",
      "Epoch [17712/20000], Training Loss: 0.03936327685577063, Validation Loss: 0.008866585173182426\n",
      "Epoch [17713/20000], Training Loss: 0.01419927726757513, Validation Loss: 0.014290900554287627\n",
      "Epoch [17714/20000], Training Loss: 0.006630829427324768, Validation Loss: 0.0056878994414972794\n",
      "Epoch [17715/20000], Training Loss: 0.005365698012090954, Validation Loss: 0.006793465138227158\n",
      "Epoch [17716/20000], Training Loss: 0.005809111104878996, Validation Loss: 0.004342077798863134\n",
      "Epoch [17717/20000], Training Loss: 0.004642035358951294, Validation Loss: 0.0070597975953738\n",
      "Epoch [17718/20000], Training Loss: 0.004079300390523193, Validation Loss: 0.009057780790442393\n",
      "Epoch [17719/20000], Training Loss: 0.015417252665169403, Validation Loss: 0.008029619903087435\n",
      "Epoch [17720/20000], Training Loss: 0.014683303111853872, Validation Loss: 0.009174985049604345\n",
      "Epoch [17721/20000], Training Loss: 0.018977916034470712, Validation Loss: 0.03479586226129835\n",
      "Epoch [17722/20000], Training Loss: 0.020464777322818657, Validation Loss: 0.016336670256328772\n",
      "Epoch [17723/20000], Training Loss: 0.016817621475118876, Validation Loss: 0.023073688619466078\n",
      "Epoch [17724/20000], Training Loss: 0.013346935452968214, Validation Loss: 0.013927063785151654\n",
      "Epoch [17725/20000], Training Loss: 0.012889305192110052, Validation Loss: 0.007283177790537364\n",
      "Epoch [17726/20000], Training Loss: 0.00635265262930521, Validation Loss: 0.011611061081696659\n",
      "Epoch [17727/20000], Training Loss: 0.010998955438740918, Validation Loss: 0.00710226413644618\n",
      "Epoch [17728/20000], Training Loss: 0.005075156204319293, Validation Loss: 0.004386974831306186\n",
      "Epoch [17729/20000], Training Loss: 0.008138976306716878, Validation Loss: 0.0033838023139562017\n",
      "Epoch [17730/20000], Training Loss: 0.010480851733258792, Validation Loss: 0.007454195650315195\n",
      "Epoch [17731/20000], Training Loss: 0.004993480987853413, Validation Loss: 0.0045192126000664756\n",
      "Epoch [17732/20000], Training Loss: 0.0056126438181049055, Validation Loss: 0.0035125452286369602\n",
      "Epoch [17733/20000], Training Loss: 0.005139741563263149, Validation Loss: 0.003584484410170928\n",
      "Epoch [17734/20000], Training Loss: 0.007420113796667595, Validation Loss: 0.0040654677578549426\n",
      "Epoch [17735/20000], Training Loss: 0.007773150294919365, Validation Loss: 0.0033050826080344314\n",
      "Epoch [17736/20000], Training Loss: 0.005409690185583064, Validation Loss: 0.0020456671255674663\n",
      "Epoch [17737/20000], Training Loss: 0.014516553583754492, Validation Loss: 0.005429828819432357\n",
      "Epoch [17738/20000], Training Loss: 0.008718543396136218, Validation Loss: 0.007029761430647026\n",
      "Epoch [17739/20000], Training Loss: 0.00612773433359897, Validation Loss: 0.0027337806286200006\n",
      "Epoch [17740/20000], Training Loss: 0.006272977748137366, Validation Loss: 0.0043423426742543415\n",
      "Epoch [17741/20000], Training Loss: 0.010740470128959194, Validation Loss: 0.010295843953175886\n",
      "Epoch [17742/20000], Training Loss: 0.01553052290962244, Validation Loss: 0.003946376085048696\n",
      "Epoch [17743/20000], Training Loss: 0.008075118919025823, Validation Loss: 0.003907201230230076\n",
      "Epoch [17744/20000], Training Loss: 0.005186387237959674, Validation Loss: 0.003883178402072624\n",
      "Epoch [17745/20000], Training Loss: 0.003580850808897854, Validation Loss: 0.003404700777665478\n",
      "Epoch [17746/20000], Training Loss: 0.009340260479378668, Validation Loss: 0.026803500425974323\n",
      "Epoch [17747/20000], Training Loss: 0.015050441049554917, Validation Loss: 0.013354491618770153\n",
      "Epoch [17748/20000], Training Loss: 0.011429679418922336, Validation Loss: 0.004603096624234431\n",
      "Epoch [17749/20000], Training Loss: 0.005648955177873306, Validation Loss: 0.0031385047066752997\n",
      "Epoch [17750/20000], Training Loss: 0.008083549329967354, Validation Loss: 0.003487798194022673\n",
      "Epoch [17751/20000], Training Loss: 0.005331544306890075, Validation Loss: 0.015253422415363664\n",
      "Epoch [17752/20000], Training Loss: 0.018661888723727316, Validation Loss: 0.008579321303733829\n",
      "Epoch [17753/20000], Training Loss: 0.0177516544437302, Validation Loss: 0.00601448489854712\n",
      "Epoch [17754/20000], Training Loss: 0.010272037823830666, Validation Loss: 0.007596971724195426\n",
      "Epoch [17755/20000], Training Loss: 0.006275142780688319, Validation Loss: 0.006284004528118154\n",
      "Epoch [17756/20000], Training Loss: 0.006195042901838731, Validation Loss: 0.004054442604886965\n",
      "Epoch [17757/20000], Training Loss: 0.005804037683576878, Validation Loss: 0.004102026306049262\n",
      "Epoch [17758/20000], Training Loss: 0.005137873643986625, Validation Loss: 0.004602785702203198\n",
      "Epoch [17759/20000], Training Loss: 0.005352534987780798, Validation Loss: 0.006595532711381072\n",
      "Epoch [17760/20000], Training Loss: 0.007257416409889369, Validation Loss: 0.004916996991712251\n",
      "Epoch [17761/20000], Training Loss: 0.00711243822297547, Validation Loss: 0.003560109583797758\n",
      "Epoch [17762/20000], Training Loss: 0.006349483532955803, Validation Loss: 0.002902822387481661\n",
      "Epoch [17763/20000], Training Loss: 0.004187886009406482, Validation Loss: 0.004440267798760881\n",
      "Epoch [17764/20000], Training Loss: 0.004422107191333323, Validation Loss: 0.0055361608491067815\n",
      "Epoch [17765/20000], Training Loss: 0.009071767172177456, Validation Loss: 0.005154589663442344\n",
      "Epoch [17766/20000], Training Loss: 0.005755267150691777, Validation Loss: 0.002737482026928108\n",
      "Epoch [17767/20000], Training Loss: 0.00684250374399044, Validation Loss: 0.010853361563911383\n",
      "Epoch [17768/20000], Training Loss: 0.01333335201135404, Validation Loss: 0.007188316172206198\n",
      "Epoch [17769/20000], Training Loss: 0.010581616068179756, Validation Loss: 0.0021198648552065436\n",
      "Epoch [17770/20000], Training Loss: 0.007782558459439315, Validation Loss: 0.0032075886228214606\n",
      "Epoch [17771/20000], Training Loss: 0.0036895765156909227, Validation Loss: 0.005120548790464292\n",
      "Epoch [17772/20000], Training Loss: 0.007484319424942701, Validation Loss: 0.007119582829698093\n",
      "Epoch [17773/20000], Training Loss: 0.028076444942728682, Validation Loss: 0.004964310675096125\n",
      "Epoch [17774/20000], Training Loss: 0.012894456259216116, Validation Loss: 0.005039581766072665\n",
      "Epoch [17775/20000], Training Loss: 0.026437405862712433, Validation Loss: 0.0029635474162666192\n",
      "Epoch [17776/20000], Training Loss: 0.04067191100018265, Validation Loss: 0.006033965769446529\n",
      "Epoch [17777/20000], Training Loss: 0.01507063614138003, Validation Loss: 0.00657051788653819\n",
      "Epoch [17778/20000], Training Loss: 0.009659438373221616, Validation Loss: 0.013124620391838236\n",
      "Epoch [17779/20000], Training Loss: 0.005624258677666408, Validation Loss: 0.004055406072014632\n",
      "Epoch [17780/20000], Training Loss: 0.005465129708422215, Validation Loss: 0.0041600641703252806\n",
      "Epoch [17781/20000], Training Loss: 0.004700178964412771, Validation Loss: 0.011641249764403819\n",
      "Epoch [17782/20000], Training Loss: 0.007503380603988522, Validation Loss: 0.005336326060663461\n",
      "Epoch [17783/20000], Training Loss: 0.006290087487061312, Validation Loss: 0.004162548570968332\n",
      "Epoch [17784/20000], Training Loss: 0.0066984834494568145, Validation Loss: 0.004194604402882314\n",
      "Epoch [17785/20000], Training Loss: 0.006916041705283403, Validation Loss: 0.0038824309134680235\n",
      "Epoch [17786/20000], Training Loss: 0.007279679181270434, Validation Loss: 0.0032989179531972873\n",
      "Epoch [17787/20000], Training Loss: 0.0043768066337049405, Validation Loss: 0.004612899660619364\n",
      "Epoch [17788/20000], Training Loss: 0.006608204160007926, Validation Loss: 0.003000203696947677\n",
      "Epoch [17789/20000], Training Loss: 0.006208427563251462, Validation Loss: 0.002728300208723563\n",
      "Epoch [17790/20000], Training Loss: 0.003069231553719679, Validation Loss: 0.007341897427032943\n",
      "Epoch [17791/20000], Training Loss: 0.005084737739317851, Validation Loss: 0.009373725692812838\n",
      "Epoch [17792/20000], Training Loss: 0.007562761564518691, Validation Loss: 0.00311116557861573\n",
      "Epoch [17793/20000], Training Loss: 0.0026076200062691767, Validation Loss: 0.00286758984778552\n",
      "Epoch [17794/20000], Training Loss: 0.007032094476345817, Validation Loss: 0.01759681910926198\n",
      "Epoch [17795/20000], Training Loss: 0.013607632749231666, Validation Loss: 0.005562835523672253\n",
      "Epoch [17796/20000], Training Loss: 0.011040434355631337, Validation Loss: 0.011139421990853467\n",
      "Epoch [17797/20000], Training Loss: 0.011920643811321392, Validation Loss: 0.0038622611025113584\n",
      "Epoch [17798/20000], Training Loss: 0.005154428059162187, Validation Loss: 0.016176118442266497\n",
      "Epoch [17799/20000], Training Loss: 0.00761323119721575, Validation Loss: 0.0027883913391304155\n",
      "Epoch [17800/20000], Training Loss: 0.005644219467498601, Validation Loss: 0.005452075282410685\n",
      "Epoch [17801/20000], Training Loss: 0.010092339327327084, Validation Loss: 0.004586457457238069\n",
      "Epoch [17802/20000], Training Loss: 0.02627424949657455, Validation Loss: 0.012773101134937768\n",
      "Epoch [17803/20000], Training Loss: 0.022359155149201473, Validation Loss: 0.011305251230574243\n",
      "Epoch [17804/20000], Training Loss: 0.024013986248974106, Validation Loss: 0.00888962689324099\n",
      "Epoch [17805/20000], Training Loss: 0.05451520549416143, Validation Loss: 0.008906493751972024\n",
      "Epoch [17806/20000], Training Loss: 0.02091090109232547, Validation Loss: 0.007402557409734405\n",
      "Epoch [17807/20000], Training Loss: 0.013011503769251118, Validation Loss: 0.007994908682786976\n",
      "Epoch [17808/20000], Training Loss: 0.00879258273510329, Validation Loss: 0.005663302882100295\n",
      "Epoch [17809/20000], Training Loss: 0.005480983025336172, Validation Loss: 0.0045687817949117615\n",
      "Epoch [17810/20000], Training Loss: 0.005383039309111025, Validation Loss: 0.006602190665298023\n",
      "Epoch [17811/20000], Training Loss: 0.006858336487701828, Validation Loss: 0.0037108059541545962\n",
      "Epoch [17812/20000], Training Loss: 0.005185825421774227, Validation Loss: 0.00460299714419118\n",
      "Epoch [17813/20000], Training Loss: 0.00536601866653135, Validation Loss: 0.004079445596228685\n",
      "Epoch [17814/20000], Training Loss: 0.007248283122212992, Validation Loss: 0.004391597336761167\n",
      "Epoch [17815/20000], Training Loss: 0.0061657264928466505, Validation Loss: 0.0044764035381064105\n",
      "Epoch [17816/20000], Training Loss: 0.007181701467028948, Validation Loss: 0.0046139018028402545\n",
      "Epoch [17817/20000], Training Loss: 0.007579079207581734, Validation Loss: 0.005380389417869732\n",
      "Epoch [17818/20000], Training Loss: 0.007800417756401755, Validation Loss: 0.006355199030617653\n",
      "Epoch [17819/20000], Training Loss: 0.007540625243564136, Validation Loss: 0.004861120089840806\n",
      "Epoch [17820/20000], Training Loss: 0.0064368197984419695, Validation Loss: 0.005883058958667269\n",
      "Epoch [17821/20000], Training Loss: 0.012015703604057697, Validation Loss: 0.0038764146319435\n",
      "Epoch [17822/20000], Training Loss: 0.009274863273146496, Validation Loss: 0.0026554198774257592\n",
      "Epoch [17823/20000], Training Loss: 0.005901512074550348, Validation Loss: 0.016753425291333293\n",
      "Epoch [17824/20000], Training Loss: 0.010886995990793886, Validation Loss: 0.01094954306420536\n",
      "Epoch [17825/20000], Training Loss: 0.010516509103451557, Validation Loss: 0.00899878234126358\n",
      "Epoch [17826/20000], Training Loss: 0.008939346266353303, Validation Loss: 0.007517027796824648\n",
      "Epoch [17827/20000], Training Loss: 0.004447161722477436, Validation Loss: 0.006986826772265148\n",
      "Epoch [17828/20000], Training Loss: 0.008451056053412944, Validation Loss: 0.010510895458468537\n",
      "Epoch [17829/20000], Training Loss: 0.005575691456670029, Validation Loss: 0.007587405602632634\n",
      "Epoch [17830/20000], Training Loss: 0.007671615751210733, Validation Loss: 0.002722119711955478\n",
      "Epoch [17831/20000], Training Loss: 0.014542328908906452, Validation Loss: 0.03382707086420844\n",
      "Epoch [17832/20000], Training Loss: 0.009933721787352365, Validation Loss: 0.005271163351738194\n",
      "Epoch [17833/20000], Training Loss: 0.006503698771536749, Validation Loss: 0.004379622735118208\n",
      "Epoch [17834/20000], Training Loss: 0.011068634441471659, Validation Loss: 0.003353779641850352\n",
      "Epoch [17835/20000], Training Loss: 0.007480090564497642, Validation Loss: 0.00492932144622384\n",
      "Epoch [17836/20000], Training Loss: 0.005817979807034135, Validation Loss: 0.00281945864693865\n",
      "Epoch [17837/20000], Training Loss: 0.008382911104750488, Validation Loss: 0.004950791533745605\n",
      "Epoch [17838/20000], Training Loss: 0.003626575636839594, Validation Loss: 0.004689615051493742\n",
      "Epoch [17839/20000], Training Loss: 0.0039300444298727855, Validation Loss: 0.007477443476725769\n",
      "Epoch [17840/20000], Training Loss: 0.010083081026030933, Validation Loss: 0.013945191445494791\n",
      "Epoch [17841/20000], Training Loss: 0.022094774679382163, Validation Loss: 0.008449046890246694\n",
      "Epoch [17842/20000], Training Loss: 0.010356371042040078, Validation Loss: 0.004956999744943046\n",
      "Epoch [17843/20000], Training Loss: 0.007895797915157996, Validation Loss: 0.002866177728628548\n",
      "Epoch [17844/20000], Training Loss: 0.006821972075288484, Validation Loss: 0.006560267278594204\n",
      "Epoch [17845/20000], Training Loss: 0.01906320617334651, Validation Loss: 0.005546771159139803\n",
      "Epoch [17846/20000], Training Loss: 0.01293188576646896, Validation Loss: 0.003837365883879126\n",
      "Epoch [17847/20000], Training Loss: 0.005569237221996965, Validation Loss: 0.004591545830010313\n",
      "Epoch [17848/20000], Training Loss: 0.003966871470246198, Validation Loss: 0.003719106746806184\n",
      "Epoch [17849/20000], Training Loss: 0.00400891932700428, Validation Loss: 0.003136918052655448\n",
      "Epoch [17850/20000], Training Loss: 0.00469165452107388, Validation Loss: 0.004496776101610018\n",
      "Epoch [17851/20000], Training Loss: 0.006622390691648304, Validation Loss: 0.014033068105803395\n",
      "Epoch [17852/20000], Training Loss: 0.006668943371600733, Validation Loss: 0.002374614322584837\n",
      "Epoch [17853/20000], Training Loss: 0.00438347451199661, Validation Loss: 0.005462373769062548\n",
      "Epoch [17854/20000], Training Loss: 0.006404696131150038, Validation Loss: 0.008591664124122588\n",
      "Epoch [17855/20000], Training Loss: 0.007586317587473397, Validation Loss: 0.002916140647767645\n",
      "Epoch [17856/20000], Training Loss: 0.0063660173826584855, Validation Loss: 0.002476515988655059\n",
      "Epoch [17857/20000], Training Loss: 0.005212311127771889, Validation Loss: 0.005945364081555786\n",
      "Epoch [17858/20000], Training Loss: 0.0059992199801074874, Validation Loss: 0.0023698991852785234\n",
      "Epoch [17859/20000], Training Loss: 0.00924940534261103, Validation Loss: 0.01245553738348526\n",
      "Epoch [17860/20000], Training Loss: 0.005341215887580931, Validation Loss: 0.00371088864713066\n",
      "Epoch [17861/20000], Training Loss: 0.004982795553847349, Validation Loss: 0.0059314209447522285\n",
      "Epoch [17862/20000], Training Loss: 0.010026314969008712, Validation Loss: 0.005632056131422068\n",
      "Epoch [17863/20000], Training Loss: 0.004974067334062836, Validation Loss: 0.004650439459456711\n",
      "Epoch [17864/20000], Training Loss: 0.006842274179299628, Validation Loss: 0.0035189480249135613\n",
      "Epoch [17865/20000], Training Loss: 0.004856254021953126, Validation Loss: 0.0071343712747865395\n",
      "Epoch [17866/20000], Training Loss: 0.011966632609463497, Validation Loss: 0.017391961747136975\n",
      "Epoch [17867/20000], Training Loss: 0.005445702843969359, Validation Loss: 0.012633172795360968\n",
      "Epoch [17868/20000], Training Loss: 0.007952540467418398, Validation Loss: 0.004490475548939189\n",
      "Epoch [17869/20000], Training Loss: 0.003859044682030799, Validation Loss: 0.005638676964443887\n",
      "Epoch [17870/20000], Training Loss: 0.0038825454604583293, Validation Loss: 0.007681573928384771\n",
      "Epoch [17871/20000], Training Loss: 0.010733795959367853, Validation Loss: 0.06359284158265437\n",
      "Epoch [17872/20000], Training Loss: 0.01009043929895727, Validation Loss: 0.006593671790843545\n",
      "Epoch [17873/20000], Training Loss: 0.010409072255177827, Validation Loss: 0.002501046515549713\n",
      "Epoch [17874/20000], Training Loss: 0.020638774936222553, Validation Loss: 0.01320461289947618\n",
      "Epoch [17875/20000], Training Loss: 0.01106173299038866, Validation Loss: 0.004700323724829237\n",
      "Epoch [17876/20000], Training Loss: 0.017602954583708197, Validation Loss: 0.022829391062826047\n",
      "Epoch [17877/20000], Training Loss: 0.00985449508430187, Validation Loss: 0.017289900420500118\n",
      "Epoch [17878/20000], Training Loss: 0.0057949036571309466, Validation Loss: 0.0027816754642660613\n",
      "Epoch [17879/20000], Training Loss: 0.008270400078929794, Validation Loss: 0.003951605861844894\n",
      "Epoch [17880/20000], Training Loss: 0.006076923810480496, Validation Loss: 0.0034061650375437785\n",
      "Epoch [17881/20000], Training Loss: 0.00369174691799604, Validation Loss: 0.029350632003375544\n",
      "Epoch [17882/20000], Training Loss: 0.025988051747325307, Validation Loss: 0.007408090087343236\n",
      "Epoch [17883/20000], Training Loss: 0.016166897268184193, Validation Loss: 0.004490935189396964\n",
      "Epoch [17884/20000], Training Loss: 0.006966364612091118, Validation Loss: 0.03001173276750738\n",
      "Epoch [17885/20000], Training Loss: 0.01589902218516467, Validation Loss: 0.042515951839724106\n",
      "Epoch [17886/20000], Training Loss: 0.03830696473180849, Validation Loss: 0.019896387969278555\n",
      "Epoch [17887/20000], Training Loss: 0.015686804012927626, Validation Loss: 0.007960344784272497\n",
      "Epoch [17888/20000], Training Loss: 0.005516264505526384, Validation Loss: 0.004285629547587837\n",
      "Epoch [17889/20000], Training Loss: 0.005089633948435741, Validation Loss: 0.0036399954471441225\n",
      "Epoch [17890/20000], Training Loss: 0.005133318694009047, Validation Loss: 0.0038119522218698393\n",
      "Epoch [17891/20000], Training Loss: 0.004331011307970455, Validation Loss: 0.00460248623673948\n",
      "Epoch [17892/20000], Training Loss: 0.004451248716837394, Validation Loss: 0.0025769159434896644\n",
      "Epoch [17893/20000], Training Loss: 0.007635463749465998, Validation Loss: 0.0037289224148016687\n",
      "Epoch [17894/20000], Training Loss: 0.007149384867391616, Validation Loss: 0.008554774470050227\n",
      "Epoch [17895/20000], Training Loss: 0.021917721336129552, Validation Loss: 0.010119030454745825\n",
      "Epoch [17896/20000], Training Loss: 0.0067706428557617725, Validation Loss: 0.007076929556731458\n",
      "Epoch [17897/20000], Training Loss: 0.004331957201689615, Validation Loss: 0.0032977002047579313\n",
      "Epoch [17898/20000], Training Loss: 0.007283094164449722, Validation Loss: 0.009602361944487825\n",
      "Epoch [17899/20000], Training Loss: 0.008256849024442434, Validation Loss: 0.005810004826982206\n",
      "Epoch [17900/20000], Training Loss: 0.0034744977832996842, Validation Loss: 0.003611269416122046\n",
      "Epoch [17901/20000], Training Loss: 0.004909942050289828, Validation Loss: 0.002750516570616388\n",
      "Epoch [17902/20000], Training Loss: 0.005721429397259011, Validation Loss: 0.0037434967448046435\n",
      "Epoch [17903/20000], Training Loss: 0.007993429381583286, Validation Loss: 0.00401310758395114\n",
      "Epoch [17904/20000], Training Loss: 0.00516590451482557, Validation Loss: 0.0047401581149943365\n",
      "Epoch [17905/20000], Training Loss: 0.0036228542832239846, Validation Loss: 0.02149290472188373\n",
      "Epoch [17906/20000], Training Loss: 0.014492889548902457, Validation Loss: 0.0033618074631177448\n",
      "Epoch [17907/20000], Training Loss: 0.003147213054554803, Validation Loss: 0.0048291689542962835\n",
      "Epoch [17908/20000], Training Loss: 0.005343668145932108, Validation Loss: 0.0023630896804396023\n",
      "Epoch [17909/20000], Training Loss: 0.004272887896929335, Validation Loss: 0.004503734478340347\n",
      "Epoch [17910/20000], Training Loss: 0.005279714543835975, Validation Loss: 0.0034609759427837103\n",
      "Epoch [17911/20000], Training Loss: 0.006498107156793205, Validation Loss: 0.0048604315328524906\n",
      "Epoch [17912/20000], Training Loss: 0.006789787822656633, Validation Loss: 0.003712819527579817\n",
      "Epoch [17913/20000], Training Loss: 0.005739505012960373, Validation Loss: 0.002398753199535721\n",
      "Epoch [17914/20000], Training Loss: 0.008320769656035867, Validation Loss: 0.007909257497185594\n",
      "Epoch [17915/20000], Training Loss: 0.012399150189594366, Validation Loss: 0.0031019243337687846\n",
      "Epoch [17916/20000], Training Loss: 0.010718639428627543, Validation Loss: 0.005946358136340178\n",
      "Epoch [17917/20000], Training Loss: 0.009865474006841981, Validation Loss: 0.006051677922480554\n",
      "Epoch [17918/20000], Training Loss: 0.011198158621742291, Validation Loss: 0.005091004527105433\n",
      "Epoch [17919/20000], Training Loss: 0.012772823424451697, Validation Loss: 0.011964395110111523\n",
      "Epoch [17920/20000], Training Loss: 0.013823296144697192, Validation Loss: 0.04356949244241561\n",
      "Epoch [17921/20000], Training Loss: 0.01934909416637051, Validation Loss: 0.009689326802398396\n",
      "Epoch [17922/20000], Training Loss: 0.01585711063619653, Validation Loss: 0.00478528705048936\n",
      "Epoch [17923/20000], Training Loss: 0.009159660811357233, Validation Loss: 0.009330815545291265\n",
      "Epoch [17924/20000], Training Loss: 0.014396731347785265, Validation Loss: 0.006009469566198585\n",
      "Epoch [17925/20000], Training Loss: 0.012362725370150787, Validation Loss: 0.008459368845351176\n",
      "Epoch [17926/20000], Training Loss: 0.013111111815305776, Validation Loss: 0.0050688184194497155\n",
      "Epoch [17927/20000], Training Loss: 0.008743640872061016, Validation Loss: 0.0042854457288790115\n",
      "Epoch [17928/20000], Training Loss: 0.007892448030491193, Validation Loss: 0.005130063749469037\n",
      "Epoch [17929/20000], Training Loss: 0.003496619032505675, Validation Loss: 0.004025986662229372\n",
      "Epoch [17930/20000], Training Loss: 0.004040396865873065, Validation Loss: 0.002768918156793175\n",
      "Epoch [17931/20000], Training Loss: 0.0033831926384989075, Validation Loss: 0.004483908366253846\n",
      "Epoch [17932/20000], Training Loss: 0.006254268379312374, Validation Loss: 0.00291022403110849\n",
      "Epoch [17933/20000], Training Loss: 0.006325274762015657, Validation Loss: 0.004634729422434069\n",
      "Epoch [17934/20000], Training Loss: 0.004837973201834497, Validation Loss: 0.0028106073882542327\n",
      "Epoch [17935/20000], Training Loss: 0.011088213290869524, Validation Loss: 0.002626982743101136\n",
      "Epoch [17936/20000], Training Loss: 0.004822502384610873, Validation Loss: 0.003083777991194684\n",
      "Epoch [17937/20000], Training Loss: 0.009131855336038695, Validation Loss: 0.0029680792610728346\n",
      "Epoch [17938/20000], Training Loss: 0.004713527524862522, Validation Loss: 0.00631860992888246\n",
      "Epoch [17939/20000], Training Loss: 0.007197923410525878, Validation Loss: 0.002436507483492889\n",
      "Epoch [17940/20000], Training Loss: 0.004387397085728091, Validation Loss: 0.006524576750371531\n",
      "Epoch [17941/20000], Training Loss: 0.011436683886651216, Validation Loss: 0.006327812717713702\n",
      "Epoch [17942/20000], Training Loss: 0.006804715334770403, Validation Loss: 0.004888813835526018\n",
      "Epoch [17943/20000], Training Loss: 0.007320906903389966, Validation Loss: 0.024025095626849947\n",
      "Epoch [17944/20000], Training Loss: 0.013772070608735833, Validation Loss: 0.007683456941967966\n",
      "Epoch [17945/20000], Training Loss: 0.02633419996761534, Validation Loss: 0.014058404271638989\n",
      "Epoch [17946/20000], Training Loss: 0.022523862293272, Validation Loss: 0.005614081300773675\n",
      "Epoch [17947/20000], Training Loss: 0.012119668801980359, Validation Loss: 0.003084181730769932\n",
      "Epoch [17948/20000], Training Loss: 0.004557533765494425, Validation Loss: 0.002554967764933786\n",
      "Epoch [17949/20000], Training Loss: 0.008071806556212582, Validation Loss: 0.002763148058973906\n",
      "Epoch [17950/20000], Training Loss: 0.01708767937842432, Validation Loss: 0.006147047917509704\n",
      "Epoch [17951/20000], Training Loss: 0.011371111332014803, Validation Loss: 0.007233544070891852\n",
      "Epoch [17952/20000], Training Loss: 0.006633152421326053, Validation Loss: 0.00558873472560754\n",
      "Epoch [17953/20000], Training Loss: 0.011172849063379025, Validation Loss: 0.004662563279045985\n",
      "Epoch [17954/20000], Training Loss: 0.003346832614624873, Validation Loss: 0.00698712122303203\n",
      "Epoch [17955/20000], Training Loss: 0.005418017093869513, Validation Loss: 0.0027019203490264592\n",
      "Epoch [17956/20000], Training Loss: 0.006174258873475732, Validation Loss: 0.012637203553759033\n",
      "Epoch [17957/20000], Training Loss: 0.017678222436188662, Validation Loss: 0.0044153195195902354\n",
      "Epoch [17958/20000], Training Loss: 0.029241641107156675, Validation Loss: 0.003912503265498556\n",
      "Epoch [17959/20000], Training Loss: 0.01838256737223024, Validation Loss: 0.008393037323741637\n",
      "Epoch [17960/20000], Training Loss: 0.01823546570968964, Validation Loss: 0.007981156420927147\n",
      "Epoch [17961/20000], Training Loss: 0.015635817630547017, Validation Loss: 0.010995391358068759\n",
      "Epoch [17962/20000], Training Loss: 0.009186941219793101, Validation Loss: 0.0037092860002992467\n",
      "Epoch [17963/20000], Training Loss: 0.01376936824297965, Validation Loss: 0.005452434259192494\n",
      "Epoch [17964/20000], Training Loss: 0.007511613855708025, Validation Loss: 0.00661243655837548\n",
      "Epoch [17965/20000], Training Loss: 0.0066186517090370345, Validation Loss: 0.007145665993019807\n",
      "Epoch [17966/20000], Training Loss: 0.005136446848544958, Validation Loss: 0.003978198606775578\n",
      "Epoch [17967/20000], Training Loss: 0.006686085588236372, Validation Loss: 0.0031292191958710897\n",
      "Epoch [17968/20000], Training Loss: 0.005307699740114913, Validation Loss: 0.007699814391370653\n",
      "Epoch [17969/20000], Training Loss: 0.007031095529133121, Validation Loss: 0.00619657667215798\n",
      "Epoch [17970/20000], Training Loss: 0.005246395025356573, Validation Loss: 0.00532996717239361\n",
      "Epoch [17971/20000], Training Loss: 0.008441233026262904, Validation Loss: 0.0047016689571334385\n",
      "Epoch [17972/20000], Training Loss: 0.005894491782912935, Validation Loss: 0.002732296095743341\n",
      "Epoch [17973/20000], Training Loss: 0.0032762412477365744, Validation Loss: 0.003953801891354823\n",
      "Epoch [17974/20000], Training Loss: 0.0034563009368347203, Validation Loss: 0.003020359011871016\n",
      "Epoch [17975/20000], Training Loss: 0.00541844791483267, Validation Loss: 0.0033411924850866186\n",
      "Epoch [17976/20000], Training Loss: 0.0028053402493242174, Validation Loss: 0.005650991731597443\n",
      "Epoch [17977/20000], Training Loss: 0.0045229220119397595, Validation Loss: 0.00792634886425477\n",
      "Epoch [17978/20000], Training Loss: 0.00678811452962691, Validation Loss: 0.0022158788021276543\n",
      "Epoch [17979/20000], Training Loss: 0.012978864067333364, Validation Loss: 0.013533931169828215\n",
      "Epoch [17980/20000], Training Loss: 0.020272058876425296, Validation Loss: 0.09163727026226809\n",
      "Epoch [17981/20000], Training Loss: 0.08825742208169426, Validation Loss: 0.030784334719750665\n",
      "Epoch [17982/20000], Training Loss: 0.016524750153101713, Validation Loss: 0.017754979597056027\n",
      "Epoch [17983/20000], Training Loss: 0.016399822356886164, Validation Loss: 0.0055550820554911085\n",
      "Epoch [17984/20000], Training Loss: 0.012499197854984751, Validation Loss: 0.012039769606705672\n",
      "Epoch [17985/20000], Training Loss: 0.009765415680947496, Validation Loss: 0.006583686220458114\n",
      "Epoch [17986/20000], Training Loss: 0.012429824410334942, Validation Loss: 0.01897589376767428\n",
      "Epoch [17987/20000], Training Loss: 0.012483546119515918, Validation Loss: 0.005305652954474128\n",
      "Epoch [17988/20000], Training Loss: 0.005642494146221517, Validation Loss: 0.007399507491776599\n",
      "Epoch [17989/20000], Training Loss: 0.007459508596054677, Validation Loss: 0.004700378717270074\n",
      "Epoch [17990/20000], Training Loss: 0.004379588150186464, Validation Loss: 0.006085163731030375\n",
      "Epoch [17991/20000], Training Loss: 0.005029797892867853, Validation Loss: 0.003519179242791992\n",
      "Epoch [17992/20000], Training Loss: 0.005533411645306582, Validation Loss: 0.004278567353677313\n",
      "Epoch [17993/20000], Training Loss: 0.00818230686023266, Validation Loss: 0.003135956241085885\n",
      "Epoch [17994/20000], Training Loss: 0.004816302112885751, Validation Loss: 0.003528297238414524\n",
      "Epoch [17995/20000], Training Loss: 0.005027818498742168, Validation Loss: 0.003504361379774374\n",
      "Epoch [17996/20000], Training Loss: 0.004558611665962482, Validation Loss: 0.002637592578956064\n",
      "Epoch [17997/20000], Training Loss: 0.0040294159670923235, Validation Loss: 0.0040899935866373005\n",
      "Epoch [17998/20000], Training Loss: 0.00642055348102336, Validation Loss: 0.0029155382908816474\n",
      "Epoch [17999/20000], Training Loss: 0.009100857519765018, Validation Loss: 0.006774488867027425\n",
      "Epoch [18000/20000], Training Loss: 0.00778498914067833, Validation Loss: 0.003944112206941749\n",
      "Epoch [18001/20000], Training Loss: 0.007409894318178496, Validation Loss: 0.009078961535221463\n",
      "Epoch [18002/20000], Training Loss: 0.007693505934314869, Validation Loss: 0.003775742426828615\n",
      "Epoch [18003/20000], Training Loss: 0.004629120992571448, Validation Loss: 0.003369981722561826\n",
      "Epoch [18004/20000], Training Loss: 0.006853985062792033, Validation Loss: 0.0125310030035897\n",
      "Epoch [18005/20000], Training Loss: 0.006883800209314879, Validation Loss: 0.008592457976807936\n",
      "Epoch [18006/20000], Training Loss: 0.011766684079962684, Validation Loss: 0.01161434981166154\n",
      "Epoch [18007/20000], Training Loss: 0.01387402435992735, Validation Loss: 0.004862482168501111\n",
      "Epoch [18008/20000], Training Loss: 0.011616270560937534, Validation Loss: 0.007141955512687608\n",
      "Epoch [18009/20000], Training Loss: 0.01079437158789785, Validation Loss: 0.004771955869824643\n",
      "Epoch [18010/20000], Training Loss: 0.0049987070629039666, Validation Loss: 0.004564442699726477\n",
      "Epoch [18011/20000], Training Loss: 0.00688278949071121, Validation Loss: 0.005569266657726804\n",
      "Epoch [18012/20000], Training Loss: 0.006596410227627659, Validation Loss: 0.0036298417769308317\n",
      "Epoch [18013/20000], Training Loss: 0.004614976621529162, Validation Loss: 0.0031258477227353865\n",
      "Epoch [18014/20000], Training Loss: 0.004881019969356463, Validation Loss: 0.00798663882868003\n",
      "Epoch [18015/20000], Training Loss: 0.004517696589443533, Validation Loss: 0.005893877535453943\n",
      "Epoch [18016/20000], Training Loss: 0.011989795086086425, Validation Loss: 0.01597159814056276\n",
      "Epoch [18017/20000], Training Loss: 0.019437295954738993, Validation Loss: 0.006423401859053498\n",
      "Epoch [18018/20000], Training Loss: 0.009130178097231172, Validation Loss: 0.007969121881240887\n",
      "Epoch [18019/20000], Training Loss: 0.010580144692929545, Validation Loss: 0.006008099825726837\n",
      "Epoch [18020/20000], Training Loss: 0.00815673732826586, Validation Loss: 0.004938756135393755\n",
      "Epoch [18021/20000], Training Loss: 0.00600165063769964, Validation Loss: 0.021211403121904077\n",
      "Epoch [18022/20000], Training Loss: 0.02011272608069703, Validation Loss: 0.004960490202547264\n",
      "Epoch [18023/20000], Training Loss: 0.009698394433923698, Validation Loss: 0.011641315322192694\n",
      "Epoch [18024/20000], Training Loss: 0.007394540740912297, Validation Loss: 0.0030525065004301916\n",
      "Epoch [18025/20000], Training Loss: 0.004274135629023996, Validation Loss: 0.0037820159245957846\n",
      "Epoch [18026/20000], Training Loss: 0.0045322946753003635, Validation Loss: 0.005935512950615651\n",
      "Epoch [18027/20000], Training Loss: 0.004736319463112133, Validation Loss: 0.004401893511076691\n",
      "Epoch [18028/20000], Training Loss: 0.005275194551878875, Validation Loss: 0.0035608655769274427\n",
      "Epoch [18029/20000], Training Loss: 0.007175127918341184, Validation Loss: 0.008389844808285878\n",
      "Epoch [18030/20000], Training Loss: 0.012873525003669783, Validation Loss: 0.005366682874081154\n",
      "Epoch [18031/20000], Training Loss: 0.010372790581511384, Validation Loss: 0.005700132328093026\n",
      "Epoch [18032/20000], Training Loss: 0.00944320235534438, Validation Loss: 0.019676325709692074\n",
      "Epoch [18033/20000], Training Loss: 0.018449832105294002, Validation Loss: 0.006619890223699893\n",
      "Epoch [18034/20000], Training Loss: 0.012389840447992486, Validation Loss: 0.003780719507995584\n",
      "Epoch [18035/20000], Training Loss: 0.004214652489671218, Validation Loss: 0.00514450307795051\n",
      "Epoch [18036/20000], Training Loss: 0.0071506660346390815, Validation Loss: 0.00393565225769115\n",
      "Epoch [18037/20000], Training Loss: 0.0050644408624896465, Validation Loss: 0.009370595616087262\n",
      "Epoch [18038/20000], Training Loss: 0.006102782477747886, Validation Loss: 0.007621534268628666\n",
      "Epoch [18039/20000], Training Loss: 0.009057521850731323, Validation Loss: 0.004749599113509992\n",
      "Epoch [18040/20000], Training Loss: 0.017965313347563745, Validation Loss: 0.003879735511486735\n",
      "Epoch [18041/20000], Training Loss: 0.004460271144872812, Validation Loss: 0.025904604953601456\n",
      "Epoch [18042/20000], Training Loss: 0.020732086800437952, Validation Loss: 0.04771052281550782\n",
      "Epoch [18043/20000], Training Loss: 0.01818026088377727, Validation Loss: 0.004506971891243795\n",
      "Epoch [18044/20000], Training Loss: 0.006746084926167636, Validation Loss: 0.0035270880909432922\n",
      "Epoch [18045/20000], Training Loss: 0.004388205366142627, Validation Loss: 0.0028595370823375327\n",
      "Epoch [18046/20000], Training Loss: 0.005447082207995534, Validation Loss: 0.005344716008843469\n",
      "Epoch [18047/20000], Training Loss: 0.006521702993333227, Validation Loss: 0.004429199309158982\n",
      "Epoch [18048/20000], Training Loss: 0.0038010225460831343, Validation Loss: 0.002353060869019652\n",
      "Epoch [18049/20000], Training Loss: 0.004927231831711002, Validation Loss: 0.0032086275815968535\n",
      "Epoch [18050/20000], Training Loss: 0.0045963660273368335, Validation Loss: 0.00462877500476517\n",
      "Epoch [18051/20000], Training Loss: 0.007436595753201151, Validation Loss: 0.006071440443569979\n",
      "Epoch [18052/20000], Training Loss: 0.007353575247439689, Validation Loss: 0.006038255657241268\n",
      "Epoch [18053/20000], Training Loss: 0.012118066157979359, Validation Loss: 0.003442462910529096\n",
      "Epoch [18054/20000], Training Loss: 0.006476005280741707, Validation Loss: 0.006744683786540203\n",
      "Epoch [18055/20000], Training Loss: 0.005513588334354479, Validation Loss: 0.004985764291090116\n",
      "Epoch [18056/20000], Training Loss: 0.005282034758887936, Validation Loss: 0.0024543946142526435\n",
      "Epoch [18057/20000], Training Loss: 0.005659501617109137, Validation Loss: 0.005200991207859649\n",
      "Epoch [18058/20000], Training Loss: 0.010483315242059433, Validation Loss: 0.002583458084898777\n",
      "Epoch [18059/20000], Training Loss: 0.005655790093961903, Validation Loss: 0.003283460319462123\n",
      "Epoch [18060/20000], Training Loss: 0.013033048386984904, Validation Loss: 0.01042658011271312\n",
      "Epoch [18061/20000], Training Loss: 0.052523425880021284, Validation Loss: 0.05232033559270417\n",
      "Epoch [18062/20000], Training Loss: 0.09548953188433577, Validation Loss: 0.0862781085800829\n",
      "Epoch [18063/20000], Training Loss: 0.05391367870782103, Validation Loss: 0.0675123808936665\n",
      "Epoch [18064/20000], Training Loss: 0.032854803172605376, Validation Loss: 0.032405618768204594\n",
      "Epoch [18065/20000], Training Loss: 0.01879441295750439, Validation Loss: 0.009924641791079769\n",
      "Epoch [18066/20000], Training Loss: 0.008212151970448238, Validation Loss: 0.008041077802512777\n",
      "Epoch [18067/20000], Training Loss: 0.008619170403108, Validation Loss: 0.0048895617905405486\n",
      "Epoch [18068/20000], Training Loss: 0.008284163330764776, Validation Loss: 0.010365468636986666\n",
      "Epoch [18069/20000], Training Loss: 0.006918811680017305, Validation Loss: 0.004202712201991484\n",
      "Epoch [18070/20000], Training Loss: 0.007331424459282841, Validation Loss: 0.004155901976836114\n",
      "Epoch [18071/20000], Training Loss: 0.00946721131913364, Validation Loss: 0.013960240560014167\n",
      "Epoch [18072/20000], Training Loss: 0.00974173576936924, Validation Loss: 0.006209716995986777\n",
      "Epoch [18073/20000], Training Loss: 0.011833906386592779, Validation Loss: 0.006048061322396896\n",
      "Epoch [18074/20000], Training Loss: 0.014279789900423825, Validation Loss: 0.0389749423704416\n",
      "Epoch [18075/20000], Training Loss: 0.020029196997971406, Validation Loss: 0.009868017473190247\n",
      "Epoch [18076/20000], Training Loss: 0.006714273254536758, Validation Loss: 0.0050607533239274615\n",
      "Epoch [18077/20000], Training Loss: 0.006453706117879067, Validation Loss: 0.004706903887322612\n",
      "Epoch [18078/20000], Training Loss: 0.004821896293183922, Validation Loss: 0.0033435356298728686\n",
      "Epoch [18079/20000], Training Loss: 0.004178791837505612, Validation Loss: 0.003376602333641391\n",
      "Epoch [18080/20000], Training Loss: 0.006239406839345715, Validation Loss: 0.006562408971920571\n",
      "Epoch [18081/20000], Training Loss: 0.020618116805313287, Validation Loss: 0.015256101536196403\n",
      "Epoch [18082/20000], Training Loss: 0.023997582844978233, Validation Loss: 0.020622988079751332\n",
      "Epoch [18083/20000], Training Loss: 0.01696182017141317, Validation Loss: 0.03818044645998141\n",
      "Epoch [18084/20000], Training Loss: 0.012857877918577287, Validation Loss: 0.015867937752333767\n",
      "Epoch [18085/20000], Training Loss: 0.011448789047595451, Validation Loss: 0.005860909013969311\n",
      "Epoch [18086/20000], Training Loss: 0.012773792828998662, Validation Loss: 0.004213815956242277\n",
      "Epoch [18087/20000], Training Loss: 0.006445477576302697, Validation Loss: 0.011778273136477375\n",
      "Epoch [18088/20000], Training Loss: 0.01821820646156474, Validation Loss: 0.0061138692311664454\n",
      "Epoch [18089/20000], Training Loss: 0.012331954328276749, Validation Loss: 0.008128730022708705\n",
      "Epoch [18090/20000], Training Loss: 0.0069551311316899955, Validation Loss: 0.00379926349519926\n",
      "Epoch [18091/20000], Training Loss: 0.0054796099481921245, Validation Loss: 0.004147740587601082\n",
      "Epoch [18092/20000], Training Loss: 0.007982395687317225, Validation Loss: 0.0040396508965626155\n",
      "Epoch [18093/20000], Training Loss: 0.0044792459604130795, Validation Loss: 0.004399661897810095\n",
      "Epoch [18094/20000], Training Loss: 0.00440580585148252, Validation Loss: 0.0030371777648180603\n",
      "Epoch [18095/20000], Training Loss: 0.0069500185314349695, Validation Loss: 0.010407382737356181\n",
      "Epoch [18096/20000], Training Loss: 0.006127218958421768, Validation Loss: 0.008248891958926054\n",
      "Epoch [18097/20000], Training Loss: 0.006989795911327487, Validation Loss: 0.014064482012663351\n",
      "Epoch [18098/20000], Training Loss: 0.008833840646730096, Validation Loss: 0.005213915761203225\n",
      "Epoch [18099/20000], Training Loss: 0.00800930442138841, Validation Loss: 0.004221102315958498\n",
      "Epoch [18100/20000], Training Loss: 0.006543160106372221, Validation Loss: 0.005456046071576078\n",
      "Epoch [18101/20000], Training Loss: 0.012919606265703416, Validation Loss: 0.0035668596934885144\n",
      "Epoch [18102/20000], Training Loss: 0.0056613322156603575, Validation Loss: 0.00244046900169183\n",
      "Epoch [18103/20000], Training Loss: 0.006281093917745498, Validation Loss: 0.006852064624516834\n",
      "Epoch [18104/20000], Training Loss: 0.007443648849987637, Validation Loss: 0.004089090643806311\n",
      "Epoch [18105/20000], Training Loss: 0.009641737098718295, Validation Loss: 0.008500841361321168\n",
      "Epoch [18106/20000], Training Loss: 0.012295292098964896, Validation Loss: 0.003699572800030637\n",
      "Epoch [18107/20000], Training Loss: 0.03832547700793449, Validation Loss: 0.017800054937640603\n",
      "Epoch [18108/20000], Training Loss: 0.02984881096719099, Validation Loss: 0.043486003536282576\n",
      "Epoch [18109/20000], Training Loss: 0.01949139281558538, Validation Loss: 0.013292937656350454\n",
      "Epoch [18110/20000], Training Loss: 0.016302622938480584, Validation Loss: 0.0054848945655611\n",
      "Epoch [18111/20000], Training Loss: 0.004926009233827784, Validation Loss: 0.004518066186627883\n",
      "Epoch [18112/20000], Training Loss: 0.005960905963937486, Validation Loss: 0.0047543722381604425\n",
      "Epoch [18113/20000], Training Loss: 0.004668001162437057, Validation Loss: 0.004955773669223618\n",
      "Epoch [18114/20000], Training Loss: 0.0073789696616586315, Validation Loss: 0.0037486693894156326\n",
      "Epoch [18115/20000], Training Loss: 0.005526938367568489, Validation Loss: 0.0031026476792632707\n",
      "Epoch [18116/20000], Training Loss: 0.004331526449927229, Validation Loss: 0.007844339953685291\n",
      "Epoch [18117/20000], Training Loss: 0.004431945645982134, Validation Loss: 0.0026543520153714845\n",
      "Epoch [18118/20000], Training Loss: 0.004890628910548652, Validation Loss: 0.0029348062361772725\n",
      "Epoch [18119/20000], Training Loss: 0.004211401632759808, Validation Loss: 0.004217083637674891\n",
      "Epoch [18120/20000], Training Loss: 0.0067391685084398235, Validation Loss: 0.002550582152214328\n",
      "Epoch [18121/20000], Training Loss: 0.004877190564002376, Validation Loss: 0.0035841699951341915\n",
      "Epoch [18122/20000], Training Loss: 0.013087316353838625, Validation Loss: 0.011775579171969777\n",
      "Epoch [18123/20000], Training Loss: 0.014888497849763585, Validation Loss: 0.005123491141903358\n",
      "Epoch [18124/20000], Training Loss: 0.011573297197173815, Validation Loss: 0.0037287680254028794\n",
      "Epoch [18125/20000], Training Loss: 0.014092514530602784, Validation Loss: 0.007225527755890328\n",
      "Epoch [18126/20000], Training Loss: 0.01339012884274585, Validation Loss: 0.003965726468773515\n",
      "Epoch [18127/20000], Training Loss: 0.007068759298168256, Validation Loss: 0.01154623599040448\n",
      "Epoch [18128/20000], Training Loss: 0.004732526384032099, Validation Loss: 0.002857306655775866\n",
      "Epoch [18129/20000], Training Loss: 0.005959172169241356, Validation Loss: 0.0026230721951208125\n",
      "Epoch [18130/20000], Training Loss: 0.002632793416133999, Validation Loss: 0.006852162158032574\n",
      "Epoch [18131/20000], Training Loss: 0.00474748247298911, Validation Loss: 0.0030602893929483344\n",
      "Epoch [18132/20000], Training Loss: 0.006202471093323376, Validation Loss: 0.002241144433790611\n",
      "Epoch [18133/20000], Training Loss: 0.005304974932284624, Validation Loss: 0.003412091008950061\n",
      "Epoch [18134/20000], Training Loss: 0.002784842698048351, Validation Loss: 0.008780103691372356\n",
      "Epoch [18135/20000], Training Loss: 0.0050082038327933075, Validation Loss: 0.006310773336350296\n",
      "Epoch [18136/20000], Training Loss: 0.007361166768451507, Validation Loss: 0.036355404030047556\n",
      "Epoch [18137/20000], Training Loss: 0.021198015374985908, Validation Loss: 0.010359320657757212\n",
      "Epoch [18138/20000], Training Loss: 0.01865426677704818, Validation Loss: 0.006872608686170218\n",
      "Epoch [18139/20000], Training Loss: 0.00986363227572237, Validation Loss: 0.006502227983262847\n",
      "Epoch [18140/20000], Training Loss: 0.0066289642854826525, Validation Loss: 0.004090651809973066\n",
      "Epoch [18141/20000], Training Loss: 0.011766710673798895, Validation Loss: 0.006119590214585254\n",
      "Epoch [18142/20000], Training Loss: 0.007858525039474833, Validation Loss: 0.0025902615100384147\n",
      "Epoch [18143/20000], Training Loss: 0.012050713763788476, Validation Loss: 0.0028487208409881\n",
      "Epoch [18144/20000], Training Loss: 0.017737756754026383, Validation Loss: 0.013911369108755218\n",
      "Epoch [18145/20000], Training Loss: 0.00961358343725546, Validation Loss: 0.045185677607811545\n",
      "Epoch [18146/20000], Training Loss: 0.023633182215040245, Validation Loss: 0.02256501956466193\n",
      "Epoch [18147/20000], Training Loss: 0.01727998877002587, Validation Loss: 0.005313172244331302\n",
      "Epoch [18148/20000], Training Loss: 0.012048372997686638, Validation Loss: 0.021617785600548296\n",
      "Epoch [18149/20000], Training Loss: 0.008420226056062217, Validation Loss: 0.006918988939458208\n",
      "Epoch [18150/20000], Training Loss: 0.006140750499038664, Validation Loss: 0.004161996886878683\n",
      "Epoch [18151/20000], Training Loss: 0.009552804655933349, Validation Loss: 0.004183804384979339\n",
      "Epoch [18152/20000], Training Loss: 0.005646774073933817, Validation Loss: 0.004336035849339075\n",
      "Epoch [18153/20000], Training Loss: 0.007153613863920327, Validation Loss: 0.0030013261521913493\n",
      "Epoch [18154/20000], Training Loss: 0.00395898351611582, Validation Loss: 0.002573309164286489\n",
      "Epoch [18155/20000], Training Loss: 0.004655161457776558, Validation Loss: 0.0033566739199721546\n",
      "Epoch [18156/20000], Training Loss: 0.005183541135920677, Validation Loss: 0.009974755486691198\n",
      "Epoch [18157/20000], Training Loss: 0.007770612128102324, Validation Loss: 0.008714422134825586\n",
      "Epoch [18158/20000], Training Loss: 0.011185098024205737, Validation Loss: 0.013268731619218346\n",
      "Epoch [18159/20000], Training Loss: 0.06267184070644102, Validation Loss: 0.010360139725801076\n",
      "Epoch [18160/20000], Training Loss: 0.029604828273867106, Validation Loss: 0.021690821943396697\n",
      "Epoch [18161/20000], Training Loss: 0.009620439236251903, Validation Loss: 0.003331415364170555\n",
      "Epoch [18162/20000], Training Loss: 0.007318887588293624, Validation Loss: 0.007496826414081527\n",
      "Epoch [18163/20000], Training Loss: 0.00714542482559669, Validation Loss: 0.0035615265620155662\n",
      "Epoch [18164/20000], Training Loss: 0.004355738593078838, Validation Loss: 0.003515780361660908\n",
      "Epoch [18165/20000], Training Loss: 0.00541487586844726, Validation Loss: 0.006613959182193493\n",
      "Epoch [18166/20000], Training Loss: 0.004385694231522004, Validation Loss: 0.004304581964520692\n",
      "Epoch [18167/20000], Training Loss: 0.00825967253019501, Validation Loss: 0.007282690918957867\n",
      "Epoch [18168/20000], Training Loss: 0.007142020066177273, Validation Loss: 0.01220232357991107\n",
      "Epoch [18169/20000], Training Loss: 0.011939699374904324, Validation Loss: 0.0032302486183673246\n",
      "Epoch [18170/20000], Training Loss: 0.019492427128300602, Validation Loss: 0.037453327805224035\n",
      "Epoch [18171/20000], Training Loss: 0.019133077206041498, Validation Loss: 0.0035890414439531276\n",
      "Epoch [18172/20000], Training Loss: 0.016150893966758822, Validation Loss: 0.011459320975256102\n",
      "Epoch [18173/20000], Training Loss: 0.011894793772285084, Validation Loss: 0.010422422339193613\n",
      "Epoch [18174/20000], Training Loss: 0.010901184593551858, Validation Loss: 0.010521305717447709\n",
      "Epoch [18175/20000], Training Loss: 0.014668727434972035, Validation Loss: 0.004285733258925006\n",
      "Epoch [18176/20000], Training Loss: 0.004001774706661568, Validation Loss: 0.004955718184427431\n",
      "Epoch [18177/20000], Training Loss: 0.004545747027051001, Validation Loss: 0.0035641087023147327\n",
      "Epoch [18178/20000], Training Loss: 0.005702621016163383, Validation Loss: 0.0048983819001315555\n",
      "Epoch [18179/20000], Training Loss: 0.0066959253311194645, Validation Loss: 0.003908913923806624\n",
      "Epoch [18180/20000], Training Loss: 0.00415721855305102, Validation Loss: 0.0036717070847852745\n",
      "Epoch [18181/20000], Training Loss: 0.0047674594344755405, Validation Loss: 0.003091115794445289\n",
      "Epoch [18182/20000], Training Loss: 0.004276848930333342, Validation Loss: 0.0036680468156983386\n",
      "Epoch [18183/20000], Training Loss: 0.0035041953503553358, Validation Loss: 0.003854082807955755\n",
      "Epoch [18184/20000], Training Loss: 0.005467089743303534, Validation Loss: 0.002887135815431715\n",
      "Epoch [18185/20000], Training Loss: 0.005097127586070981, Validation Loss: 0.005042042672863646\n",
      "Epoch [18186/20000], Training Loss: 0.006722153321399154, Validation Loss: 0.003226097331897953\n",
      "Epoch [18187/20000], Training Loss: 0.006467070222632694, Validation Loss: 0.016185668594399516\n",
      "Epoch [18188/20000], Training Loss: 0.01456847778501924, Validation Loss: 0.003336572428744538\n",
      "Epoch [18189/20000], Training Loss: 0.004888720689840349, Validation Loss: 0.004218642583745285\n",
      "Epoch [18190/20000], Training Loss: 0.004164224101779317, Validation Loss: 0.004332385245998945\n",
      "Epoch [18191/20000], Training Loss: 0.0039363241737321785, Validation Loss: 0.002988273197752887\n",
      "Epoch [18192/20000], Training Loss: 0.003858445509519827, Validation Loss: 0.002444671037106819\n",
      "Epoch [18193/20000], Training Loss: 0.0031319895156879545, Validation Loss: 0.0033021891750907783\n",
      "Epoch [18194/20000], Training Loss: 0.004974810849359658, Validation Loss: 0.0024161724063154516\n",
      "Epoch [18195/20000], Training Loss: 0.004580403129823806, Validation Loss: 0.007939131818626979\n",
      "Epoch [18196/20000], Training Loss: 0.009399186681028888, Validation Loss: 0.003446924735750859\n",
      "Epoch [18197/20000], Training Loss: 0.011671521080903144, Validation Loss: 0.004754044021807553\n",
      "Epoch [18198/20000], Training Loss: 0.007631107120297591, Validation Loss: 0.004347377775143205\n",
      "Epoch [18199/20000], Training Loss: 0.014423081354087637, Validation Loss: 0.0031208377158874\n",
      "Epoch [18200/20000], Training Loss: 0.005001911955852327, Validation Loss: 0.004965627731101969\n",
      "Epoch [18201/20000], Training Loss: 0.011768669701625965, Validation Loss: 0.003116502269540839\n",
      "Epoch [18202/20000], Training Loss: 0.01468181004059131, Validation Loss: 0.010624426499108506\n",
      "Epoch [18203/20000], Training Loss: 0.021075529107702908, Validation Loss: 0.011036609732140537\n",
      "Epoch [18204/20000], Training Loss: 0.011279840657412674, Validation Loss: 0.00653366720043737\n",
      "Epoch [18205/20000], Training Loss: 0.0061829785975403085, Validation Loss: 0.003274024758427783\n",
      "Epoch [18206/20000], Training Loss: 0.0037833936374746763, Validation Loss: 0.0031563905866534014\n",
      "Epoch [18207/20000], Training Loss: 0.003919648970362947, Validation Loss: 0.002628169354177804\n",
      "Epoch [18208/20000], Training Loss: 0.005751762100512029, Validation Loss: 0.005849850350291064\n",
      "Epoch [18209/20000], Training Loss: 0.00390028758140813, Validation Loss: 0.005543075450304968\n",
      "Epoch [18210/20000], Training Loss: 0.0061411457884657595, Validation Loss: 0.0025702394815902657\n",
      "Epoch [18211/20000], Training Loss: 0.007905789261972782, Validation Loss: 0.004404779704823828\n",
      "Epoch [18212/20000], Training Loss: 0.008042674578194107, Validation Loss: 0.009500537119192996\n",
      "Epoch [18213/20000], Training Loss: 0.0063274940657720435, Validation Loss: 0.0028167408196212917\n",
      "Epoch [18214/20000], Training Loss: 0.004036678399903134, Validation Loss: 0.0027451673813629895\n",
      "Epoch [18215/20000], Training Loss: 0.00477148042097854, Validation Loss: 0.008612566079315391\n",
      "Epoch [18216/20000], Training Loss: 0.013008493095834897, Validation Loss: 0.009864947807921227\n",
      "Epoch [18217/20000], Training Loss: 0.010359798842678458, Validation Loss: 0.007068822076042837\n",
      "Epoch [18218/20000], Training Loss: 0.011034465717135131, Validation Loss: 0.0029012586022949855\n",
      "Epoch [18219/20000], Training Loss: 0.012684609893247918, Validation Loss: 0.004824847978422245\n",
      "Epoch [18220/20000], Training Loss: 0.007292313912330428, Validation Loss: 0.0036426416160537656\n",
      "Epoch [18221/20000], Training Loss: 0.005549312065340928, Validation Loss: 0.016538654898981115\n",
      "Epoch [18222/20000], Training Loss: 0.010426294921282013, Validation Loss: 0.0028506764841478927\n",
      "Epoch [18223/20000], Training Loss: 0.008209386423134544, Validation Loss: 0.0036369830883542825\n",
      "Epoch [18224/20000], Training Loss: 0.01568524167409383, Validation Loss: 0.02172616659686355\n",
      "Epoch [18225/20000], Training Loss: 0.027881909698148126, Validation Loss: 0.004159179433978112\n",
      "Epoch [18226/20000], Training Loss: 0.007719199830880795, Validation Loss: 0.006679296355644381\n",
      "Epoch [18227/20000], Training Loss: 0.004842784352409321, Validation Loss: 0.01051470014080369\n",
      "Epoch [18228/20000], Training Loss: 0.011476837488381924, Validation Loss: 0.0027466868125054183\n",
      "Epoch [18229/20000], Training Loss: 0.007545599589840484, Validation Loss: 0.01470581200498928\n",
      "Epoch [18230/20000], Training Loss: 0.00787013240174669, Validation Loss: 0.002506566403125378\n",
      "Epoch [18231/20000], Training Loss: 0.005449687625514343, Validation Loss: 0.00506894758445802\n",
      "Epoch [18232/20000], Training Loss: 0.005355868262477819, Validation Loss: 0.010873676621630348\n",
      "Epoch [18233/20000], Training Loss: 0.014765812127279787, Validation Loss: 0.008559846386405263\n",
      "Epoch [18234/20000], Training Loss: 0.015459657781061, Validation Loss: 0.006419500827328355\n",
      "Epoch [18235/20000], Training Loss: 0.008882594308358551, Validation Loss: 0.024047552116286904\n",
      "Epoch [18236/20000], Training Loss: 0.009555963308230275, Validation Loss: 0.004200283956007768\n",
      "Epoch [18237/20000], Training Loss: 0.00923472095564648, Validation Loss: 0.0035435251755129927\n",
      "Epoch [18238/20000], Training Loss: 0.004072246370404693, Validation Loss: 0.005400333623383245\n",
      "Epoch [18239/20000], Training Loss: 0.006609125601316228, Validation Loss: 0.0031404270302287174\n",
      "Epoch [18240/20000], Training Loss: 0.005038662511383661, Validation Loss: 0.002976918521984625\n",
      "Epoch [18241/20000], Training Loss: 0.004015639231511159, Validation Loss: 0.003947007921522123\n",
      "Epoch [18242/20000], Training Loss: 0.005464051076290032, Validation Loss: 0.0030579575097279743\n",
      "Epoch [18243/20000], Training Loss: 0.008630682109339562, Validation Loss: 0.0030392783335072083\n",
      "Epoch [18244/20000], Training Loss: 0.0029926236373804776, Validation Loss: 0.003888769083405614\n",
      "Epoch [18245/20000], Training Loss: 0.012377319122606423, Validation Loss: 0.005660287057395408\n",
      "Epoch [18246/20000], Training Loss: 0.029365884917362046, Validation Loss: 0.008382847051328619\n",
      "Epoch [18247/20000], Training Loss: 0.010374521709828903, Validation Loss: 0.003081660727688578\n",
      "Epoch [18248/20000], Training Loss: 0.005934516491314363, Validation Loss: 0.009794223075885304\n",
      "Epoch [18249/20000], Training Loss: 0.004942228419947371, Validation Loss: 0.0039901391394485996\n",
      "Epoch [18250/20000], Training Loss: 0.004975738724104823, Validation Loss: 0.0046110155627161475\n",
      "Epoch [18251/20000], Training Loss: 0.004316205822362917, Validation Loss: 0.009206997255168556\n",
      "Epoch [18252/20000], Training Loss: 0.005381322983079632, Validation Loss: 0.002976587853124459\n",
      "Epoch [18253/20000], Training Loss: 0.008143386758872242, Validation Loss: 0.007552541906053304\n",
      "Epoch [18254/20000], Training Loss: 0.012097538592310489, Validation Loss: 0.002413611430944635\n",
      "Epoch [18255/20000], Training Loss: 0.011436108941519965, Validation Loss: 0.00911082256781169\n",
      "Epoch [18256/20000], Training Loss: 0.031035166031220536, Validation Loss: 0.008637478430858079\n",
      "Epoch [18257/20000], Training Loss: 0.015437320326173318, Validation Loss: 0.00887928045881381\n",
      "Epoch [18258/20000], Training Loss: 0.004462853888981044, Validation Loss: 0.005661202761230706\n",
      "Epoch [18259/20000], Training Loss: 0.004284008377518538, Validation Loss: 0.00495421326702074\n",
      "Epoch [18260/20000], Training Loss: 0.004667676759709138, Validation Loss: 0.0028531246180532677\n",
      "Epoch [18261/20000], Training Loss: 0.005184061446925625, Validation Loss: 0.0034073264457999486\n",
      "Epoch [18262/20000], Training Loss: 0.008226176787892265, Validation Loss: 0.0032055972940087584\n",
      "Epoch [18263/20000], Training Loss: 0.004630113439053015, Validation Loss: 0.008720718540370598\n",
      "Epoch [18264/20000], Training Loss: 0.00750430299090762, Validation Loss: 0.02166804638160816\n",
      "Epoch [18265/20000], Training Loss: 0.021809825393834865, Validation Loss: 0.012872768556074934\n",
      "Epoch [18266/20000], Training Loss: 0.01026901008032967, Validation Loss: 0.0066117475612251\n",
      "Epoch [18267/20000], Training Loss: 0.005740962711570319, Validation Loss: 0.011878679752904782\n",
      "Epoch [18268/20000], Training Loss: 0.010112772126116656, Validation Loss: 0.004099461259648203\n",
      "Epoch [18269/20000], Training Loss: 0.007084860075597784, Validation Loss: 0.0029378499920897283\n",
      "Epoch [18270/20000], Training Loss: 0.0036986722438346726, Validation Loss: 0.003712793556299435\n",
      "Epoch [18271/20000], Training Loss: 0.006681533090678775, Validation Loss: 0.00263310219573246\n",
      "Epoch [18272/20000], Training Loss: 0.007255770221230965, Validation Loss: 0.007130356459819203\n",
      "Epoch [18273/20000], Training Loss: 0.0072587551523091475, Validation Loss: 0.00708246736395982\n",
      "Epoch [18274/20000], Training Loss: 0.00685530741871584, Validation Loss: 0.005293240598993419\n",
      "Epoch [18275/20000], Training Loss: 0.011134896248612287, Validation Loss: 0.01476599895102331\n",
      "Epoch [18276/20000], Training Loss: 0.008835971409488204, Validation Loss: 0.016249148324088174\n",
      "Epoch [18277/20000], Training Loss: 0.012520634498027252, Validation Loss: 0.012981602365000709\n",
      "Epoch [18278/20000], Training Loss: 0.01167070316088419, Validation Loss: 0.004084130268945988\n",
      "Epoch [18279/20000], Training Loss: 0.018233440214966583, Validation Loss: 0.005029875305857396\n",
      "Epoch [18280/20000], Training Loss: 0.010014789315131825, Validation Loss: 0.007449240151325753\n",
      "Epoch [18281/20000], Training Loss: 0.006034819096385036, Validation Loss: 0.0062534162683274275\n",
      "Epoch [18282/20000], Training Loss: 0.006603558041596054, Validation Loss: 0.006088813295215394\n",
      "Epoch [18283/20000], Training Loss: 0.00532706270077402, Validation Loss: 0.002835123847603427\n",
      "Epoch [18284/20000], Training Loss: 0.005223622956171832, Validation Loss: 0.003142762860975828\n",
      "Epoch [18285/20000], Training Loss: 0.008406090656665453, Validation Loss: 0.002913764054777549\n",
      "Epoch [18286/20000], Training Loss: 0.004802659847233112, Validation Loss: 0.00702095050653614\n",
      "Epoch [18287/20000], Training Loss: 0.005535890527101271, Validation Loss: 0.0024290915127319784\n",
      "Epoch [18288/20000], Training Loss: 0.005022695073941057, Validation Loss: 0.003921345430951208\n",
      "Epoch [18289/20000], Training Loss: 0.008915814482082039, Validation Loss: 0.0028423771455225377\n",
      "Epoch [18290/20000], Training Loss: 0.00937855199819231, Validation Loss: 0.01578022250011481\n",
      "Epoch [18291/20000], Training Loss: 0.005255768492913505, Validation Loss: 0.003772909017267726\n",
      "Epoch [18292/20000], Training Loss: 0.006656649054743217, Validation Loss: 0.006487074106292187\n",
      "Epoch [18293/20000], Training Loss: 0.025454207787723653, Validation Loss: 0.022627412192249303\n",
      "Epoch [18294/20000], Training Loss: 0.012102296751566297, Validation Loss: 0.0052620739853692965\n",
      "Epoch [18295/20000], Training Loss: 0.013397213110563046, Validation Loss: 0.008804688355106853\n",
      "Epoch [18296/20000], Training Loss: 0.013821256007369291, Validation Loss: 0.009217969546511995\n",
      "Epoch [18297/20000], Training Loss: 0.006397170011041453, Validation Loss: 0.003938974336759112\n",
      "Epoch [18298/20000], Training Loss: 0.007993675229954533, Validation Loss: 0.0026516200064707262\n",
      "Epoch [18299/20000], Training Loss: 0.012139491208878488, Validation Loss: 0.007946073357077563\n",
      "Epoch [18300/20000], Training Loss: 0.00784209331322927, Validation Loss: 0.003432077993659886\n",
      "Epoch [18301/20000], Training Loss: 0.003967417095368805, Validation Loss: 0.003045934800407427\n",
      "Epoch [18302/20000], Training Loss: 0.004538089407495656, Validation Loss: 0.004769569265173246\n",
      "Epoch [18303/20000], Training Loss: 0.005496648473906264, Validation Loss: 0.004698841267595201\n",
      "Epoch [18304/20000], Training Loss: 0.006459530534942003, Validation Loss: 0.0033302747567032854\n",
      "Epoch [18305/20000], Training Loss: 0.00578956883275948, Validation Loss: 0.006885349212746306\n",
      "Epoch [18306/20000], Training Loss: 0.00929171578536625, Validation Loss: 0.006833268745804588\n",
      "Epoch [18307/20000], Training Loss: 0.00559199382301553, Validation Loss: 0.005378011516053708\n",
      "Epoch [18308/20000], Training Loss: 0.005657109221959088, Validation Loss: 0.002598826644066727\n",
      "Epoch [18309/20000], Training Loss: 0.012836829900818105, Validation Loss: 0.0034842832412859286\n",
      "Epoch [18310/20000], Training Loss: 0.04258441535348619, Validation Loss: 0.03139963752241063\n",
      "Epoch [18311/20000], Training Loss: 0.014116572672719485, Validation Loss: 0.02164303347658222\n",
      "Epoch [18312/20000], Training Loss: 0.013928866252464494, Validation Loss: 0.005503813910465283\n",
      "Epoch [18313/20000], Training Loss: 0.015334527340850659, Validation Loss: 0.006738552187040828\n",
      "Epoch [18314/20000], Training Loss: 0.00784435442515782, Validation Loss: 0.0033691002007612625\n",
      "Epoch [18315/20000], Training Loss: 0.005895563532249071, Validation Loss: 0.0033947419450279476\n",
      "Epoch [18316/20000], Training Loss: 0.007961485628974125, Validation Loss: 0.004623999406195115\n",
      "Epoch [18317/20000], Training Loss: 0.0050562019007754444, Validation Loss: 0.0031103891245649606\n",
      "Epoch [18318/20000], Training Loss: 0.0039462567988916165, Validation Loss: 0.003239637639826043\n",
      "Epoch [18319/20000], Training Loss: 0.010475993466505835, Validation Loss: 0.004383698112891616\n",
      "Epoch [18320/20000], Training Loss: 0.011050210531850877, Validation Loss: 0.004240323202455646\n",
      "Epoch [18321/20000], Training Loss: 0.00432118715847147, Validation Loss: 0.007906560962851472\n",
      "Epoch [18322/20000], Training Loss: 0.008492204031491772, Validation Loss: 0.007842101395768353\n",
      "Epoch [18323/20000], Training Loss: 0.012208667857391577, Validation Loss: 0.015271223524985953\n",
      "Epoch [18324/20000], Training Loss: 0.007248464468361249, Validation Loss: 0.003549943392879332\n",
      "Epoch [18325/20000], Training Loss: 0.006693815269370264, Validation Loss: 0.0024193940096266836\n",
      "Epoch [18326/20000], Training Loss: 0.009998904620130946, Validation Loss: 0.003148096284612783\n",
      "Epoch [18327/20000], Training Loss: 0.004265350958965948, Validation Loss: 0.018574656115934878\n",
      "Epoch [18328/20000], Training Loss: 0.009561651540155123, Validation Loss: 0.003129306983229948\n",
      "Epoch [18329/20000], Training Loss: 0.006429873007229097, Validation Loss: 0.0027403939825974633\n",
      "Epoch [18330/20000], Training Loss: 0.004295496993076345, Validation Loss: 0.002408036555744429\n",
      "Epoch [18331/20000], Training Loss: 0.002956200132757658, Validation Loss: 0.009720484139743009\n",
      "Epoch [18332/20000], Training Loss: 0.004479406913430596, Validation Loss: 0.004820192909956437\n",
      "Epoch [18333/20000], Training Loss: 0.007953025340871786, Validation Loss: 0.0026537377892559277\n",
      "Epoch [18334/20000], Training Loss: 0.004095752426597495, Validation Loss: 0.012390396211372057\n",
      "Epoch [18335/20000], Training Loss: 0.011526933231821397, Validation Loss: 0.007898872427997996\n",
      "Epoch [18336/20000], Training Loss: 0.010205965683521103, Validation Loss: 0.004525771933559598\n",
      "Epoch [18337/20000], Training Loss: 0.012960480472039697, Validation Loss: 0.0033705512802304177\n",
      "Epoch [18338/20000], Training Loss: 0.006535975367244516, Validation Loss: 0.0026404546114235927\n",
      "Epoch [18339/20000], Training Loss: 0.0071772235730480005, Validation Loss: 0.013297735595285434\n",
      "Epoch [18340/20000], Training Loss: 0.004909646821033675, Validation Loss: 0.0018737364175101284\n",
      "Epoch [18341/20000], Training Loss: 0.004609213516232558, Validation Loss: 0.022005859351684975\n",
      "Epoch [18342/20000], Training Loss: 0.012530809087690744, Validation Loss: 0.008131809332998887\n",
      "Epoch [18343/20000], Training Loss: 0.021750735896992928, Validation Loss: 0.0042308603231572105\n",
      "Epoch [18344/20000], Training Loss: 0.009135448712708565, Validation Loss: 0.03574060386407447\n",
      "Epoch [18345/20000], Training Loss: 0.024046986738410072, Validation Loss: 0.004824107981603122\n",
      "Epoch [18346/20000], Training Loss: 0.04595918383191539, Validation Loss: 0.004409919137218919\n",
      "Epoch [18347/20000], Training Loss: 0.020604866623346294, Validation Loss: 0.006645075553885097\n",
      "Epoch [18348/20000], Training Loss: 0.010074357111339591, Validation Loss: 0.009481793040759865\n",
      "Epoch [18349/20000], Training Loss: 0.01033010332944936, Validation Loss: 0.004536459215882717\n",
      "Epoch [18350/20000], Training Loss: 0.011308245136336024, Validation Loss: 0.005031054032855309\n",
      "Epoch [18351/20000], Training Loss: 0.010026027497328218, Validation Loss: 0.004561114684386374\n",
      "Epoch [18352/20000], Training Loss: 0.0055362972198054194, Validation Loss: 0.006586278065867646\n",
      "Epoch [18353/20000], Training Loss: 0.006946714208295036, Validation Loss: 0.004841340558321606\n",
      "Epoch [18354/20000], Training Loss: 0.005816351655084873, Validation Loss: 0.00686677149354052\n",
      "Epoch [18355/20000], Training Loss: 0.004011665406453956, Validation Loss: 0.010674618294445841\n",
      "Epoch [18356/20000], Training Loss: 0.008748687057699758, Validation Loss: 0.003090327420528029\n",
      "Epoch [18357/20000], Training Loss: 0.004587716201191716, Validation Loss: 0.0027746225404326553\n",
      "Epoch [18358/20000], Training Loss: 0.005211802840806611, Validation Loss: 0.003574025060546049\n",
      "Epoch [18359/20000], Training Loss: 0.005319100481106683, Validation Loss: 0.014323627519875768\n",
      "Epoch [18360/20000], Training Loss: 0.029828504292449258, Validation Loss: 0.010364142195165524\n",
      "Epoch [18361/20000], Training Loss: 0.025810089392637434, Validation Loss: 0.0033343342794946045\n",
      "Epoch [18362/20000], Training Loss: 0.016577124213134602, Validation Loss: 0.025942923171690273\n",
      "Epoch [18363/20000], Training Loss: 0.009482181133729941, Validation Loss: 0.007994333849783939\n",
      "Epoch [18364/20000], Training Loss: 0.007617415934094295, Validation Loss: 0.003704316319348168\n",
      "Epoch [18365/20000], Training Loss: 0.005412965017187942, Validation Loss: 0.0033657707603969656\n",
      "Epoch [18366/20000], Training Loss: 0.005525738041667084, Validation Loss: 0.0035828161067007003\n",
      "Epoch [18367/20000], Training Loss: 0.005236524945108353, Validation Loss: 0.0027485013132623032\n",
      "Epoch [18368/20000], Training Loss: 0.0037172964948695153, Validation Loss: 0.005337229491347628\n",
      "Epoch [18369/20000], Training Loss: 0.004077846963420078, Validation Loss: 0.0029276084300816152\n",
      "Epoch [18370/20000], Training Loss: 0.010000171257810766, Validation Loss: 0.0042420683295306206\n",
      "Epoch [18371/20000], Training Loss: 0.0060166784444553355, Validation Loss: 0.01612275546698478\n",
      "Epoch [18372/20000], Training Loss: 0.008484528467566374, Validation Loss: 0.0025029304726069362\n",
      "Epoch [18373/20000], Training Loss: 0.007095746700153021, Validation Loss: 0.009396909638805287\n",
      "Epoch [18374/20000], Training Loss: 0.027233209218024967, Validation Loss: 0.006598642389038544\n",
      "Epoch [18375/20000], Training Loss: 0.008851400834308671, Validation Loss: 0.007894035782056465\n",
      "Epoch [18376/20000], Training Loss: 0.01027694943123996, Validation Loss: 0.022969935632168363\n",
      "Epoch [18377/20000], Training Loss: 0.009525033081185288, Validation Loss: 0.007671694129297991\n",
      "Epoch [18378/20000], Training Loss: 0.010898095578470799, Validation Loss: 0.00449022262509402\n",
      "Epoch [18379/20000], Training Loss: 0.0052685494842015556, Validation Loss: 0.0029513644016187107\n",
      "Epoch [18380/20000], Training Loss: 0.0052118137672161014, Validation Loss: 0.002552091780476725\n",
      "Epoch [18381/20000], Training Loss: 0.004208391817103673, Validation Loss: 0.00289982439030315\n",
      "Epoch [18382/20000], Training Loss: 0.004615800960891647, Validation Loss: 0.0025347501638037784\n",
      "Epoch [18383/20000], Training Loss: 0.006317964585572814, Validation Loss: 0.002862388541649758\n",
      "Epoch [18384/20000], Training Loss: 0.01273092147727896, Validation Loss: 0.008603641879628603\n",
      "Epoch [18385/20000], Training Loss: 0.02558970331353651, Validation Loss: 0.018438368504062046\n",
      "Epoch [18386/20000], Training Loss: 0.013068066675365182, Validation Loss: 0.021872727926246886\n",
      "Epoch [18387/20000], Training Loss: 0.008516214998491835, Validation Loss: 0.00969909203163206\n",
      "Epoch [18388/20000], Training Loss: 0.010070397992379299, Validation Loss: 0.00329179771940196\n",
      "Epoch [18389/20000], Training Loss: 0.008470837823215862, Validation Loss: 0.00643930921341226\n",
      "Epoch [18390/20000], Training Loss: 0.0055013973739862975, Validation Loss: 0.0037202331984015507\n",
      "Epoch [18391/20000], Training Loss: 0.005887096890156889, Validation Loss: 0.0027995207719117487\n",
      "Epoch [18392/20000], Training Loss: 0.004717625624315198, Validation Loss: 0.00575080678044542\n",
      "Epoch [18393/20000], Training Loss: 0.004905954834872058, Validation Loss: 0.0038158542605413815\n",
      "Epoch [18394/20000], Training Loss: 0.0061469780318605316, Validation Loss: 0.005009974630382885\n",
      "Epoch [18395/20000], Training Loss: 0.00685750917182304, Validation Loss: 0.002999033730345348\n",
      "Epoch [18396/20000], Training Loss: 0.005901529304017978, Validation Loss: 0.004289257043028881\n",
      "Epoch [18397/20000], Training Loss: 0.005137961488799192, Validation Loss: 0.003177565748301398\n",
      "Epoch [18398/20000], Training Loss: 0.00483843371850006, Validation Loss: 0.0035470398965640826\n",
      "Epoch [18399/20000], Training Loss: 0.006735466470802619, Validation Loss: 0.003140287211326357\n",
      "Epoch [18400/20000], Training Loss: 0.007826961524967504, Validation Loss: 0.01747843106203487\n",
      "Epoch [18401/20000], Training Loss: 0.010925953530074497, Validation Loss: 0.01584532081683346\n",
      "Epoch [18402/20000], Training Loss: 0.010852356846433915, Validation Loss: 0.01262372023709446\n",
      "Epoch [18403/20000], Training Loss: 0.01729121228189407, Validation Loss: 0.0060081457127958105\n",
      "Epoch [18404/20000], Training Loss: 0.01079889731564825, Validation Loss: 0.003381893971680126\n",
      "Epoch [18405/20000], Training Loss: 0.004989133286601698, Validation Loss: 0.003263776401394937\n",
      "Epoch [18406/20000], Training Loss: 0.010477526139084148, Validation Loss: 0.002459547005071069\n",
      "Epoch [18407/20000], Training Loss: 0.011166623108043236, Validation Loss: 0.01094868542186995\n",
      "Epoch [18408/20000], Training Loss: 0.011469724916033945, Validation Loss: 0.004568341468025697\n",
      "Epoch [18409/20000], Training Loss: 0.013785606232354601, Validation Loss: 0.010021354676570837\n",
      "Epoch [18410/20000], Training Loss: 0.03872093542200413, Validation Loss: 0.007447583194980163\n",
      "Epoch [18411/20000], Training Loss: 0.0384308729085855, Validation Loss: 0.06555649696227712\n",
      "Epoch [18412/20000], Training Loss: 0.026620840807611654, Validation Loss: 0.004549277566312071\n",
      "Epoch [18413/20000], Training Loss: 0.00809159952249112, Validation Loss: 0.004293319493299411\n",
      "Epoch [18414/20000], Training Loss: 0.004959291582776716, Validation Loss: 0.006180523462514235\n",
      "Epoch [18415/20000], Training Loss: 0.0075086679020647095, Validation Loss: 0.0044835777712251\n",
      "Epoch [18416/20000], Training Loss: 0.010123122753741751, Validation Loss: 0.006854040369293506\n",
      "Epoch [18417/20000], Training Loss: 0.004645038038558725, Validation Loss: 0.0033424383669277296\n",
      "Epoch [18418/20000], Training Loss: 0.004892041039007771, Validation Loss: 0.0029237261115479135\n",
      "Epoch [18419/20000], Training Loss: 0.0059732100754834915, Validation Loss: 0.003859031413402444\n",
      "Epoch [18420/20000], Training Loss: 0.01316242400727268, Validation Loss: 0.007267038681514697\n",
      "Epoch [18421/20000], Training Loss: 0.005501254430003298, Validation Loss: 0.003364556154861325\n",
      "Epoch [18422/20000], Training Loss: 0.009186414889752217, Validation Loss: 0.01903057554330978\n",
      "Epoch [18423/20000], Training Loss: 0.006531602908840536, Validation Loss: 0.0042162285053061665\n",
      "Epoch [18424/20000], Training Loss: 0.009641827207815368, Validation Loss: 0.004766944906148548\n",
      "Epoch [18425/20000], Training Loss: 0.0035081069502861544, Validation Loss: 0.0035470462641488914\n",
      "Epoch [18426/20000], Training Loss: 0.004807675006304635, Validation Loss: 0.003008493290051563\n",
      "Epoch [18427/20000], Training Loss: 0.0068214372305582015, Validation Loss: 0.002530301829505025\n",
      "Epoch [18428/20000], Training Loss: 0.00475049048899174, Validation Loss: 0.003020776672799969\n",
      "Epoch [18429/20000], Training Loss: 0.012439818109247167, Validation Loss: 0.008760927357281036\n",
      "Epoch [18430/20000], Training Loss: 0.006012092243046092, Validation Loss: 0.03793928497512779\n",
      "Epoch [18431/20000], Training Loss: 0.017399427889033956, Validation Loss: 0.006703710251128798\n",
      "Epoch [18432/20000], Training Loss: 0.004931956943307471, Validation Loss: 0.00978418986023171\n",
      "Epoch [18433/20000], Training Loss: 0.0093515194313843, Validation Loss: 0.002990937201716853\n",
      "Epoch [18434/20000], Training Loss: 0.007599667287649936, Validation Loss: 0.035579542639060366\n",
      "Epoch [18435/20000], Training Loss: 0.016230289229757285, Validation Loss: 0.034500098063692475\n",
      "Epoch [18436/20000], Training Loss: 0.016233568379123296, Validation Loss: 0.012520306864071731\n",
      "Epoch [18437/20000], Training Loss: 0.006179658422687291, Validation Loss: 0.019032316821948836\n",
      "Epoch [18438/20000], Training Loss: 0.009837632173522641, Validation Loss: 0.007190173049505612\n",
      "Epoch [18439/20000], Training Loss: 0.014339397070995932, Validation Loss: 0.005495173707925817\n",
      "Epoch [18440/20000], Training Loss: 0.011392566355685372, Validation Loss: 0.005402846213720685\n",
      "Epoch [18441/20000], Training Loss: 0.0176404614114501, Validation Loss: 0.012435931580164603\n",
      "Epoch [18442/20000], Training Loss: 0.011467876771933203, Validation Loss: 0.00607875679065525\n",
      "Epoch [18443/20000], Training Loss: 0.007104440991367612, Validation Loss: 0.01584297277994397\n",
      "Epoch [18444/20000], Training Loss: 0.02010223086602179, Validation Loss: 0.009980808626128887\n",
      "Epoch [18445/20000], Training Loss: 0.013428238505444565, Validation Loss: 0.0163504411003313\n",
      "Epoch [18446/20000], Training Loss: 0.006839551618343519, Validation Loss: 0.0033210466129455695\n",
      "Epoch [18447/20000], Training Loss: 0.0044573032451724615, Validation Loss: 0.00421374174715058\n",
      "Epoch [18448/20000], Training Loss: 0.0067207253251930455, Validation Loss: 0.0032272528917250804\n",
      "Epoch [18449/20000], Training Loss: 0.0058369844012174455, Validation Loss: 0.003371219897842995\n",
      "Epoch [18450/20000], Training Loss: 0.006362896440675415, Validation Loss: 0.0027474290253165024\n",
      "Epoch [18451/20000], Training Loss: 0.005957158966339193, Validation Loss: 0.0026906317204148217\n",
      "Epoch [18452/20000], Training Loss: 0.008744487478127536, Validation Loss: 0.005931890337876666\n",
      "Epoch [18453/20000], Training Loss: 0.011657697587582203, Validation Loss: 0.007278025271460312\n",
      "Epoch [18454/20000], Training Loss: 0.006328090715604568, Validation Loss: 0.004637789255142926\n",
      "Epoch [18455/20000], Training Loss: 0.0052953592551472995, Validation Loss: 0.013051357755918909\n",
      "Epoch [18456/20000], Training Loss: 0.010071504567390224, Validation Loss: 0.004835503256440633\n",
      "Epoch [18457/20000], Training Loss: 0.004523081896127427, Validation Loss: 0.0027396118607206127\n",
      "Epoch [18458/20000], Training Loss: 0.003058795996789091, Validation Loss: 0.01380326833571921\n",
      "Epoch [18459/20000], Training Loss: 0.004676000101816109, Validation Loss: 0.002946275104586401\n",
      "Epoch [18460/20000], Training Loss: 0.004557041219572154, Validation Loss: 0.002030614630235001\n",
      "Epoch [18461/20000], Training Loss: 0.007054228100709484, Validation Loss: 0.016137040452869438\n",
      "Epoch [18462/20000], Training Loss: 0.0072623512990373585, Validation Loss: 0.00289749843795839\n",
      "Epoch [18463/20000], Training Loss: 0.005081343357232981, Validation Loss: 0.009324749086760318\n",
      "Epoch [18464/20000], Training Loss: 0.009654063943993865, Validation Loss: 0.0026290844873265557\n",
      "Epoch [18465/20000], Training Loss: 0.005010354997856277, Validation Loss: 0.017892171075640666\n",
      "Epoch [18466/20000], Training Loss: 0.01226689387996365, Validation Loss: 0.008566966111305061\n",
      "Epoch [18467/20000], Training Loss: 0.015823525563277485, Validation Loss: 0.008751746992982186\n",
      "Epoch [18468/20000], Training Loss: 0.011554894893509169, Validation Loss: 0.004917893491039597\n",
      "Epoch [18469/20000], Training Loss: 0.008605637078289874, Validation Loss: 0.004804212538905363\n",
      "Epoch [18470/20000], Training Loss: 0.06251445871352448, Validation Loss: 0.03356534452190577\n",
      "Epoch [18471/20000], Training Loss: 0.0517835306402828, Validation Loss: 0.029891129944898256\n",
      "Epoch [18472/20000], Training Loss: 0.031158695503368757, Validation Loss: 0.013875948109997134\n",
      "Epoch [18473/20000], Training Loss: 0.011429991494878777, Validation Loss: 0.006689583265719128\n",
      "Epoch [18474/20000], Training Loss: 0.007938982314954046, Validation Loss: 0.007247478958216139\n",
      "Epoch [18475/20000], Training Loss: 0.007092833518202367, Validation Loss: 0.006141855921372651\n",
      "Epoch [18476/20000], Training Loss: 0.0062216795974693795, Validation Loss: 0.0057804193453258835\n",
      "Epoch [18477/20000], Training Loss: 0.006383915879788609, Validation Loss: 0.00509185261311172\n",
      "Epoch [18478/20000], Training Loss: 0.00542775889958388, Validation Loss: 0.004660848521731558\n",
      "Epoch [18479/20000], Training Loss: 0.0060765129227157945, Validation Loss: 0.009307848914668324\n",
      "Epoch [18480/20000], Training Loss: 0.007836930832959459, Validation Loss: 0.0048848516923847285\n",
      "Epoch [18481/20000], Training Loss: 0.005586961822700687, Validation Loss: 0.004897368575271344\n",
      "Epoch [18482/20000], Training Loss: 0.005267163272947073, Validation Loss: 0.008403507071439062\n",
      "Epoch [18483/20000], Training Loss: 0.005969591035474358, Validation Loss: 0.0033350799696779837\n",
      "Epoch [18484/20000], Training Loss: 0.006006455751568345, Validation Loss: 0.0043835425434874\n",
      "Epoch [18485/20000], Training Loss: 0.006688683398741497, Validation Loss: 0.003617935799629229\n",
      "Epoch [18486/20000], Training Loss: 0.005056966202703604, Validation Loss: 0.0032403787637664194\n",
      "Epoch [18487/20000], Training Loss: 0.003602474940375292, Validation Loss: 0.002632449349440894\n",
      "Epoch [18488/20000], Training Loss: 0.004363396656117402, Validation Loss: 0.002405029955358121\n",
      "Epoch [18489/20000], Training Loss: 0.0060060638151558775, Validation Loss: 0.026386284747144155\n",
      "Epoch [18490/20000], Training Loss: 0.016083638588107924, Validation Loss: 0.010753098463813972\n",
      "Epoch [18491/20000], Training Loss: 0.014556397370012877, Validation Loss: 0.005774474508707986\n",
      "Epoch [18492/20000], Training Loss: 0.010402320634706743, Validation Loss: 0.00477074600286593\n",
      "Epoch [18493/20000], Training Loss: 0.008007202279259218, Validation Loss: 0.004766053692396355\n",
      "Epoch [18494/20000], Training Loss: 0.009016910349600948, Validation Loss: 0.00572858145590465\n",
      "Epoch [18495/20000], Training Loss: 0.005360260829947704, Validation Loss: 0.0055495369270793\n",
      "Epoch [18496/20000], Training Loss: 0.007537970911081564, Validation Loss: 0.003669604784414661\n",
      "Epoch [18497/20000], Training Loss: 0.01488562504372177, Validation Loss: 0.003077113228226125\n",
      "Epoch [18498/20000], Training Loss: 0.00813310270781845, Validation Loss: 0.01456267284561871\n",
      "Epoch [18499/20000], Training Loss: 0.016837989016494248, Validation Loss: 0.0031539428984816353\n",
      "Epoch [18500/20000], Training Loss: 0.023460887971720825, Validation Loss: 0.02665790564844071\n",
      "Epoch [18501/20000], Training Loss: 0.007572296549911682, Validation Loss: 0.004424609614486842\n",
      "Epoch [18502/20000], Training Loss: 0.007655992800469643, Validation Loss: 0.004469321715248498\n",
      "Epoch [18503/20000], Training Loss: 0.004591576135510422, Validation Loss: 0.01690986849923653\n",
      "Epoch [18504/20000], Training Loss: 0.007277988718539875, Validation Loss: 0.006678219829508082\n",
      "Epoch [18505/20000], Training Loss: 0.006516323564158354, Validation Loss: 0.0038090901292966756\n",
      "Epoch [18506/20000], Training Loss: 0.005319285861332901, Validation Loss: 0.026181529681322844\n",
      "Epoch [18507/20000], Training Loss: 0.004371351258149454, Validation Loss: 0.027745628672810035\n",
      "Epoch [18508/20000], Training Loss: 0.015465840019065322, Validation Loss: 0.01860949659147642\n",
      "Epoch [18509/20000], Training Loss: 0.009498780042901802, Validation Loss: 0.00408655101734163\n",
      "Epoch [18510/20000], Training Loss: 0.013423507094038152, Validation Loss: 0.005562207098676148\n",
      "Epoch [18511/20000], Training Loss: 0.009955854566458453, Validation Loss: 0.011956371728391193\n",
      "Epoch [18512/20000], Training Loss: 0.031259739736144966, Validation Loss: 0.010938458883233335\n",
      "Epoch [18513/20000], Training Loss: 0.013961597307310772, Validation Loss: 0.017921250586855057\n",
      "Epoch [18514/20000], Training Loss: 0.006300649494473224, Validation Loss: 0.004766491312712974\n",
      "Epoch [18515/20000], Training Loss: 0.005188137996550982, Validation Loss: 0.003176385147866207\n",
      "Epoch [18516/20000], Training Loss: 0.00514473053668293, Validation Loss: 0.009305908913818581\n",
      "Epoch [18517/20000], Training Loss: 0.010135948597702995, Validation Loss: 0.0030366639934535605\n",
      "Epoch [18518/20000], Training Loss: 0.008934164684822983, Validation Loss: 0.007685609345765597\n",
      "Epoch [18519/20000], Training Loss: 0.007598368515443456, Validation Loss: 0.0030363926068354935\n",
      "Epoch [18520/20000], Training Loss: 0.005668167318983056, Validation Loss: 0.010022952220034702\n",
      "Epoch [18521/20000], Training Loss: 0.011642908669143383, Validation Loss: 0.006369201310723176\n",
      "Epoch [18522/20000], Training Loss: 0.005968208567927442, Validation Loss: 0.0037444845303293206\n",
      "Epoch [18523/20000], Training Loss: 0.00715990992827642, Validation Loss: 0.004020457157179992\n",
      "Epoch [18524/20000], Training Loss: 0.008796192420829487, Validation Loss: 0.0049212265123778865\n",
      "Epoch [18525/20000], Training Loss: 0.008793144548625216, Validation Loss: 0.002200874169625016\n",
      "Epoch [18526/20000], Training Loss: 0.007986225049015567, Validation Loss: 0.002260029555812488\n",
      "Epoch [18527/20000], Training Loss: 0.009783927812325796, Validation Loss: 0.002650779503785624\n",
      "Epoch [18528/20000], Training Loss: 0.004794895661429369, Validation Loss: 0.0027024771197327524\n",
      "Epoch [18529/20000], Training Loss: 0.004663710286617321, Validation Loss: 0.005560565458486012\n",
      "Epoch [18530/20000], Training Loss: 0.0054850420870674755, Validation Loss: 0.0041967542049669805\n",
      "Epoch [18531/20000], Training Loss: 0.00491683659129194, Validation Loss: 0.00227296506669461\n",
      "Epoch [18532/20000], Training Loss: 0.006474852281651725, Validation Loss: 0.010968598107144853\n",
      "Epoch [18533/20000], Training Loss: 0.01198404032475823, Validation Loss: 0.0032311229813259956\n",
      "Epoch [18534/20000], Training Loss: 0.00927437654172536, Validation Loss: 0.0032212349352234476\n",
      "Epoch [18535/20000], Training Loss: 0.014380465313869146, Validation Loss: 0.010205727953348287\n",
      "Epoch [18536/20000], Training Loss: 0.027097991953476303, Validation Loss: 0.00825605244879059\n",
      "Epoch [18537/20000], Training Loss: 0.02189549102019685, Validation Loss: 0.004306328536582527\n",
      "Epoch [18538/20000], Training Loss: 0.010896253131505773, Validation Loss: 0.010908785877465021\n",
      "Epoch [18539/20000], Training Loss: 0.004367512248011606, Validation Loss: 0.009069706220202534\n",
      "Epoch [18540/20000], Training Loss: 0.008913657505867636, Validation Loss: 0.005587214808753365\n",
      "Epoch [18541/20000], Training Loss: 0.00477026787289755, Validation Loss: 0.0038504325262087896\n",
      "Epoch [18542/20000], Training Loss: 0.0042633104011266755, Validation Loss: 0.008660113510937302\n",
      "Epoch [18543/20000], Training Loss: 0.005407807277931299, Validation Loss: 0.004096855247021937\n",
      "Epoch [18544/20000], Training Loss: 0.0051182571503107155, Validation Loss: 0.00327905714981063\n",
      "Epoch [18545/20000], Training Loss: 0.004578717159997073, Validation Loss: 0.013685312016987654\n",
      "Epoch [18546/20000], Training Loss: 0.01142802584720549, Validation Loss: 0.015088464464781737\n",
      "Epoch [18547/20000], Training Loss: 0.019878858564620065, Validation Loss: 0.004499952206240871\n",
      "Epoch [18548/20000], Training Loss: 0.006709955160512696, Validation Loss: 0.006758519205890577\n",
      "Epoch [18549/20000], Training Loss: 0.010366872381252636, Validation Loss: 0.005061475947447848\n",
      "Epoch [18550/20000], Training Loss: 0.004083445889721459, Validation Loss: 0.0032610206005208348\n",
      "Epoch [18551/20000], Training Loss: 0.0067930259517327484, Validation Loss: 0.0034975058529980174\n",
      "Epoch [18552/20000], Training Loss: 0.03494070871939974, Validation Loss: 0.04640610027625858\n",
      "Epoch [18553/20000], Training Loss: 0.024138309376991986, Validation Loss: 0.024545431288132642\n",
      "Epoch [18554/20000], Training Loss: 0.010298946472396788, Validation Loss: 0.006474149083208504\n",
      "Epoch [18555/20000], Training Loss: 0.008191477269455, Validation Loss: 0.015273015102317393\n",
      "Epoch [18556/20000], Training Loss: 0.009042877381619161, Validation Loss: 0.0040961218417737855\n",
      "Epoch [18557/20000], Training Loss: 0.008128184958747755, Validation Loss: 0.011405363813615057\n",
      "Epoch [18558/20000], Training Loss: 0.006940876742841543, Validation Loss: 0.005717650812111381\n",
      "Epoch [18559/20000], Training Loss: 0.005954435773414194, Validation Loss: 0.004265249704050892\n",
      "Epoch [18560/20000], Training Loss: 0.005731337566560667, Validation Loss: 0.002784365298735127\n",
      "Epoch [18561/20000], Training Loss: 0.005726911550612256, Validation Loss: 0.003414702441689193\n",
      "Epoch [18562/20000], Training Loss: 0.008474080249600644, Validation Loss: 0.003345288695549747\n",
      "Epoch [18563/20000], Training Loss: 0.02371659148669027, Validation Loss: 0.009713020424226901\n",
      "Epoch [18564/20000], Training Loss: 0.022477998506993963, Validation Loss: 0.01316824949080748\n",
      "Epoch [18565/20000], Training Loss: 0.013243359582052758, Validation Loss: 0.007608872025634079\n",
      "Epoch [18566/20000], Training Loss: 0.006150622109998949, Validation Loss: 0.005984926313463542\n",
      "Epoch [18567/20000], Training Loss: 0.010640375398257415, Validation Loss: 0.006242150122327571\n",
      "Epoch [18568/20000], Training Loss: 0.005136871218772805, Validation Loss: 0.002752326186610889\n",
      "Epoch [18569/20000], Training Loss: 0.003514276250436004, Validation Loss: 0.0060484531893076265\n",
      "Epoch [18570/20000], Training Loss: 0.005092190466192343, Validation Loss: 0.003597827210325834\n",
      "Epoch [18571/20000], Training Loss: 0.005007492270643914, Validation Loss: 0.00237771002289985\n",
      "Epoch [18572/20000], Training Loss: 0.004809384366256252, Validation Loss: 0.002648607847624821\n",
      "Epoch [18573/20000], Training Loss: 0.003893541256340021, Validation Loss: 0.01349913795809053\n",
      "Epoch [18574/20000], Training Loss: 0.007064554484843809, Validation Loss: 0.002952518295736236\n",
      "Epoch [18575/20000], Training Loss: 0.0038686471733464195, Validation Loss: 0.003004972216291892\n",
      "Epoch [18576/20000], Training Loss: 0.007403001615395104, Validation Loss: 0.004048443915037987\n",
      "Epoch [18577/20000], Training Loss: 0.006578543694644168, Validation Loss: 0.004491388610316806\n",
      "Epoch [18578/20000], Training Loss: 0.005472700410401947, Validation Loss: 0.006120611169018885\n",
      "Epoch [18579/20000], Training Loss: 0.004709264186816752, Validation Loss: 0.0029030979257088163\n",
      "Epoch [18580/20000], Training Loss: 0.004053804149277441, Validation Loss: 0.005757405607054876\n",
      "Epoch [18581/20000], Training Loss: 0.009554967972819992, Validation Loss: 0.0019225186464844057\n",
      "Epoch [18582/20000], Training Loss: 0.028871448344684074, Validation Loss: 0.0021454454075668294\n",
      "Epoch [18583/20000], Training Loss: 0.00574297167570746, Validation Loss: 0.0046797622243508795\n",
      "Epoch [18584/20000], Training Loss: 0.005759134919604796, Validation Loss: 0.002747617109986373\n",
      "Epoch [18585/20000], Training Loss: 0.0026653752512564616, Validation Loss: 0.002018336494734945\n",
      "Epoch [18586/20000], Training Loss: 0.005286028405992381, Validation Loss: 0.0019443727887128606\n",
      "Epoch [18587/20000], Training Loss: 0.003514330216473484, Validation Loss: 0.0018932374974982885\n",
      "Epoch [18588/20000], Training Loss: 0.006902984795039353, Validation Loss: 0.0031783606135320497\n",
      "Epoch [18589/20000], Training Loss: 0.009484254255775528, Validation Loss: 0.013712248716008074\n",
      "Epoch [18590/20000], Training Loss: 0.014577811540220864, Validation Loss: 0.0026583787094772555\n",
      "Epoch [18591/20000], Training Loss: 0.013142177432200697, Validation Loss: 0.00968908544437446\n",
      "Epoch [18592/20000], Training Loss: 0.006905933037549923, Validation Loss: 0.01046439908603907\n",
      "Epoch [18593/20000], Training Loss: 0.010230661976882922, Validation Loss: 0.0053367352823409575\n",
      "Epoch [18594/20000], Training Loss: 0.005850829641920947, Validation Loss: 0.0033964922858731633\n",
      "Epoch [18595/20000], Training Loss: 0.00715912932979076, Validation Loss: 0.003081823899434407\n",
      "Epoch [18596/20000], Training Loss: 0.010498442356817708, Validation Loss: 0.004205628240318861\n",
      "Epoch [18597/20000], Training Loss: 0.006220343672404332, Validation Loss: 0.002395590085328081\n",
      "Epoch [18598/20000], Training Loss: 0.005573265442373148, Validation Loss: 0.004023561084774181\n",
      "Epoch [18599/20000], Training Loss: 0.009775986851309426, Validation Loss: 0.0030207060336600983\n",
      "Epoch [18600/20000], Training Loss: 0.0047222342525076655, Validation Loss: 0.002236488438234271\n",
      "Epoch [18601/20000], Training Loss: 0.006116362272483197, Validation Loss: 0.0024270785460766475\n",
      "Epoch [18602/20000], Training Loss: 0.005596262367362215, Validation Loss: 0.00379891648951539\n",
      "Epoch [18603/20000], Training Loss: 0.002631146019926486, Validation Loss: 0.0062675297981229505\n",
      "Epoch [18604/20000], Training Loss: 0.010633658137521707, Validation Loss: 0.008775651028874596\n",
      "Epoch [18605/20000], Training Loss: 0.019010697803813464, Validation Loss: 0.014639332292565638\n",
      "Epoch [18606/20000], Training Loss: 0.01147128360383525, Validation Loss: 0.004336518015203897\n",
      "Epoch [18607/20000], Training Loss: 0.0101095184982114, Validation Loss: 0.01607095802556966\n",
      "Epoch [18608/20000], Training Loss: 0.008849800318785128, Validation Loss: 0.00734925175358495\n",
      "Epoch [18609/20000], Training Loss: 0.010352035043719557, Validation Loss: 0.003256582545884409\n",
      "Epoch [18610/20000], Training Loss: 0.01260149943430276, Validation Loss: 0.016336153308701258\n",
      "Epoch [18611/20000], Training Loss: 0.008278730339173177, Validation Loss: 0.0047737075106389125\n",
      "Epoch [18612/20000], Training Loss: 0.004336751894113279, Validation Loss: 0.007228334380199287\n",
      "Epoch [18613/20000], Training Loss: 0.009786360045836773, Validation Loss: 0.005688459510827621\n",
      "Epoch [18614/20000], Training Loss: 0.012690314507998275, Validation Loss: 0.003944456671906745\n",
      "Epoch [18615/20000], Training Loss: 0.007725838012577567, Validation Loss: 0.004156904367140462\n",
      "Epoch [18616/20000], Training Loss: 0.0054795028306762516, Validation Loss: 0.002786991759050902\n",
      "Epoch [18617/20000], Training Loss: 0.007410946952404629, Validation Loss: 0.0019009389878290911\n",
      "Epoch [18618/20000], Training Loss: 0.004808871419498928, Validation Loss: 0.004485052912271621\n",
      "Epoch [18619/20000], Training Loss: 0.011169387039089429, Validation Loss: 0.0023376003783161436\n",
      "Epoch [18620/20000], Training Loss: 0.007525114477695232, Validation Loss: 0.0025390384835878776\n",
      "Epoch [18621/20000], Training Loss: 0.00599056198760601, Validation Loss: 0.012983379101277543\n",
      "Epoch [18622/20000], Training Loss: 0.008599440257447506, Validation Loss: 0.0018547951816813171\n",
      "Epoch [18623/20000], Training Loss: 0.011515804791802762, Validation Loss: 0.023523635641146345\n",
      "Epoch [18624/20000], Training Loss: 0.008560301700656834, Validation Loss: 0.00704362853738681\n",
      "Epoch [18625/20000], Training Loss: 0.007507071148990819, Validation Loss: 0.005189785489526295\n",
      "Epoch [18626/20000], Training Loss: 0.004775355886002737, Validation Loss: 0.0016942087259300212\n",
      "Epoch [18627/20000], Training Loss: 0.005640378233822828, Validation Loss: 0.003867067709978197\n",
      "Epoch [18628/20000], Training Loss: 0.006206404965823253, Validation Loss: 0.002122271450890802\n",
      "Epoch [18629/20000], Training Loss: 0.025385409374783712, Validation Loss: 0.012179649624415438\n",
      "Epoch [18630/20000], Training Loss: 0.012546362260244288, Validation Loss: 0.010410320190005524\n",
      "Epoch [18631/20000], Training Loss: 0.007346944603341399, Validation Loss: 0.0030786323698218814\n",
      "Epoch [18632/20000], Training Loss: 0.00799859701477804, Validation Loss: 0.008763436197003038\n",
      "Epoch [18633/20000], Training Loss: 0.006581505366609365, Validation Loss: 0.003292570513004023\n",
      "Epoch [18634/20000], Training Loss: 0.008028320979065029, Validation Loss: 0.006200160173164144\n",
      "Epoch [18635/20000], Training Loss: 0.006855043541041336, Validation Loss: 0.005211481010354164\n",
      "Epoch [18636/20000], Training Loss: 0.004217636406143096, Validation Loss: 0.002325414985599699\n",
      "Epoch [18637/20000], Training Loss: 0.0038820241035344744, Validation Loss: 0.004993253842276779\n",
      "Epoch [18638/20000], Training Loss: 0.005388731857432764, Validation Loss: 0.0025026913975269493\n",
      "Epoch [18639/20000], Training Loss: 0.004119050195555474, Validation Loss: 0.004426471017933079\n",
      "Epoch [18640/20000], Training Loss: 0.005282024038319376, Validation Loss: 0.003768118286945819\n",
      "Epoch [18641/20000], Training Loss: 0.007135791023236899, Validation Loss: 0.0020451788750831057\n",
      "Epoch [18642/20000], Training Loss: 0.0062155817473207465, Validation Loss: 0.0029480414645023855\n",
      "Epoch [18643/20000], Training Loss: 0.015593624237746033, Validation Loss: 0.009275984176708465\n",
      "Epoch [18644/20000], Training Loss: 0.006524891564497791, Validation Loss: 0.0035465962879003754\n",
      "Epoch [18645/20000], Training Loss: 0.005360216707411729, Validation Loss: 0.002991595562442027\n",
      "Epoch [18646/20000], Training Loss: 0.004054438524430485, Validation Loss: 0.004344462545609369\n",
      "Epoch [18647/20000], Training Loss: 0.006132013993521828, Validation Loss: 0.0020778162512239567\n",
      "Epoch [18648/20000], Training Loss: 0.0034170690103759887, Validation Loss: 0.0030301809259601115\n",
      "Epoch [18649/20000], Training Loss: 0.007533385606880724, Validation Loss: 0.002955564692492791\n",
      "Epoch [18650/20000], Training Loss: 0.008530159507894755, Validation Loss: 0.026791394821837032\n",
      "Epoch [18651/20000], Training Loss: 0.02551080741652965, Validation Loss: 0.01648458479253304\n",
      "Epoch [18652/20000], Training Loss: 0.027499449587724354, Validation Loss: 0.048971245887774524\n",
      "Epoch [18653/20000], Training Loss: 0.041682048260944224, Validation Loss: 0.043591563515408084\n",
      "Epoch [18654/20000], Training Loss: 0.016307211009136933, Validation Loss: 0.013183437216412892\n",
      "Epoch [18655/20000], Training Loss: 0.008041595372820407, Validation Loss: 0.004309895316158726\n",
      "Epoch [18656/20000], Training Loss: 0.00631589354764271, Validation Loss: 0.00519600512342728\n",
      "Epoch [18657/20000], Training Loss: 0.005444546556516018, Validation Loss: 0.006059116000175819\n",
      "Epoch [18658/20000], Training Loss: 0.004402357303271336, Validation Loss: 0.0027121924567105843\n",
      "Epoch [18659/20000], Training Loss: 0.005703971904393613, Validation Loss: 0.005050907107261082\n",
      "Epoch [18660/20000], Training Loss: 0.004127323732453598, Validation Loss: 0.009000866341617971\n",
      "Epoch [18661/20000], Training Loss: 0.004329406395949523, Validation Loss: 0.013228483379634781\n",
      "Epoch [18662/20000], Training Loss: 0.006604451289332148, Validation Loss: 0.003209058218171184\n",
      "Epoch [18663/20000], Training Loss: 0.004847950846721817, Validation Loss: 0.0067995222553849455\n",
      "Epoch [18664/20000], Training Loss: 0.007508421990288687, Validation Loss: 0.02247398958674501\n",
      "Epoch [18665/20000], Training Loss: 0.009704098878760956, Validation Loss: 0.0064763912202327545\n",
      "Epoch [18666/20000], Training Loss: 0.005917301711243843, Validation Loss: 0.005142945702237532\n",
      "Epoch [18667/20000], Training Loss: 0.008525839183546071, Validation Loss: 0.012321803053574902\n",
      "Epoch [18668/20000], Training Loss: 0.011045619862540792, Validation Loss: 0.008169163950065747\n",
      "Epoch [18669/20000], Training Loss: 0.006579653236258309, Validation Loss: 0.0096890227205055\n",
      "Epoch [18670/20000], Training Loss: 0.011051850555369518, Validation Loss: 0.0046180369019534085\n",
      "Epoch [18671/20000], Training Loss: 0.008359788558014511, Validation Loss: 0.017110496442699383\n",
      "Epoch [18672/20000], Training Loss: 0.023406575789392394, Validation Loss: 0.08779923841068792\n",
      "Epoch [18673/20000], Training Loss: 0.05150735011297262, Validation Loss: 0.07988541536178673\n",
      "Epoch [18674/20000], Training Loss: 0.035429642026429065, Validation Loss: 0.017986346970698844\n",
      "Epoch [18675/20000], Training Loss: 0.01547590019513986, Validation Loss: 0.00825632133504379\n",
      "Epoch [18676/20000], Training Loss: 0.01440705921517552, Validation Loss: 0.013782741447566511\n",
      "Epoch [18677/20000], Training Loss: 0.009946075885506746, Validation Loss: 0.004716818812832051\n",
      "Epoch [18678/20000], Training Loss: 0.006153671347419731, Validation Loss: 0.0038094785597390262\n",
      "Epoch [18679/20000], Training Loss: 0.004658722676270243, Validation Loss: 0.00335385407732604\n",
      "Epoch [18680/20000], Training Loss: 0.004539686529564538, Validation Loss: 0.003521247482727371\n",
      "Epoch [18681/20000], Training Loss: 0.005631575406109083, Validation Loss: 0.00340810930185041\n",
      "Epoch [18682/20000], Training Loss: 0.004257872095747318, Validation Loss: 0.005003917255893141\n",
      "Epoch [18683/20000], Training Loss: 0.006270663993616056, Validation Loss: 0.00636259650990167\n",
      "Epoch [18684/20000], Training Loss: 0.005300322857302879, Validation Loss: 0.0033819661103731136\n",
      "Epoch [18685/20000], Training Loss: 0.007660837712007508, Validation Loss: 0.0028777999961359262\n",
      "Epoch [18686/20000], Training Loss: 0.004246446887139298, Validation Loss: 0.0031781848492644016\n",
      "Epoch [18687/20000], Training Loss: 0.006228832419895168, Validation Loss: 0.006924584678416628\n",
      "Epoch [18688/20000], Training Loss: 0.009038147353066961, Validation Loss: 0.008695841827651358\n",
      "Epoch [18689/20000], Training Loss: 0.007711597017727659, Validation Loss: 0.005582311477642258\n",
      "Epoch [18690/20000], Training Loss: 0.004300992054173288, Validation Loss: 0.01180143865557939\n",
      "Epoch [18691/20000], Training Loss: 0.009279744469111506, Validation Loss: 0.0035482471959572387\n",
      "Epoch [18692/20000], Training Loss: 0.003878171266362886, Validation Loss: 0.0031416248547779663\n",
      "Epoch [18693/20000], Training Loss: 0.0039949420456001205, Validation Loss: 0.004634842820302872\n",
      "Epoch [18694/20000], Training Loss: 0.0060912142811527, Validation Loss: 0.004127699827858509\n",
      "Epoch [18695/20000], Training Loss: 0.020977117756680336, Validation Loss: 0.06357179092745875\n",
      "Epoch [18696/20000], Training Loss: 0.029227858074055542, Validation Loss: 0.04291106388593237\n",
      "Epoch [18697/20000], Training Loss: 0.055050696535545285, Validation Loss: 0.014525452711950493\n",
      "Epoch [18698/20000], Training Loss: 0.0366047422804903, Validation Loss: 0.12763963347404886\n",
      "Epoch [18699/20000], Training Loss: 0.05090642112190835, Validation Loss: 0.007920808200827014\n",
      "Epoch [18700/20000], Training Loss: 0.013034807268663176, Validation Loss: 0.008824609096231315\n",
      "Epoch [18701/20000], Training Loss: 0.007523650461475232, Validation Loss: 0.00648762444872781\n",
      "Epoch [18702/20000], Training Loss: 0.007613139497493648, Validation Loss: 0.004977314153442032\n",
      "Epoch [18703/20000], Training Loss: 0.004696312402042427, Validation Loss: 0.003880417111776272\n",
      "Epoch [18704/20000], Training Loss: 0.004901275931583119, Validation Loss: 0.00590081283981395\n",
      "Epoch [18705/20000], Training Loss: 0.00417343232532273, Validation Loss: 0.003179404289962789\n",
      "Epoch [18706/20000], Training Loss: 0.004752661906033089, Validation Loss: 0.0028552347887567586\n",
      "Epoch [18707/20000], Training Loss: 0.003808867311038609, Validation Loss: 0.0027676380445277315\n",
      "Epoch [18708/20000], Training Loss: 0.005676420557912414, Validation Loss: 0.0028083616048980736\n",
      "Epoch [18709/20000], Training Loss: 0.0035286153574166485, Validation Loss: 0.0046129003231074025\n",
      "Epoch [18710/20000], Training Loss: 0.009118691295790475, Validation Loss: 0.002230932636516509\n",
      "Epoch [18711/20000], Training Loss: 0.024979753167177217, Validation Loss: 0.03193444040204828\n",
      "Epoch [18712/20000], Training Loss: 0.030165809818234784, Validation Loss: 0.01061992687032162\n",
      "Epoch [18713/20000], Training Loss: 0.021135626380003356, Validation Loss: 0.01164364220392855\n",
      "Epoch [18714/20000], Training Loss: 0.00810685558800677, Validation Loss: 0.008275505471090325\n",
      "Epoch [18715/20000], Training Loss: 0.00688487526791245, Validation Loss: 0.003995783671078763\n",
      "Epoch [18716/20000], Training Loss: 0.005895607247631622, Validation Loss: 0.008245972822863724\n",
      "Epoch [18717/20000], Training Loss: 0.004662260575193906, Validation Loss: 0.004031277652887054\n",
      "Epoch [18718/20000], Training Loss: 0.004504803551991898, Validation Loss: 0.0031953329269313207\n",
      "Epoch [18719/20000], Training Loss: 0.003479763760359284, Validation Loss: 0.0036992381251325568\n",
      "Epoch [18720/20000], Training Loss: 0.005632579826591869, Validation Loss: 0.002891818788853584\n",
      "Epoch [18721/20000], Training Loss: 0.005236858526820194, Validation Loss: 0.003948119848896976\n",
      "Epoch [18722/20000], Training Loss: 0.006087415166413328, Validation Loss: 0.003861545016505773\n",
      "Epoch [18723/20000], Training Loss: 0.0050752043644024525, Validation Loss: 0.0026546625195870937\n",
      "Epoch [18724/20000], Training Loss: 0.006145518861059835, Validation Loss: 0.02118233465713664\n",
      "Epoch [18725/20000], Training Loss: 0.010390167696121872, Validation Loss: 0.0039370753543522125\n",
      "Epoch [18726/20000], Training Loss: 0.01590604247758165, Validation Loss: 0.006751589360815755\n",
      "Epoch [18727/20000], Training Loss: 0.017991460089563458, Validation Loss: 0.014337862187393932\n",
      "Epoch [18728/20000], Training Loss: 0.013129569253578666, Validation Loss: 0.024591389610605696\n",
      "Epoch [18729/20000], Training Loss: 0.010219854070369041, Validation Loss: 0.009358080038736215\n",
      "Epoch [18730/20000], Training Loss: 0.015385886069322754, Validation Loss: 0.003964566639264129\n",
      "Epoch [18731/20000], Training Loss: 0.006845380678052087, Validation Loss: 0.003942467087211655\n",
      "Epoch [18732/20000], Training Loss: 0.00415812759414231, Validation Loss: 0.006657076952910552\n",
      "Epoch [18733/20000], Training Loss: 0.005042715170930023, Validation Loss: 0.005156589211919384\n",
      "Epoch [18734/20000], Training Loss: 0.006613763192685188, Validation Loss: 0.00299435456387306\n",
      "Epoch [18735/20000], Training Loss: 0.005086780857319744, Validation Loss: 0.0024935316703958827\n",
      "Epoch [18736/20000], Training Loss: 0.005394381696013235, Validation Loss: 0.0025809373590306123\n",
      "Epoch [18737/20000], Training Loss: 0.005027402272389736, Validation Loss: 0.0032405814233344037\n",
      "Epoch [18738/20000], Training Loss: 0.005058417082182132, Validation Loss: 0.005190810660111713\n",
      "Epoch [18739/20000], Training Loss: 0.005645917928884121, Validation Loss: 0.0028611475497036004\n",
      "Epoch [18740/20000], Training Loss: 0.0034600759891613314, Validation Loss: 0.004346975763481562\n",
      "Epoch [18741/20000], Training Loss: 0.004030811179420978, Validation Loss: 0.003383601442241765\n",
      "Epoch [18742/20000], Training Loss: 0.0060573484806809574, Validation Loss: 0.003963311532232743\n",
      "Epoch [18743/20000], Training Loss: 0.0058293890393607785, Validation Loss: 0.002616047663628623\n",
      "Epoch [18744/20000], Training Loss: 0.0038643638654320966, Validation Loss: 0.003113572172639741\n",
      "Epoch [18745/20000], Training Loss: 0.013581509502186253, Validation Loss: 0.01060584768685199\n",
      "Epoch [18746/20000], Training Loss: 0.010322717798771919, Validation Loss: 0.004493116768100189\n",
      "Epoch [18747/20000], Training Loss: 0.01029181118370356, Validation Loss: 0.007112376925087147\n",
      "Epoch [18748/20000], Training Loss: 0.01827547366617312, Validation Loss: 0.022912008421790385\n",
      "Epoch [18749/20000], Training Loss: 0.025750614015643287, Validation Loss: 0.028187296220252837\n",
      "Epoch [18750/20000], Training Loss: 0.012441591580552216, Validation Loss: 0.032583072515471594\n",
      "Epoch [18751/20000], Training Loss: 0.01705784719004961, Validation Loss: 0.00451655175079286\n",
      "Epoch [18752/20000], Training Loss: 0.016490355574725463, Validation Loss: 0.006114117488620277\n",
      "Epoch [18753/20000], Training Loss: 0.019044979703072125, Validation Loss: 0.004122254421837303\n",
      "Epoch [18754/20000], Training Loss: 0.0064385904171838804, Validation Loss: 0.007043588678413926\n",
      "Epoch [18755/20000], Training Loss: 0.00528836641103985, Validation Loss: 0.005506532409439673\n",
      "Epoch [18756/20000], Training Loss: 0.007603983675705551, Validation Loss: 0.006030415003654334\n",
      "Epoch [18757/20000], Training Loss: 0.008147137597136731, Validation Loss: 0.010847448921959466\n",
      "Epoch [18758/20000], Training Loss: 0.011333994900009461, Validation Loss: 0.004585451223854596\n",
      "Epoch [18759/20000], Training Loss: 0.005682227928973295, Validation Loss: 0.003300760213305054\n",
      "Epoch [18760/20000], Training Loss: 0.006361586431272112, Validation Loss: 0.006435573101841724\n",
      "Epoch [18761/20000], Training Loss: 0.004990479233258936, Validation Loss: 0.003056476692815074\n",
      "Epoch [18762/20000], Training Loss: 0.0064860034056307215, Validation Loss: 0.005051613737087346\n",
      "Epoch [18763/20000], Training Loss: 0.005295373767564472, Validation Loss: 0.00646007753131068\n",
      "Epoch [18764/20000], Training Loss: 0.008709221523791453, Validation Loss: 0.0027987471094362676\n",
      "Epoch [18765/20000], Training Loss: 0.005450226525811429, Validation Loss: 0.0032574795800296994\n",
      "Epoch [18766/20000], Training Loss: 0.00383296276517545, Validation Loss: 0.00312162679170519\n",
      "Epoch [18767/20000], Training Loss: 0.007319323357348496, Validation Loss: 0.007707978257887557\n",
      "Epoch [18768/20000], Training Loss: 0.01907419942264304, Validation Loss: 0.00947732673820058\n",
      "Epoch [18769/20000], Training Loss: 0.03560988797671273, Validation Loss: 0.021875538777521797\n",
      "Epoch [18770/20000], Training Loss: 0.018643089034022915, Validation Loss: 0.009438355156492304\n",
      "Epoch [18771/20000], Training Loss: 0.009911533071967174, Validation Loss: 0.0048483687698391365\n",
      "Epoch [18772/20000], Training Loss: 0.009589644015899726, Validation Loss: 0.003403007327648311\n",
      "Epoch [18773/20000], Training Loss: 0.003772184100983265, Validation Loss: 0.006772213735985133\n",
      "Epoch [18774/20000], Training Loss: 0.011181102293059146, Validation Loss: 0.003966497117813462\n",
      "Epoch [18775/20000], Training Loss: 0.005061524728911796, Validation Loss: 0.00705605902219567\n",
      "Epoch [18776/20000], Training Loss: 0.00583533103408345, Validation Loss: 0.007498098060100996\n",
      "Epoch [18777/20000], Training Loss: 0.010273594718358123, Validation Loss: 0.007344184671377921\n",
      "Epoch [18778/20000], Training Loss: 0.009216120114225694, Validation Loss: 0.00784014713861809\n",
      "Epoch [18779/20000], Training Loss: 0.011888761360361957, Validation Loss: 0.02597690533708424\n",
      "Epoch [18780/20000], Training Loss: 0.0472994242549508, Validation Loss: 0.01824377130132504\n",
      "Epoch [18781/20000], Training Loss: 0.025532085858035965, Validation Loss: 0.0136701036485455\n",
      "Epoch [18782/20000], Training Loss: 0.024237516856373986, Validation Loss: 0.038426801285303167\n",
      "Epoch [18783/20000], Training Loss: 0.012999017838670366, Validation Loss: 0.0138215278165876\n",
      "Epoch [18784/20000], Training Loss: 0.006782271577061952, Validation Loss: 0.007212748185598424\n",
      "Epoch [18785/20000], Training Loss: 0.007883507317663836, Validation Loss: 0.006009965307693932\n",
      "Epoch [18786/20000], Training Loss: 0.009924480957644326, Validation Loss: 0.02679667515548917\n",
      "Epoch [18787/20000], Training Loss: 0.014231599212507717, Validation Loss: 0.006104153575253386\n",
      "Epoch [18788/20000], Training Loss: 0.00577678663207085, Validation Loss: 0.003353518811621825\n",
      "Epoch [18789/20000], Training Loss: 0.005369480830268003, Validation Loss: 0.003313497806463569\n",
      "Epoch [18790/20000], Training Loss: 0.005621980459961508, Validation Loss: 0.005029530950553506\n",
      "Epoch [18791/20000], Training Loss: 0.008199236191493193, Validation Loss: 0.007332253489906057\n",
      "Epoch [18792/20000], Training Loss: 0.004180205177233022, Validation Loss: 0.007105261404051687\n",
      "Epoch [18793/20000], Training Loss: 0.006450468794875113, Validation Loss: 0.005930416260388657\n",
      "Epoch [18794/20000], Training Loss: 0.008114463955020515, Validation Loss: 0.003725022085432731\n",
      "Epoch [18795/20000], Training Loss: 0.0069370920963202866, Validation Loss: 0.005152492007969324\n",
      "Epoch [18796/20000], Training Loss: 0.009226011084267936, Validation Loss: 0.003300678016690333\n",
      "Epoch [18797/20000], Training Loss: 0.011994130265000942, Validation Loss: 0.0042772625913247075\n",
      "Epoch [18798/20000], Training Loss: 0.008269580739059685, Validation Loss: 0.003207443642830908\n",
      "Epoch [18799/20000], Training Loss: 0.012279063825969518, Validation Loss: 0.005255692983889203\n",
      "Epoch [18800/20000], Training Loss: 0.016819331549673473, Validation Loss: 0.03074481976883753\n",
      "Epoch [18801/20000], Training Loss: 0.018674614321623397, Validation Loss: 0.021207923868803998\n",
      "Epoch [18802/20000], Training Loss: 0.010085740122370757, Validation Loss: 0.006676868016901735\n",
      "Epoch [18803/20000], Training Loss: 0.009988005203402801, Validation Loss: 0.006923485781201911\n",
      "Epoch [18804/20000], Training Loss: 0.007478376463820625, Validation Loss: 0.004155203726312173\n",
      "Epoch [18805/20000], Training Loss: 0.005190271244381555, Validation Loss: 0.0031473919818727175\n",
      "Epoch [18806/20000], Training Loss: 0.006152374535304261, Validation Loss: 0.0032057112544344895\n",
      "Epoch [18807/20000], Training Loss: 0.006170738546010008, Validation Loss: 0.003299456731351569\n",
      "Epoch [18808/20000], Training Loss: 0.005213037345259052, Validation Loss: 0.004845141232817715\n",
      "Epoch [18809/20000], Training Loss: 0.005469973506738565, Validation Loss: 0.006863805939924454\n",
      "Epoch [18810/20000], Training Loss: 0.007730598249638986, Validation Loss: 0.0032251282784697527\n",
      "Epoch [18811/20000], Training Loss: 0.005612280013697354, Validation Loss: 0.0038609865911861823\n",
      "Epoch [18812/20000], Training Loss: 0.004188310754346146, Validation Loss: 0.004505224876027445\n",
      "Epoch [18813/20000], Training Loss: 0.004264848899245928, Validation Loss: 0.0025476212679244525\n",
      "Epoch [18814/20000], Training Loss: 0.004262404345614154, Validation Loss: 0.007096366959639446\n",
      "Epoch [18815/20000], Training Loss: 0.005945283956472329, Validation Loss: 0.002453961859540499\n",
      "Epoch [18816/20000], Training Loss: 0.004020584342340173, Validation Loss: 0.0062190970863315875\n",
      "Epoch [18817/20000], Training Loss: 0.01117303121970638, Validation Loss: 0.0042472450271585005\n",
      "Epoch [18818/20000], Training Loss: 0.015740446610184984, Validation Loss: 0.005806782317170737\n",
      "Epoch [18819/20000], Training Loss: 0.033174729292243556, Validation Loss: 0.02510743455124092\n",
      "Epoch [18820/20000], Training Loss: 0.023790036170144697, Validation Loss: 0.008626025359100302\n",
      "Epoch [18821/20000], Training Loss: 0.03266570145803728, Validation Loss: 0.009963011103153332\n",
      "Epoch [18822/20000], Training Loss: 0.03214169378874691, Validation Loss: 0.006651626585167587\n",
      "Epoch [18823/20000], Training Loss: 0.01273681377642788, Validation Loss: 0.004714456645386024\n",
      "Epoch [18824/20000], Training Loss: 0.004759922861142384, Validation Loss: 0.0066576785108349535\n",
      "Epoch [18825/20000], Training Loss: 0.009466211490299819, Validation Loss: 0.005844920282811472\n",
      "Epoch [18826/20000], Training Loss: 0.008745403902139515, Validation Loss: 0.0046262406365583176\n",
      "Epoch [18827/20000], Training Loss: 0.00972697558505128, Validation Loss: 0.003577954956936326\n",
      "Epoch [18828/20000], Training Loss: 0.01386431875839987, Validation Loss: 0.008034142557245131\n",
      "Epoch [18829/20000], Training Loss: 0.008861477697791997, Validation Loss: 0.015156066852079878\n",
      "Epoch [18830/20000], Training Loss: 0.01157531732418907, Validation Loss: 0.03980636327592687\n",
      "Epoch [18831/20000], Training Loss: 0.016647729128765474, Validation Loss: 0.007066094817296127\n",
      "Epoch [18832/20000], Training Loss: 0.007490396120260812, Validation Loss: 0.004969096353635548\n",
      "Epoch [18833/20000], Training Loss: 0.0049057210604327595, Validation Loss: 0.004531706574644951\n",
      "Epoch [18834/20000], Training Loss: 0.005018225139273065, Validation Loss: 0.0035857591208354428\n",
      "Epoch [18835/20000], Training Loss: 0.006145331759138831, Validation Loss: 0.007986537474201734\n",
      "Epoch [18836/20000], Training Loss: 0.01085259633133059, Validation Loss: 0.003910172202941098\n",
      "Epoch [18837/20000], Training Loss: 0.00997547798864876, Validation Loss: 0.01611044949722621\n",
      "Epoch [18838/20000], Training Loss: 0.007778994464388234, Validation Loss: 0.003692454959536442\n",
      "Epoch [18839/20000], Training Loss: 0.006922500028200115, Validation Loss: 0.006147383955911729\n",
      "Epoch [18840/20000], Training Loss: 0.004375715115519897, Validation Loss: 0.0028771991977605904\n",
      "Epoch [18841/20000], Training Loss: 0.003958730648654247, Validation Loss: 0.003297951804359965\n",
      "Epoch [18842/20000], Training Loss: 0.003389553786421727, Validation Loss: 0.004089139670615901\n",
      "Epoch [18843/20000], Training Loss: 0.003379623391521948, Validation Loss: 0.003176573950835549\n",
      "Epoch [18844/20000], Training Loss: 0.009311351059295703, Validation Loss: 0.003299268996709007\n",
      "Epoch [18845/20000], Training Loss: 0.007894856862776578, Validation Loss: 0.024744100602609786\n",
      "Epoch [18846/20000], Training Loss: 0.012629375054789957, Validation Loss: 0.015477793357734191\n",
      "Epoch [18847/20000], Training Loss: 0.01740111127040375, Validation Loss: 0.00311118115892306\n",
      "Epoch [18848/20000], Training Loss: 0.0073717398883153306, Validation Loss: 0.0061866703931917144\n",
      "Epoch [18849/20000], Training Loss: 0.009698759085430149, Validation Loss: 0.00830890963534817\n",
      "Epoch [18850/20000], Training Loss: 0.007391673741681514, Validation Loss: 0.006726665655198906\n",
      "Epoch [18851/20000], Training Loss: 0.003999778544052138, Validation Loss: 0.007177033761130198\n",
      "Epoch [18852/20000], Training Loss: 0.007425681546530021, Validation Loss: 0.004150920864122119\n",
      "Epoch [18853/20000], Training Loss: 0.003982278950778502, Validation Loss: 0.01372388790228655\n",
      "Epoch [18854/20000], Training Loss: 0.00973409370583665, Validation Loss: 0.002578245402111728\n",
      "Epoch [18855/20000], Training Loss: 0.011506601806169263, Validation Loss: 0.004323581303383252\n",
      "Epoch [18856/20000], Training Loss: 0.005341087804741359, Validation Loss: 0.0027679903713440823\n",
      "Epoch [18857/20000], Training Loss: 0.003745702620238132, Validation Loss: 0.0025800947077309145\n",
      "Epoch [18858/20000], Training Loss: 0.004223582583238438, Validation Loss: 0.008225289239966524\n",
      "Epoch [18859/20000], Training Loss: 0.004911425639355522, Validation Loss: 0.00310209293674358\n",
      "Epoch [18860/20000], Training Loss: 0.0025423116213849945, Validation Loss: 0.0032180840594580496\n",
      "Epoch [18861/20000], Training Loss: 0.007192183870201089, Validation Loss: 0.010648332203606291\n",
      "Epoch [18862/20000], Training Loss: 0.02595550948964436, Validation Loss: 0.004759144954927511\n",
      "Epoch [18863/20000], Training Loss: 0.008803942508035638, Validation Loss: 0.007108879410611579\n",
      "Epoch [18864/20000], Training Loss: 0.044103950894558305, Validation Loss: 0.002498486322133025\n",
      "Epoch [18865/20000], Training Loss: 0.15231958580885216, Validation Loss: 0.11252363910117795\n",
      "Epoch [18866/20000], Training Loss: 0.07644309404921452, Validation Loss: 0.050100667180590364\n",
      "Epoch [18867/20000], Training Loss: 0.0494752466079912, Validation Loss: 0.01606151238172539\n",
      "Epoch [18868/20000], Training Loss: 0.024334214561219727, Validation Loss: 0.021304774463975003\n",
      "Epoch [18869/20000], Training Loss: 0.020350275112182965, Validation Loss: 0.011420104401540954\n",
      "Epoch [18870/20000], Training Loss: 0.0098850139849154, Validation Loss: 0.010888487714388106\n",
      "Epoch [18871/20000], Training Loss: 0.008999789517214854, Validation Loss: 0.010162968288458616\n",
      "Epoch [18872/20000], Training Loss: 0.008237026848032005, Validation Loss: 0.011283863695959283\n",
      "Epoch [18873/20000], Training Loss: 0.010071636061184108, Validation Loss: 0.006463518166649291\n",
      "Epoch [18874/20000], Training Loss: 0.006293230350170883, Validation Loss: 0.004479280040998649\n",
      "Epoch [18875/20000], Training Loss: 0.0025646374124335125, Validation Loss: 0.030586311726697853\n",
      "Epoch [18876/20000], Training Loss: 0.021881770957926556, Validation Loss: 0.011253659356124943\n",
      "Epoch [18877/20000], Training Loss: 0.00969307853457784, Validation Loss: 0.007596436771044475\n",
      "Epoch [18878/20000], Training Loss: 0.008516377604662142, Validation Loss: 0.0055918746781701755\n",
      "Epoch [18879/20000], Training Loss: 0.0060523798061434975, Validation Loss: 0.006220451718036202\n",
      "Epoch [18880/20000], Training Loss: 0.006107763262532119, Validation Loss: 0.0050263721113879\n",
      "Epoch [18881/20000], Training Loss: 0.006629124238055998, Validation Loss: 0.0050138835106606295\n",
      "Epoch [18882/20000], Training Loss: 0.006623115297916878, Validation Loss: 0.005272402908043919\n",
      "Epoch [18883/20000], Training Loss: 0.00649521410793698, Validation Loss: 0.004571687273840196\n",
      "Epoch [18884/20000], Training Loss: 0.004767048160374543, Validation Loss: 0.004095313817742863\n",
      "Epoch [18885/20000], Training Loss: 0.004987179122898462, Validation Loss: 0.004339242118995961\n",
      "Epoch [18886/20000], Training Loss: 0.004720233326744554, Validation Loss: 0.005206312610815595\n",
      "Epoch [18887/20000], Training Loss: 0.005496619845091898, Validation Loss: 0.004851666979967766\n",
      "Epoch [18888/20000], Training Loss: 0.006658299631193846, Validation Loss: 0.0031353415860004524\n",
      "Epoch [18889/20000], Training Loss: 0.003796555962513334, Validation Loss: 0.00296582103387014\n",
      "Epoch [18890/20000], Training Loss: 0.005715668197288843, Validation Loss: 0.003510798824988472\n",
      "Epoch [18891/20000], Training Loss: 0.008669430718457858, Validation Loss: 0.004155351660510779\n",
      "Epoch [18892/20000], Training Loss: 0.007705485690647456, Validation Loss: 0.013966917767893605\n",
      "Epoch [18893/20000], Training Loss: 0.009373417727017243, Validation Loss: 0.0038227614344188104\n",
      "Epoch [18894/20000], Training Loss: 0.005579329890939074, Validation Loss: 0.002716722775825734\n",
      "Epoch [18895/20000], Training Loss: 0.004512188597540704, Validation Loss: 0.006337271838135912\n",
      "Epoch [18896/20000], Training Loss: 0.0075415220186820465, Validation Loss: 0.003566266329065521\n",
      "Epoch [18897/20000], Training Loss: 0.008337938463747767, Validation Loss: 0.0037977041974954773\n",
      "Epoch [18898/20000], Training Loss: 0.003778739888470487, Validation Loss: 0.003109608798370443\n",
      "Epoch [18899/20000], Training Loss: 0.004766947787873376, Validation Loss: 0.004126581220939103\n",
      "Epoch [18900/20000], Training Loss: 0.005729802914304726, Validation Loss: 0.0025064654149998334\n",
      "Epoch [18901/20000], Training Loss: 0.004494167288482588, Validation Loss: 0.003220321624569676\n",
      "Epoch [18902/20000], Training Loss: 0.01170955622053173, Validation Loss: 0.005740773625463483\n",
      "Epoch [18903/20000], Training Loss: 0.008300014734361736, Validation Loss: 0.006538342743160357\n",
      "Epoch [18904/20000], Training Loss: 0.004734502666191734, Validation Loss: 0.006776769718401608\n",
      "Epoch [18905/20000], Training Loss: 0.003745869201728575, Validation Loss: 0.003277832010614284\n",
      "Epoch [18906/20000], Training Loss: 0.0038697764196383233, Validation Loss: 0.00251243892527445\n",
      "Epoch [18907/20000], Training Loss: 0.013342429880659827, Validation Loss: 0.004483693712598195\n",
      "Epoch [18908/20000], Training Loss: 0.034684861670289786, Validation Loss: 0.009433689670828986\n",
      "Epoch [18909/20000], Training Loss: 0.03786631496408128, Validation Loss: 0.04579497221857309\n",
      "Epoch [18910/20000], Training Loss: 0.05112491702727441, Validation Loss: 0.012992601886838071\n",
      "Epoch [18911/20000], Training Loss: 0.017792962957173586, Validation Loss: 0.011025945154317793\n",
      "Epoch [18912/20000], Training Loss: 0.01278656303786972, Validation Loss: 0.00691982792949984\n",
      "Epoch [18913/20000], Training Loss: 0.007577412071571287, Validation Loss: 0.008325056013656296\n",
      "Epoch [18914/20000], Training Loss: 0.008286695418064483, Validation Loss: 0.007645411839348526\n",
      "Epoch [18915/20000], Training Loss: 0.006123038476549222, Validation Loss: 0.0061703571689378\n",
      "Epoch [18916/20000], Training Loss: 0.005502395057452044, Validation Loss: 0.003850986028445342\n",
      "Epoch [18917/20000], Training Loss: 0.004966607508582196, Validation Loss: 0.004643358587242931\n",
      "Epoch [18918/20000], Training Loss: 0.005354177104891278, Validation Loss: 0.004547370331269158\n",
      "Epoch [18919/20000], Training Loss: 0.006755141081937056, Validation Loss: 0.008445509062647794\n",
      "Epoch [18920/20000], Training Loss: 0.0048520175207938466, Validation Loss: 0.00625875446261099\n",
      "Epoch [18921/20000], Training Loss: 0.009391760349119847, Validation Loss: 0.0037262049802053404\n",
      "Epoch [18922/20000], Training Loss: 0.007664454388889551, Validation Loss: 0.0040966230792173364\n",
      "Epoch [18923/20000], Training Loss: 0.007317684459849261, Validation Loss: 0.011760196116638453\n",
      "Epoch [18924/20000], Training Loss: 0.011030928976002283, Validation Loss: 0.004558555901982889\n",
      "Epoch [18925/20000], Training Loss: 0.004457317638817975, Validation Loss: 0.0054063361644856355\n",
      "Epoch [18926/20000], Training Loss: 0.004530375771407437, Validation Loss: 0.004881564912664679\n",
      "Epoch [18927/20000], Training Loss: 0.006512019068655458, Validation Loss: 0.003220246171200155\n",
      "Epoch [18928/20000], Training Loss: 0.008714158561945493, Validation Loss: 0.007361497864188235\n",
      "Epoch [18929/20000], Training Loss: 0.00824427318652202, Validation Loss: 0.01203941164673261\n",
      "Epoch [18930/20000], Training Loss: 0.009329932099067914, Validation Loss: 0.0034148135040165278\n",
      "Epoch [18931/20000], Training Loss: 0.00561162006592245, Validation Loss: 0.005150707797586725\n",
      "Epoch [18932/20000], Training Loss: 0.006108316276887698, Validation Loss: 0.005420805753585357\n",
      "Epoch [18933/20000], Training Loss: 0.003982770812789178, Validation Loss: 0.0040431902113308394\n",
      "Epoch [18934/20000], Training Loss: 0.007561611595304998, Validation Loss: 0.0045251109268717405\n",
      "Epoch [18935/20000], Training Loss: 0.00853055692433762, Validation Loss: 0.01613994008188716\n",
      "Epoch [18936/20000], Training Loss: 0.017430902070243195, Validation Loss: 0.003615349950127459\n",
      "Epoch [18937/20000], Training Loss: 0.016701803791355423, Validation Loss: 0.02367856340216739\n",
      "Epoch [18938/20000], Training Loss: 0.011662166243955394, Validation Loss: 0.008952512473168599\n",
      "Epoch [18939/20000], Training Loss: 0.008823045909853786, Validation Loss: 0.00556712141191643\n",
      "Epoch [18940/20000], Training Loss: 0.006028082337122344, Validation Loss: 0.0030191357165412847\n",
      "Epoch [18941/20000], Training Loss: 0.004552122488218758, Validation Loss: 0.0036474524190014163\n",
      "Epoch [18942/20000], Training Loss: 0.0044189786713104695, Validation Loss: 0.005906105065215732\n",
      "Epoch [18943/20000], Training Loss: 0.0032982676894854584, Validation Loss: 0.012815990884425922\n",
      "Epoch [18944/20000], Training Loss: 0.005240816364156282, Validation Loss: 0.008443777801127388\n",
      "Epoch [18945/20000], Training Loss: 0.015180923059233464, Validation Loss: 0.011777020982143378\n",
      "Epoch [18946/20000], Training Loss: 0.007437609066463275, Validation Loss: 0.002591899717409823\n",
      "Epoch [18947/20000], Training Loss: 0.0069412661999064896, Validation Loss: 0.0029193026260190586\n",
      "Epoch [18948/20000], Training Loss: 0.004614945904385033, Validation Loss: 0.009313221289117306\n",
      "Epoch [18949/20000], Training Loss: 0.006966476514727609, Validation Loss: 0.008904161247691238\n",
      "Epoch [18950/20000], Training Loss: 0.010529948938126057, Validation Loss: 0.005672016952205793\n",
      "Epoch [18951/20000], Training Loss: 0.005471283935029143, Validation Loss: 0.0048677415825458025\n",
      "Epoch [18952/20000], Training Loss: 0.020532778570278815, Validation Loss: 0.005956083972509909\n",
      "Epoch [18953/20000], Training Loss: 0.017083142306838845, Validation Loss: 0.005826612993964935\n",
      "Epoch [18954/20000], Training Loss: 0.009876728735564808, Validation Loss: 0.0033249429031770505\n",
      "Epoch [18955/20000], Training Loss: 0.005253951439954108, Validation Loss: 0.029758865811995442\n",
      "Epoch [18956/20000], Training Loss: 0.01316631690964901, Validation Loss: 0.01893410991413347\n",
      "Epoch [18957/20000], Training Loss: 0.010947530896373792, Validation Loss: 0.021107330918356527\n",
      "Epoch [18958/20000], Training Loss: 0.007965453191510668, Validation Loss: 0.0059175518626292545\n",
      "Epoch [18959/20000], Training Loss: 0.011763271687544017, Validation Loss: 0.007589199061385443\n",
      "Epoch [18960/20000], Training Loss: 0.037900465991177565, Validation Loss: 0.024341487767352583\n",
      "Epoch [18961/20000], Training Loss: 0.024807393677682348, Validation Loss: 0.003954138476687622\n",
      "Epoch [18962/20000], Training Loss: 0.0067925012865868795, Validation Loss: 0.009361355049934817\n",
      "Epoch [18963/20000], Training Loss: 0.006488801390722594, Validation Loss: 0.006109950626880943\n",
      "Epoch [18964/20000], Training Loss: 0.007833874999960244, Validation Loss: 0.0074842632083705965\n",
      "Epoch [18965/20000], Training Loss: 0.011271901431817761, Validation Loss: 0.006968575928918618\n",
      "Epoch [18966/20000], Training Loss: 0.023929931068258675, Validation Loss: 0.0605347634118516\n",
      "Epoch [18967/20000], Training Loss: 0.020120266798910182, Validation Loss: 0.032564754730888774\n",
      "Epoch [18968/20000], Training Loss: 0.007657120507376801, Validation Loss: 0.007448021037167304\n",
      "Epoch [18969/20000], Training Loss: 0.011755820434440725, Validation Loss: 0.003057857018104317\n",
      "Epoch [18970/20000], Training Loss: 0.01651322828703477, Validation Loss: 0.0037798733187628386\n",
      "Epoch [18971/20000], Training Loss: 0.0046803796410261255, Validation Loss: 0.004582450721720337\n",
      "Epoch [18972/20000], Training Loss: 0.004471043418610472, Validation Loss: 0.003156920616664901\n",
      "Epoch [18973/20000], Training Loss: 0.00550881979454841, Validation Loss: 0.00413344630273736\n",
      "Epoch [18974/20000], Training Loss: 0.009333875117590651, Validation Loss: 0.004576263393200212\n",
      "Epoch [18975/20000], Training Loss: 0.013225021209499184, Validation Loss: 0.0060110642792169855\n",
      "Epoch [18976/20000], Training Loss: 0.009727189845372257, Validation Loss: 0.004740700931101476\n",
      "Epoch [18977/20000], Training Loss: 0.004780448262213862, Validation Loss: 0.0091859576316987\n",
      "Epoch [18978/20000], Training Loss: 0.014872662904443652, Validation Loss: 0.005644757504219897\n",
      "Epoch [18979/20000], Training Loss: 0.01021027154222663, Validation Loss: 0.0045482398897399535\n",
      "Epoch [18980/20000], Training Loss: 0.0054097630220764715, Validation Loss: 0.0036004270665696845\n",
      "Epoch [18981/20000], Training Loss: 0.005112536552835374, Validation Loss: 0.0030981255379306356\n",
      "Epoch [18982/20000], Training Loss: 0.0036056256949359117, Validation Loss: 0.0049929853669169804\n",
      "Epoch [18983/20000], Training Loss: 0.008118545106429207, Validation Loss: 0.0029852456252995713\n",
      "Epoch [18984/20000], Training Loss: 0.011790798580067628, Validation Loss: 0.010218832321190316\n",
      "Epoch [18985/20000], Training Loss: 0.009314127700885624, Validation Loss: 0.0028142829989591866\n",
      "Epoch [18986/20000], Training Loss: 0.006631327410494643, Validation Loss: 0.0036173163534709693\n",
      "Epoch [18987/20000], Training Loss: 0.005452214209071826, Validation Loss: 0.006870642570523054\n",
      "Epoch [18988/20000], Training Loss: 0.005612409004243091, Validation Loss: 0.0029819275697877045\n",
      "Epoch [18989/20000], Training Loss: 0.00470913466831137, Validation Loss: 0.0032786243435385296\n",
      "Epoch [18990/20000], Training Loss: 0.004684729198093659, Validation Loss: 0.002527180006057208\n",
      "Epoch [18991/20000], Training Loss: 0.005199643689723287, Validation Loss: 0.002961646203653499\n",
      "Epoch [18992/20000], Training Loss: 0.005979872162112899, Validation Loss: 0.00302320790536175\n",
      "Epoch [18993/20000], Training Loss: 0.008141447900145846, Validation Loss: 0.006152481510382997\n",
      "Epoch [18994/20000], Training Loss: 0.007875549028410336, Validation Loss: 0.013759077659675056\n",
      "Epoch [18995/20000], Training Loss: 0.010610157764209103, Validation Loss: 0.0037612931059344745\n",
      "Epoch [18996/20000], Training Loss: 0.010909590837919885, Validation Loss: 0.006381475781742625\n",
      "Epoch [18997/20000], Training Loss: 0.014922106135080477, Validation Loss: 0.010617233280670471\n",
      "Epoch [18998/20000], Training Loss: 0.020575224214033887, Validation Loss: 0.0046821246758396145\n",
      "Epoch [18999/20000], Training Loss: 0.03851127722009551, Validation Loss: 0.004822078145141027\n",
      "Epoch [19000/20000], Training Loss: 0.02107047859577246, Validation Loss: 0.005682384997273031\n",
      "Epoch [19001/20000], Training Loss: 0.009083591708206638, Validation Loss: 0.0064498475974801295\n",
      "Epoch [19002/20000], Training Loss: 0.004601118994157462, Validation Loss: 0.003941296078435487\n",
      "Epoch [19003/20000], Training Loss: 0.011120749399456795, Validation Loss: 0.004408233139786587\n",
      "Epoch [19004/20000], Training Loss: 0.014488093941638778, Validation Loss: 0.006687163774937623\n",
      "Epoch [19005/20000], Training Loss: 0.0064020507119754, Validation Loss: 0.003815840986362673\n",
      "Epoch [19006/20000], Training Loss: 0.00466075316424914, Validation Loss: 0.0031617018233553213\n",
      "Epoch [19007/20000], Training Loss: 0.004166606352165607, Validation Loss: 0.005773117898962178\n",
      "Epoch [19008/20000], Training Loss: 0.006113614858415011, Validation Loss: 0.003443089649940794\n",
      "Epoch [19009/20000], Training Loss: 0.003927857725995766, Validation Loss: 0.003241686449621548\n",
      "Epoch [19010/20000], Training Loss: 0.004465372224980716, Validation Loss: 0.004387345897284358\n",
      "Epoch [19011/20000], Training Loss: 0.0035736548099001603, Validation Loss: 0.0026051777810408056\n",
      "Epoch [19012/20000], Training Loss: 0.004135605370850369, Validation Loss: 0.0035046858585725317\n",
      "Epoch [19013/20000], Training Loss: 0.003980493483790529, Validation Loss: 0.004741889557689159\n",
      "Epoch [19014/20000], Training Loss: 0.006236998224267154, Validation Loss: 0.0033238847151574806\n",
      "Epoch [19015/20000], Training Loss: 0.0074109169883221125, Validation Loss: 0.0024827202077198113\n",
      "Epoch [19016/20000], Training Loss: 0.006156997018868944, Validation Loss: 0.00334304441538047\n",
      "Epoch [19017/20000], Training Loss: 0.005884639839418274, Validation Loss: 0.0025992789251898366\n",
      "Epoch [19018/20000], Training Loss: 0.00571334088454023, Validation Loss: 0.0032361715935752705\n",
      "Epoch [19019/20000], Training Loss: 0.004429592031686168, Validation Loss: 0.00843846678201816\n",
      "Epoch [19020/20000], Training Loss: 0.004192917484293243, Validation Loss: 0.0029553894670990505\n",
      "Epoch [19021/20000], Training Loss: 0.005287393409941744, Validation Loss: 0.003498776806235528\n",
      "Epoch [19022/20000], Training Loss: 0.01900115648520893, Validation Loss: 0.004366081457461279\n",
      "Epoch [19023/20000], Training Loss: 0.004521814564733566, Validation Loss: 0.008579273042934448\n",
      "Epoch [19024/20000], Training Loss: 0.007425924135272258, Validation Loss: 0.0028359804483826167\n",
      "Epoch [19025/20000], Training Loss: 0.01084165319999946, Validation Loss: 0.0027496891979602507\n",
      "Epoch [19026/20000], Training Loss: 0.008171194138152973, Validation Loss: 0.003955179900709219\n",
      "Epoch [19027/20000], Training Loss: 0.005932250719134962, Validation Loss: 0.0035107906151414865\n",
      "Epoch [19028/20000], Training Loss: 0.005707329987801911, Validation Loss: 0.002475935496925078\n",
      "Epoch [19029/20000], Training Loss: 0.006284998452299208, Validation Loss: 0.007088788319379383\n",
      "Epoch [19030/20000], Training Loss: 0.006499831667627275, Validation Loss: 0.002923627947306178\n",
      "Epoch [19031/20000], Training Loss: 0.004429782692958335, Validation Loss: 0.0025726116978413345\n",
      "Epoch [19032/20000], Training Loss: 0.00523885883116496, Validation Loss: 0.005763627222872413\n",
      "Epoch [19033/20000], Training Loss: 0.00590941857418719, Validation Loss: 0.003052945099384987\n",
      "Epoch [19034/20000], Training Loss: 0.004407688764331397, Validation Loss: 0.006470918206364993\n",
      "Epoch [19035/20000], Training Loss: 0.007682643326656294, Validation Loss: 0.0030850525779150743\n",
      "Epoch [19036/20000], Training Loss: 0.010127352308960067, Validation Loss: 0.006982338843951362\n",
      "Epoch [19037/20000], Training Loss: 0.009358129540487425, Validation Loss: 0.0076400911453909995\n",
      "Epoch [19038/20000], Training Loss: 0.011693305119739048, Validation Loss: 0.015271801235420364\n",
      "Epoch [19039/20000], Training Loss: 0.015663281820155004, Validation Loss: 0.00542049879939962\n",
      "Epoch [19040/20000], Training Loss: 0.019432741837850438, Validation Loss: 0.06484299495688486\n",
      "Epoch [19041/20000], Training Loss: 0.035195172304156586, Validation Loss: 0.015135892068168946\n",
      "Epoch [19042/20000], Training Loss: 0.049714525998881855, Validation Loss: 0.030673098605932734\n",
      "Epoch [19043/20000], Training Loss: 0.013721306660694868, Validation Loss: 0.01958882968340601\n",
      "Epoch [19044/20000], Training Loss: 0.01472007788834162, Validation Loss: 0.005846683096491181\n",
      "Epoch [19045/20000], Training Loss: 0.007975978431724278, Validation Loss: 0.0045203762099753474\n",
      "Epoch [19046/20000], Training Loss: 0.005681611919758974, Validation Loss: 0.004479469796433421\n",
      "Epoch [19047/20000], Training Loss: 0.006671826771448001, Validation Loss: 0.00838775041200207\n",
      "Epoch [19048/20000], Training Loss: 0.010735833125571454, Validation Loss: 0.0029846800942694507\n",
      "Epoch [19049/20000], Training Loss: 0.0040787955721108505, Validation Loss: 0.004632604108894349\n",
      "Epoch [19050/20000], Training Loss: 0.0056784378622458985, Validation Loss: 0.011637085250445784\n",
      "Epoch [19051/20000], Training Loss: 0.006613766178588776, Validation Loss: 0.010460851404007241\n",
      "Epoch [19052/20000], Training Loss: 0.006013440471308838, Validation Loss: 0.0035389318949649784\n",
      "Epoch [19053/20000], Training Loss: 0.005910795901270051, Validation Loss: 0.0027401794413765873\n",
      "Epoch [19054/20000], Training Loss: 0.007759075971469949, Validation Loss: 0.006981321344418215\n",
      "Epoch [19055/20000], Training Loss: 0.006324542759102769, Validation Loss: 0.003482697436163632\n",
      "Epoch [19056/20000], Training Loss: 0.0044714884627735174, Validation Loss: 0.007444795753275182\n",
      "Epoch [19057/20000], Training Loss: 0.005405815658117977, Validation Loss: 0.0032277243278559862\n",
      "Epoch [19058/20000], Training Loss: 0.004810408724422034, Validation Loss: 0.003761075643769945\n",
      "Epoch [19059/20000], Training Loss: 0.007068581960733614, Validation Loss: 0.006142210780775981\n",
      "Epoch [19060/20000], Training Loss: 0.005146831181004278, Validation Loss: 0.0032997592892849382\n",
      "Epoch [19061/20000], Training Loss: 0.006625873072022971, Validation Loss: 0.0025101878107418457\n",
      "Epoch [19062/20000], Training Loss: 0.0037299745543870294, Validation Loss: 0.0030944821258368166\n",
      "Epoch [19063/20000], Training Loss: 0.005518407509433538, Validation Loss: 0.0027189996026052176\n",
      "Epoch [19064/20000], Training Loss: 0.005349247480913802, Validation Loss: 0.0049064555640657825\n",
      "Epoch [19065/20000], Training Loss: 0.00408602259111441, Validation Loss: 0.0056076591634856895\n",
      "Epoch [19066/20000], Training Loss: 0.006637250120028122, Validation Loss: 0.004624630590634869\n",
      "Epoch [19067/20000], Training Loss: 0.00480755653243741, Validation Loss: 0.004582278742859864\n",
      "Epoch [19068/20000], Training Loss: 0.003413883224961215, Validation Loss: 0.003053104322009826\n",
      "Epoch [19069/20000], Training Loss: 0.005902703890959466, Validation Loss: 0.008628298761323096\n",
      "Epoch [19070/20000], Training Loss: 0.009586679583922628, Validation Loss: 0.005748201194884536\n",
      "Epoch [19071/20000], Training Loss: 0.008518512308780504, Validation Loss: 0.003160347461495163\n",
      "Epoch [19072/20000], Training Loss: 0.0042539114902064535, Validation Loss: 0.0025359529370030873\n",
      "Epoch [19073/20000], Training Loss: 0.004545696129623268, Validation Loss: 0.0029651419109936908\n",
      "Epoch [19074/20000], Training Loss: 0.011839085728362468, Validation Loss: 0.012794429989511968\n",
      "Epoch [19075/20000], Training Loss: 0.013509468819523005, Validation Loss: 0.012813653852670193\n",
      "Epoch [19076/20000], Training Loss: 0.01391244464736831, Validation Loss: 0.005347679967931547\n",
      "Epoch [19077/20000], Training Loss: 0.003580638839740199, Validation Loss: 0.004857903679034564\n",
      "Epoch [19078/20000], Training Loss: 0.004570532131245792, Validation Loss: 0.0037868876270348403\n",
      "Epoch [19079/20000], Training Loss: 0.00409649746989414, Validation Loss: 0.023138896163020815\n",
      "Epoch [19080/20000], Training Loss: 0.013086258078536568, Validation Loss: 0.0029739299352612983\n",
      "Epoch [19081/20000], Training Loss: 0.007403640406534707, Validation Loss: 0.0032761895946082876\n",
      "Epoch [19082/20000], Training Loss: 0.008205814079181957, Validation Loss: 0.025306616510663713\n",
      "Epoch [19083/20000], Training Loss: 0.016152200523980094, Validation Loss: 0.0226444113733513\n",
      "Epoch [19084/20000], Training Loss: 0.024349690758323765, Validation Loss: 0.017353998339206807\n",
      "Epoch [19085/20000], Training Loss: 0.019574016361730173, Validation Loss: 0.005389552180921424\n",
      "Epoch [19086/20000], Training Loss: 0.013662180184058213, Validation Loss: 0.01531704949189816\n",
      "Epoch [19087/20000], Training Loss: 0.009972943185338019, Validation Loss: 0.004433024244336076\n",
      "Epoch [19088/20000], Training Loss: 0.005054849075219993, Validation Loss: 0.009959087540794696\n",
      "Epoch [19089/20000], Training Loss: 0.005057584945981424, Validation Loss: 0.00472590995698168\n",
      "Epoch [19090/20000], Training Loss: 0.009151277693360629, Validation Loss: 0.00397465886947302\n",
      "Epoch [19091/20000], Training Loss: 0.0061158727273453095, Validation Loss: 0.004863704022552353\n",
      "Epoch [19092/20000], Training Loss: 0.005880886637070505, Validation Loss: 0.002436117851654151\n",
      "Epoch [19093/20000], Training Loss: 0.004368436349685518, Validation Loss: 0.007941246498376133\n",
      "Epoch [19094/20000], Training Loss: 0.010410756647745854, Validation Loss: 0.005231497011015459\n",
      "Epoch [19095/20000], Training Loss: 0.005216475029127666, Validation Loss: 0.003316017924307657\n",
      "Epoch [19096/20000], Training Loss: 0.005873518494939033, Validation Loss: 0.00183835631492283\n",
      "Epoch [19097/20000], Training Loss: 0.002962166215898573, Validation Loss: 0.01730778015085629\n",
      "Epoch [19098/20000], Training Loss: 0.008734748109450916, Validation Loss: 0.0036247919063809838\n",
      "Epoch [19099/20000], Training Loss: 0.007737844920484349, Validation Loss: 0.00674227475454765\n",
      "Epoch [19100/20000], Training Loss: 0.009119357156870527, Validation Loss: 0.008668998089579614\n",
      "Epoch [19101/20000], Training Loss: 0.021080857962390707, Validation Loss: 0.01190713745693256\n",
      "Epoch [19102/20000], Training Loss: 0.008265909339700426, Validation Loss: 0.005217662081122459\n",
      "Epoch [19103/20000], Training Loss: 0.003018595402993794, Validation Loss: 0.007779665075816233\n",
      "Epoch [19104/20000], Training Loss: 0.008121867296202774, Validation Loss: 0.003472499773910118\n",
      "Epoch [19105/20000], Training Loss: 0.012401001970699457, Validation Loss: 0.0031915091843036773\n",
      "Epoch [19106/20000], Training Loss: 0.02543427096965648, Validation Loss: 0.004484573213911382\n",
      "Epoch [19107/20000], Training Loss: 0.048781730593288684, Validation Loss: 0.008402138150196218\n",
      "Epoch [19108/20000], Training Loss: 0.016065358271589503, Validation Loss: 0.009609995276801534\n",
      "Epoch [19109/20000], Training Loss: 0.010036561953964882, Validation Loss: 0.008435544462957165\n",
      "Epoch [19110/20000], Training Loss: 0.013198950585709619, Validation Loss: 0.017179567233792374\n",
      "Epoch [19111/20000], Training Loss: 0.015217787471296365, Validation Loss: 0.014785630502716035\n",
      "Epoch [19112/20000], Training Loss: 0.010035040994158148, Validation Loss: 0.0208680337028844\n",
      "Epoch [19113/20000], Training Loss: 0.009304119076529917, Validation Loss: 0.015110198809809714\n",
      "Epoch [19114/20000], Training Loss: 0.013656459209934317, Validation Loss: 0.0028186263820589564\n",
      "Epoch [19115/20000], Training Loss: 0.006181792375822884, Validation Loss: 0.0026501487762501546\n",
      "Epoch [19116/20000], Training Loss: 0.005882440192570877, Validation Loss: 0.002736101945105683\n",
      "Epoch [19117/20000], Training Loss: 0.006773565854798237, Validation Loss: 0.005773271217810774\n",
      "Epoch [19118/20000], Training Loss: 0.004762710933261717, Validation Loss: 0.007511505191883874\n",
      "Epoch [19119/20000], Training Loss: 0.007916823552607508, Validation Loss: 0.014438091644218989\n",
      "Epoch [19120/20000], Training Loss: 0.012351236657455697, Validation Loss: 0.006722992083688999\n",
      "Epoch [19121/20000], Training Loss: 0.008994003479618382, Validation Loss: 0.0040250814139038945\n",
      "Epoch [19122/20000], Training Loss: 0.007705220142919903, Validation Loss: 0.003053172923946907\n",
      "Epoch [19123/20000], Training Loss: 0.005958630674285814, Validation Loss: 0.0027431701165261503\n",
      "Epoch [19124/20000], Training Loss: 0.0037762497881524176, Validation Loss: 0.002378007344102605\n",
      "Epoch [19125/20000], Training Loss: 0.005654952314476499, Validation Loss: 0.002172286080297775\n",
      "Epoch [19126/20000], Training Loss: 0.024060245977515087, Validation Loss: 0.01982552132436214\n",
      "Epoch [19127/20000], Training Loss: 0.02008155025194875, Validation Loss: 0.007357885021707899\n",
      "Epoch [19128/20000], Training Loss: 0.009398247069839272, Validation Loss: 0.003982911565978091\n",
      "Epoch [19129/20000], Training Loss: 0.0068920078961777365, Validation Loss: 0.0040105565842228465\n",
      "Epoch [19130/20000], Training Loss: 0.007825570225707321, Validation Loss: 0.006143844520141696\n",
      "Epoch [19131/20000], Training Loss: 0.007698935631846585, Validation Loss: 0.004611520978091018\n",
      "Epoch [19132/20000], Training Loss: 0.00874031691273558, Validation Loss: 0.004867727563879669\n",
      "Epoch [19133/20000], Training Loss: 0.006113112728469007, Validation Loss: 0.008117194819663266\n",
      "Epoch [19134/20000], Training Loss: 0.006261420677349504, Validation Loss: 0.0033841751104810793\n",
      "Epoch [19135/20000], Training Loss: 0.005908735030711146, Validation Loss: 0.005607301087463513\n",
      "Epoch [19136/20000], Training Loss: 0.0038847156018683954, Validation Loss: 0.0032670376160913064\n",
      "Epoch [19137/20000], Training Loss: 0.004589401179276008, Validation Loss: 0.007648979912378983\n",
      "Epoch [19138/20000], Training Loss: 0.007204657461053492, Validation Loss: 0.0030740918492420896\n",
      "Epoch [19139/20000], Training Loss: 0.00984098364694676, Validation Loss: 0.005658304663873952\n",
      "Epoch [19140/20000], Training Loss: 0.01825139583706914, Validation Loss: 0.004985290413238104\n",
      "Epoch [19141/20000], Training Loss: 0.00631431991675007, Validation Loss: 0.004438003110519594\n",
      "Epoch [19142/20000], Training Loss: 0.002958877087491731, Validation Loss: 0.0068399666675509506\n",
      "Epoch [19143/20000], Training Loss: 0.008323879905999223, Validation Loss: 0.004406414024875709\n",
      "Epoch [19144/20000], Training Loss: 0.005933795354524461, Validation Loss: 0.003659719647008799\n",
      "Epoch [19145/20000], Training Loss: 0.004724726755479683, Validation Loss: 0.004077434148586325\n",
      "Epoch [19146/20000], Training Loss: 0.009824533961364068, Validation Loss: 0.01676826099199908\n",
      "Epoch [19147/20000], Training Loss: 0.01534094755145229, Validation Loss: 0.03809700799839837\n",
      "Epoch [19148/20000], Training Loss: 0.03009083597941396, Validation Loss: 0.03754980117082596\n",
      "Epoch [19149/20000], Training Loss: 0.015083357739578267, Validation Loss: 0.0029306530655303395\n",
      "Epoch [19150/20000], Training Loss: 0.010916615094174631, Validation Loss: 0.003738633626462778\n",
      "Epoch [19151/20000], Training Loss: 0.0040337368887516534, Validation Loss: 0.0031068547064120053\n",
      "Epoch [19152/20000], Training Loss: 0.0037691305790628704, Validation Loss: 0.0029139925606518455\n",
      "Epoch [19153/20000], Training Loss: 0.0032103289309556465, Validation Loss: 0.002671454187531143\n",
      "Epoch [19154/20000], Training Loss: 0.004608655506412366, Validation Loss: 0.002596691633666684\n",
      "Epoch [19155/20000], Training Loss: 0.0053329393280624315, Validation Loss: 0.01054536940140289\n",
      "Epoch [19156/20000], Training Loss: 0.012838413651999352, Validation Loss: 0.003397216845869771\n",
      "Epoch [19157/20000], Training Loss: 0.009712327645892012, Validation Loss: 0.017281559343302118\n",
      "Epoch [19158/20000], Training Loss: 0.017841263905990803, Validation Loss: 0.0058227012159651915\n",
      "Epoch [19159/20000], Training Loss: 0.008133460963368375, Validation Loss: 0.03239554805415018\n",
      "Epoch [19160/20000], Training Loss: 0.010685523636701484, Validation Loss: 0.004850991545294304\n",
      "Epoch [19161/20000], Training Loss: 0.003834386010566959, Validation Loss: 0.006490555660483168\n",
      "Epoch [19162/20000], Training Loss: 0.009949154152439275, Validation Loss: 0.004192283213263254\n",
      "Epoch [19163/20000], Training Loss: 0.01154841093999234, Validation Loss: 0.008817278513950961\n",
      "Epoch [19164/20000], Training Loss: 0.008432156388412946, Validation Loss: 0.01316899194249088\n",
      "Epoch [19165/20000], Training Loss: 0.012097985962450406, Validation Loss: 0.004643353483905833\n",
      "Epoch [19166/20000], Training Loss: 0.010672162870443052, Validation Loss: 0.003222264948172549\n",
      "Epoch [19167/20000], Training Loss: 0.007787998784351657, Validation Loss: 0.006345602032037048\n",
      "Epoch [19168/20000], Training Loss: 0.009637866696721176, Validation Loss: 0.0039789236298099166\n",
      "Epoch [19169/20000], Training Loss: 0.01637093934524663, Validation Loss: 0.004150983274627158\n",
      "Epoch [19170/20000], Training Loss: 0.03251817896463243, Validation Loss: 0.007623031164946527\n",
      "Epoch [19171/20000], Training Loss: 0.01186519661496277, Validation Loss: 0.011623697355389595\n",
      "Epoch [19172/20000], Training Loss: 0.006784528254814047, Validation Loss: 0.0031196206673917604\n",
      "Epoch [19173/20000], Training Loss: 0.007148048470428746, Validation Loss: 0.006647641589644847\n",
      "Epoch [19174/20000], Training Loss: 0.013163136621057805, Validation Loss: 0.004204365690905812\n",
      "Epoch [19175/20000], Training Loss: 0.014861790473722587, Validation Loss: 0.007277262230803031\n",
      "Epoch [19176/20000], Training Loss: 0.007875880106439581, Validation Loss: 0.006679909037694816\n",
      "Epoch [19177/20000], Training Loss: 0.00834553117809784, Validation Loss: 0.004780763893264238\n",
      "Epoch [19178/20000], Training Loss: 0.007934604438458694, Validation Loss: 0.004405163619335326\n",
      "Epoch [19179/20000], Training Loss: 0.004312178614912098, Validation Loss: 0.0028570202908452533\n",
      "Epoch [19180/20000], Training Loss: 0.005060525737852524, Validation Loss: 0.0024842161308343855\n",
      "Epoch [19181/20000], Training Loss: 0.008767347965268917, Validation Loss: 0.004168961330318348\n",
      "Epoch [19182/20000], Training Loss: 0.004946597130453613, Validation Loss: 0.002734169264471191\n",
      "Epoch [19183/20000], Training Loss: 0.003190973465409895, Validation Loss: 0.00352293327783476\n",
      "Epoch [19184/20000], Training Loss: 0.003892134674710438, Validation Loss: 0.002850257442340585\n",
      "Epoch [19185/20000], Training Loss: 0.004879248095676303, Validation Loss: 0.009197265109313386\n",
      "Epoch [19186/20000], Training Loss: 0.006889912713502001, Validation Loss: 0.0050874968624806905\n",
      "Epoch [19187/20000], Training Loss: 0.010728718885470048, Validation Loss: 0.002771677615786336\n",
      "Epoch [19188/20000], Training Loss: 0.005763291975849175, Validation Loss: 0.003543453058228445\n",
      "Epoch [19189/20000], Training Loss: 0.0031553136588107528, Validation Loss: 0.0030395583210061673\n",
      "Epoch [19190/20000], Training Loss: 0.004485791963816155, Validation Loss: 0.00440769689157609\n",
      "Epoch [19191/20000], Training Loss: 0.004483529629819064, Validation Loss: 0.0034052774239749575\n",
      "Epoch [19192/20000], Training Loss: 0.005317740712533643, Validation Loss: 0.01852083658533437\n",
      "Epoch [19193/20000], Training Loss: 0.015951088912385916, Validation Loss: 0.07847539016178676\n",
      "Epoch [19194/20000], Training Loss: 0.04262470670905064, Validation Loss: 0.09030164139611381\n",
      "Epoch [19195/20000], Training Loss: 0.03996882950429738, Validation Loss: 0.03288724113787927\n",
      "Epoch [19196/20000], Training Loss: 0.015354160460576947, Validation Loss: 0.0030838308453143663\n",
      "Epoch [19197/20000], Training Loss: 0.005812757376198923, Validation Loss: 0.0030715104620440653\n",
      "Epoch [19198/20000], Training Loss: 0.00480207567358905, Validation Loss: 0.0029371711928847582\n",
      "Epoch [19199/20000], Training Loss: 0.00518405205565082, Validation Loss: 0.02261683557714735\n",
      "Epoch [19200/20000], Training Loss: 0.00747255420453387, Validation Loss: 0.0037760937184192034\n",
      "Epoch [19201/20000], Training Loss: 0.010041899653775286, Validation Loss: 0.004133549559107968\n",
      "Epoch [19202/20000], Training Loss: 0.00593102910728963, Validation Loss: 0.005076025080468184\n",
      "Epoch [19203/20000], Training Loss: 0.004960627711365565, Validation Loss: 0.0037162684152253966\n",
      "Epoch [19204/20000], Training Loss: 0.004689009791036369, Validation Loss: 0.0032820232528559174\n",
      "Epoch [19205/20000], Training Loss: 0.005808310750313247, Validation Loss: 0.017004012795431272\n",
      "Epoch [19206/20000], Training Loss: 0.009265472035622224, Validation Loss: 0.0029139827903653767\n",
      "Epoch [19207/20000], Training Loss: 0.008504863013513386, Validation Loss: 0.00455366946490749\n",
      "Epoch [19208/20000], Training Loss: 0.005213473826838059, Validation Loss: 0.00469838974322668\n",
      "Epoch [19209/20000], Training Loss: 0.004611647696167764, Validation Loss: 0.0035522976125191763\n",
      "Epoch [19210/20000], Training Loss: 0.005032256025775236, Validation Loss: 0.0032281714709999727\n",
      "Epoch [19211/20000], Training Loss: 0.0038286305888440048, Validation Loss: 0.008611267099955253\n",
      "Epoch [19212/20000], Training Loss: 0.008684925257901861, Validation Loss: 0.0023734725462992066\n",
      "Epoch [19213/20000], Training Loss: 0.0036867253659563304, Validation Loss: 0.002367800168163459\n",
      "Epoch [19214/20000], Training Loss: 0.004319533104186439, Validation Loss: 0.002426190043999425\n",
      "Epoch [19215/20000], Training Loss: 0.0047816572934087686, Validation Loss: 0.005511619483253791\n",
      "Epoch [19216/20000], Training Loss: 0.010927366248064832, Validation Loss: 0.008230333567524771\n",
      "Epoch [19217/20000], Training Loss: 0.011169869492212976, Validation Loss: 0.04616292076830827\n",
      "Epoch [19218/20000], Training Loss: 0.020211686368708928, Validation Loss: 0.013861499121702536\n",
      "Epoch [19219/20000], Training Loss: 0.017907551734034705, Validation Loss: 0.007580619035825894\n",
      "Epoch [19220/20000], Training Loss: 0.025037632256194393, Validation Loss: 0.037024440985987894\n",
      "Epoch [19221/20000], Training Loss: 0.037644129283112955, Validation Loss: 0.0482763082584045\n",
      "Epoch [19222/20000], Training Loss: 0.03934501674458651, Validation Loss: 0.03483489102550915\n",
      "Epoch [19223/20000], Training Loss: 0.01656908319953995, Validation Loss: 0.009284756063217785\n",
      "Epoch [19224/20000], Training Loss: 0.011336858215807004, Validation Loss: 0.004590936540656942\n",
      "Epoch [19225/20000], Training Loss: 0.008140689901275826, Validation Loss: 0.009944732833121504\n",
      "Epoch [19226/20000], Training Loss: 0.0069486552238231525, Validation Loss: 0.004013455389922905\n",
      "Epoch [19227/20000], Training Loss: 0.008633764117673439, Validation Loss: 0.0038467902091542387\n",
      "Epoch [19228/20000], Training Loss: 0.00586421560521038, Validation Loss: 0.006676573360372072\n",
      "Epoch [19229/20000], Training Loss: 0.005185676968072325, Validation Loss: 0.0035166057891156018\n",
      "Epoch [19230/20000], Training Loss: 0.0062408649163054565, Validation Loss: 0.005347434597622075\n",
      "Epoch [19231/20000], Training Loss: 0.005091377538877039, Validation Loss: 0.003072174399028523\n",
      "Epoch [19232/20000], Training Loss: 0.005135879001630071, Validation Loss: 0.0031292851615169837\n",
      "Epoch [19233/20000], Training Loss: 0.005372349938884976, Validation Loss: 0.002702529259459157\n",
      "Epoch [19234/20000], Training Loss: 0.0068352304918205065, Validation Loss: 0.016414122870599814\n",
      "Epoch [19235/20000], Training Loss: 0.018454283028924174, Validation Loss: 0.0035833723377455265\n",
      "Epoch [19236/20000], Training Loss: 0.006519101569192998, Validation Loss: 0.006566016801767576\n",
      "Epoch [19237/20000], Training Loss: 0.007437952108115756, Validation Loss: 0.009334368975147167\n",
      "Epoch [19238/20000], Training Loss: 0.014098923511483008, Validation Loss: 0.006292630174566057\n",
      "Epoch [19239/20000], Training Loss: 0.012824827473585694, Validation Loss: 0.02481795847415924\n",
      "Epoch [19240/20000], Training Loss: 0.015171810062124027, Validation Loss: 0.013211288356355258\n",
      "Epoch [19241/20000], Training Loss: 0.007742468319650341, Validation Loss: 0.005520988106092262\n",
      "Epoch [19242/20000], Training Loss: 0.0107097753356733, Validation Loss: 0.003999289474450059\n",
      "Epoch [19243/20000], Training Loss: 0.008483887153293576, Validation Loss: 0.004137349614341346\n",
      "Epoch [19244/20000], Training Loss: 0.004087142959698602, Validation Loss: 0.0035589189834094536\n",
      "Epoch [19245/20000], Training Loss: 0.005081385667186363, Validation Loss: 0.0030212691386379492\n",
      "Epoch [19246/20000], Training Loss: 0.004317337181419134, Validation Loss: 0.0037163031354014403\n",
      "Epoch [19247/20000], Training Loss: 0.0040745385444357195, Validation Loss: 0.002665310499391839\n",
      "Epoch [19248/20000], Training Loss: 0.005768185063581248, Validation Loss: 0.002708938164037891\n",
      "Epoch [19249/20000], Training Loss: 0.013711543921609908, Validation Loss: 0.0034719492916472326\n",
      "Epoch [19250/20000], Training Loss: 0.012431689234550245, Validation Loss: 0.00485533256349819\n",
      "Epoch [19251/20000], Training Loss: 0.011697394412262188, Validation Loss: 0.004996722497578242\n",
      "Epoch [19252/20000], Training Loss: 0.013906626832197486, Validation Loss: 0.007228589350623745\n",
      "Epoch [19253/20000], Training Loss: 0.008487380359188787, Validation Loss: 0.00413991225650534\n",
      "Epoch [19254/20000], Training Loss: 0.005845285956248907, Validation Loss: 0.00338313451253012\n",
      "Epoch [19255/20000], Training Loss: 0.01431754783802067, Validation Loss: 0.0056659840006406895\n",
      "Epoch [19256/20000], Training Loss: 0.012664661663750718, Validation Loss: 0.005122243958924508\n",
      "Epoch [19257/20000], Training Loss: 0.01130433984820099, Validation Loss: 0.043715160872255056\n",
      "Epoch [19258/20000], Training Loss: 0.03394864612086427, Validation Loss: 0.008495440507041556\n",
      "Epoch [19259/20000], Training Loss: 0.021145375760430137, Validation Loss: 0.007861479551398329\n",
      "Epoch [19260/20000], Training Loss: 0.008841332282047785, Validation Loss: 0.0142076701543244\n",
      "Epoch [19261/20000], Training Loss: 0.012676813583571598, Validation Loss: 0.02008998260966369\n",
      "Epoch [19262/20000], Training Loss: 0.007269827192690822, Validation Loss: 0.0039017018778915113\n",
      "Epoch [19263/20000], Training Loss: 0.006221764491783688, Validation Loss: 0.003708217097612689\n",
      "Epoch [19264/20000], Training Loss: 0.00497854980806421, Validation Loss: 0.00580184707151992\n",
      "Epoch [19265/20000], Training Loss: 0.007731628774698558, Validation Loss: 0.003580051948249872\n",
      "Epoch [19266/20000], Training Loss: 0.01379228783688242, Validation Loss: 0.014585305882574244\n",
      "Epoch [19267/20000], Training Loss: 0.010827888704992501, Validation Loss: 0.012330199949377205\n",
      "Epoch [19268/20000], Training Loss: 0.009724638924775977, Validation Loss: 0.005833027219133717\n",
      "Epoch [19269/20000], Training Loss: 0.004932249642929979, Validation Loss: 0.0035544459803661865\n",
      "Epoch [19270/20000], Training Loss: 0.003808550939928474, Validation Loss: 0.005121143368471946\n",
      "Epoch [19271/20000], Training Loss: 0.0047102245360812435, Validation Loss: 0.0026512477779760957\n",
      "Epoch [19272/20000], Training Loss: 0.003910715908984587, Validation Loss: 0.0034650459607031996\n",
      "Epoch [19273/20000], Training Loss: 0.0051824428457101545, Validation Loss: 0.0024590864444949795\n",
      "Epoch [19274/20000], Training Loss: 0.006823532631001074, Validation Loss: 0.002930421515234879\n",
      "Epoch [19275/20000], Training Loss: 0.004554084847248825, Validation Loss: 0.0025875125928515835\n",
      "Epoch [19276/20000], Training Loss: 0.005427427234410841, Validation Loss: 0.004959832681509267\n",
      "Epoch [19277/20000], Training Loss: 0.005241934889714425, Validation Loss: 0.014471039708171572\n",
      "Epoch [19278/20000], Training Loss: 0.006702584314812806, Validation Loss: 0.004802994030926909\n",
      "Epoch [19279/20000], Training Loss: 0.006388005299543563, Validation Loss: 0.005082536122894713\n",
      "Epoch [19280/20000], Training Loss: 0.005687062141987553, Validation Loss: 0.005293005752084696\n",
      "Epoch [19281/20000], Training Loss: 0.00442174592743478, Validation Loss: 0.0026133411530672875\n",
      "Epoch [19282/20000], Training Loss: 0.0042856531737405544, Validation Loss: 0.0029228889200437285\n",
      "Epoch [19283/20000], Training Loss: 0.008379720138951339, Validation Loss: 0.0023196594639953165\n",
      "Epoch [19284/20000], Training Loss: 0.010376548316490439, Validation Loss: 0.02057156977908952\n",
      "Epoch [19285/20000], Training Loss: 0.010196820308919996, Validation Loss: 0.009564966620278679\n",
      "Epoch [19286/20000], Training Loss: 0.007403097822264369, Validation Loss: 0.01978096299405609\n",
      "Epoch [19287/20000], Training Loss: 0.012959187831646497, Validation Loss: 0.002801773936620961\n",
      "Epoch [19288/20000], Training Loss: 0.005712331550377822, Validation Loss: 0.0029688796535733038\n",
      "Epoch [19289/20000], Training Loss: 0.00613365853288477, Validation Loss: 0.0035014386687970277\n",
      "Epoch [19290/20000], Training Loss: 0.00749216408920412, Validation Loss: 0.0036533575039356947\n",
      "Epoch [19291/20000], Training Loss: 0.008545408216637693, Validation Loss: 0.002533243503422277\n",
      "Epoch [19292/20000], Training Loss: 0.005186501241759218, Validation Loss: 0.004805395828693041\n",
      "Epoch [19293/20000], Training Loss: 0.009371996308183432, Validation Loss: 0.0031184320437855043\n",
      "Epoch [19294/20000], Training Loss: 0.017050430831399614, Validation Loss: 0.0033090608527345843\n",
      "Epoch [19295/20000], Training Loss: 0.015583316347628726, Validation Loss: 0.0027039619162680796\n",
      "Epoch [19296/20000], Training Loss: 0.010103206856001634, Validation Loss: 0.009117199975922387\n",
      "Epoch [19297/20000], Training Loss: 0.013478659839577745, Validation Loss: 0.007130712495527547\n",
      "Epoch [19298/20000], Training Loss: 0.007198621678201432, Validation Loss: 0.009293476086375969\n",
      "Epoch [19299/20000], Training Loss: 0.0066759964444957276, Validation Loss: 0.004377371199162945\n",
      "Epoch [19300/20000], Training Loss: 0.008026468397214817, Validation Loss: 0.004085637504354911\n",
      "Epoch [19301/20000], Training Loss: 0.0043222855130333115, Validation Loss: 0.002779978830536103\n",
      "Epoch [19302/20000], Training Loss: 0.004945187321157262, Validation Loss: 0.002386239024678628\n",
      "Epoch [19303/20000], Training Loss: 0.004590730744114678, Validation Loss: 0.003660239379054734\n",
      "Epoch [19304/20000], Training Loss: 0.003647006584807449, Validation Loss: 0.005042247507455093\n",
      "Epoch [19305/20000], Training Loss: 0.00607967183337418, Validation Loss: 0.0024899125648001307\n",
      "Epoch [19306/20000], Training Loss: 0.006105019486962452, Validation Loss: 0.005597300094902623\n",
      "Epoch [19307/20000], Training Loss: 0.013218215606424824, Validation Loss: 0.002402240127724212\n",
      "Epoch [19308/20000], Training Loss: 0.011466643346856082, Validation Loss: 0.007519532627026949\n",
      "Epoch [19309/20000], Training Loss: 0.011628049263909594, Validation Loss: 0.004396147793158889\n",
      "Epoch [19310/20000], Training Loss: 0.008825875531848786, Validation Loss: 0.00453329172783669\n",
      "Epoch [19311/20000], Training Loss: 0.009838381586242966, Validation Loss: 0.007456315947430474\n",
      "Epoch [19312/20000], Training Loss: 0.007362677575656562, Validation Loss: 0.005539027042686939\n",
      "Epoch [19313/20000], Training Loss: 0.008216472084791957, Validation Loss: 0.00455460896981614\n",
      "Epoch [19314/20000], Training Loss: 0.008587497183595718, Validation Loss: 0.005786861289379885\n",
      "Epoch [19315/20000], Training Loss: 0.00442308039938715, Validation Loss: 0.0031345389184675625\n",
      "Epoch [19316/20000], Training Loss: 0.0033590683212553684, Validation Loss: 0.0037289826931165802\n",
      "Epoch [19317/20000], Training Loss: 0.003739640859618833, Validation Loss: 0.002384117366870799\n",
      "Epoch [19318/20000], Training Loss: 0.005555972988694131, Validation Loss: 0.009172587505807834\n",
      "Epoch [19319/20000], Training Loss: 0.003945641906650746, Validation Loss: 0.00810789680209484\n",
      "Epoch [19320/20000], Training Loss: 0.008062458141359993, Validation Loss: 0.00486782201863165\n",
      "Epoch [19321/20000], Training Loss: 0.004233040970055819, Validation Loss: 0.007201443825449262\n",
      "Epoch [19322/20000], Training Loss: 0.007907989950857492, Validation Loss: 0.05386807130915778\n",
      "Epoch [19323/20000], Training Loss: 0.030270367526100017, Validation Loss: 0.022594938254249946\n",
      "Epoch [19324/20000], Training Loss: 0.058456598012266374, Validation Loss: 0.042188904028378715\n",
      "Epoch [19325/20000], Training Loss: 0.026805217613728467, Validation Loss: 0.0179509488599641\n",
      "Epoch [19326/20000], Training Loss: 0.029471546581979573, Validation Loss: 0.06858736982478661\n",
      "Epoch [19327/20000], Training Loss: 0.02698257349506353, Validation Loss: 0.008319315384142236\n",
      "Epoch [19328/20000], Training Loss: 0.007825996067757452, Validation Loss: 0.004953332943959608\n",
      "Epoch [19329/20000], Training Loss: 0.009206081094289347, Validation Loss: 0.013524969640587057\n",
      "Epoch [19330/20000], Training Loss: 0.010948605421747613, Validation Loss: 0.012690301337483183\n",
      "Epoch [19331/20000], Training Loss: 0.009755123620769674, Validation Loss: 0.0038180494947106414\n",
      "Epoch [19332/20000], Training Loss: 0.009010502607582436, Validation Loss: 0.009495648688503675\n",
      "Epoch [19333/20000], Training Loss: 0.012104326272882255, Validation Loss: 0.003936544676045936\n",
      "Epoch [19334/20000], Training Loss: 0.007533823136847266, Validation Loss: 0.009996408330542701\n",
      "Epoch [19335/20000], Training Loss: 0.005982354141451651, Validation Loss: 0.007446903874489731\n",
      "Epoch [19336/20000], Training Loss: 0.00741474240203388, Validation Loss: 0.005428278278804016\n",
      "Epoch [19337/20000], Training Loss: 0.004119483262391961, Validation Loss: 0.004835473772670544\n",
      "Epoch [19338/20000], Training Loss: 0.005537806438464291, Validation Loss: 0.007150559270355318\n",
      "Epoch [19339/20000], Training Loss: 0.00554894852431888, Validation Loss: 0.004020206264596939\n",
      "Epoch [19340/20000], Training Loss: 0.006466489845999084, Validation Loss: 0.003151414620724328\n",
      "Epoch [19341/20000], Training Loss: 0.016632338501949562, Validation Loss: 0.0034567802213132837\n",
      "Epoch [19342/20000], Training Loss: 0.009472506457638832, Validation Loss: 0.018010482458131655\n",
      "Epoch [19343/20000], Training Loss: 0.017460109414709483, Validation Loss: 0.0084176021960697\n",
      "Epoch [19344/20000], Training Loss: 0.016383504930542716, Validation Loss: 0.003576081292243027\n",
      "Epoch [19345/20000], Training Loss: 0.0127830230942761, Validation Loss: 0.00357303209603154\n",
      "Epoch [19346/20000], Training Loss: 0.00844982797415079, Validation Loss: 0.03799162592206683\n",
      "Epoch [19347/20000], Training Loss: 0.02006146053255569, Validation Loss: 0.032874728952135356\n",
      "Epoch [19348/20000], Training Loss: 0.011500783587377685, Validation Loss: 0.010318011114061703\n",
      "Epoch [19349/20000], Training Loss: 0.008119961113802024, Validation Loss: 0.0031377132982085385\n",
      "Epoch [19350/20000], Training Loss: 0.008402718028394571, Validation Loss: 0.0029930158052597503\n",
      "Epoch [19351/20000], Training Loss: 0.006135197263834665, Validation Loss: 0.004430027379255222\n",
      "Epoch [19352/20000], Training Loss: 0.005716996797543418, Validation Loss: 0.003654048823851599\n",
      "Epoch [19353/20000], Training Loss: 0.0035194697487830645, Validation Loss: 0.002673823452954268\n",
      "Epoch [19354/20000], Training Loss: 0.004103061060180023, Validation Loss: 0.002731092523658257\n",
      "Epoch [19355/20000], Training Loss: 0.004259929125899882, Validation Loss: 0.0038659225849966916\n",
      "Epoch [19356/20000], Training Loss: 0.00597231267848589, Validation Loss: 0.012792086627866541\n",
      "Epoch [19357/20000], Training Loss: 0.008447945419123113, Validation Loss: 0.007415002078882286\n",
      "Epoch [19358/20000], Training Loss: 0.023379478227330504, Validation Loss: 0.002953590758677048\n",
      "Epoch [19359/20000], Training Loss: 0.004152496007658816, Validation Loss: 0.00529647059738636\n",
      "Epoch [19360/20000], Training Loss: 0.008124089419392735, Validation Loss: 0.015189793493066515\n",
      "Epoch [19361/20000], Training Loss: 0.02018249995879679, Validation Loss: 0.007741850335150957\n",
      "Epoch [19362/20000], Training Loss: 0.0080904210564248, Validation Loss: 0.004599395939814193\n",
      "Epoch [19363/20000], Training Loss: 0.012249880168902954, Validation Loss: 0.0053273198261327495\n",
      "Epoch [19364/20000], Training Loss: 0.01456429269455839, Validation Loss: 0.00776091351040772\n",
      "Epoch [19365/20000], Training Loss: 0.0055789287339821515, Validation Loss: 0.003793239926121103\n",
      "Epoch [19366/20000], Training Loss: 0.0045022555766211425, Validation Loss: 0.0023424088805254046\n",
      "Epoch [19367/20000], Training Loss: 0.005940316444528955, Validation Loss: 0.0037545105941327564\n",
      "Epoch [19368/20000], Training Loss: 0.016444012345995622, Validation Loss: 0.011708945035934448\n",
      "Epoch [19369/20000], Training Loss: 0.011145586465967685, Validation Loss: 0.004006026212924295\n",
      "Epoch [19370/20000], Training Loss: 0.0059851484620838035, Validation Loss: 0.002250715408341513\n",
      "Epoch [19371/20000], Training Loss: 0.0035088803197140805, Validation Loss: 0.002229503825741726\n",
      "Epoch [19372/20000], Training Loss: 0.004971431954930138, Validation Loss: 0.002228737392995319\n",
      "Epoch [19373/20000], Training Loss: 0.0038882366412248564, Validation Loss: 0.003993633269731488\n",
      "Epoch [19374/20000], Training Loss: 0.0051729861120942845, Validation Loss: 0.0021281818751179887\n",
      "Epoch [19375/20000], Training Loss: 0.007205056256810037, Validation Loss: 0.003460531651013482\n",
      "Epoch [19376/20000], Training Loss: 0.023447489092047493, Validation Loss: 0.0023141865219657386\n",
      "Epoch [19377/20000], Training Loss: 0.018416068803990777, Validation Loss: 0.010284633866311716\n",
      "Epoch [19378/20000], Training Loss: 0.00977372788241025, Validation Loss: 0.009745798472847261\n",
      "Epoch [19379/20000], Training Loss: 0.020010222013297607, Validation Loss: 0.007560625539294312\n",
      "Epoch [19380/20000], Training Loss: 0.021124847061271015, Validation Loss: 0.04733251993145261\n",
      "Epoch [19381/20000], Training Loss: 0.020361123032801385, Validation Loss: 0.04089859766619546\n",
      "Epoch [19382/20000], Training Loss: 0.02240750635974109, Validation Loss: 0.014827448476455831\n",
      "Epoch [19383/20000], Training Loss: 0.011029734623402223, Validation Loss: 0.0064307660130517846\n",
      "Epoch [19384/20000], Training Loss: 0.00875804472681401, Validation Loss: 0.017805979187999452\n",
      "Epoch [19385/20000], Training Loss: 0.01362860647870028, Validation Loss: 0.013375240245039147\n",
      "Epoch [19386/20000], Training Loss: 0.016709878367172287, Validation Loss: 0.014767005756950568\n",
      "Epoch [19387/20000], Training Loss: 0.010353593615166443, Validation Loss: 0.006638761584846596\n",
      "Epoch [19388/20000], Training Loss: 0.010353514587872528, Validation Loss: 0.013567030962024416\n",
      "Epoch [19389/20000], Training Loss: 0.009160426412043827, Validation Loss: 0.006093517478030583\n",
      "Epoch [19390/20000], Training Loss: 0.007918306252707095, Validation Loss: 0.007543993535372566\n",
      "Epoch [19391/20000], Training Loss: 0.011860358068101309, Validation Loss: 0.004544531577266934\n",
      "Epoch [19392/20000], Training Loss: 0.010904105677452338, Validation Loss: 0.009946496026324374\n",
      "Epoch [19393/20000], Training Loss: 0.00564140309143113, Validation Loss: 0.007698039391211101\n",
      "Epoch [19394/20000], Training Loss: 0.0057178188193834755, Validation Loss: 0.005139066274363632\n",
      "Epoch [19395/20000], Training Loss: 0.00457730887033644, Validation Loss: 0.004777282742517335\n",
      "Epoch [19396/20000], Training Loss: 0.007859193492289964, Validation Loss: 0.011066477080540997\n",
      "Epoch [19397/20000], Training Loss: 0.007248453686770517, Validation Loss: 0.009098748630744714\n",
      "Epoch [19398/20000], Training Loss: 0.0046031406553603505, Validation Loss: 0.021150643538151468\n",
      "Epoch [19399/20000], Training Loss: 0.008714106788309956, Validation Loss: 0.018531021795102527\n",
      "Epoch [19400/20000], Training Loss: 0.007015292978974027, Validation Loss: 0.01919908540084041\n",
      "Epoch [19401/20000], Training Loss: 0.011847751993491354, Validation Loss: 0.012367997085675597\n",
      "Epoch [19402/20000], Training Loss: 0.005079027090687305, Validation Loss: 0.006504106867526266\n",
      "Epoch [19403/20000], Training Loss: 0.009762359075206664, Validation Loss: 0.003003422836107867\n",
      "Epoch [19404/20000], Training Loss: 0.010202710311334937, Validation Loss: 0.00971503263073308\n",
      "Epoch [19405/20000], Training Loss: 0.008517520822221871, Validation Loss: 0.006362434443352478\n",
      "Epoch [19406/20000], Training Loss: 0.009353991234092973, Validation Loss: 0.012805422606263429\n",
      "Epoch [19407/20000], Training Loss: 0.02496279348581863, Validation Loss: 0.02669557991729246\n",
      "Epoch [19408/20000], Training Loss: 0.019905994072489972, Validation Loss: 0.05224342916205517\n",
      "Epoch [19409/20000], Training Loss: 0.03920237872937703, Validation Loss: 0.005776697869818359\n",
      "Epoch [19410/20000], Training Loss: 0.011693944863509387, Validation Loss: 0.013661010190844536\n",
      "Epoch [19411/20000], Training Loss: 0.004432668864735335, Validation Loss: 0.0041389199323021994\n",
      "Epoch [19412/20000], Training Loss: 0.005894183123018593, Validation Loss: 0.005708776264742082\n",
      "Epoch [19413/20000], Training Loss: 0.006471594766480848, Validation Loss: 0.0053465827368199825\n",
      "Epoch [19414/20000], Training Loss: 0.005532268726840682, Validation Loss: 0.008503203612885304\n",
      "Epoch [19415/20000], Training Loss: 0.012120213124684856, Validation Loss: 0.0053567022966624885\n",
      "Epoch [19416/20000], Training Loss: 0.009499334511669335, Validation Loss: 0.0038761960292636134\n",
      "Epoch [19417/20000], Training Loss: 0.006704100733648894, Validation Loss: 0.006039278748046074\n",
      "Epoch [19418/20000], Training Loss: 0.002677972452667226, Validation Loss: 0.004243103870456912\n",
      "Epoch [19419/20000], Training Loss: 0.004996525128652658, Validation Loss: 0.0033146799874625038\n",
      "Epoch [19420/20000], Training Loss: 0.004671931079688615, Validation Loss: 0.005226070630930856\n",
      "Epoch [19421/20000], Training Loss: 0.006945742441790311, Validation Loss: 0.008295766543596983\n",
      "Epoch [19422/20000], Training Loss: 0.0050141783358412795, Validation Loss: 0.009518751913544425\n",
      "Epoch [19423/20000], Training Loss: 0.00872877292567864, Validation Loss: 0.004936695732924698\n",
      "Epoch [19424/20000], Training Loss: 0.009048368724896656, Validation Loss: 0.015380245367331164\n",
      "Epoch [19425/20000], Training Loss: 0.00913137121863526, Validation Loss: 0.003166821485917483\n",
      "Epoch [19426/20000], Training Loss: 0.0066372516381047065, Validation Loss: 0.0026628697274385963\n",
      "Epoch [19427/20000], Training Loss: 0.006716953388864307, Validation Loss: 0.007334781698435869\n",
      "Epoch [19428/20000], Training Loss: 0.011616456879242574, Validation Loss: 0.0033293998068464658\n",
      "Epoch [19429/20000], Training Loss: 0.009891433523340052, Validation Loss: 0.00323914091235825\n",
      "Epoch [19430/20000], Training Loss: 0.007294903702391561, Validation Loss: 0.0026134098547377755\n",
      "Epoch [19431/20000], Training Loss: 0.005060645768285862, Validation Loss: 0.010408300241189343\n",
      "Epoch [19432/20000], Training Loss: 0.005804167840519118, Validation Loss: 0.0059569158765957215\n",
      "Epoch [19433/20000], Training Loss: 0.004203174861946276, Validation Loss: 0.007928031097565378\n",
      "Epoch [19434/20000], Training Loss: 0.005014750432175268, Validation Loss: 0.00803975678926331\n",
      "Epoch [19435/20000], Training Loss: 0.014081400020586443, Validation Loss: 0.006209775957229053\n",
      "Epoch [19436/20000], Training Loss: 0.01649756958667378, Validation Loss: 0.006690721274935669\n",
      "Epoch [19437/20000], Training Loss: 0.008498228494058171, Validation Loss: 0.0036161416958618376\n",
      "Epoch [19438/20000], Training Loss: 0.007076467210676388, Validation Loss: 0.004828835540268173\n",
      "Epoch [19439/20000], Training Loss: 0.019012142509122247, Validation Loss: 0.029591911605426242\n",
      "Epoch [19440/20000], Training Loss: 0.014234702916707778, Validation Loss: 0.015395148258124079\n",
      "Epoch [19441/20000], Training Loss: 0.009813602169742808, Validation Loss: 0.004745903092303446\n",
      "Epoch [19442/20000], Training Loss: 0.006901336025293858, Validation Loss: 0.005245398771616498\n",
      "Epoch [19443/20000], Training Loss: 0.006802677267322517, Validation Loss: 0.00450931562642966\n",
      "Epoch [19444/20000], Training Loss: 0.004726694284207562, Validation Loss: 0.010532554505126817\n",
      "Epoch [19445/20000], Training Loss: 0.007791975152940722, Validation Loss: 0.006345117173623294\n",
      "Epoch [19446/20000], Training Loss: 0.007069135075458429, Validation Loss: 0.0034299001480186625\n",
      "Epoch [19447/20000], Training Loss: 0.006002105851491381, Validation Loss: 0.005768786716674056\n",
      "Epoch [19448/20000], Training Loss: 0.008010135611584701, Validation Loss: 0.008552018376317047\n",
      "Epoch [19449/20000], Training Loss: 0.008481103188907062, Validation Loss: 0.009748088462012154\n",
      "Epoch [19450/20000], Training Loss: 0.010153751597889433, Validation Loss: 0.012741073167749814\n",
      "Epoch [19451/20000], Training Loss: 0.021626311839229726, Validation Loss: 0.004335177916800603\n",
      "Epoch [19452/20000], Training Loss: 0.015352940042191676, Validation Loss: 0.05087244515328036\n",
      "Epoch [19453/20000], Training Loss: 0.02470405398902845, Validation Loss: 0.004646511681910076\n",
      "Epoch [19454/20000], Training Loss: 0.011207042008339028, Validation Loss: 0.015843424147793224\n",
      "Epoch [19455/20000], Training Loss: 0.007776363943409034, Validation Loss: 0.005992349212257457\n",
      "Epoch [19456/20000], Training Loss: 0.007405942523616561, Validation Loss: 0.0032326356906976017\n",
      "Epoch [19457/20000], Training Loss: 0.0037135217592419523, Validation Loss: 0.003182748382511948\n",
      "Epoch [19458/20000], Training Loss: 0.00372352877963879, Validation Loss: 0.00358736515045166\n",
      "Epoch [19459/20000], Training Loss: 0.008377923871583854, Validation Loss: 0.05284237906985954\n",
      "Epoch [19460/20000], Training Loss: 0.12091528278376375, Validation Loss: 0.029909242902379316\n",
      "Epoch [19461/20000], Training Loss: 0.06618101864920131, Validation Loss: 0.016875144859472994\n",
      "Epoch [19462/20000], Training Loss: 0.011513822541538892, Validation Loss: 0.011395370654229606\n",
      "Epoch [19463/20000], Training Loss: 0.013673704383628709, Validation Loss: 0.007943355721571217\n",
      "Epoch [19464/20000], Training Loss: 0.007053791790635192, Validation Loss: 0.006229846095915751\n",
      "Epoch [19465/20000], Training Loss: 0.005873031408295252, Validation Loss: 0.00591707852592143\n",
      "Epoch [19466/20000], Training Loss: 0.005959513879913304, Validation Loss: 0.011606578861704162\n",
      "Epoch [19467/20000], Training Loss: 0.007493140200884747, Validation Loss: 0.0040368385732238365\n",
      "Epoch [19468/20000], Training Loss: 0.0061233126512628844, Validation Loss: 0.004191000096527985\n",
      "Epoch [19469/20000], Training Loss: 0.006406524256038081, Validation Loss: 0.005143298260333852\n",
      "Epoch [19470/20000], Training Loss: 0.008085655211678906, Validation Loss: 0.01094948534111089\n",
      "Epoch [19471/20000], Training Loss: 0.02802478806032533, Validation Loss: 0.0064554847173699925\n",
      "Epoch [19472/20000], Training Loss: 0.013979380225854194, Validation Loss: 0.005480797234651748\n",
      "Epoch [19473/20000], Training Loss: 0.004888640717971222, Validation Loss: 0.008681864731276587\n",
      "Epoch [19474/20000], Training Loss: 0.012862215108076011, Validation Loss: 0.009646628220926752\n",
      "Epoch [19475/20000], Training Loss: 0.012060647193980653, Validation Loss: 0.00474306741637877\n",
      "Epoch [19476/20000], Training Loss: 0.004756657760091392, Validation Loss: 0.00294813330361064\n",
      "Epoch [19477/20000], Training Loss: 0.00806803038059505, Validation Loss: 0.007041403661820108\n",
      "Epoch [19478/20000], Training Loss: 0.010543012827318827, Validation Loss: 0.005900833814065111\n",
      "Epoch [19479/20000], Training Loss: 0.01072073919671571, Validation Loss: 0.004968834166801018\n",
      "Epoch [19480/20000], Training Loss: 0.010767061544076537, Validation Loss: 0.0035137449777055607\n",
      "Epoch [19481/20000], Training Loss: 0.007983013955838811, Validation Loss: 0.003920140207201582\n",
      "Epoch [19482/20000], Training Loss: 0.006683670197162428, Validation Loss: 0.0033323386386103137\n",
      "Epoch [19483/20000], Training Loss: 0.0030723679031195517, Validation Loss: 0.0054565193025674875\n",
      "Epoch [19484/20000], Training Loss: 0.009477321004461763, Validation Loss: 0.004050833057955775\n",
      "Epoch [19485/20000], Training Loss: 0.009794638071834274, Validation Loss: 0.004033972099737834\n",
      "Epoch [19486/20000], Training Loss: 0.008715409008021067, Validation Loss: 0.025524301716359332\n",
      "Epoch [19487/20000], Training Loss: 0.014492580169254714, Validation Loss: 0.00673105460481338\n",
      "Epoch [19488/20000], Training Loss: 0.008476487279819724, Validation Loss: 0.004040551423372563\n",
      "Epoch [19489/20000], Training Loss: 0.003469591093692413, Validation Loss: 0.003696836049844673\n",
      "Epoch [19490/20000], Training Loss: 0.0038718899158993736, Validation Loss: 0.003079039392235115\n",
      "Epoch [19491/20000], Training Loss: 0.004707290713018405, Validation Loss: 0.00244083342175015\n",
      "Epoch [19492/20000], Training Loss: 0.0036853067695119535, Validation Loss: 0.008403963854206609\n",
      "Epoch [19493/20000], Training Loss: 0.008451440821642921, Validation Loss: 0.0026227326263616696\n",
      "Epoch [19494/20000], Training Loss: 0.009668283992401225, Validation Loss: 0.004764439710267457\n",
      "Epoch [19495/20000], Training Loss: 0.004695165057325669, Validation Loss: 0.0028647541745791777\n",
      "Epoch [19496/20000], Training Loss: 0.005679581845210383, Validation Loss: 0.003040201415931928\n",
      "Epoch [19497/20000], Training Loss: 0.0037565195337135393, Validation Loss: 0.014759008596491137\n",
      "Epoch [19498/20000], Training Loss: 0.005696112574176888, Validation Loss: 0.0035082376435866536\n",
      "Epoch [19499/20000], Training Loss: 0.00702831562362657, Validation Loss: 0.007321074251836178\n",
      "Epoch [19500/20000], Training Loss: 0.006774381095380834, Validation Loss: 0.003599325245927178\n",
      "Epoch [19501/20000], Training Loss: 0.0033128476404402007, Validation Loss: 0.002224943584589515\n",
      "Epoch [19502/20000], Training Loss: 0.005477681277527934, Validation Loss: 0.00591981554040696\n",
      "Epoch [19503/20000], Training Loss: 0.007855967244332922, Validation Loss: 0.010556513547300028\n",
      "Epoch [19504/20000], Training Loss: 0.009643432438305613, Validation Loss: 0.004724711564713451\n",
      "Epoch [19505/20000], Training Loss: 0.00781309040719082, Validation Loss: 0.0049407814916061655\n",
      "Epoch [19506/20000], Training Loss: 0.00775476055319554, Validation Loss: 0.0021815412384163274\n",
      "Epoch [19507/20000], Training Loss: 0.011202141399345627, Validation Loss: 0.01660811482318536\n",
      "Epoch [19508/20000], Training Loss: 0.008722096345341665, Validation Loss: 0.011939539092578246\n",
      "Epoch [19509/20000], Training Loss: 0.012734928564896109, Validation Loss: 0.01137653244286087\n",
      "Epoch [19510/20000], Training Loss: 0.013137039325426616, Validation Loss: 0.0029288346182901087\n",
      "Epoch [19511/20000], Training Loss: 0.007955789243690337, Validation Loss: 0.007423753593781969\n",
      "Epoch [19512/20000], Training Loss: 0.00510398144251667, Validation Loss: 0.001846754633934974\n",
      "Epoch [19513/20000], Training Loss: 0.008016631822524036, Validation Loss: 0.0037583500400769155\n",
      "Epoch [19514/20000], Training Loss: 0.009254607897316549, Validation Loss: 0.008508251504951301\n",
      "Epoch [19515/20000], Training Loss: 0.009634017977987241, Validation Loss: 0.0019362501892032924\n",
      "Epoch [19516/20000], Training Loss: 0.007583635898169762, Validation Loss: 0.006965130013575\n",
      "Epoch [19517/20000], Training Loss: 0.004944175771275435, Validation Loss: 0.008739925287305337\n",
      "Epoch [19518/20000], Training Loss: 0.019832014278758185, Validation Loss: 0.004894213522204284\n",
      "Epoch [19519/20000], Training Loss: 0.018021397405976, Validation Loss: 0.03442197570335078\n",
      "Epoch [19520/20000], Training Loss: 0.008640561223598655, Validation Loss: 0.007103871943846636\n",
      "Epoch [19521/20000], Training Loss: 0.008043016903684475, Validation Loss: 0.022163120583773628\n",
      "Epoch [19522/20000], Training Loss: 0.010262281902604238, Validation Loss: 0.012638701439105457\n",
      "Epoch [19523/20000], Training Loss: 0.051973968322300834, Validation Loss: 0.00589723306930117\n",
      "Epoch [19524/20000], Training Loss: 0.02281529141831535, Validation Loss: 0.016586047718841916\n",
      "Epoch [19525/20000], Training Loss: 0.016869069894905157, Validation Loss: 0.0055500492455199234\n",
      "Epoch [19526/20000], Training Loss: 0.011816292409743514, Validation Loss: 0.003698744535378345\n",
      "Epoch [19527/20000], Training Loss: 0.006974218929794266, Validation Loss: 0.006140369447458787\n",
      "Epoch [19528/20000], Training Loss: 0.0050983338987862226, Validation Loss: 0.005181889954769758\n",
      "Epoch [19529/20000], Training Loss: 0.004399825080326991, Validation Loss: 0.0032698225464205927\n",
      "Epoch [19530/20000], Training Loss: 0.003784678167513838, Validation Loss: 0.0031764174289605762\n",
      "Epoch [19531/20000], Training Loss: 0.005368428897262285, Validation Loss: 0.005480643685107874\n",
      "Epoch [19532/20000], Training Loss: 0.004394667077576742, Validation Loss: 0.009325838764474221\n",
      "Epoch [19533/20000], Training Loss: 0.005441731615324638, Validation Loss: 0.020047581896436992\n",
      "Epoch [19534/20000], Training Loss: 0.007461112086316136, Validation Loss: 0.0033903590812087714\n",
      "Epoch [19535/20000], Training Loss: 0.010770402076689476, Validation Loss: 0.0030390248165872435\n",
      "Epoch [19536/20000], Training Loss: 0.010844713444774763, Validation Loss: 0.008235681029727207\n",
      "Epoch [19537/20000], Training Loss: 0.006419974390675114, Validation Loss: 0.003970495854933428\n",
      "Epoch [19538/20000], Training Loss: 0.005659062806444126, Validation Loss: 0.0056234136344244234\n",
      "Epoch [19539/20000], Training Loss: 0.006258618891089489, Validation Loss: 0.0028274152698242816\n",
      "Epoch [19540/20000], Training Loss: 0.005904863441953369, Validation Loss: 0.006066537966262331\n",
      "Epoch [19541/20000], Training Loss: 0.0064097778028683805, Validation Loss: 0.004599417384984658\n",
      "Epoch [19542/20000], Training Loss: 0.01073578920217447, Validation Loss: 0.015964897607986472\n",
      "Epoch [19543/20000], Training Loss: 0.014041693022591062, Validation Loss: 0.01322147295963459\n",
      "Epoch [19544/20000], Training Loss: 0.024115853385800228, Validation Loss: 0.0051081128799036635\n",
      "Epoch [19545/20000], Training Loss: 0.003939584351298565, Validation Loss: 0.007526188420675746\n",
      "Epoch [19546/20000], Training Loss: 0.004068059494394609, Validation Loss: 0.003497133831599188\n",
      "Epoch [19547/20000], Training Loss: 0.007350877891959888, Validation Loss: 0.022733083874003666\n",
      "Epoch [19548/20000], Training Loss: 0.026606927824884354, Validation Loss: 0.01674876587780919\n",
      "Epoch [19549/20000], Training Loss: 0.010029074299755589, Validation Loss: 0.007285477069184968\n",
      "Epoch [19550/20000], Training Loss: 0.016441075158452025, Validation Loss: 0.009522158945269357\n",
      "Epoch [19551/20000], Training Loss: 0.008863209681294393, Validation Loss: 0.008986983280729126\n",
      "Epoch [19552/20000], Training Loss: 0.009928194704116322, Validation Loss: 0.0039529223450317985\n",
      "Epoch [19553/20000], Training Loss: 0.01547106854143619, Validation Loss: 0.011922920071790648\n",
      "Epoch [19554/20000], Training Loss: 0.01434945189703285, Validation Loss: 0.013648322137782998\n",
      "Epoch [19555/20000], Training Loss: 0.01257092107594612, Validation Loss: 0.00361574538853152\n",
      "Epoch [19556/20000], Training Loss: 0.009743627162866428, Validation Loss: 0.01632240876521556\n",
      "Epoch [19557/20000], Training Loss: 0.01142387765503372, Validation Loss: 0.010132352623843741\n",
      "Epoch [19558/20000], Training Loss: 0.013733953888212065, Validation Loss: 0.016170409486871255\n",
      "Epoch [19559/20000], Training Loss: 0.009586283821720696, Validation Loss: 0.004526905981752601\n",
      "Epoch [19560/20000], Training Loss: 0.006198663058707358, Validation Loss: 0.010360800884033747\n",
      "Epoch [19561/20000], Training Loss: 0.012911383086394121, Validation Loss: 0.008463721588961042\n",
      "Epoch [19562/20000], Training Loss: 0.011722042616124131, Validation Loss: 0.005731098695085686\n",
      "Epoch [19563/20000], Training Loss: 0.0043239923472810915, Validation Loss: 0.0038270900835186694\n",
      "Epoch [19564/20000], Training Loss: 0.006191346933649454, Validation Loss: 0.0028172314894569744\n",
      "Epoch [19565/20000], Training Loss: 0.007961707954694117, Validation Loss: 0.024589427128327252\n",
      "Epoch [19566/20000], Training Loss: 0.013145858216505855, Validation Loss: 0.003366539766878824\n",
      "Epoch [19567/20000], Training Loss: 0.006889909435163385, Validation Loss: 0.005278932019775441\n",
      "Epoch [19568/20000], Training Loss: 0.005892305907244528, Validation Loss: 0.0034054345325590213\n",
      "Epoch [19569/20000], Training Loss: 0.0031745667012208806, Validation Loss: 0.0034403739288758777\n",
      "Epoch [19570/20000], Training Loss: 0.008618627100076992, Validation Loss: 0.002504813678172013\n",
      "Epoch [19571/20000], Training Loss: 0.018207282847080414, Validation Loss: 0.006471494723829859\n",
      "Epoch [19572/20000], Training Loss: 0.009029936433892414, Validation Loss: 0.003631025257415591\n",
      "Epoch [19573/20000], Training Loss: 0.006541006042555507, Validation Loss: 0.004781348478413098\n",
      "Epoch [19574/20000], Training Loss: 0.006872603101198495, Validation Loss: 0.026977073648432843\n",
      "Epoch [19575/20000], Training Loss: 0.03254013879658097, Validation Loss: 0.10312553287910475\n",
      "Epoch [19576/20000], Training Loss: 0.0630796839460085, Validation Loss: 0.006275995240944862\n",
      "Epoch [19577/20000], Training Loss: 0.033949927154545936, Validation Loss: 0.04012055125602793\n",
      "Epoch [19578/20000], Training Loss: 0.049896871881043935, Validation Loss: 0.01575486399172275\n",
      "Epoch [19579/20000], Training Loss: 0.02056824216145157, Validation Loss: 0.013004171858288569\n",
      "Epoch [19580/20000], Training Loss: 0.01765020888914088, Validation Loss: 0.015415024048706982\n",
      "Epoch [19581/20000], Training Loss: 0.009956668069987375, Validation Loss: 0.010847982321917828\n",
      "Epoch [19582/20000], Training Loss: 0.009572814314326803, Validation Loss: 0.011743648046571511\n",
      "Epoch [19583/20000], Training Loss: 0.010803145872029876, Validation Loss: 0.005426804678466267\n",
      "Epoch [19584/20000], Training Loss: 0.005794151806704966, Validation Loss: 0.004163147764302266\n",
      "Epoch [19585/20000], Training Loss: 0.004990540798254577, Validation Loss: 0.006333280413595536\n",
      "Epoch [19586/20000], Training Loss: 0.005591818511935084, Validation Loss: 0.004221782729928236\n",
      "Epoch [19587/20000], Training Loss: 0.005517552412389445, Validation Loss: 0.004565733557910789\n",
      "Epoch [19588/20000], Training Loss: 0.003982080208353571, Validation Loss: 0.003589239858099193\n",
      "Epoch [19589/20000], Training Loss: 0.0055107191583374515, Validation Loss: 0.0035503536827751043\n",
      "Epoch [19590/20000], Training Loss: 0.004472727400882702, Validation Loss: 0.0032724280974077502\n",
      "Epoch [19591/20000], Training Loss: 0.004724916376289912, Validation Loss: 0.016188506738282222\n",
      "Epoch [19592/20000], Training Loss: 0.010264614839115893, Validation Loss: 0.0035276661907183943\n",
      "Epoch [19593/20000], Training Loss: 0.004514808201877584, Validation Loss: 0.0036228562371921464\n",
      "Epoch [19594/20000], Training Loss: 0.005204743836657144, Validation Loss: 0.007066743031982371\n",
      "Epoch [19595/20000], Training Loss: 0.005276208718506885, Validation Loss: 0.004964117116888441\n",
      "Epoch [19596/20000], Training Loss: 0.004517006945596742, Validation Loss: 0.003554099918233078\n",
      "Epoch [19597/20000], Training Loss: 0.003540015284670517, Validation Loss: 0.00426201035541648\n",
      "Epoch [19598/20000], Training Loss: 0.004442904693340617, Validation Loss: 0.0033190211961807237\n",
      "Epoch [19599/20000], Training Loss: 0.0058383973706181026, Validation Loss: 0.0030386634493798476\n",
      "Epoch [19600/20000], Training Loss: 0.004841115536172376, Validation Loss: 0.0029129060019629704\n",
      "Epoch [19601/20000], Training Loss: 0.004693121380017017, Validation Loss: 0.004278039907494043\n",
      "Epoch [19602/20000], Training Loss: 0.00524444229189872, Validation Loss: 0.002586145267563162\n",
      "Epoch [19603/20000], Training Loss: 0.005462369066143375, Validation Loss: 0.0036452595468087373\n",
      "Epoch [19604/20000], Training Loss: 0.0040809892987973785, Validation Loss: 0.006677570432405641\n",
      "Epoch [19605/20000], Training Loss: 0.008552580277734836, Validation Loss: 0.010318301553510878\n",
      "Epoch [19606/20000], Training Loss: 0.0029210688984936234, Validation Loss: 0.0032778094140358655\n",
      "Epoch [19607/20000], Training Loss: 0.004561900752118423, Validation Loss: 0.0034747533426196598\n",
      "Epoch [19608/20000], Training Loss: 0.00426908824298725, Validation Loss: 0.002736336575186523\n",
      "Epoch [19609/20000], Training Loss: 0.005686379898439294, Validation Loss: 0.00635768709307121\n",
      "Epoch [19610/20000], Training Loss: 0.010114188544840934, Validation Loss: 0.002478277114176665\n",
      "Epoch [19611/20000], Training Loss: 0.00554847782772023, Validation Loss: 0.012377665726213698\n",
      "Epoch [19612/20000], Training Loss: 0.017962559541047085, Validation Loss: 0.013044224352175402\n",
      "Epoch [19613/20000], Training Loss: 0.017155214671220165, Validation Loss: 0.008723040180066326\n",
      "Epoch [19614/20000], Training Loss: 0.018097110491388384, Validation Loss: 0.03306436079870118\n",
      "Epoch [19615/20000], Training Loss: 0.020182641663788154, Validation Loss: 0.0302968567359382\n",
      "Epoch [19616/20000], Training Loss: 0.014718945192206385, Validation Loss: 0.003746240089473726\n",
      "Epoch [19617/20000], Training Loss: 0.019637585950217078, Validation Loss: 0.004536029918361757\n",
      "Epoch [19618/20000], Training Loss: 0.042313686468073035, Validation Loss: 0.0416539799095647\n",
      "Epoch [19619/20000], Training Loss: 0.02437629638214795, Validation Loss: 0.008861590476889043\n",
      "Epoch [19620/20000], Training Loss: 0.023662957017742365, Validation Loss: 0.0053780306220038\n",
      "Epoch [19621/20000], Training Loss: 0.018078366175700467, Validation Loss: 0.008265382580118188\n",
      "Epoch [19622/20000], Training Loss: 0.014104289169024144, Validation Loss: 0.02056765092636072\n",
      "Epoch [19623/20000], Training Loss: 0.014940027771185018, Validation Loss: 0.009305096781320441\n",
      "Epoch [19624/20000], Training Loss: 0.0108157015825522, Validation Loss: 0.008415210522278877\n",
      "Epoch [19625/20000], Training Loss: 0.012096027702292693, Validation Loss: 0.006256942377311654\n",
      "Epoch [19626/20000], Training Loss: 0.005124601934637342, Validation Loss: 0.004112860555713789\n",
      "Epoch [19627/20000], Training Loss: 0.0052381997458204365, Validation Loss: 0.003302609230494064\n",
      "Epoch [19628/20000], Training Loss: 0.00527865337374221, Validation Loss: 0.00326872480289759\n",
      "Epoch [19629/20000], Training Loss: 0.004110540217620188, Validation Loss: 0.003488915067074621\n",
      "Epoch [19630/20000], Training Loss: 0.003766092663551847, Validation Loss: 0.004293860646612302\n",
      "Epoch [19631/20000], Training Loss: 0.0060810379852763875, Validation Loss: 0.003040911347560333\n",
      "Epoch [19632/20000], Training Loss: 0.004511194760604147, Validation Loss: 0.0033589330206983243\n",
      "Epoch [19633/20000], Training Loss: 0.004787258315835581, Validation Loss: 0.0030122672834360337\n",
      "Epoch [19634/20000], Training Loss: 0.005729697723706652, Validation Loss: 0.00686619413110228\n",
      "Epoch [19635/20000], Training Loss: 0.005597563121097794, Validation Loss: 0.003421709164561954\n",
      "Epoch [19636/20000], Training Loss: 0.004521833425055125, Validation Loss: 0.004122043815868892\n",
      "Epoch [19637/20000], Training Loss: 0.007076120210902965, Validation Loss: 0.004654731191392004\n",
      "Epoch [19638/20000], Training Loss: 0.005211702979327778, Validation Loss: 0.0028080851202828072\n",
      "Epoch [19639/20000], Training Loss: 0.0041557417217908165, Validation Loss: 0.004983823110641035\n",
      "Epoch [19640/20000], Training Loss: 0.006965473615439676, Validation Loss: 0.003353530161932992\n",
      "Epoch [19641/20000], Training Loss: 0.010562513642794005, Validation Loss: 0.00324726047771914\n",
      "Epoch [19642/20000], Training Loss: 0.004491119694908515, Validation Loss: 0.003335082448960033\n",
      "Epoch [19643/20000], Training Loss: 0.004551140261485541, Validation Loss: 0.002605835846095163\n",
      "Epoch [19644/20000], Training Loss: 0.00767800653037349, Validation Loss: 0.00556772113873194\n",
      "Epoch [19645/20000], Training Loss: 0.005265972991765011, Validation Loss: 0.014447314862193496\n",
      "Epoch [19646/20000], Training Loss: 0.010252156758984452, Validation Loss: 0.0032364921859969593\n",
      "Epoch [19647/20000], Training Loss: 0.017854843013213082, Validation Loss: 0.0071176381075994345\n",
      "Epoch [19648/20000], Training Loss: 0.010377535786704226, Validation Loss: 0.0029490844663249455\n",
      "Epoch [19649/20000], Training Loss: 0.013304710154573383, Validation Loss: 0.012273207477051074\n",
      "Epoch [19650/20000], Training Loss: 0.009835189678207306, Validation Loss: 0.004091609301131679\n",
      "Epoch [19651/20000], Training Loss: 0.009711254148766914, Validation Loss: 0.005566997504826888\n",
      "Epoch [19652/20000], Training Loss: 0.006878001647107469, Validation Loss: 0.003398292016884495\n",
      "Epoch [19653/20000], Training Loss: 0.010619266928415527, Validation Loss: 0.01031097048204239\n",
      "Epoch [19654/20000], Training Loss: 0.00667337670685291, Validation Loss: 0.005787626038763197\n",
      "Epoch [19655/20000], Training Loss: 0.0072312013528841946, Validation Loss: 0.008853401307610349\n",
      "Epoch [19656/20000], Training Loss: 0.006478920450068212, Validation Loss: 0.006690881933199729\n",
      "Epoch [19657/20000], Training Loss: 0.00870412444419344, Validation Loss: 0.005631330861474128\n",
      "Epoch [19658/20000], Training Loss: 0.00835590750960234, Validation Loss: 0.0038452869171286303\n",
      "Epoch [19659/20000], Training Loss: 0.005321448059735953, Validation Loss: 0.006557430430904431\n",
      "Epoch [19660/20000], Training Loss: 0.0070714039795478, Validation Loss: 0.003182208150259872\n",
      "Epoch [19661/20000], Training Loss: 0.014661945190320174, Validation Loss: 0.01330851009241282\n",
      "Epoch [19662/20000], Training Loss: 0.021488148122443818, Validation Loss: 0.010975046996496271\n",
      "Epoch [19663/20000], Training Loss: 0.016023520559039234, Validation Loss: 0.005522050412586006\n",
      "Epoch [19664/20000], Training Loss: 0.004743272054481297, Validation Loss: 0.0074689805277167965\n",
      "Epoch [19665/20000], Training Loss: 0.008269272146467952, Validation Loss: 0.004157564357269296\n",
      "Epoch [19666/20000], Training Loss: 0.0049983634245498775, Validation Loss: 0.004333771224651173\n",
      "Epoch [19667/20000], Training Loss: 0.007192683605438788, Validation Loss: 0.0047268619305863725\n",
      "Epoch [19668/20000], Training Loss: 0.006321825234668462, Validation Loss: 0.010103391114887878\n",
      "Epoch [19669/20000], Training Loss: 0.006863513437329922, Validation Loss: 0.00315802222326868\n",
      "Epoch [19670/20000], Training Loss: 0.014320258151721126, Validation Loss: 0.0029175873280402292\n",
      "Epoch [19671/20000], Training Loss: 0.009857597402775095, Validation Loss: 0.03686357534070339\n",
      "Epoch [19672/20000], Training Loss: 0.018748553715080822, Validation Loss: 0.00351591703287519\n",
      "Epoch [19673/20000], Training Loss: 0.005740893931423281, Validation Loss: 0.005674331509234369\n",
      "Epoch [19674/20000], Training Loss: 0.00720549227477412, Validation Loss: 0.0030248190890672178\n",
      "Epoch [19675/20000], Training Loss: 0.003821034841166693, Validation Loss: 0.006498824719658712\n",
      "Epoch [19676/20000], Training Loss: 0.012143885624287318, Validation Loss: 0.01240673290370913\n",
      "Epoch [19677/20000], Training Loss: 0.011932390055465345, Validation Loss: 0.046722249403450586\n",
      "Epoch [19678/20000], Training Loss: 0.024918300469965158, Validation Loss: 0.04098580805027601\n",
      "Epoch [19679/20000], Training Loss: 0.028433613030106893, Validation Loss: 0.00792856346578219\n",
      "Epoch [19680/20000], Training Loss: 0.03470114767385114, Validation Loss: 0.006556838050122289\n",
      "Epoch [19681/20000], Training Loss: 0.025741901629122106, Validation Loss: 0.011890586898240534\n",
      "Epoch [19682/20000], Training Loss: 0.014598110977593544, Validation Loss: 0.004181817899377295\n",
      "Epoch [19683/20000], Training Loss: 0.00798328147876808, Validation Loss: 0.008285983532240542\n",
      "Epoch [19684/20000], Training Loss: 0.0048234410954007345, Validation Loss: 0.0029534365775946625\n",
      "Epoch [19685/20000], Training Loss: 0.004890103294331182, Validation Loss: 0.011199411830298394\n",
      "Epoch [19686/20000], Training Loss: 0.008949675483953408, Validation Loss: 0.003506975287905334\n",
      "Epoch [19687/20000], Training Loss: 0.005301466547929782, Validation Loss: 0.003747135929712288\n",
      "Epoch [19688/20000], Training Loss: 0.005881252915839598, Validation Loss: 0.008527689293621117\n",
      "Epoch [19689/20000], Training Loss: 0.004809179529339807, Validation Loss: 0.0034407638262824287\n",
      "Epoch [19690/20000], Training Loss: 0.007479116811217474, Validation Loss: 0.00275692802883246\n",
      "Epoch [19691/20000], Training Loss: 0.00470844580974829, Validation Loss: 0.00353584617409654\n",
      "Epoch [19692/20000], Training Loss: 0.007128368675954074, Validation Loss: 0.005126576804118242\n",
      "Epoch [19693/20000], Training Loss: 0.006906189108641618, Validation Loss: 0.0034501867861211266\n",
      "Epoch [19694/20000], Training Loss: 0.006715412836326452, Validation Loss: 0.0035985197237096145\n",
      "Epoch [19695/20000], Training Loss: 0.004612559249965541, Validation Loss: 0.0035331469392515047\n",
      "Epoch [19696/20000], Training Loss: 0.005869137942292062, Validation Loss: 0.010867204327301547\n",
      "Epoch [19697/20000], Training Loss: 0.009325903633712837, Validation Loss: 0.005532489726558002\n",
      "Epoch [19698/20000], Training Loss: 0.016647524346418714, Validation Loss: 0.007582172498505965\n",
      "Epoch [19699/20000], Training Loss: 0.0058719737753563095, Validation Loss: 0.0036838282762070257\n",
      "Epoch [19700/20000], Training Loss: 0.003252888359514015, Validation Loss: 0.004834472766296878\n",
      "Epoch [19701/20000], Training Loss: 0.005942489748122171, Validation Loss: 0.0077887493552911535\n",
      "Epoch [19702/20000], Training Loss: 0.007101334581550743, Validation Loss: 0.00423047159646777\n",
      "Epoch [19703/20000], Training Loss: 0.006523187019411125, Validation Loss: 0.002644646766461684\n",
      "Epoch [19704/20000], Training Loss: 0.00579470637421764, Validation Loss: 0.006036347254903619\n",
      "Epoch [19705/20000], Training Loss: 0.00420135423649169, Validation Loss: 0.008736340105891876\n",
      "Epoch [19706/20000], Training Loss: 0.004710162676149464, Validation Loss: 0.006238173632426529\n",
      "Epoch [19707/20000], Training Loss: 0.004597456999493131, Validation Loss: 0.0025437957511351378\n",
      "Epoch [19708/20000], Training Loss: 0.009878573045300851, Validation Loss: 0.002343877458825107\n",
      "Epoch [19709/20000], Training Loss: 0.013038298021066501, Validation Loss: 0.00671319874992398\n",
      "Epoch [19710/20000], Training Loss: 0.013451618130992367, Validation Loss: 0.003688817377313366\n",
      "Epoch [19711/20000], Training Loss: 0.0057539856796730805, Validation Loss: 0.020183840326646014\n",
      "Epoch [19712/20000], Training Loss: 0.00912106472527999, Validation Loss: 0.007297921190380562\n",
      "Epoch [19713/20000], Training Loss: 0.004646288920152334, Validation Loss: 0.0028120337798002637\n",
      "Epoch [19714/20000], Training Loss: 0.003767384533213252, Validation Loss: 0.002928784328694129\n",
      "Epoch [19715/20000], Training Loss: 0.005427959133937422, Validation Loss: 0.0025056389544074265\n",
      "Epoch [19716/20000], Training Loss: 0.003194978137409115, Validation Loss: 0.006617481920879901\n",
      "Epoch [19717/20000], Training Loss: 0.004828776072827168, Validation Loss: 0.0033348771992913434\n",
      "Epoch [19718/20000], Training Loss: 0.003955583828639776, Validation Loss: 0.01957604281758815\n",
      "Epoch [19719/20000], Training Loss: 0.013514796697563725, Validation Loss: 0.004756593911890507\n",
      "Epoch [19720/20000], Training Loss: 0.005893360037589446, Validation Loss: 0.014202249317771174\n",
      "Epoch [19721/20000], Training Loss: 0.008290395132332509, Validation Loss: 0.005929476670304317\n",
      "Epoch [19722/20000], Training Loss: 0.01030996156857457, Validation Loss: 0.0030241973052085314\n",
      "Epoch [19723/20000], Training Loss: 0.019220361562474864, Validation Loss: 0.014870688935169645\n",
      "Epoch [19724/20000], Training Loss: 0.018154751520861673, Validation Loss: 0.004847243988097359\n",
      "Epoch [19725/20000], Training Loss: 0.013944306399935158, Validation Loss: 0.008044237857506329\n",
      "Epoch [19726/20000], Training Loss: 0.01811134282797866, Validation Loss: 0.008914940134648728\n",
      "Epoch [19727/20000], Training Loss: 0.014943652187607117, Validation Loss: 0.0038035444729993223\n",
      "Epoch [19728/20000], Training Loss: 0.008065910180968265, Validation Loss: 0.010004413313018696\n",
      "Epoch [19729/20000], Training Loss: 0.00817149787949997, Validation Loss: 0.006150326153095266\n",
      "Epoch [19730/20000], Training Loss: 0.007267749132422198, Validation Loss: 0.009342242328717784\n",
      "Epoch [19731/20000], Training Loss: 0.0074914706929649, Validation Loss: 0.0025814431195913456\n",
      "Epoch [19732/20000], Training Loss: 0.0059189055194061825, Validation Loss: 0.0049639258208295746\n",
      "Epoch [19733/20000], Training Loss: 0.004333596662036143, Validation Loss: 0.0035028820334892835\n",
      "Epoch [19734/20000], Training Loss: 0.005056648783335861, Validation Loss: 0.007152178681405782\n",
      "Epoch [19735/20000], Training Loss: 0.007378079953403878, Validation Loss: 0.00967729678656529\n",
      "Epoch [19736/20000], Training Loss: 0.011200193995007015, Validation Loss: 0.005019999625438183\n",
      "Epoch [19737/20000], Training Loss: 0.00788609343830363, Validation Loss: 0.0038216543917956513\n",
      "Epoch [19738/20000], Training Loss: 0.007730148036249115, Validation Loss: 0.005030728742227097\n",
      "Epoch [19739/20000], Training Loss: 0.012968251527386851, Validation Loss: 0.007777573195703269\n",
      "Epoch [19740/20000], Training Loss: 0.006537455496852219, Validation Loss: 0.0044126554586471555\n",
      "Epoch [19741/20000], Training Loss: 0.007040843933802015, Validation Loss: 0.005113738176849633\n",
      "Epoch [19742/20000], Training Loss: 0.00866990650131681, Validation Loss: 0.006245065494803319\n",
      "Epoch [19743/20000], Training Loss: 0.01653668023936916, Validation Loss: 0.002130766967301919\n",
      "Epoch [19744/20000], Training Loss: 0.011857896654484128, Validation Loss: 0.003234950077587736\n",
      "Epoch [19745/20000], Training Loss: 0.006900734968705168, Validation Loss: 0.012431683225269044\n",
      "Epoch [19746/20000], Training Loss: 0.008191107443833192, Validation Loss: 0.0022985300368044853\n",
      "Epoch [19747/20000], Training Loss: 0.0038213727025972082, Validation Loss: 0.0028637737216854475\n",
      "Epoch [19748/20000], Training Loss: 0.0053037504419210435, Validation Loss: 0.002856656846627662\n",
      "Epoch [19749/20000], Training Loss: 0.0045186364134549096, Validation Loss: 0.0067279985305417666\n",
      "Epoch [19750/20000], Training Loss: 0.004318995965637181, Validation Loss: 0.006680757634939027\n",
      "Epoch [19751/20000], Training Loss: 0.009329291052251523, Validation Loss: 0.0025640215187101895\n",
      "Epoch [19752/20000], Training Loss: 0.0031564252224468093, Validation Loss: 0.00796262075774631\n",
      "Epoch [19753/20000], Training Loss: 0.010336763199331602, Validation Loss: 0.003603261437160451\n",
      "Epoch [19754/20000], Training Loss: 0.007559360146322953, Validation Loss: 0.0037077648423543713\n",
      "Epoch [19755/20000], Training Loss: 0.008099059603673757, Validation Loss: 0.004990487929281885\n",
      "Epoch [19756/20000], Training Loss: 0.011662827848340385, Validation Loss: 0.005709861567569062\n",
      "Epoch [19757/20000], Training Loss: 0.010624263832661589, Validation Loss: 0.017723625329433088\n",
      "Epoch [19758/20000], Training Loss: 0.011465904070064425, Validation Loss: 0.004746414346998661\n",
      "Epoch [19759/20000], Training Loss: 0.008450852070382098, Validation Loss: 0.005991130744665237\n",
      "Epoch [19760/20000], Training Loss: 0.009911487223819546, Validation Loss: 0.0038615955544604014\n",
      "Epoch [19761/20000], Training Loss: 0.009587356100155375, Validation Loss: 0.0036251694301552917\n",
      "Epoch [19762/20000], Training Loss: 0.004891467931364397, Validation Loss: 0.00223898468426244\n",
      "Epoch [19763/20000], Training Loss: 0.00709316083729001, Validation Loss: 0.003768407603497995\n",
      "Epoch [19764/20000], Training Loss: 0.012833886306387805, Validation Loss: 0.027337557425456436\n",
      "Epoch [19765/20000], Training Loss: 0.006834731537050435, Validation Loss: 0.00328687927668625\n",
      "Epoch [19766/20000], Training Loss: 0.007902118154951105, Validation Loss: 0.00261127192769095\n",
      "Epoch [19767/20000], Training Loss: 0.006375863703267116, Validation Loss: 0.007942254210488744\n",
      "Epoch [19768/20000], Training Loss: 0.007425522396239614, Validation Loss: 0.0046505515485130945\n",
      "Epoch [19769/20000], Training Loss: 0.011966610431305267, Validation Loss: 0.004187987652106325\n",
      "Epoch [19770/20000], Training Loss: 0.008417168626660245, Validation Loss: 0.0090368382338722\n",
      "Epoch [19771/20000], Training Loss: 0.0028428138247887447, Validation Loss: 0.0028877637967172276\n",
      "Epoch [19772/20000], Training Loss: 0.010970640983681992, Validation Loss: 0.0030746771318307864\n",
      "Epoch [19773/20000], Training Loss: 0.007135671349325483, Validation Loss: 0.006635683268095859\n",
      "Epoch [19774/20000], Training Loss: 0.004963252280292961, Validation Loss: 0.002358762438820098\n",
      "Epoch [19775/20000], Training Loss: 0.005003297173451366, Validation Loss: 0.0023333286721387886\n",
      "Epoch [19776/20000], Training Loss: 0.007703780024582686, Validation Loss: 0.009740787054992341\n",
      "Epoch [19777/20000], Training Loss: 0.0075875318449598285, Validation Loss: 0.0022314662103844235\n",
      "Epoch [19778/20000], Training Loss: 0.003692460768174247, Validation Loss: 0.00438248055263151\n",
      "Epoch [19779/20000], Training Loss: 0.004854863955340453, Validation Loss: 0.0036395349666418853\n",
      "Epoch [19780/20000], Training Loss: 0.0067354542698012665, Validation Loss: 0.0041287233434808\n",
      "Epoch [19781/20000], Training Loss: 0.00932282485389026, Validation Loss: 0.013548961942654156\n",
      "Epoch [19782/20000], Training Loss: 0.007696384987087056, Validation Loss: 0.011124872521645355\n",
      "Epoch [19783/20000], Training Loss: 0.0065516819521625235, Validation Loss: 0.007191639929743587\n",
      "Epoch [19784/20000], Training Loss: 0.00569023814928674, Validation Loss: 0.008606709297351602\n",
      "Epoch [19785/20000], Training Loss: 0.024864437455887258, Validation Loss: 0.018989363190010117\n",
      "Epoch [19786/20000], Training Loss: 0.008543680573244014, Validation Loss: 0.00700023154278132\n",
      "Epoch [19787/20000], Training Loss: 0.012382004344544839, Validation Loss: 0.005515909812690519\n",
      "Epoch [19788/20000], Training Loss: 0.009756183868635812, Validation Loss: 0.0028203648735904657\n",
      "Epoch [19789/20000], Training Loss: 0.03528816624956172, Validation Loss: 0.002906499071223883\n",
      "Epoch [19790/20000], Training Loss: 0.02685724135829202, Validation Loss: 0.014708530096200434\n",
      "Epoch [19791/20000], Training Loss: 0.029803855378206987, Validation Loss: 0.07291202672891638\n",
      "Epoch [19792/20000], Training Loss: 0.03478228351329855, Validation Loss: 0.009470227056268194\n",
      "Epoch [19793/20000], Training Loss: 0.012161272251562747, Validation Loss: 0.004674321782532941\n",
      "Epoch [19794/20000], Training Loss: 0.006411329585327101, Validation Loss: 0.00342181952003021\n",
      "Epoch [19795/20000], Training Loss: 0.0050094326449782655, Validation Loss: 0.003417369101437576\n",
      "Epoch [19796/20000], Training Loss: 0.0046431568542694935, Validation Loss: 0.003798236632836246\n",
      "Epoch [19797/20000], Training Loss: 0.004963988016243093, Validation Loss: 0.0031246431304968575\n",
      "Epoch [19798/20000], Training Loss: 0.006985015888598615, Validation Loss: 0.003030372903887318\n",
      "Epoch [19799/20000], Training Loss: 0.009416197468194045, Validation Loss: 0.00605545386864037\n",
      "Epoch [19800/20000], Training Loss: 0.005745729949142385, Validation Loss: 0.0029982794112705896\n",
      "Epoch [19801/20000], Training Loss: 0.007358658907053593, Validation Loss: 0.017930656130732\n",
      "Epoch [19802/20000], Training Loss: 0.01334634394567859, Validation Loss: 0.00453239499873348\n",
      "Epoch [19803/20000], Training Loss: 0.006045173154104434, Validation Loss: 0.004091180227469238\n",
      "Epoch [19804/20000], Training Loss: 0.004691819627104061, Validation Loss: 0.002491275441636885\n",
      "Epoch [19805/20000], Training Loss: 0.005015753384546835, Validation Loss: 0.0030709393753686038\n",
      "Epoch [19806/20000], Training Loss: 0.005737305964950272, Validation Loss: 0.0022892398758703587\n",
      "Epoch [19807/20000], Training Loss: 0.004480729918993477, Validation Loss: 0.004693727416340364\n",
      "Epoch [19808/20000], Training Loss: 0.009870444181095601, Validation Loss: 0.007779914550656234\n",
      "Epoch [19809/20000], Training Loss: 0.01559724182266109, Validation Loss: 0.024399531127997927\n",
      "Epoch [19810/20000], Training Loss: 0.01189416643322667, Validation Loss: 0.0028373208645068155\n",
      "Epoch [19811/20000], Training Loss: 0.013564144516879293, Validation Loss: 0.0030100934630305737\n",
      "Epoch [19812/20000], Training Loss: 0.008065332291672738, Validation Loss: 0.004291669207304949\n",
      "Epoch [19813/20000], Training Loss: 0.0077900346555647305, Validation Loss: 0.0034747738633384984\n",
      "Epoch [19814/20000], Training Loss: 0.004033992323262022, Validation Loss: 0.004851447375425469\n",
      "Epoch [19815/20000], Training Loss: 0.005573743326489681, Validation Loss: 0.0034948610751338754\n",
      "Epoch [19816/20000], Training Loss: 0.007344727677783729, Validation Loss: 0.028798823174513797\n",
      "Epoch [19817/20000], Training Loss: 0.01954556527743989, Validation Loss: 0.010341267532958958\n",
      "Epoch [19818/20000], Training Loss: 0.008323142144685594, Validation Loss: 0.004513745598971346\n",
      "Epoch [19819/20000], Training Loss: 0.005033403612547512, Validation Loss: 0.011920753598905114\n",
      "Epoch [19820/20000], Training Loss: 0.013372541908730844, Validation Loss: 0.0029275968269197555\n",
      "Epoch [19821/20000], Training Loss: 0.00849501019339576, Validation Loss: 0.003344333263896715\n",
      "Epoch [19822/20000], Training Loss: 0.005057929561839306, Validation Loss: 0.006350579598159608\n",
      "Epoch [19823/20000], Training Loss: 0.011591910006180635, Validation Loss: 0.005481662327284705\n",
      "Epoch [19824/20000], Training Loss: 0.007985674562015837, Validation Loss: 0.022188324243633213\n",
      "Epoch [19825/20000], Training Loss: 0.0089688142387396, Validation Loss: 0.003291468033812147\n",
      "Epoch [19826/20000], Training Loss: 0.009218273894865498, Validation Loss: 0.0022353216188568986\n",
      "Epoch [19827/20000], Training Loss: 0.007549656898001038, Validation Loss: 0.018510988509482167\n",
      "Epoch [19828/20000], Training Loss: 0.00964954964833201, Validation Loss: 0.005477686269480435\n",
      "Epoch [19829/20000], Training Loss: 0.020148955698687132, Validation Loss: 0.010332958833089887\n",
      "Epoch [19830/20000], Training Loss: 0.08340280862843688, Validation Loss: 0.12889115432435183\n",
      "Epoch [19831/20000], Training Loss: 0.0343433793289023, Validation Loss: 0.0075108151396699385\n",
      "Epoch [19832/20000], Training Loss: 0.013892284672432911, Validation Loss: 0.018036579224306997\n",
      "Epoch [19833/20000], Training Loss: 0.014694417943246663, Validation Loss: 0.005626135905247901\n",
      "Epoch [19834/20000], Training Loss: 0.008373792744740578, Validation Loss: 0.007689889276016564\n",
      "Epoch [19835/20000], Training Loss: 0.005538081209773996, Validation Loss: 0.0044596466723955275\n",
      "Epoch [19836/20000], Training Loss: 0.0064446580716841185, Validation Loss: 0.008737861717055548\n",
      "Epoch [19837/20000], Training Loss: 0.008900883709429763, Validation Loss: 0.004155801453399428\n",
      "Epoch [19838/20000], Training Loss: 0.007271758049942686, Validation Loss: 0.007256697137198184\n",
      "Epoch [19839/20000], Training Loss: 0.013849626118566707, Validation Loss: 0.0041425152194035164\n",
      "Epoch [19840/20000], Training Loss: 0.00857191342402075, Validation Loss: 0.007558163581803358\n",
      "Epoch [19841/20000], Training Loss: 0.005454591435068323, Validation Loss: 0.004180412179065586\n",
      "Epoch [19842/20000], Training Loss: 0.008596487997108073, Validation Loss: 0.0037607866102819442\n",
      "Epoch [19843/20000], Training Loss: 0.00825404241706045, Validation Loss: 0.004725134854282942\n",
      "Epoch [19844/20000], Training Loss: 0.009108126593803587, Validation Loss: 0.013897898156460744\n",
      "Epoch [19845/20000], Training Loss: 0.006892946340875434, Validation Loss: 0.005000742563193431\n",
      "Epoch [19846/20000], Training Loss: 0.006516836366017742, Validation Loss: 0.005689041949276584\n",
      "Epoch [19847/20000], Training Loss: 0.006135176713412095, Validation Loss: 0.004836810787294\n",
      "Epoch [19848/20000], Training Loss: 0.009868337919345192, Validation Loss: 0.015119127428273973\n",
      "Epoch [19849/20000], Training Loss: 0.010960010848845871, Validation Loss: 0.006697026592474588\n",
      "Epoch [19850/20000], Training Loss: 0.008532790365669436, Validation Loss: 0.0058823231034332525\n",
      "Epoch [19851/20000], Training Loss: 0.008964708960515313, Validation Loss: 0.004097567336990389\n",
      "Epoch [19852/20000], Training Loss: 0.006002766458550468, Validation Loss: 0.010659393151804863\n",
      "Epoch [19853/20000], Training Loss: 0.007298216500203125, Validation Loss: 0.00466848793612838\n",
      "Epoch [19854/20000], Training Loss: 0.005057402527069955, Validation Loss: 0.003093776249743639\n",
      "Epoch [19855/20000], Training Loss: 0.004974167596498903, Validation Loss: 0.004723651528706796\n",
      "Epoch [19856/20000], Training Loss: 0.008245122402773373, Validation Loss: 0.015194398538405096\n",
      "Epoch [19857/20000], Training Loss: 0.008380538082877007, Validation Loss: 0.0056915289437086914\n",
      "Epoch [19858/20000], Training Loss: 0.010219581907741875, Validation Loss: 0.004616443685645574\n",
      "Epoch [19859/20000], Training Loss: 0.0058779127024795996, Validation Loss: 0.010392315395457834\n",
      "Epoch [19860/20000], Training Loss: 0.01390141002567751, Validation Loss: 0.006572093033712788\n",
      "Epoch [19861/20000], Training Loss: 0.013271377650588485, Validation Loss: 0.00430326761183762\n",
      "Epoch [19862/20000], Training Loss: 0.008484612929480915, Validation Loss: 0.006185430036486496\n",
      "Epoch [19863/20000], Training Loss: 0.009573068349648597, Validation Loss: 0.005919845163709121\n",
      "Epoch [19864/20000], Training Loss: 0.006547161396676009, Validation Loss: 0.003132491963570873\n",
      "Epoch [19865/20000], Training Loss: 0.003925590917658285, Validation Loss: 0.0030200189000006666\n",
      "Epoch [19866/20000], Training Loss: 0.003577928040093476, Validation Loss: 0.0034859156854452067\n",
      "Epoch [19867/20000], Training Loss: 0.0033737338827839786, Validation Loss: 0.005661085904693307\n",
      "Epoch [19868/20000], Training Loss: 0.003896407942322964, Validation Loss: 0.01794369669854642\n",
      "Epoch [19869/20000], Training Loss: 0.010611987675344738, Validation Loss: 0.011007973806964546\n",
      "Epoch [19870/20000], Training Loss: 0.017740889130176844, Validation Loss: 0.016037002525623376\n",
      "Epoch [19871/20000], Training Loss: 0.02755460046319058, Validation Loss: 0.027742696495287027\n",
      "Epoch [19872/20000], Training Loss: 0.02427468089786089, Validation Loss: 0.06421029145885118\n",
      "Epoch [19873/20000], Training Loss: 0.0368218596540828, Validation Loss: 0.03830599054656757\n",
      "Epoch [19874/20000], Training Loss: 0.03483077816365819, Validation Loss: 0.0072649686696329195\n",
      "Epoch [19875/20000], Training Loss: 0.00835171268304943, Validation Loss: 0.0099856155301349\n",
      "Epoch [19876/20000], Training Loss: 0.006809973474154114, Validation Loss: 0.0037667943872220023\n",
      "Epoch [19877/20000], Training Loss: 0.005999362010957806, Validation Loss: 0.0038239256685212047\n",
      "Epoch [19878/20000], Training Loss: 0.0043445239766047705, Validation Loss: 0.004888760115207447\n",
      "Epoch [19879/20000], Training Loss: 0.004347060637038729, Validation Loss: 0.0031378826117369435\n",
      "Epoch [19880/20000], Training Loss: 0.0046632165883368415, Validation Loss: 0.008347023976815495\n",
      "Epoch [19881/20000], Training Loss: 0.00747612800997948, Validation Loss: 0.005934709367707569\n",
      "Epoch [19882/20000], Training Loss: 0.006192206216691635, Validation Loss: 0.01051944426985431\n",
      "Epoch [19883/20000], Training Loss: 0.011776988028681703, Validation Loss: 0.004890707891127428\n",
      "Epoch [19884/20000], Training Loss: 0.0049960067782584315, Validation Loss: 0.019007519865565165\n",
      "Epoch [19885/20000], Training Loss: 0.018983403053425718, Validation Loss: 0.006446064460272944\n",
      "Epoch [19886/20000], Training Loss: 0.009442270647923579, Validation Loss: 0.009469770698921362\n",
      "Epoch [19887/20000], Training Loss: 0.006446536025708026, Validation Loss: 0.005454053163154933\n",
      "Epoch [19888/20000], Training Loss: 0.006678742601902091, Validation Loss: 0.0046085703918247335\n",
      "Epoch [19889/20000], Training Loss: 0.004541213341456439, Validation Loss: 0.0034817682677820627\n",
      "Epoch [19890/20000], Training Loss: 0.0038253890130103435, Validation Loss: 0.004734046396086354\n",
      "Epoch [19891/20000], Training Loss: 0.0060247419057434725, Validation Loss: 0.002657513107838635\n",
      "Epoch [19892/20000], Training Loss: 0.00550150779391905, Validation Loss: 0.0029140620101598203\n",
      "Epoch [19893/20000], Training Loss: 0.006388439642510743, Validation Loss: 0.0060730511649264175\n",
      "Epoch [19894/20000], Training Loss: 0.007132045338429245, Validation Loss: 0.005004354715630909\n",
      "Epoch [19895/20000], Training Loss: 0.005346794505645188, Validation Loss: 0.0028373928018682803\n",
      "Epoch [19896/20000], Training Loss: 0.003904280910709141, Validation Loss: 0.004314865644009355\n",
      "Epoch [19897/20000], Training Loss: 0.004073151760573117, Validation Loss: 0.007204228098797775\n",
      "Epoch [19898/20000], Training Loss: 0.004473214201945146, Validation Loss: 0.007449567627678074\n",
      "Epoch [19899/20000], Training Loss: 0.007560149876683552, Validation Loss: 0.005775303125727557\n",
      "Epoch [19900/20000], Training Loss: 0.0038003121999721223, Validation Loss: 0.0047329834001861625\n",
      "Epoch [19901/20000], Training Loss: 0.004549520587066321, Validation Loss: 0.007998667517880951\n",
      "Epoch [19902/20000], Training Loss: 0.0043125191826090615, Validation Loss: 0.012543216715043468\n",
      "Epoch [19903/20000], Training Loss: 0.019941204573093785, Validation Loss: 0.04010141745844261\n",
      "Epoch [19904/20000], Training Loss: 0.05847452672397984, Validation Loss: 0.022874681073553966\n",
      "Epoch [19905/20000], Training Loss: 0.11279253776072957, Validation Loss: 0.07415685972994927\n",
      "Epoch [19906/20000], Training Loss: 0.04115715907287917, Validation Loss: 0.032282164203699115\n",
      "Epoch [19907/20000], Training Loss: 0.020852019511429325, Validation Loss: 0.014372282336499407\n",
      "Epoch [19908/20000], Training Loss: 0.016021530842408538, Validation Loss: 0.011244704616850803\n",
      "Epoch [19909/20000], Training Loss: 0.010468947501587016, Validation Loss: 0.008782944549766723\n",
      "Epoch [19910/20000], Training Loss: 0.009410394491169427, Validation Loss: 0.006240132880833471\n",
      "Epoch [19911/20000], Training Loss: 0.006892541872470507, Validation Loss: 0.004809724254300818\n",
      "Epoch [19912/20000], Training Loss: 0.008587160671595484, Validation Loss: 0.0051878254659826996\n",
      "Epoch [19913/20000], Training Loss: 0.0057400757785736846, Validation Loss: 0.0036159347872570485\n",
      "Epoch [19914/20000], Training Loss: 0.005229291281596359, Validation Loss: 0.002640552677571049\n",
      "Epoch [19915/20000], Training Loss: 0.003955025785087075, Validation Loss: 0.0028407446495243454\n",
      "Epoch [19916/20000], Training Loss: 0.006561744165082928, Validation Loss: 0.00228139343143471\n",
      "Epoch [19917/20000], Training Loss: 0.008678736943206136, Validation Loss: 0.01073898801485981\n",
      "Epoch [19918/20000], Training Loss: 0.010384999897983757, Validation Loss: 0.002861902499315632\n",
      "Epoch [19919/20000], Training Loss: 0.01084489137718135, Validation Loss: 0.0019470755378974453\n",
      "Epoch [19920/20000], Training Loss: 0.006270918252994306, Validation Loss: 0.0017577090925696229\n",
      "Epoch [19921/20000], Training Loss: 0.006622125833798366, Validation Loss: 0.008323630091531\n",
      "Epoch [19922/20000], Training Loss: 0.014988098669294101, Validation Loss: 0.007578976137986209\n",
      "Epoch [19923/20000], Training Loss: 0.008782970573422842, Validation Loss: 0.016622702834461274\n",
      "Epoch [19924/20000], Training Loss: 0.007952364749404037, Validation Loss: 0.002231804828115485\n",
      "Epoch [19925/20000], Training Loss: 0.012328459027360492, Validation Loss: 0.017915810258583877\n",
      "Epoch [19926/20000], Training Loss: 0.025899556082419752, Validation Loss: 0.03774257806242271\n",
      "Epoch [19927/20000], Training Loss: 0.02008780415781075, Validation Loss: 0.022391510877317295\n",
      "Epoch [19928/20000], Training Loss: 0.014836782990771877, Validation Loss: 0.0070425259761344805\n",
      "Epoch [19929/20000], Training Loss: 0.008708800495826705, Validation Loss: 0.012680408939319574\n",
      "Epoch [19930/20000], Training Loss: 0.009506334719065177, Validation Loss: 0.0023414977750394633\n",
      "Epoch [19931/20000], Training Loss: 0.005843207053008622, Validation Loss: 0.029405326804750942\n",
      "Epoch [19932/20000], Training Loss: 0.03316355256695098, Validation Loss: 0.021481475507726524\n",
      "Epoch [19933/20000], Training Loss: 0.025082839295334582, Validation Loss: 0.02144734191113987\n",
      "Epoch [19934/20000], Training Loss: 0.01484229946800042, Validation Loss: 0.01644575543070914\n",
      "Epoch [19935/20000], Training Loss: 0.005839171013836416, Validation Loss: 0.010750235472894585\n",
      "Epoch [19936/20000], Training Loss: 0.004731399038454194, Validation Loss: 0.009478882379720435\n",
      "Epoch [19937/20000], Training Loss: 0.010094861376273911, Validation Loss: 0.005677870811658587\n",
      "Epoch [19938/20000], Training Loss: 0.005905099444914542, Validation Loss: 0.0029788228343581935\n",
      "Epoch [19939/20000], Training Loss: 0.007771042642236888, Validation Loss: 0.007693481203783595\n",
      "Epoch [19940/20000], Training Loss: 0.010134301516310578, Validation Loss: 0.0027740381415338584\n",
      "Epoch [19941/20000], Training Loss: 0.014812081325154785, Validation Loss: 0.044809528237872216\n",
      "Epoch [19942/20000], Training Loss: 0.011004507516710354, Validation Loss: 0.0033524854908318752\n",
      "Epoch [19943/20000], Training Loss: 0.014342341912976866, Validation Loss: 0.0064056600375147965\n",
      "Epoch [19944/20000], Training Loss: 0.014437823010356152, Validation Loss: 0.023953987598529482\n",
      "Epoch [19945/20000], Training Loss: 0.01464987486438726, Validation Loss: 0.005323434372447977\n",
      "Epoch [19946/20000], Training Loss: 0.008921729968278669, Validation Loss: 0.004935698324638439\n",
      "Epoch [19947/20000], Training Loss: 0.00628465913385818, Validation Loss: 0.005263556664980383\n",
      "Epoch [19948/20000], Training Loss: 0.007268832584356589, Validation Loss: 0.030030144563254067\n",
      "Epoch [19949/20000], Training Loss: 0.018580117067058057, Validation Loss: 0.004397313438215684\n",
      "Epoch [19950/20000], Training Loss: 0.0052354672495442045, Validation Loss: 0.004730448678574005\n",
      "Epoch [19951/20000], Training Loss: 0.006068896097401323, Validation Loss: 0.010570245630041037\n",
      "Epoch [19952/20000], Training Loss: 0.010850455873878673, Validation Loss: 0.011924200360845865\n",
      "Epoch [19953/20000], Training Loss: 0.008186567926064267, Validation Loss: 0.007117571909826584\n",
      "Epoch [19954/20000], Training Loss: 0.008256058030383429, Validation Loss: 0.004288266733099582\n",
      "Epoch [19955/20000], Training Loss: 0.008080457495192863, Validation Loss: 0.012576201897058072\n",
      "Epoch [19956/20000], Training Loss: 0.01188941332890993, Validation Loss: 0.0028485664719280885\n",
      "Epoch [19957/20000], Training Loss: 0.011079295236933311, Validation Loss: 0.005564787204828495\n",
      "Epoch [19958/20000], Training Loss: 0.004534380534486055, Validation Loss: 0.00397601406925168\n",
      "Epoch [19959/20000], Training Loss: 0.0046507383025084425, Validation Loss: 0.0042625982090128845\n",
      "Epoch [19960/20000], Training Loss: 0.0037263485407623064, Validation Loss: 0.005293888772656479\n",
      "Epoch [19961/20000], Training Loss: 0.010517307274442698, Validation Loss: 0.0035142306342826096\n",
      "Epoch [19962/20000], Training Loss: 0.008510849308160167, Validation Loss: 0.018066134116642374\n",
      "Epoch [19963/20000], Training Loss: 0.006081530518388588, Validation Loss: 0.0037659847649193423\n",
      "Epoch [19964/20000], Training Loss: 0.014366743808620543, Validation Loss: 0.007204339833908843\n",
      "Epoch [19965/20000], Training Loss: 0.006285046102021852, Validation Loss: 0.0026973433437511574\n",
      "Epoch [19966/20000], Training Loss: 0.003708651138627569, Validation Loss: 0.00324846203789824\n",
      "Epoch [19967/20000], Training Loss: 0.003510644161646529, Validation Loss: 0.005514942698647195\n",
      "Epoch [19968/20000], Training Loss: 0.007199870665967215, Validation Loss: 0.0024203151837330745\n",
      "Epoch [19969/20000], Training Loss: 0.019368151772466393, Validation Loss: 0.002524757177012203\n",
      "Epoch [19970/20000], Training Loss: 0.005349094076726553, Validation Loss: 0.010831758320740268\n",
      "Epoch [19971/20000], Training Loss: 0.007828587247006778, Validation Loss: 0.0042694333021976606\n",
      "Epoch [19972/20000], Training Loss: 0.0061885795980093205, Validation Loss: 0.0027424979969636254\n",
      "Epoch [19973/20000], Training Loss: 0.010461091453537717, Validation Loss: 0.023537142180747\n",
      "Epoch [19974/20000], Training Loss: 0.0065632174228085205, Validation Loss: 0.004516910146687282\n",
      "Epoch [19975/20000], Training Loss: 0.007789298917227175, Validation Loss: 0.005421680012136372\n",
      "Epoch [19976/20000], Training Loss: 0.011584597082609045, Validation Loss: 0.005792864052924784\n",
      "Epoch [19977/20000], Training Loss: 0.03664822386287726, Validation Loss: 0.012873646920591295\n",
      "Epoch [19978/20000], Training Loss: 0.015736381704170656, Validation Loss: 0.006341214838381047\n",
      "Epoch [19979/20000], Training Loss: 0.006582217942195712, Validation Loss: 0.0038562418804514004\n",
      "Epoch [19980/20000], Training Loss: 0.009833256300095985, Validation Loss: 0.004012808725350782\n",
      "Epoch [19981/20000], Training Loss: 0.01296934185497126, Validation Loss: 0.056530995075365374\n",
      "Epoch [19982/20000], Training Loss: 0.04918650847789, Validation Loss: 0.04423415948533812\n",
      "Epoch [19983/20000], Training Loss: 0.014874747434597728, Validation Loss: 0.013353166296882688\n",
      "Epoch [19984/20000], Training Loss: 0.0065221396655813025, Validation Loss: 0.004736997237031915\n",
      "Epoch [19985/20000], Training Loss: 0.005498270770268781, Validation Loss: 0.003931635577777521\n",
      "Epoch [19986/20000], Training Loss: 0.00516588832916958, Validation Loss: 0.003637456699378459\n",
      "Epoch [19987/20000], Training Loss: 0.005687605213357269, Validation Loss: 0.0037196022004139756\n",
      "Epoch [19988/20000], Training Loss: 0.004445890663191676, Validation Loss: 0.004331892078945084\n",
      "Epoch [19989/20000], Training Loss: 0.004981866498253241, Validation Loss: 0.0031507505374313333\n",
      "Epoch [19990/20000], Training Loss: 0.005067336851165497, Validation Loss: 0.003491796414005575\n",
      "Epoch [19991/20000], Training Loss: 0.005550267330753351, Validation Loss: 0.003037186110974334\n",
      "Epoch [19992/20000], Training Loss: 0.0035592392045405825, Validation Loss: 0.003440510987603166\n",
      "Epoch [19993/20000], Training Loss: 0.004046257265144959, Validation Loss: 0.002735239376598792\n",
      "Epoch [19994/20000], Training Loss: 0.004334010004510803, Validation Loss: 0.004519375137499638\n",
      "Epoch [19995/20000], Training Loss: 0.005272970098425438, Validation Loss: 0.00982031700746333\n",
      "Epoch [19996/20000], Training Loss: 0.005412524044524096, Validation Loss: 0.004699693570723846\n",
      "Epoch [19997/20000], Training Loss: 0.005844647441075982, Validation Loss: 0.002205266055305799\n",
      "Epoch [19998/20000], Training Loss: 0.006130751240561949, Validation Loss: 0.002131693976029985\n",
      "Epoch [19999/20000], Training Loss: 0.013862281017022074, Validation Loss: 0.0038175391385147756\n",
      "Epoch [20000/20000], Training Loss: 0.005410260755005376, Validation Loss: 0.002566698946261476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 3\n",
    "output_dim = 1\n",
    "hidden_dims = [36, 76, 355, 469, 100, 302]\n",
    "learning_rate = 0.003627873890923467\n",
    "batch_size = 75\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleNN(input_dim, hidden_dims, output_dim).to(device)\n",
    "# model = model.double()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # Reshape to [batch_size, 1]\n",
    "x_val_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)  # Reshape to [batch_size, 1]\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "patience = 200\n",
    "best_val_loss = float('inf')\n",
    "best_model_wts = None\n",
    "epochs_no_improve = 0\n",
    "num_epochs = 20000\n",
    "accuracy_old = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        targets = targets.view(-1, 1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_targets in test_dataloader:\n",
    "            val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "            val_targets = val_targets.view(-1, 1)\n",
    "            val_outputs = model(val_inputs)\n",
    "            v_loss = criterion(val_outputs, val_targets)\n",
    "            val_loss += v_loss.item()\n",
    "        \n",
    "            \n",
    "\n",
    "    avg_val_loss = val_loss / len(test_dataloader)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(x_val_tensor)\n",
    "        binary = Binarizer(threshold=0.5)\n",
    "        val_outs = binary.transform(pred.numpy())\n",
    "        accuracy = accuracy_score(y_val_tensor.numpy(), val_outs)\n",
    "    \n",
    "\n",
    "    if accuracy > accuracy_old:\n",
    "        accuracy_old = accuracy\n",
    "        best_model_wts = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # if epochs_no_improve >= patience:\n",
    "    #     # break\n",
    "\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfa7d056-cb4b-4c80-8517-233a7d8f12be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51025548-e9d5-4278-9f92-1ec47200b545",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(x_val_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72835751-656c-4b3e-8341-b19c7b327774",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = Binarizer(threshold=0.5)\n",
    "val_predict = binary.transform(pred.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d022679-4ae2-409e-a336-2c36aa584e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_val_tensor.numpy(), val_predict)\n",
    "print(f'DNN Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f115344-e24c-497a-aaf0-b252e7a6ea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5f6032b7-2fdf-4449-902c-0b4c1fb14373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b64e326a8934a7bb2d31d8996fc4ece",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=640.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib ipympl\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(y_val_tensor.numpy(), val_predict))\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2086222-eb3e-4e5a-81c0-508698e8e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'DNN_new_Aug25.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e09da8a-691c-4f05-9ddf-dcda58e497a7",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c1da46a-9cf8-44bc-8a03-864966c2927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata1 = 360\n",
    "ntest1 = np.maximum(20, int(ndata1/10))\n",
    "x_train1, y_train1 = create_data(ndata1, mat_h, extend=True) \n",
    "x_test1, y_test1 = create_data(ntest1, mat_h, rand=True)\n",
    "x_train1 *= mat_h.sy\n",
    "x_test1 *= mat_h.sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b824ed00-b306-4e19-9a24-df8fc9b90cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10080, 3)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5c4b06e4-fc4c-4ef5-bf42-8a4ddca47886",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stress = x_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cc08509d-c4bf-44bd-bd22-0723bc174a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stress = np.append(x_train1, x_test1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f99a4549-8663-4357-8338-3da8419737df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10944, 3)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_stress.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "890577f6-d7e5-4568-9436-334e6d4ea9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hill function\n",
    "def hill_func(pstress):\n",
    "    hill = np.sqrt(0.5*(0.7*(pstress[:, 0]-pstress[:, 1])**2+1*(pstress[:, 1]-pstress[:, 2])**2\n",
    "                        +1.4*(pstress[:, 2]-pstress[:, 0])**2))\n",
    "    return hill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2785e467-a098-4489-8dd8-4dc94b4f0dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_hill = hill_func(p_stress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f82afcf4-250a-4141-8b61-39a2b5fe6bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.88248506, 1.88880467, 1.89507733, 1.90129584, 1.90745316,\n",
       "       1.91354236, 1.91955668, 1.92548948, 1.93133428, 1.93708473,\n",
       "       1.94273468, 1.94827809, 1.95370912, 1.95902207, 1.96421141,\n",
       "       1.9692718 , 1.97419804, 1.97898513, 1.98362822, 1.98812266,\n",
       "       1.99246397, 1.99664784, 2.00067015, 2.00452696, 2.00821452,\n",
       "       2.01172923, 2.01506773, 2.01822679, 2.0212034 , 2.02399473,\n",
       "       2.02659812, 2.02901113, 2.03123146, 2.03325705, 2.03508599,\n",
       "       2.03671657, 2.03814729, 2.03937679, 2.04040395, 2.04122781,\n",
       "       2.04184761, 2.04226278, 2.04247293, 2.04247786, 2.04227758,\n",
       "       2.04187227, 2.0412623 , 2.04044823, 2.03943083, 2.03821103,\n",
       "       2.03678997, 2.03516896, 2.03334952, 2.03133335, 2.02912234,\n",
       "       2.02671855, 2.02412426, 2.02134191, 2.01837414, 2.01522379,\n",
       "       2.01189385, 2.00838753, 2.00470821, 2.00085946, 1.99684502,\n",
       "       1.99266883, 1.98833499, 1.98384781, 1.97921176, 1.97443149,\n",
       "       1.96951183, 1.96445778, 1.95927451, 1.95396738, 1.94854191,\n",
       "       1.94300377, 1.93735881, 1.93161304, 1.92577264, 1.91984393,\n",
       "       1.91383338, 1.90774762, 1.90159342, 1.89537769, 1.88910748,\n",
       "       1.88278996, 1.87643243, 1.87004231, 1.86362714, 1.85719456,\n",
       "       1.85075231, 1.84430822, 1.83787021, 1.83144627, 1.82504446,\n",
       "       1.81867291, 1.81233976, 1.80605324, 1.79982157, 1.793653  ])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_hill[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bd8353c3-7d2b-4a44-a42e-2646da323068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.674279134157869\n"
     ]
    }
   ],
   "source": [
    "print(np.min(result_hill))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c068ff14-380e-4fbd-a958-0cf9665dbbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 3)\n",
      "207\n",
      "(208,)\n"
     ]
    }
   ],
   "source": [
    "plot_ps_hill=np.zeros((1,3))\n",
    "value_store_hill = np.zeros(1)\n",
    "#print(value_store_dp)\n",
    "#print(plot_ps_dp.shape,)\n",
    "#np.concatenate((plot_ps_dp, [pstrs[20,:]]), axis=0)\n",
    "#print(plot_ps_dp, [pstrs[20,:]])\n",
    "last_count_hill=0\n",
    "for i in range(len(result_hill)):\n",
    "    if abs(result_hill[i]-150)<3:\n",
    "        plot_ps_hill=np.append(plot_ps_hill, [p_stress[i,:]], axis=0)\n",
    "        value_store_hill=np.append(value_store_hill, [result_hill[i]], axis=0)\n",
    "        #print(plot_ps, pstrs[i,:])\n",
    "        last_count_hill +=1\n",
    "print(plot_ps_hill.shape)\n",
    "print(last_count_hill)\n",
    "print(value_store_hill.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "261bc49e-a620-4334-8e07-cf99ff31e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pstr = x_train\n",
    "pstr = np.append(x_train, x_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ec7c3239-c8ba-4c72-a471-8efc364a7931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1488, 3)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pstr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bf31a83b-36ac-417b-adb0-4791b8ddda43",
   "metadata": {},
   "outputs": [],
   "source": [
    "pstress_tensor = torch.tensor(pstr, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "dafb60ba-80be-4de5-8959-bcb5d72849f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    new_pred = model(pstress_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6e23f5d6-e3af-49a4-96c9-dfa1cf6a9e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = Binarizer(threshold=0.5)\n",
    "pred_np = binary.transform(new_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "43176943-9ad4-4232-9eb7-9abc24b95323",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_np = pred_np.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "31d437c3-e423-4706-a057-902ea8725aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1488,)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "09aaad12-9a95-43cf-841d-c5da34de8422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(pred_np), np.min(pred_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a1cee1bf-730f-46ec-8baf-99ad146fb2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(774, 3)\n",
      "773\n",
      "(774,)\n"
     ]
    }
   ],
   "source": [
    "plot_ps_ml=np.zeros((1,3))\n",
    "value_store_ml = np.zeros(1)\n",
    "#print(value_store_dp)\n",
    "#print(plot_ps_dp.shape,)\n",
    "#np.concatenate((plot_ps_dp, [pstrs[20,:]]), axis=0)\n",
    "#print(plot_ps_dp, [pstrs[20,:]])\n",
    "last_count_ml=0\n",
    "for i in range(len(pred_np)):\n",
    "    if abs(pred_np[i])==0:\n",
    "        plot_ps_ml=np.append(plot_ps_ml, [pstr[i,:]], axis=0)\n",
    "        value_store_ml=np.append(value_store_ml, [pred_np[i]], axis=0)\n",
    "        #print(plot_ps, pstrs[i,:])\n",
    "        last_count_ml +=1\n",
    "print(plot_ps_ml.shape)\n",
    "print(last_count_ml)\n",
    "print(value_store_ml.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "abca8d6f-2598-43d8-ae3f-0000a2f22893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.92, 'Hill Yield Locus and ML Predicted Elastic Stress States')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644392da67d0405eb000fa75a849e448",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=1000.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib ipympl\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.axes(projection = '3d')\n",
    "fig = ax.scatter3D(plot_ps_hill[1:,0], plot_ps_hill[1:,1], plot_ps_hill[1:,2], color ='r')\n",
    "fig = ax.scatter3D(plot_ps_ml[1:,0], plot_ps_ml[1:,1], plot_ps_ml[1:,2], color = 'b')\n",
    "ax.set_xlabel('$\\\\sigma_1$')\n",
    "ax.set_ylabel('$\\\\sigma_2$')\n",
    "ax.set_zlabel('$\\\\sigma_3$')\n",
    "ax.set_title('Hill Yield Locus and ML Predicted Elastic Stress States')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b486db-82c3-477a-8048-554a1eb1989f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
